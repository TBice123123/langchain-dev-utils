{"config":{"lang":["en","zh"],"separator":"[\\s\\u200b\\u3000\\-\u3001\u3002\uff0c\uff0e\uff1f\uff01\uff1b]+","pipeline":["stemmer"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"\ud83e\udd9c\ufe0f\ud83e\uddf0 langchain-dev-utils","text":"<p> \ud83d\ude80 A high-efficiency toolkit designed specifically for LangChain and LangGraph developers </p> <p> </p>"},{"location":"#why-choose-langchain-dev-utils","title":"Why choose langchain-dev-utils?","text":"<p>Tired of writing repetitive code in LangChain development? <code>langchain-dev-utils</code> is exactly the solution you need! This lightweight yet powerful toolkit is designed specifically to enhance the development experience of LangChain and LangGraph, helping you:</p> <ul> <li>Improve development efficiency - Reduce boilerplate code, allowing you to focus on core functionality</li> <li>Simplify complex processes - Easily manage multi-model, multi-tool, and multi-agent applications</li> <li>Enhance code quality - Improve consistency and readability, reducing maintenance costs</li> <li>Accelerate prototype development - Quickly implement ideas, iterate and validate faster</li> </ul>"},{"location":"#core-features","title":"Core Features","text":"<ul> <li> <p> Unified Model Management</p> <p>Easily switch and combine different models by specifying model providers through strings</p> </li> <li> <p> Flexible Message Processing</p> <p>Support chain-of-thought concatenation, streaming processing, and message formatting</p> </li> <li> <p> Powerful Tool Invocation</p> <p>Built-in tool invocation detection, parameter parsing, and manual review features    </p> </li> <li> <p> Efficient Agent Development</p> <p>Simplify the agent creation process and expand more common middleware</p> </li> <li> <p> Flexible State Graph Composition</p> <p>Support serial and parallel composition of multiple StateGraphs</p> </li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>1. Install <code>langchain-dev-utils</code></p> <pre><code>pip install -U \"langchain-dev-utils[standard]\"\n</code></pre> <p>2. Get Started</p> <pre><code>from langchain.tools import tool\nfrom langchain_core.messages import HumanMessage\nfrom langchain_dev_utils.chat_models import register_model_provider, load_chat_model\nfrom langchain_dev_utils.agents import create_agent\n\n# Register model provider\nregister_model_provider(\"vllm\", \"openai-compatible\", base_url=\"http://localhost:8000/v1\")\n\n@tool\ndef get_current_weather(location: str) -&gt; str:\n    \"\"\"Get the current weather for the specified location\"\"\"\n    return f\"25 degrees, {location}\"\n\n# Dynamically load model using string\nmodel = load_chat_model(\"vllm:qwen3-4b\")\nresponse = model.invoke(\"Hello\")\nprint(response)\n\n# Create agent\nagent = create_agent(\"vllm:qwen3-4b\", tools=[get_current_weather])\nresponse = agent.invoke({\"messages\": [HumanMessage(content=\"What's the weather like in New York today?\")]})\nprint(response)\n</code></pre>"},{"location":"#github-repository","title":"GitHub Repository","text":"<p>Visit the GitHub Repository to view source code and issues.</p>"},{"location":"example-project/","title":"Langchain-dev-utils Example Project","text":"<p>This repository provides an example project <code>langchain-dev-utils-example</code> designed to help developers quickly understand how to efficiently build two typical intelligent agent (agent) systems using the utility functions provided by <code>langchain-dev-utils</code>:</p> <ul> <li>Single Agent: Suitable for executing simple tasks and tasks related to long-term memory storage.</li> <li>Supervisor-Multi-Agent Architecture: Coordinates multiple specialized agents through a central supervisor, suitable for complex scenarios that require task decomposition, planning, and iterative optimization.</li> </ul> <p> </p>"},{"location":"example-project/#quick-start","title":"Quick Start","text":"<ol> <li>Clone this repository: <pre><code>git clone https://github.com/TBice123123/langchain-dev-utils-example.git  \ncd langchain-dev-utils-example\n</code></pre></li> <li>Install dependencies using uv: <pre><code>uv sync\n</code></pre></li> <li>Create .env file <pre><code>cp .env.example .env\n</code></pre></li> <li> <p>Edit the <code>.env</code> file and fill in your API keys (requires API keys from <code>ZhipuAI</code> and <code>Tavily</code>).</p> </li> <li> <p>Start the project <pre><code>langgraph dev\n</code></pre></p> </li> </ol>"},{"location":"example-project/#features-used","title":"Features Used","text":"<p>Single Agent:</p> <p>Features from this library used:</p> <ul> <li>Chat model management: <code>register_model_provider</code>, <code>load_chat_model</code></li> <li>Embedding model management: <code>register_embeddings_provider</code>, <code>load_embeddings</code></li> <li>Format sequence: <code>format_sequence</code></li> <li>Middleware: <code>format_prompt</code></li> </ul> <p>Supervisor-Multi-Agent Architecture:</p> <p>Features from this library used:</p> <ul> <li>Chat model management: <code>register_model_provider</code>, <code>load_chat_model</code></li> <li>Multi-agent construction: <code>wrap_agent_as_tool</code></li> </ul>"},{"location":"example-project/#how-to-customize","title":"How to Customize","text":"<p>You can customize this project according to your actual needs.</p>"},{"location":"example-project/#1-replace-chat-model-provider","title":"1. Replace Chat Model Provider","text":"<p>This project uses ZhipuAI's GLM series as the core model by default, specifically as follows:</p> <ul> <li><code>GLM-4.7</code>: Used for <code>simple-agent</code></li> <li><code>GLM-4.6</code>: Used for <code>supervisor-agent</code>'s <code>supervisor</code></li> <li><code>GLM-4.5</code>: Used for the <code>supervisor-agent</code>'s <code>subagent</code></li> </ul> <p>If you want to customize your model provider, you need to modify the content in <code>src/utils/providers/chat_models/register.py</code>, and register your model provider using the <code>register_model_provider</code> function in the <code>register_all_model_providers</code> function.</p> <p>It is also recommended to modify the content in <code>src/utils/providers/chat_models/load.py</code>, and add the loading logic for your model provider in the <code>load_chat_model</code> function.</p> <p>Chat Model Management Best Practice</p> <p>For additional parameters of different chat model classes, the <code>load_chat_model</code> function receives them through keyword arguments (the corresponding LangChain function also uses this method). Although this approach enhances universality, it weakens IDE type hints and increases the risk of parameter misuse. Therefore, if the specific provider is already determined, you can extend the parameter signature for its integrated chat model class (or embedding model class) to restore type hints. You can refer to the content in <code>src\\utils\\providers\\chat_models\\load.py</code> for targeted modifications.</p>"},{"location":"example-project/#2-register-embedding-model-provider","title":"2. Register Embedding Model Provider","text":"<p>Similar to chat model providers, you can also register custom embedding model providers as needed. You need to modify the content in <code>src/utils/providers/embeddings/register.py</code>, and register your embedding model provider using the <code>register_embeddings_provider</code> function in the <code>register_all_embeddings_providers</code> function.</p> <p>If needed, you can also modify the content in <code>src/utils/providers/embeddings/load.py</code>, and add the loading logic for your embedding model provider in the <code>load_embeddings</code> function.</p>"},{"location":"example-project/#3-customize-tools","title":"3. Customize Tools","text":"<p>Single Agent (simple-agent) Tool implementations are located in <code>src/agents/simple_agent/tools.py</code>, with built-in: - <code>save_user_memory</code> - Persist user memory - <code>get_user_memory</code> - Read user memory  </p> <p>For extensions, simply add corresponding tool implementations in this file.</p> <p>Supervisor-Multi-Agent (supervisor-agent) Tool implementations are located in <code>src/agents/supervisor/subagent/tools.py</code>. These are tool implementations for sub-agents. If you need to add custom tools for sub-agents, simply add corresponding tool implementations in this file.</p> <p>Note: The <code>supervisor</code> only holds two tools for \"calling sub-agents\" by default. If you need to add custom tools for the <code>supervisor</code>, it is recommended to create a new <code>tools.py</code> under <code>src/agents/supervisor/</code>, and after writing, import it in <code>src/agents/supervisor/agent.py</code> and pass it to the <code>create_agent</code> function.</p>"},{"location":"adavance-guide/human-in-the-loop/","title":"Adding Human Review to Tool Calls","text":""},{"location":"adavance-guide/human-in-the-loop/#overview","title":"Overview","text":"<p>This library provides decorator functions to add \"human-in-the-loop\" review support for tool calls, enabling human review during tool execution.</p> Decorator Applicable Scenario <code>human_in_the_loop</code> For synchronous tool functions <code>human_in_the_loop_async</code> For asynchronous tool functions"},{"location":"adavance-guide/human-in-the-loop/#parameter-description","title":"Parameter Description","text":"Parameter Description <code>handler</code> Custom handler function. If <code>None</code>, the default handler is used.Type: <code>Callable</code>Required: No"},{"location":"adavance-guide/human-in-the-loop/#usage-examples","title":"Usage Examples","text":""},{"location":"adavance-guide/human-in-the-loop/#using-the-default-handler","title":"Using the Default Handler","text":"<pre><code>from langchain_dev_utils.tool_calling import human_in_the_loop\nimport datetime\n\n\n@human_in_the_loop\ndef get_current_time() -&gt; str:\n    \"\"\"Gets the current timestamp\"\"\"\n    return str(datetime.datetime.now().timestamp())\n</code></pre>"},{"location":"adavance-guide/human-in-the-loop/#async-tool-example","title":"Async Tool Example","text":"<pre><code>from langchain_dev_utils.tool_calling import human_in_the_loop_async\nimport asyncio\nimport datetime\n\n\n@human_in_the_loop_async\nasync def async_get_current_time() -&gt; str:\n    \"\"\"Asynchronously gets the current timestamp\"\"\"\n    await asyncio.sleep(1)\n    return str(datetime.datetime.now().timestamp())\n</code></pre>"},{"location":"adavance-guide/human-in-the-loop/#default-handler-implementation","title":"Default Handler Implementation","text":"<p>The implementation of the default handler is as follows:</p> <pre><code>def _get_human_in_the_loop_request(params: InterruptParams) -&gt; dict[str, Any]:\n    return {\n        \"action_request\": {\n            \"action\": params[\"tool_call_name\"],\n            \"args\": params[\"tool_call_args\"],\n        },\n        \"config\": {\n            \"allow_accept\": True,\n            \"allow_edit\": True,\n            \"allow_respond\": True,\n        },\n        \"description\": f\"Please review tool call: {params['tool_call_name']}\",\n    }\n\n\ndef default_handler(params: InterruptParams) -&gt; Any:\n    request = _get_human_in_the_loop_request(params)\n    response = interrupt(request)\n\n    if response[\"type\"] == \"accept\":\n        return params[\"tool\"].invoke(params[\"tool_call_args\"])\n    elif response[\"type\"] == \"edit\":\n        updated_args = response[\"args\"]\n        return params[\"tool\"].invoke(updated_args)\n    elif response[\"type\"] == \"response\":\n        return response[\"args\"]\n    else:\n        raise ValueError(f\"Unsupported interrupt response type: {response['type']}\")\n</code></pre>"},{"location":"adavance-guide/human-in-the-loop/#interrupt-request-format","title":"Interrupt Request Format","text":"<p>An interrupt sends a request in the following JSON Schema format:</p> Field Description <code>action_request.action</code> The name of the tool call.Type: <code>str</code> <code>action_request.args</code> The arguments for the tool call.Type: <code>dict</code> <code>config.allow_accept</code> Whether to allow accepting the action.Type: <code>bool</code> <code>config.allow_edit</code> Whether to allow editing the arguments.Type: <code>bool</code> <code>config.allow_respond</code> Whether to allow responding directly.Type: <code>bool</code> <code>description</code> A description of the action.Type: <code>str</code>"},{"location":"adavance-guide/human-in-the-loop/#interrupt-response-format","title":"Interrupt Response Format","text":"<p>The response must be returned in the following JSON Schema format:</p> Field Description <code>type</code> Response type, with optional values <code>accept</code>, <code>edit</code>, <code>response</code>.Type: <code>str</code>Required: Yes <code>args</code> When <code>type</code> is <code>edit</code> or <code>response</code>, contains the updated arguments or response content.Type: <code>dict</code>Required: No"},{"location":"adavance-guide/human-in-the-loop/#custom-handler-example","title":"Custom Handler Example","text":"<p>You can have full control over the interrupt behavior, for example, by only allowing \"accept/reject\" or customizing the prompt:</p> <pre><code>from typing import Any\nfrom langchain_dev_utils.tool_calling import human_in_the_loop_async, InterruptParams\nfrom langgraph.types import interrupt\n\n\nasync def custom_handler(params: InterruptParams) -&gt; Any:\n    response = interrupt(\n        f\"I am about to call the tool {params['tool_call_name']} with arguments {params['tool_call_args']}. Please confirm if I should proceed.\"\n    )\n    if response[\"type\"] == \"accept\":\n        return await params[\"tool\"].ainvoke(params[\"tool_call_args\"])\n    elif response[\"type\"] == \"reject\":\n        return \"The user rejected calling this tool.\"\n    else:\n        raise ValueError(f\"Unsupported response type: {response['type']}\")\n\n\n@human_in_the_loop_async(handler=custom_handler)\nasync def get_weather(city: str) -&gt; str:\n    \"\"\"Gets weather information\"\"\"\n    return f\"The weather in {city} is sunny.\"\n</code></pre> <p>Best Practice</p> <p>When implementing custom human-in-the-loop logic with this decorator, you need to pass the <code>handler</code> parameter. This <code>handler</code> parameter is a function that must internally use LangGraph's <code>interrupt</code> function to perform the interrupt operation. Therefore, if you are only adding custom human-in-the-loop logic for a single tool, it is recommended to use LangGraph's <code>interrupt</code> function directly. When multiple tools require the same custom human-in-the-loop logic, using this decorator can effectively avoid code duplication.</p>"},{"location":"adavance-guide/middleware/","title":"Middleware","text":""},{"location":"adavance-guide/middleware/#overview","title":"Overview","text":"<p>Middleware is a component specifically built for LangChain's pre-built Agents. The official library provides some built-in middleware, and this library offers more practical middleware based on actual usage scenarios.</p> <p>The middleware provided by this library includes:</p> <ul> <li><code>PlanMiddleware</code>: Task planning that breaks down complex tasks into ordered subtasks</li> <li><code>ModelRouterMiddleware</code>: Dynamically routes to the most suitable model based on input content</li> <li><code>HandoffAgentMiddleware</code>: Flexibly switches between multiple sub-agents</li> <li><code>ToolCallRepairMiddleware</code>: Automatically fixes invalid tool calls from large models</li> <li><code>format_prompt</code>: Dynamically formats placeholders in system prompts</li> </ul> <p>Additionally, this library extends the functionality of official middleware, supporting model specification through string parameters:</p> <ul> <li>SummarizationMiddleware</li> <li>LLMToolSelectorMiddleware</li> <li>ModelFallbackMiddleware</li> <li>LLMToolEmulator</li> </ul>"},{"location":"adavance-guide/middleware/#task-planning","title":"Task Planning","text":"<p><code>PlanMiddleware</code> is a middleware for structured decomposition and process management before executing complex tasks.</p> <p>Additional Information</p> <p>Task planning is an efficient context engineering management strategy. Before executing a task, the large model first breaks down the overall task into multiple ordered subtasks, forming a task planning list (called a \"plan\" in this library). It then executes each subtask in sequence and dynamically updates the task status after completing each step until all subtasks are finished.</p>"},{"location":"adavance-guide/middleware/#parameter-description","title":"Parameter Description","text":"Parameter Description <code>system_prompt</code> System prompt. If <code>None</code>, the default prompt is used.Type: <code>str</code>Required: No <code>custom_plan_tool_descriptions</code> Custom descriptions for plan-related tools.Type: <code>dict</code>Required: No <code>use_read_plan_tool</code> Whether to enable the read plan tool.Type: <code>bool</code>Required: NoDefault: <code>True</code> <p>The keys in the <code>custom_plan_tool_descriptions</code> dictionary can take the following three values:</p> Key Description <code>write_plan</code> Description of the write plan tool <code>finish_sub_plan</code> Description of the finish subplan tool <code>read_plan</code> Description of the read plan tool"},{"location":"adavance-guide/middleware/#usage-example","title":"Usage Example","text":"<pre><code>from langchain_dev_utils.agents.middleware import PlanMiddleware\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    middleware=[\n        PlanMiddleware(\n            custom_plan_tool_descriptions={\n                \"write_plan\": \"Used for writing plans, breaking tasks into multiple ordered subtasks.\",\n                \"finish_sub_plan\": \"Used for completing subtasks, updating subtask status to completed.\",\n                \"read_plan\": \"Used for querying the current task planning list.\"\n            },\n            use_read_plan_tool=True,  # If not using the read plan tool, set this parameter to False\n        )\n    ],\n)\n\nresponse = agent.invoke(\n    {\"messages\": [HumanMessage(content=\"I want to visit New York for a few days, help me plan my itinerary\")]}\n)\nprint(response)\n</code></pre>"},{"location":"adavance-guide/middleware/#tool-description","title":"Tool Description","text":"<p><code>PlanMiddleware</code> requires the use of two tools: <code>write_plan</code> and <code>finish_sub_plan</code>, while the <code>read_plan</code> tool is enabled by default; if not needed, the <code>use_read_plan_tool</code> parameter can be set to <code>False</code>.</p>"},{"location":"adavance-guide/middleware/#comparison-with-official-to-do-list-middleware","title":"Comparison with Official To-do List Middleware","text":"<p>This middleware has a similar functional positioning to LangChain's official To-do list middleware, but there are differences in tool design:</p> Feature Official To-do List Middleware This Library's PlanMiddleware Number of Tools 1 (<code>write_todo</code>) 3 (<code>write_plan</code>, <code>finish_sub_plan</code>, <code>read_plan</code>) Functional Positioning Focused on to-do lists Specifically for planning lists Operation Method Adding and modifying done through one tool Writing, modifying, and querying done through different tools <p>Whether it's <code>todo</code> or <code>plan</code>, they are essentially the same concept. The key difference between this middleware and the official one is that it provides three specialized tools:</p> <ul> <li><code>write_plan</code>: Used for writing or updating plan content</li> <li><code>finish_sub_plan</code>: Used to update the status after completing a subtask</li> <li><code>read_plan</code>: Used for querying plan content</li> </ul>"},{"location":"adavance-guide/middleware/#model-routing","title":"Model Routing","text":"<p><code>ModelRouterMiddleware</code> is a middleware for dynamically routing to the most suitable model based on input content. It analyzes user requests through a \"routing model\" and selects the most appropriate model from a predefined list to handle the current task.</p>"},{"location":"adavance-guide/middleware/#parameter-description_1","title":"Parameter Description","text":"Parameter Description <code>router_model</code> Model used for routing decisions.Type: <code>str</code> | <code>BaseChatModel</code>Required: Yes <code>model_list</code> List of model configurations.Type: <code>list[ModelDict]</code>Required: Yes <code>router_prompt</code> Custom prompt for the routing model.Type: <code>str</code>Required: No"},{"location":"adavance-guide/middleware/#model_list-configuration-description","title":"<code>model_list</code> Configuration Description","text":"<p>Each model configuration is a dictionary containing the following fields:</p> Field Description <code>model_name</code> Unique identifier for the model, using <code>provider:model-name</code> format.Type: <code>str</code>Required: Yes <code>model_description</code> Brief description of the model's capabilities or applicable scenarios.Type: <code>str</code>Required: Yes <code>tools</code> Whitelist of tools that this model can call.Type: <code>list[BaseTool]</code>Required: No <code>model_kwargs</code> Additional parameters when loading the model.Type: <code>dict</code>Required: No <code>model_system_prompt</code> System-level prompt for the model.Type: <code>str</code>Required: No <code>model_instance</code> Instantiated model object.Type: <code>BaseChatModel</code>Required: No <p>model_instance Field Description</p> <ul> <li>If provided: Directly uses this instance, <code>model_name</code> is only for identification, <code>model_kwargs</code> is ignored; suitable for cases not using this library's conversation model management functionality.</li> <li>If not provided: Loads the model using <code>load_chat_model</code> based on <code>model_name</code> and <code>model_kwargs</code>.</li> <li>Naming format: In either case, it's recommended to use the <code>provider:model-name</code> format for <code>model_name</code>.</li> </ul>"},{"location":"adavance-guide/middleware/#usage-example_1","title":"Usage Example","text":""},{"location":"adavance-guide/middleware/#step-1-define-the-model-list","title":"Step 1: Define the Model List","text":"<pre><code>from langchain_dev_utils.agents.middleware.model_router import ModelDict\n\nmodel_list: list[ModelDict] = [\n    {\n        \"model_name\": \"vllm:qwen3-8b\",\n        \"model_description\": \"Suitable for ordinary tasks, such as dialogue, text generation, etc.\",\n        \"model_kwargs\": {\n            \"temperature\": 0.7,\n            \"extra_body\": {\"chat_template_kwargs\": {\"enable_thinking\": False}},\n        },\n        \"model_system_prompt\": \"You are an assistant, good at handling ordinary tasks, such as dialogue, text generation, etc.\",\n    },\n    {\n        \"model_name\": \"vllm:qwen3-vl-2b\",\n        \"model_description\": \"Suitable for visual tasks\",\n        \"tools\": [],  # If this model doesn't need any tools, set this field to an empty list []\n    },\n    {\n        \"model_name\": \"vllm:qwen3-coder-flash\",\n        \"model_description\": \"Suitable for code generation tasks\",\n        \"tools\": [run_python_code],  # Only allow the use of run_python_code tool\n    },\n    {\n        \"model_name\": \"openai:gpt-4o\",\n        \"model_description\": \"Suitable for comprehensive high-difficulty tasks\",\n        \"model_system_prompt\": \"You are an assistant, good at handling comprehensive high-difficulty tasks\",\n        \"model_instance\": ChatOpenAI(\n            model_name=\"gpt-4o\"\n        ),  # Directly pass the instance, where model_name is only for identification, model_kwargs is ignored\n    },\n]\n</code></pre>"},{"location":"adavance-guide/middleware/#step-2-create-agent-and-enable-middleware","title":"Step 2: Create Agent and Enable Middleware","text":"<pre><code>from langchain_dev_utils.agents.middleware import ModelRouterMiddleware\nfrom langchain_core.messages import HumanMessage\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",  # This model is only a placeholder, actually dynamically replaced by middleware\n    tools=[run_python_code, get_current_time],\n    middleware=[\n        ModelRouterMiddleware(\n            router_model=\"vllm:qwen3-4b\",\n            model_list=model_list,\n        )\n    ],\n)\n\n# The routing middleware will automatically select the most suitable model based on input content\nresponse = agent.invoke({\"messages\": [HumanMessage(content=\"Help me write a bubble sort code\")]})\nprint(response)\n</code></pre> <p>Through <code>ModelRouterMiddleware</code>, you can easily build a multi-model, multi-capability Agent that automatically selects the optimal model based on task type, improving response quality and efficiency.</p>"},{"location":"adavance-guide/middleware/#agent-handoff","title":"Agent Handoff","text":"<p><code>HandoffAgentMiddleware</code> is a middleware for flexibly switching between multiple sub-agents, fully implementing LangChain's official <code>handoffs</code> multi-agent collaboration solution.</p>"},{"location":"adavance-guide/middleware/#parameter-description_2","title":"Parameter Description","text":"Parameter Description <code>agents_config</code> Dictionary of agent configurations, with agent names as keys and agent configuration dictionaries as values.Type: <code>dict[str, AgentConfig]</code>Required: Yes <code>custom_handoffs_tool_descriptions</code> Custom descriptions for handoff tools, with agent names as keys and corresponding handoff tool descriptions as values.Type: <code>dict[str, str]</code>Required: No"},{"location":"adavance-guide/middleware/#agents_config-configuration-description","title":"<code>agents_config</code> Configuration Description","text":"<p>Each agent configuration is a dictionary containing the following fields:</p> Field Description <code>model</code> Specifies the model used by this agent; if not passed, it inherits the model corresponding to the <code>model</code> parameter of <code>create_agent</code>. Supports strings (must be in <code>provider:model-name</code> format, such as <code>vllm:qwen3-4b</code>) or <code>BaseChatModel</code> instances.Type: <code>str</code> | <code>BaseChatModel</code>Required: No <code>prompt</code> System prompt for the agent.Type: <code>str</code> | <code>SystemMessage</code>Required: Yes <code>tools</code> List of tools that the agent can call.Type: <code>list[BaseTool]</code>Required: No <code>default</code> Whether to set as the default agent; defaults to <code>False</code>. Only one agent in all configurations must be set to <code>True</code>.Type: <code>bool</code>Required: No <code>handoffs</code> List of other agent names that this agent can hand off to. If set to <code>\"all\"</code>, it means this agent can hand off to all other agents.Type: <code>list[str]</code> | <code>str</code>Required: Yes <p>For this paradigm of multi-agent implementation, a tool for handoffs is often needed. This middleware automatically creates corresponding handoff tools for each agent based on their <code>handoffs</code> configuration. If you want to customize the description of handoff tools, you can achieve this through the <code>custom_handoffs_tool_descriptions</code> parameter.</p> <p>Usage Example</p> <p>In this example, we will use four agents: <code>time_agent</code>, <code>weather_agent</code>, <code>code_agent</code>, and <code>default_agent</code>.</p> <p>Next, we need to create the corresponding agent configuration dictionary <code>agent_config</code>.</p> <pre><code>from langchain_dev_utils.agents.middleware.handoffs import AgentConfig\n\nagent_config: dict[str, AgentConfig] = {\n    \"time_agent\": {\n        \"model\": \"vllm:qwen3-8b\",\n        \"prompt\": \"You are a time assistant\",\n        \"tools\": [get_current_time],\n        \"handoffs\": [\"default_agent\"],  # This agent can only hand off to default_agent\n    },\n    \"weather_agent\": {\n        \"prompt\": \"You are a weather assistant\",\n        \"tools\": [get_current_weather, get_current_city],\n        \"handoffs\": [\"default_agent\"],\n    },\n    \"code_agent\": {\n        \"model\": load_chat_model(\"vllm:qwen3-coder-flash\"),\n        \"prompt\": \"You are a code assistant\",\n        \"tools\": [\n            run_code,\n        ],\n        \"handoffs\": [\"default_agent\"],\n    },\n    \"default_agent\": {\n        \"prompt\": \"You are an assistant\",\n        \"default\": True, # Set as the default agent\n        \"handoffs\": \"all\",  # This agent can hand off to all other agents\n    },\n}\n</code></pre> <p>Finally, pass this configuration to <code>HandoffAgentMiddleware</code>.</p> <pre><code>from langchain_dev_utils.agents.middleware import HandoffAgentMiddleware\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    tools=[\n        get_current_time,\n        get_current_weather,\n        get_current_city,\n        run_code,\n    ],\n    middleware=[HandoffAgentMiddleware(agents_config=agent_config)],\n)\n\nresponse = agent.invoke({\"messages\": [HumanMessage(content=\"What is the current time?\")]})\nprint(response)\n</code></pre> <p>If you want to customize the description of handoff tools, you can pass a second parameter <code>custom_handoffs_tool_descriptions</code>.</p> <pre><code>from langchain_dev_utils.agents.middleware import HandoffAgentMiddleware\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    tools=[\n        get_current_time,\n        get_current_weather,\n        get_current_city,\n        run_code,\n    ],\n    middleware=[\n        HandoffAgentMiddleware(\n            agents_config=agent_config,\n            custom_handoffs_tool_descriptions={\n                \"time_agent\": \"This tool is used to hand off to the time assistant to solve time query problems\",\n                \"weather_agent\": \"This tool is used to hand off to the weather assistant to solve weather query problems\",\n                \"code_agent\": \"This tool is used to hand off to the code assistant to solve code problems\",\n                \"default_agent\": \"This tool is used to hand off to the default assistant\",\n            },\n        )\n    ],\n)\n</code></pre>"},{"location":"adavance-guide/middleware/#tool-call-repair","title":"Tool Call Repair","text":"<p><code>ToolCallRepairMiddleware</code> is a middleware for automatically fixing invalid tool calls (<code>invalid_tool_calls</code>) from large models.</p> <p>When large models output the JSON Schema for tool calls, they may generate JSON format errors due to the model's own reasons (errors are common in the <code>arguments</code> field), leading to JSON parsing failures. Such calls are stored in the <code>invalid_tool_calls</code> field. <code>ToolCallRepairMiddleware</code> will automatically detect <code>invalid_tool_calls</code> after the model returns results and attempt to fix them by calling <code>json-repair</code>, allowing the tool calls to execute normally.</p> <p>Please ensure you have installed <code>langchain-dev-utils[standard]</code>, see the Installation Guide for details.</p>"},{"location":"adavance-guide/middleware/#parameter-description_3","title":"Parameter Description","text":"<p>This middleware is zero-configuration and ready to use out of the box, no additional parameters required.</p>"},{"location":"adavance-guide/middleware/#usage-example_2","title":"Usage Example","text":"<pre><code>from langchain_dev_utils.agents.middleware import ToolCallRepairMiddleware\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    tools=[run_python_code, get_current_time],\n    middleware=[\n        ToolCallRepairMiddleware()\n    ],\n)\n</code></pre> <p>Note</p> <p>This middleware cannot guarantee 100% fixing of all invalid tool calls; the actual effect depends on the repair capability of <code>json-repair</code>. Additionally, it only acts on invalid tool call content in the <code>invalid_tool_calls</code> field.</p>"},{"location":"adavance-guide/middleware/#formatting-system-prompts","title":"Formatting System Prompts","text":"<p><code>format_prompt</code> is a decorator that allows you to use <code>f-string</code> style placeholders (like <code>{name}</code>) in <code>system_prompt</code> and dynamically replace them with actual values at runtime.</p>"},{"location":"adavance-guide/middleware/#parameter-description_4","title":"Parameter Description","text":"<p>The values of variables in placeholders follow a clear resolution order:</p> <ol> <li>First look up in <code>state</code>: It first looks for fields with the same name as the placeholder in the <code>state</code> dictionary</li> <li>Then look up in <code>context</code>: If the field is not found in <code>state</code>, it continues to look in the <code>context</code> object</li> </ol> <p>This order means that values in <code>state</code> have higher priority and can override values with the same name in <code>context</code>.</p>"},{"location":"adavance-guide/middleware/#usage-example_3","title":"Usage Example","text":""},{"location":"adavance-guide/middleware/#getting-variables-only-from-state","title":"Getting Variables Only from <code>state</code>","text":"<p>This is the most basic usage, where all placeholder variables are provided by <code>state</code>.</p> <pre><code>from langchain_dev_utils.agents.middleware import format_prompt\nfrom langchain.agents import AgentState\n\nclass AssistantState(AgentState):\n    name: str\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    system_prompt=\"You are an intelligent assistant, your name is {name}.\",\n    middleware=[format_prompt],\n    state_schema=AssistantState,\n)\n\n# When calling, you must provide a value for 'name' in state\nresponse = agent.invoke(\n    {\"messages\": [HumanMessage(content=\"Hello\")], \"name\": \"assistant\"}\n)\nprint(response)\n</code></pre>"},{"location":"adavance-guide/middleware/#getting-variables-from-both-state-and-context","title":"Getting Variables from Both <code>state</code> and <code>context</code>","text":"<p>Using both <code>state</code> and <code>context</code> simultaneously:</p> <pre><code>from dataclasses import dataclass\n\n@dataclass\nclass Context:\n    user: str\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    # {name} will be obtained from state, {user} will be obtained from context\n    system_prompt=\"You are an intelligent assistant, your name is {name}. Your user is named {user}.\",\n    middleware=[format_prompt],\n    state_schema=AssistantState,\n    context_schema=Context,\n)\n\n# When calling, provide 'name' for state and 'user' for context\nresponse = agent.invoke(\n    {\n        \"messages\": [HumanMessage(content=\"I want to visit New York for a few days, help me plan my itinerary\")],\n        \"name\": \"assistant\",\n    },\n    context=Context(user=\"Zhang San\"),\n)\nprint(response)\n</code></pre>"},{"location":"adavance-guide/middleware/#variable-override-example","title":"Variable Override Example","text":"<p>This example shows that when <code>state</code> and <code>context</code> have variables with the same name, the value in <code>state</code> takes precedence.</p> <pre><code>from dataclasses import dataclass\n\n@dataclass\nclass Context:\n    # 'name' is defined in context\n    name: str\n    user: str\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    system_prompt=\"You are an intelligent assistant, your name is {name}. Your user is named {user}.\",\n    middleware=[format_prompt],\n    state_schema=AssistantState, # 'name' is also defined in state\n    context_schema=Context,\n)\n\n# When calling, both state and context provide a value for 'name'\nresponse = agent.invoke(\n    {\n        \"messages\": [HumanMessage(content=\"What is your name?\")],\n        \"name\": \"assistant-1\",\n    },\n    context=Context(name=\"assistant-2\", user=\"Zhang San\"),\n)\n\n# The final system prompt will be \"You are an intelligent assistant, your name is assistant-1. Your user is named Zhang San.\"\n# Because state has higher priority\nprint(response)\n</code></pre> <p>Note</p> <p>There are two ways to implement custom middleware: decorators or class inheritance. - Class inheritance implementation: <code>PlanMiddleware</code>, <code>ModelMiddleware</code>, <code>HandoffAgentMiddleware</code>, <code>ToolCallRepairMiddleware</code> - Decorator implementation: <code>format_prompt</code> (the decorator directly turns the function into a middleware instance, so no manual instantiation is needed to use it)</p> <p>Official Middleware Extensions</p> <p>This library extends the following official middleware, supporting model specification through string parameters for models registered with <code>register_model_provider</code>:</p> <p>You only need to import these middleware from this library to use string parameters for models registered with <code>register_model_provider</code>. The usage of the middleware remains consistent with the official middleware, for example: <pre><code>from langchain_core.messages import AIMessage\nfrom langchain_dev_utils.agents.middleware import SummarizationMiddleware\nfrom langchain_dev_utils.chat_models import register_model_provider\n\nregister_model_provider(\n    provider_name=\"vllm\",\n    chat_model=\"openai-compatible\",\n    base_url=\"http://localhost:8000/v1\",\n)\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    middleware=[\n        SummarizationMiddleware(\n            model=\"vllm:qwen3-4b\",\n            trigger=(\"tokens\", 50),\n            keep=(\"messages\", 1),\n        )\n    ],\n    system_prompt=\"You are an intelligent AI assistant that can solve user problems\",\n)\n# big_text is a text containing a lot of content, omitted here\nbig_messages = [\n    HumanMessage(content=\"Hello, who are you\"),\n    AIMessage(content=\"I am your AI assistant\"),\n    HumanMessage(content=\"Write a beautiful long text\"),\n    AIMessage(content=f\"Okay, I will write a beautiful long text, the content is: {big_text}\"),\n    HumanMessage(content=\"Why did you write this long text?\"),\n]\nresponse = agent.invoke({\"messages\": big_messages})\nprint(response)\n</code></pre></p>"},{"location":"adavance-guide/multi-agent/","title":"Multi-Agent Construction","text":""},{"location":"adavance-guide/multi-agent/#overview","title":"Overview","text":"<p>When building complex AI applications, multi-agent collaboration is a powerful architectural pattern. By assigning different responsibilities to specialized agents, specialized division of labor and efficient collaboration can be achieved.</p> <p>There are various ways to implement multi-agent collaboration, among which tool calling is a common and flexible approach. By encapsulating subagents as tools, a master agent can dynamically delegate tasks to specialized subagents based on requirements.</p> <p>This library provides two pre-built functions to simplify this implementation:</p> Function Name Description <code>wrap_agent_as_tool</code> Wraps a single agent instance as an independent tool <code>wrap_all_agents_as_tool</code> Wraps multiple agent instances as a unified tool, specifying which subagent to call via parameters"},{"location":"adavance-guide/multi-agent/#wrapping-a-single-agent-as-a-tool","title":"Wrapping a Single Agent as a Tool","text":"<p>Wrapping a single agent involves just three steps:</p> <ol> <li>Import <code>wrap_agent_as_tool</code></li> <li>Pass the agent instance as a parameter</li> <li>Obtain a tool object that can be directly called by other agents</li> </ol>"},{"location":"adavance-guide/multi-agent/#function-parameter-description","title":"Function Parameter Description","text":"Parameter Description <code>agent</code> The agent instance, must have a <code>name</code> attribute defined.Type: <code>CompiledStateGraph</code>Required: Yes <code>tool_name</code> The tool name, defaults to <code>transfer_to_{agent_name}</code>.Type: <code>str</code>Required: No <code>tool_description</code> The tool description, defaults to <code>This tool transforms input to {agent_name}</code>.Type: <code>str</code>Required: No <code>pre_input_hooks</code> Hook functions before the agent runs.Type: <code>tuple</code>Required: No <code>post_output_hooks</code> Hook functions after the agent runs.Type: <code>tuple</code>Required: No"},{"location":"adavance-guide/multi-agent/#usage-example","title":"Usage Example","text":"<p>Below, we use the <code>supervisor</code> agent as an example to introduce how to wrap subagents as tools using <code>wrap_agent_as_tool</code>.</p> <p>First, implement two subagents: one for sending emails and one for calendar query and scheduling.</p>"},{"location":"adavance-guide/multi-agent/#email-agent","title":"Email Agent","text":"<pre><code>from langchain_core.tools import tool\nfrom langchain_dev_utils.chat_models import register_model_provider\nfrom langchain_dev_utils.agents import create_agent, wrap_agent_as_tool \n\nregister_model_provider(\n    \"vllm\",\n    \"openai-compatible\",\n    base_url=\"http://localhost:8000/v1\",\n)\n\n\n@tool\ndef send_email(\n    to: list[str],  # Email addresses\n    subject: str,\n    body: str,\n    cc: list[str] = [],\n) -&gt; str:\n    \"\"\"Send emails via the Email API. Requires properly formatted addresses.\"\"\"\n    # Stub: In a real application, SendGrid, Gmail API, etc. would be called here\n    return f\"Email sent to {', '.join(to)} - Subject: {subject}\"\n\n\nEMAIL_AGENT_PROMPT = (\n    \"You are an email assistant. \"\n    \"Draft professional emails based on natural language requests. \"\n    \"Extract recipient information and create appropriate subject lines and body content. \"\n    \"Use send_email to send the email. \"\n    \"Always confirm what was sent in the final reply.\"\n)\n\nemail_agent = create_agent(\n    \"vllm:qwen3-4b\",\n    tools=[send_email],\n    system_prompt=EMAIL_AGENT_PROMPT,\n    name=\"email_agent\",\n)\n</code></pre>"},{"location":"adavance-guide/multi-agent/#calendar-agent","title":"Calendar Agent","text":"<pre><code>@tool\ndef create_calendar_event(\n    title: str,\n    start_time: str,  # ISO format: \"2024-01-15T14:00:00\"\n    end_time: str,  # ISO format: \"2024-01-15T15:00:00\"\n    attendees: list[str],  # Email addresses\n    location: str = \"\",\n) -&gt; str:\n    \"\"\"Create a calendar event. Requires precise ISO date-time format.\"\"\"\n    # Stub: In a real application, Google Calendar API, Outlook API, etc. would be called here\n    return f\"Event created: {title} from {start_time} to {end_time} with {len(attendees)} participants\"\n\n\n@tool\ndef get_available_time_slots(\n    attendees: list[str],\n    date: str,  # ISO format: \"2024-01-15\"\n    duration_minutes: int,\n) -&gt; list[str]:\n    \"\"\"Query calendar availability for attendees on a specific date.\"\"\"\n    # Stub: In a real application, the calendar API would be queried here\n    return [\"09:00\", \"14:00\", \"16:00\"]\n\n\nCALENDAR_AGENT_PROMPT = (\n    \"You are a calendar scheduling assistant. \"\n    \"Parse natural language scheduling requests (e.g., 'next Tuesday at 2 PM') into the correct ISO date-time format. \"\n    \"Use get_available_time_slots to check availability when needed. \"\n    \"Use create_calendar_event to schedule events. \"\n    \"Always confirm what was scheduled in the final reply.\"\n)\n\ncalendar_agent = create_agent(\n    \"vllm:qwen3-4b\",\n    tools=[create_calendar_event, get_available_time_slots],\n    system_prompt=CALENDAR_AGENT_PROMPT,\n    name=\"calendar_agent\",\n)\n</code></pre> <p>Next, use <code>wrap_agent_as_tool</code> to wrap these two subagents as tools.</p> <pre><code>schedule_event = wrap_agent_as_tool(\n    calendar_agent,\n    tool_name=\"schedule_event\",\n    tool_description=(\n        \"Schedule calendar events using natural language. \"\n        \"Use this when the user wants to create, modify, or check calendar appointments. \"\n        \"Can handle date/time parsing, querying available times, and creating events. \"\n        \"Input: Natural language scheduling request (e.g., 'meeting with design team next Tuesday at 2 PM')\"\n    ),\n)\nmanage_email = wrap_agent_as_tool(\n    email_agent,\n    tool_name=\"manage_email\",\n    tool_description=(\n        \"Send emails using natural language. \"\n        \"Use this when the user wants to send notifications, reminders, or any email communication. \"\n        \"Capable of extracting recipient info, subject generation, and email drafting. \"\n        \"Input: Natural language email request (e.g., 'send them a meeting reminder')\"\n    ),\n)\n</code></pre> <p>Finally, create a <code>supervisor_agent</code> that can call these two tools.</p> <pre><code>SUPERVISOR_PROMPT = (\n    \"You are a helpful personal assistant. \"\n    \"You can schedule calendar events and send emails. \"\n    \"Break down user requests into appropriate tool calls and coordinate results. \"\n    \"When a request involves multiple operations, please use multiple tools sequentially.\"\n)\n\n\nsupervisor_agent = create_agent(\n    \"vllm:qwen3-4b\",\n    tools=[schedule_event, manage_email],\n    system_prompt=SUPERVISOR_PROMPT,\n)\n\nprint(\n    supervisor_agent.invoke({\"messages\": [HumanMessage(content=\"Query free time for tomorrow\")]})\n)\nprint(\n    supervisor_agent.invoke(\n        {\"messages\": [HumanMessage(content=\"Send an email meeting reminder to test@123.com\")]}\n    )\n)\n</code></pre> <p>Tip</p> <p>In the example above, we imported <code>create_agent</code> from <code>langchain_dev_utils.agents</code> instead of <code>langchain.agents</code>. This is because this library also provides a function with exactly the same functionality as the official <code>create_agent</code>, but with the added capability to specify models via strings. This allows direct use of models registered via <code>register_model_provider</code> without needing to initialize and pass in model instances.</p>"},{"location":"adavance-guide/multi-agent/#wrapping-multiple-agents-as-a-single-tool","title":"Wrapping Multiple Agents as a Single Tool","text":"<p>Wrapping multiple agents as a single tool involves just three steps:</p> <ol> <li>Import <code>wrap_all_agents_as_tool</code></li> <li>Pass multiple agent instances as a list at once</li> <li>Obtain a unified tool object that can be directly called by other agents</li> </ol>"},{"location":"adavance-guide/multi-agent/#function-parameter-description_1","title":"Function Parameter Description","text":"Parameter Description <code>agents</code> List of agent instances.Type: <code>list[CompiledStateGraph]</code>Required: Yes <code>tool_name</code> The tool name, defaults to <code>task</code>.Type: <code>str</code>Required: No <code>tool_description</code> The tool description, defaults to containing information about all available agents.Type: <code>str</code>Required: No <code>pre_input_hooks</code> Hook functions before the agent runs.Type: <code>tuple</code>Required: No <code>post_output_hooks</code> Hook functions after the agent runs.Type: <code>tuple</code>Required: No"},{"location":"adavance-guide/multi-agent/#usage-example_1","title":"Usage Example","text":"<p>For the <code>calendar_agent</code> and <code>email_agent</code> from the previous example, we can wrap them into a single tool <code>call_subagent</code>:</p> <pre><code>call_subagent_tool = wrap_all_agents_as_tool(\n    [calendar_agent, email_agent],\n    tool_name=\"call_subagent\",\n    tool_description=(\n        \"Call subagents to execute tasks. \"\n        \"Available agents are: \"\n        \"- calendar_agent: for scheduling calendar events\"\n        \"- email_agent: for sending emails\"\n    ),\n)\n\nMAIN_AGENT_PROMPT = (\n    \"You are a helpful personal assistant. \"\n    \"You can use the **call_subagent** tool to call subagents to execute tasks. \"\n    \"Break down user requests into appropriate tool calls and coordinate results. \"\n    \"When a request involves multiple operations, please use multiple tools sequentially.\"\n)\n\nmain_agent = create_agent(\n    \"vllm:qwen3-4b\",\n    tools=[call_subagent_tool],\n    system_prompt=MAIN_AGENT_PROMPT,\n)\n</code></pre> <p>Tip</p> <p>In addition to using <code>wrap_all_agents_as_tool</code> provided by this library to wrap multiple agents into a single tool, you can also use the <code>SubAgentMiddleware</code> middleware provided by the <code>deepagents</code> library to achieve a similar effect.</p>"},{"location":"adavance-guide/multi-agent/#hook-functions","title":"Hook Functions","text":"<p>This library has a built-in flexible hook mechanism that allows inserting custom logic before and after subagent execution. This mechanism applies to both <code>wrap_agent_as_tool</code> and <code>wrap_all_agents_as_tool</code>. The following section uses <code>wrap_agent_as_tool</code> as an example for illustration.</p>"},{"location":"adavance-guide/multi-agent/#1-pre_input_hooks","title":"1. pre_input_hooks","text":"<p>Preprocess the input before the agent runs. Useful for input augmentation, context injection, format validation, permission checks, etc.</p>"},{"location":"adavance-guide/multi-agent/#supported-input-types","title":"Supported Input Types","text":"Type Description Single synchronous function Used for both synchronous (<code>invoke</code>) and asynchronous (<code>ainvoke</code>) call paths (it will be called directly in the async path without <code>await</code>) Binary tuple <code>(sync_func, async_func)</code> The first function is for the synchronous call path; the second function (must be <code>async def</code>) is for the asynchronous call path and will be <code>await</code>ed"},{"location":"adavance-guide/multi-agent/#function-signature","title":"Function Signature","text":"<pre><code>def pre_input_hook(request: str, runtime: ToolRuntime) -&gt; str:\n    \"\"\"\n    Args:\n        request: The original tool call input\n        runtime: langchain's ToolRuntime\n\n    Returns:\n        The processed str, serving as the actual input for the agent\n    \"\"\"\n</code></pre>"},{"location":"adavance-guide/multi-agent/#usage-example_2","title":"Usage Example","text":"<pre><code>def process_input(request: str, runtime: ToolRuntime) -&gt; str:\n    return \"&lt;task_description&gt;\" + request + \"&lt;/task_description&gt;\"\n\n# Or support async\nasync def process_input_async(request: str, runtime: ToolRuntime) -&gt; str:\n    return \"&lt;task_description&gt;\" + request + \"&lt;/task_description&gt;\"\n\n# Usage\ncall_agent_tool = wrap_agent_as_tool(\n    agent,\n    pre_input_hooks=(process_input, process_input_async)\n)\n</code></pre> <p>Tip</p> <p>The example above is relatively simple. In practice, you can add more complex logic based on the <code>state</code> or <code>context</code> within <code>runtime</code>.</p>"},{"location":"adavance-guide/multi-agent/#2-post_output_hooks","title":"2. post_output_hooks","text":"<p>Post-process the complete list of messages returned by the agent after it finishes running to generate the final return value of the tool. Useful for result extraction, structured transformation, etc.</p>"},{"location":"adavance-guide/multi-agent/#supported-input-types_1","title":"Supported Input Types","text":"Type Description Single function Used for both synchronous and asynchronous paths (it will not be <code>await</code>ed in the async path) Binary tuple <code>(sync_func, async_func)</code> The first is for the synchronous path; the second (<code>async def</code>) is for the asynchronous path and will be <code>await</code>ed"},{"location":"adavance-guide/multi-agent/#function-signature_1","title":"Function Signature","text":"<pre><code>def post_output_hook(request: str, messages: list, runtime: ToolRuntime) -&gt; Union[str, Command]:\n    \"\"\"\n    Args:\n        request: The original input (possibly processed)\n        messages: The complete message history returned by the agent (from response[\"messages\"])\n        runtime: langchain's ToolRuntime\n\n    Returns:\n        A value that can be serialized to a string, or a Command object\n    \"\"\"\n</code></pre>"},{"location":"adavance-guide/multi-agent/#usage-example_3","title":"Usage Example","text":"<pre><code>from langgraph.types import Command\n\ndef process_output_sync(request: str, messages: list, runtime: ToolRuntime) -&gt; Command:\n    return Command(update={\n        \"messages\":[ToolMessage(content=messages[-1].content, tool_call_id=runtime.tool_call_id)]\n    })\n\nasync def process_output_async(request: str, messages: list, runtime: ToolRuntime) -&gt; Command:\n    return Command(update={\n        \"messages\":[ToolMessage(content=messages[-1].content, tool_call_id=runtime.tool_call_id)]\n    })\n\n# Usage\ncall_agent_tool = wrap_agent_as_tool(\n    agent,\n    post_output_hooks=(process_output_sync, process_output_async)\n)\n</code></pre> <p>Tip</p> <p>The example above is relatively simple. In practice, you can add more complex logic based on the <code>state</code> or <code>context</code> within <code>runtime</code>.</p>"},{"location":"adavance-guide/multi-agent/#default-behavior","title":"Default Behavior","text":"<ul> <li>If <code>pre_input_hooks</code> is not provided, the input is passed through as-is.</li> <li>If <code>post_output_hooks</code> is not provided, it defaults to returning <code>response[\"messages\"][-1].content</code> (i.e., the text content of the last message).</li> </ul>"},{"location":"adavance-guide/openai-compatible/","title":"OpenAI Compatible API Model Provider Integration","text":""},{"location":"adavance-guide/openai-compatible/#overview","title":"Overview","text":"<p>Many model providers support OpenAI compatible API services, such as: vLLM, OpenRouter, Together AI, etc. This library provides a complete OpenAI compatible API integration solution, supporting both chat models and embedding models, especially suitable for scenarios where there is no corresponding LangChain integration yet but the provider offers an OpenAI compatible API.</p> <p>Tip</p> <p>The common approach to integrate with OpenAI compatible APIs is to directly use <code>ChatOpenAI</code> or <code>OpenAIEmbeddings</code> from <code>langchain-openai</code>, simply by passing in <code>base_url</code> and <code>api_key</code>. However, this approach only works for simple scenarios and has many compatibility issues, especially for chat models, including:</p> <ol> <li>Unable to display reasoning content (<code>reasoning_content</code>) from non-OpenAI official inference models</li> <li>No support for video type content_block</li> <li>Low coverage rate for default structured output strategies</li> </ol> <p>This library provides this functionality to solve the above compatibility issues. For simple scenarios (especially those with low compatibility requirements), you can directly use <code>ChatOpenAI</code> without using this feature. <code>OpenAIEmbeddings</code> has good compatibility, just set <code>check_embedding_ctx_length</code> to <code>False</code>. Additionally, for developers' convenience, we also provide embedding model OpenAI compatible integration class functionality.</p>"},{"location":"adavance-guide/openai-compatible/#creating-corresponding-integration-classes","title":"Creating Corresponding Integration Classes","text":"<p>This library provides two utility functions for creating corresponding chat model integration classes and embedding model integration classes. Specifically:</p> Function Name Description <code>create_openai_compatible_model</code> Create chat model integration class <code>create_openai_compatible_embedding</code> Create embedding model integration class"},{"location":"adavance-guide/openai-compatible/#creating-chat-model-class","title":"Creating Chat Model Class","text":"<p>You can use the <code>create_openai_compatible_model</code> function to create a chat model integration class. This function accepts the following parameters:</p> Parameter Description <code>model_provider</code> Model provider name, e.g., <code>vllm</code>.Type: <code>str</code>Required: Yes <code>base_url</code> Default API address of the model provider.Type: <code>str</code>Required: No <code>compatibility_options</code> Compatibility options configuration.Type: <code>dict</code>Required: No <code>model_profiles</code> Profiles corresponding to the models provided by this model provider.Type: <code>dict</code>Required: No <code>chat_model_cls_name</code> Chat model class name, default value is <code>Chat{model_provider}</code> (where <code>{model_provider}</code> is capitalized).Type: <code>str</code>Required: No <p>This library builds a chat model class corresponding to a specific provider using the built-in <code>BaseChatOpenAICompatible</code> class based on the above parameters passed by the user. This class inherits from <code>BaseChatOpenAI</code> of <code>langchain-openai</code> and enhances the following capabilities:</p> <ul> <li>Support for more formats of reasoning content: Compared to <code>ChatOpenAI</code> which can only output official reasoning content, this class also supports outputting more formats of reasoning content (e.g., <code>vLLM</code>).</li> <li>Support for <code>video</code> type content_block: <code>ChatOpenAI</code> cannot convert <code>type=video</code> <code>content_block</code>, this implementation has added support.</li> <li>Dynamically adapt and select the most suitable structured output method: By default, it can automatically select the optimal structured output method (<code>function_calling</code> or <code>json_schema</code>) based on the actual support of the model provider.</li> <li>Fine-tune differences through compatibility_options: By configuring provider compatibility options, resolve support differences for parameters such as <code>tool_choice</code>, <code>response_format</code>, etc.</li> </ul> <p>Note</p> <p>When using this feature, you must install the standard version of the <code>langchain-dev-utils</code> library. Please refer to the installation section for details.</p>"},{"location":"adavance-guide/openai-compatible/#code-example","title":"Code Example","text":"<p>We take integrating vLLM as an example to show how to use the <code>create_openai_compatible_model</code> function to create a chat model integration class.</p> <p>Additional Information</p> <p>vLLM is a commonly used large model inference framework that can deploy large models as OpenAI compatible APIs, such as Qwen3-4B in this example:</p> <p><pre><code>vllm serve Qwen/Qwen3-4B \\\n--reasoning-parser qwen3 \\\n--enable-auto-tool-choice --tool-call-parser hermes \\\n--host 0.0.0.0 --port 8000 \\\n--served-model-name qwen3-4b\n</code></pre> The service address is <code>http://localhost:8000/v1</code>.</p> <p>Use the following code to create a chat model class:</p> <pre><code>from langchain_dev_utils.chat_models.adapters import create_openai_compatible_model\n\nChatVLLM = create_openai_compatible_model(\n    model_provider=\"vllm\",\n    base_url=\"http://localhost:8000/v1\",\n    chat_model_cls_name=\"ChatVLLM\",\n)\n\nmodel = ChatVLLM(model=\"qwen3-4b\")\nprint(model.invoke(\"\u4f60\u597d\"))\n</code></pre> <p>It's worth noting that when creating a chat model class, the <code>base_url</code> parameter can be omitted. If not passed, this library will read the corresponding environment variable by default. For example:</p> <pre><code>export VLLM_API_BASE=http://localhost:8000/v1\n</code></pre> <p>In this case, the code can omit the <code>base_url</code> parameter:</p> <pre><code>from langchain_dev_utils.chat_models.adapters import create_openai_compatible_model\n\nChatVLLM = create_openai_compatible_model(\n    model_provider=\"vllm\",\n    chat_model_cls_name=\"ChatVLLM\",\n)\n\nmodel = ChatVLLM(model=\"qwen3-4b\")\nprint(model.invoke(\"\u4f60\u597d\"))\n</code></pre> <p>Note: The prerequisite for the above code to run successfully is that you have set the <code>VLLM_API_KEY</code> environment variable. Although vLLM does not require passing an API Key, the chat model class initialization requires an API Key.</p> <p>Tip</p> <p>Naming rules for environment variables of the created chat model class:</p> <ul> <li> <p>API address: <code>${PROVIDER_NAME}_API_BASE</code> (all uppercase, separated by underscores).</p> </li> <li> <p>API Key: <code>${PROVIDER_NAME}_API_KEY</code> (all uppercase, separated by underscores).</p> </li> </ul>"},{"location":"adavance-guide/openai-compatible/#compatibility-parameters","title":"Compatibility Parameters","text":"<p><code>compatibility_options</code> is a dictionary used to declare the provider's support for some features of the OpenAI API to improve compatibility and stability.</p> <p>Currently supported configuration items:</p> Configuration Item Description <code>supported_tool_choice</code> List of supported <code>tool_choice</code> strategies.Type: <code>list[str]</code>Default Value: <code>[\"auto\"]</code> <code>supported_response_format</code> List of supported <code>response_format</code> formats (<code>json_schema</code>, <code>json_object</code>).Type: <code>list[str]</code>Default Value: <code>[]</code> <code>reasoning_keep_policy</code> Retention policy for the <code>reasoning_content</code> field in historical messages.Type: <code>str</code>Default Value: <code>\"never\"</code> <code>include_usage</code> Whether to include <code>usage</code> information in streaming return results.Type: <code>bool</code>Default Value: <code>True</code> <p>Additional Information</p> <p>Since different models from the same model provider have different support for parameters such as <code>tool_choice</code> and <code>response_format</code>, these four compatibility options are instance attributes of the class. Therefore, when creating a chat model class, values can be passed in as global defaults (representing the configuration supported by most models of this provider). If fine-tuning is needed for specific models later, the same-named parameters can be overridden during instantiation.</p> <p>Detailed introductions to these configuration items are as follows:</p> 1. supported_tool_choice <p><code>tool_choice</code> is used to control whether and which external tools the large model calls during responses to improve accuracy, reliability, and controllability. Common values include:</p> <ul> <li><code>\"auto\"</code>: The model autonomously decides whether to call tools (default behavior);</li> <li><code>\"none\"</code>: Prohibit calling tools;</li> <li><code>\"required\"</code>: Force calling at least one tool;</li> <li>Specify a specific tool (in OpenAI compatible API, specifically <code>{\"type\": \"function\", \"function\": {\"name\": \"xxx\"}}</code>).</li> </ul> <p>Different providers support different ranges. To avoid errors, this library defaults <code>supported_tool_choice</code> to <code>[\"auto\"]</code>, which means when using <code>bind_tools</code>, the <code>tool_choice</code> parameter can only be passed as <code>auto</code>, and other values will be filtered out.</p> <p>If you need to support passing other <code>tool_choice</code> values, you must configure the supported items. The configuration value is a list of strings, with each optional value being:</p> <ul> <li><code>\"auto\"</code>, <code>\"none\"</code>, <code>\"required\"</code>: Corresponding to standard strategies;</li> <li><code>\"specific\"</code>: A unique identifier in this library, indicating support for specifying a specific tool.</li> </ul> <p>For example, vLLM supports all strategies:</p> <pre><code>from langchain_dev_utils.chat_models.adapters import create_openai_compatible_model\n\nChatVLLM = create_openai_compatible_model(\n    model_provider=\"vllm\",\n    chat_model_cls_name=\"ChatVLLM\",\n    compatibility_options={\n        \"supported_tool_choice\": [\"auto\", \"required\", \"none\", \"specific\"]\n    },\n)\n\nmodel = ChatVLLM(model=\"qwen3-4b\")\n</code></pre> <p>Tip</p> <p>If there are no special requirements, you can keep the default (i.e., <code>[\"auto\"]</code>). If business scenarios require the model to must call a specific tool or select one from a given list, and the model provider supports the corresponding strategy, enable as needed:</p> <ol> <li> <p>If you require at least one tool to be called and the model provider supports <code>required</code>, you can set it to <code>[\"required\"]</code> (at the same time, when calling <code>bind_tools</code>, you need to explicitly pass <code>tool_choice=\"required\"</code>)</p> </li> <li> <p>If you require calling a specific tool and the model provider supports specifying a specific tool call, you can set it to <code>[\"specific\"]</code> (in <code>function_calling</code> structured output, this configuration is very useful to ensure the model calls the specified structured output tool to ensure the stability of structured output. Because in the <code>with_structured_output</code> method, its internal implementation will pass in a <code>tool_choice</code> value that can force calling the specified tool when calling <code>bind_tools</code>, but if <code>\"specific\"</code> is not in <code>supported_tool_choice</code>, this parameter will be filtered out. Therefore, if you want to ensure that <code>tool_choice</code> can be passed normally, you must add <code>\"specific\"</code> to <code>supported_tool_choice</code>.)</p> </li> </ol> <p>This parameter can be set uniformly when creating, or can be dynamically overridden for a single model during instantiation; it is recommended to declare the <code>tool_choice</code> support situation for most models of this provider when creating, and for some models with different support situations, specify separately during instantiation.</p> 2. supported_response_format <p>Currently, there are three common methods for structured output.</p> <ul> <li><code>function_calling</code>: Generate structured output by calling a tool that conforms to a specified schema.</li> <li><code>json_schema</code>: A feature provided by the model provider specifically for generating structured output, in OpenAI compatible API, specifically <code>response_format={\"type\": \"json_schema\", \"json_schema\": {...}}</code>.</li> <li><code>json_mode</code>: A feature provided by some providers before they launched <code>json_schema</code>, which can generate valid JSON, but the schema must be described in the prompt. In OpenAI compatible API, specifically <code>response_format={\"type\": \"json_object\"}</code>).</li> </ul> <p>Among them, <code>json_schema</code> is supported by only a few OpenAI compatible API providers (such as <code>OpenRouter</code>, <code>TogetherAI</code>); <code>json_mode</code> has higher support and is compatible with most providers; while <code>function_calling</code> is the most universal, as long as the model supports tool calls, it can be used.</p> <p>This parameter is used to declare the model provider's support for <code>response_format</code>. By default, it is <code>[]</code>, which means the model provider supports neither <code>json_mode</code> nor <code>json_schema</code>. In this case, the <code>method</code> parameter in the <code>with_structured_output</code> method can only be passed as <code>function_calling</code>. If <code>json_mode</code> or <code>json_schema</code> is passed, it will be automatically converted to <code>function_calling</code>. If you want to enable <code>json_mode</code> or <code>json_schema</code> structured output implementation, you need to explicitly set this parameter.</p> <p>For example, if the model deployed by vLLM supports the <code>json_schema</code> structured output method, you can declare it during registration:</p> <pre><code>from langchain_dev_utils.chat_models.adapters import create_openai_compatible_model\n\nChatVLLM = create_openai_compatible_model(\n    model_provider=\"vllm\",\n    chat_model_cls_name=\"ChatVLLM\",\n    compatibility_options={\"supported_response_format\": [\"json_schema\"]},\n)\n\nmodel = ChatVLLM(model=\"qwen3-4b\")\n</code></pre> <p>Tip</p> <p>Generally, there is no need to configure this. It only needs to be considered when using the <code>with_structured_output</code> method. At this time, if the model provider supports <code>json_schema</code>, you can consider configuring this parameter (because the stability of <code>json_schema</code> structured output is better than <code>function_calling</code>). To ensure the stability of structured output. For <code>json_mode</code>, because it can only guarantee JSON output, it is generally not necessary to set it. Only when the model does not support tool calls and only supports setting <code>response_format={\"type\":\"json_object\"}</code>, it is necessary to configure this parameter to include <code>json_mode</code>.</p> <p>Similarly, this parameter can be set uniformly when creating, or can be dynamically overridden for a single model during instantiation; it is recommended to declare the <code>response_format</code> support situation for most models of this provider when creating, and for some models with different support situations, specify separately during instantiation.</p> <p>Note</p> <p>This parameter currently only affects the <code>model.with_structured_output</code> method. For structured output in <code>create_agent</code>, if you need to use the <code>json_schema</code> implementation, you need to ensure that the corresponding model's <code>profile</code> contains the <code>structured_output</code> field, and its value is <code>True</code>.</p> 3. reasoning_keep_policy <p>Used to control the retention policy of the <code>reasoning_content</code> field in historical messages (messages), mainly adapted to different thinking modes of models from different model providers.</p> <p>Supports the following values:</p> <ul> <li> <p><code>never</code>: Do not retain any reasoning content in historical messages (default);</p> </li> <li> <p><code>current</code>: Only retain the <code>reasoning_content</code> field in the current conversation;</p> </li> <li> <p><code>all</code>: Retain the <code>reasoning_content</code> field in all conversations.</p> </li> </ul> <p>For example: For example, the user first asks \"How is the weather in New York?\", then follows up with \"How is the weather in London?\", and is currently about to have the second round of conversation, and is about to make the last model call.</p> <ul> <li>When the value is <code>never</code></li> </ul> <p>When the value is <code>never</code>, the final messages passed to the model will not have any <code>reasoning_content</code> field. The final messages received by the model will be:</p> <pre><code>messages = [\n    {\"content\": \"How is the weather in New York?\", \"role\": \"user\"},\n    {\"content\": \"\", \"role\": \"assistant\", \"tool_calls\": [...]},\n    {\"content\": \"Cloudy 7~13\u00b0C\", \"role\": \"tool\", \"tool_call_id\": \"...\"},\n    {\"content\": \"The weather in New York today is cloudy, 7~13\u00b0C.\", \"role\": \"assistant\"},\n    {\"content\": \"How is the weather in London?\", \"role\": \"user\"},\n    {\"content\": \"\", \"role\": \"assistant\", \"tool_calls\": [...]},\n    {\"content\": \"Rainy, 14~20\u00b0C\", \"role\": \"tool\", \"tool_call_id\": \"...\"},\n]\n</code></pre> <ul> <li>When the value is <code>current</code></li> </ul> <p>When the value is <code>current</code>, only the <code>reasoning_content</code> field in the current conversation is retained. The final messages received by the model will be: <pre><code>messages = [\n    {\"content\": \"How is the weather in New York?\", \"role\": \"user\"},\n    {\"content\": \"\", \"role\": \"assistant\", \"tool_calls\": [...]},\n    {\"content\": \"Cloudy 7~13\u00b0C\", \"role\": \"tool\", \"tool_call_id\": \"...\"},\n    {\"content\": \"The weather in New York today is cloudy, 7~13\u00b0C.\", \"role\": \"assistant\"},\n    {\"content\": \"How is the weather in London?\", \"role\": \"user\"},\n    {\n        \"content\": \"\",\n        \"reasoning_content\": \"Check London weather, need to directly call the weather tool.\",  # Only retain reasoning_content of this round of conversation\n        \"role\": \"assistant\",\n        \"tool_calls\": [...],\n    },\n    {\"content\": \"Rainy, 14~20\u00b0C\", \"role\": \"tool\", \"tool_call_id\": \"...\"},\n]\n</code></pre></p> <ul> <li>When the value is <code>all</code></li> </ul> <p>When the value is <code>all</code>, the <code>reasoning_content</code> field in all conversations is retained. The final messages received by the model will be: <pre><code>messages = [\n    {\"content\": \"How is the weather in New York?\", \"role\": \"user\"},\n    {\n        \"content\": \"\",\n        \"reasoning_content\": \"Check New York weather, need to directly call the weather tool.\",  # Retain reasoning_content\n        \"role\": \"assistant\",\n        \"tool_calls\": [...],\n    },\n    {\"content\": \"Cloudy 7~13\u00b0C\", \"role\": \"tool\", \"tool_call_id\": \"...\"},\n    {\n        \"content\": \"The weather in New York today is cloudy, 7~13\u00b0C.\",\n        \"reasoning_content\": \"Directly return New York weather result.\",  # Retain reasoning_content\n        \"role\": \"assistant\",\n    },\n    {\"content\": \"How is the weather in London?\", \"role\": \"user\"},\n    {\n        \"content\": \"\",\n        \"reasoning_content\": \"Check London weather, need to directly call the weather tool.\",  # Retain reasoning_content\n        \"role\": \"assistant\",\n        \"tool_calls\": [...],\n    },\n    {\"content\": \"Rainy, 14~20\u00b0C\", \"role\": \"tool\", \"tool_call_id\": \"...\"},\n]\n</code></pre></p> <p>Note: If the current round of conversation does not involve tool calls, the effect of <code>current</code> is the same as <code>never</code>.</p> <p>Tip</p> <p>Configure flexibly according to the model provider's requirements for retaining <code>reasoning_content</code>:</p> <ul> <li>If the provider requires retaining reasoning content throughout, set to <code>all</code>;  </li> <li>If only required to retain in this round of tool calls, set to <code>current</code>;  </li> <li>If there are no special requirements, keep the default <code>never</code>.</li> </ul> <p>Similarly, this parameter can be set uniformly when creating, or can be dynamically overridden for a single model during instantiation; it is generally recommended to specify separately during instantiation, in which case no need to set when creating.</p> 4. include_usage <p><code>include_usage</code> is a parameter in the OpenAI compatible API used to control whether to append a message containing token usage information (such as <code>prompt_tokens</code> and <code>completion_tokens</code>) at the end of the streaming response. Since standard streaming responses do not return usage information by default, enabling this option allows clients to directly obtain complete token consumption data for billing, monitoring, or logging.</p> <p>It is usually enabled through <code>stream_options={\"include_usage\": true}</code>. Considering that some model providers do not support this parameter, this library sets it as a compatibility option with a default value of <code>True</code>, because most model providers support this parameter. If not supported, it can be explicitly set to <code>False</code>.</p> <p>Tip</p> <p>This parameter generally does not need to be set, just keep the default value. Only when the model provider does not support it, it needs to be set to <code>False</code>.</p>"},{"location":"adavance-guide/openai-compatible/#model_profiles-parameter-setting","title":"model_profiles Parameter Setting","text":"<p>If you want to use the <code>model.profile</code> parameter, you must explicitly pass it in when creating.</p> <p>For example:</p> <pre><code>from langchain_dev_utils.chat_models.adapters import create_openai_compatible_model\n\nmodel_profiles = {\n    \"qwen3-4b\": {\n        \"max_input_tokens\": 131072,\n        \"max_output_tokens\": 8192,\n        \"image_inputs\": False,\n        \"audio_inputs\": False,\n        \"video_inputs\": False,\n        \"image_outputs\": False,\n        \"audio_outputs\": False,\n        \"video_outputs\": False,\n        \"reasoning_output\": True,\n        \"tool_calling\": True,\n    }\n    # More model profiles can be written here\n}\n\nChatVLLM = create_openai_compatible_model(\n    model_provider=\"vllm\",\n    base_url=\"http://localhost:8000/v1\",\n    model_profiles=model_profiles,\n)\n\nmodel = ChatVLLM(\n    model=\"qwen3-4b\",\n)\nprint(model.profile)\n</code></pre> <p>Note</p> <p>Although the above compatibility configurations have been provided, this library still cannot guarantee 100% compatibility with all OpenAI compatible interfaces. If the model provider already has an official or community integration class, please prioritize using that integration class. If you encounter any compatibility issues, welcome to submit an issue in this library's GitHub repository.</p>"},{"location":"adavance-guide/openai-compatible/#creating-embedding-model-class","title":"Creating Embedding Model Class","text":"<p>Similar to the chat model class, you can use <code>create_openai_compatible_embedding</code> to create an embedding model class.</p>"},{"location":"adavance-guide/openai-compatible/#example-code","title":"Example Code","text":"<p>Similarly, we use <code>create_openai_compatible_embedding</code> to integrate vLLM's embedding model.</p> <p>Additional Information</p> <p>vLLM can deploy embedding models and expose OpenAI compatible interfaces, for example:</p> <pre><code>vllm serve Qwen/Qwen3-Embedding-4B \\\n--task embed \\\n--served-model-name qwen3-embedding-4b \\\n--host 0.0.0.0 --port 8000\n</code></pre> <p>The service address is <code>http://localhost:8000/v1</code>.</p> <pre><code>from langchain_dev_utils.embeddings.adapters import create_openai_compatible_embedding\n\nVLLMEmbedding = create_openai_compatible_embedding(\n    embedding_provider=\"vllm\",\n    base_url=\"http://localhost:8000/v1\",\n    embedding_model_cls_name=\"VLLMEmbedding\",\n)\n\nembedding = VLLMEmbedding(model=\"qwen3-embedding-8b\")\n\nprint(embedding.embed_query(\"\u4f60\u597d\"))\n</code></pre> <p>Similarly, <code>base_url</code> can be omitted, in which case you need to set the environment variable <code>VLLM_API_BASE</code>.</p> <pre><code>export VLLM_API_BASE=\"http://localhost:8000/v1\"\n</code></pre> <p>The code can omit <code>base_url</code>.</p> <pre><code>from langchain_dev_utils.embeddings.adapters import create_openai_compatible_embedding\n\nVLLMEmbedding = create_openai_compatible_embedding(\n    embedding_provider=\"vllm\",\n    embedding_model_cls_name=\"VLLMEmbedding\",\n)\n\nembedding = VLLMEmbedding(model=\"qwen3-embedding-8b\")\n\nprint(embedding.embed_query(\"\u4f60\u597d\"))\n</code></pre>"},{"location":"adavance-guide/openai-compatible/#using-integration-classes","title":"Using Integration Classes","text":""},{"location":"adavance-guide/openai-compatible/#using-chat-model-class","title":"Using Chat Model Class","text":"<p>First, we need to create a chat model class. We use the previously created <code>ChatVLLM</code> class.</p> <ul> <li>Supports methods such as <code>invoke</code>, <code>ainvoke</code>, <code>stream</code>, <code>astream</code>, etc.</li> </ul> Regular Call <p>Supports using <code>invoke</code> for simple calls:</p> <pre><code>from langchain_core.messages import HumanMessage\n\nmodel = ChatVLLM(\"qwen3-4b\")\nresponse = model.invoke([HumanMessage(\"Hello\")])\nprint(response)\n</code></pre> <p>Also supports using <code>ainvoke</code> for asynchronous calls:</p> <pre><code>from langchain_core.messages import HumanMessage\n\nmodel = ChatVLLM(\"qwen3-4b\")\nresponse = await model.ainvoke([HumanMessage(\"Hello\")])\nprint(response)\n</code></pre> Streaming Output <p>Supports using <code>stream</code> for streaming output:</p> <pre><code>from langchain_core.messages import HumanMessage\n\nmodel = ChatVLLM(\"qwen3-4b\")\nfor chunk in model.stream([HumanMessage(\"Hello\")]):\n    print(chunk)\n</code></pre> <p>And using <code>astream</code> for asynchronous streaming calls:</p> <pre><code>from langchain_core.messages import HumanMessage\n\nmodel = ChatVLLM(\"qwen3-4b\")\nasync for chunk in model.astream([HumanMessage(\"Hello\")]):\n    print(chunk)\n</code></pre> <ul> <li>Supports the <code>bind_tools</code> method for tool calling.</li> </ul> <p>If the model itself supports tool calling, you can directly use the <code>bind_tools</code> method for tool calling:</p> Tool Calling <pre><code>from langchain_core.messages import HumanMessage\nfrom langchain_core.tools import tool\nimport datetime\n\n@tool\ndef get_current_time() -&gt; str:\n    \"\"\"Get current timestamp\"\"\"\n    return str(datetime.datetime.now().timestamp())\n\nmodel = ChatVLLM(\"qwen3-4b\").bind_tools([get_current_time])\nresponse = model.invoke([HumanMessage(\"Get current timestamp\")])\nprint(response)\n</code></pre> <ul> <li>Supports the <code>with_structured_output</code> method for structured output.</li> </ul> <p>If the <code>supported_response_format</code> parameter of this model class contains <code>json_schema</code>, then <code>with_structured_output</code> will prioritize using <code>json_schema</code> for structured output, otherwise fall back to <code>function_calling</code>; if you need <code>json_mode</code>, explicitly specify <code>method=\"json_mode\"</code> and ensure <code>json_mode</code> is included during registration.</p> Structured Output <pre><code>from langchain_core.messages import HumanMessage\nfrom langchain_core.tools import tool\nfrom pydantic import BaseModel\n\nclass User(BaseModel):\n    name: str\n    age: int\n\nmodel = ChatVLLM(\"qwen3-4b\").with_structured_output(User)\nresponse = model.invoke([HumanMessage(\"Hello, my name is Zhang San, I'm 25 years old\")])\nprint(response)\n</code></pre> <ul> <li>Supports passing parameters of <code>BaseChatOpenAI</code>, such as <code>temperature</code>, <code>top_p</code>, <code>max_tokens</code>, etc.</li> </ul> <p>In addition, since this class inherits from <code>BaseChatOpenAI</code>, it supports passing model parameters of <code>BaseChatOpenAI</code>, such as <code>temperature</code>, <code>extra_body</code>, etc.:</p> Passing Model Parameters <pre><code>from langchain_core.messages import HumanMessage\n\nmodel = ChatVLLM(\"qwen3-4b\", extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}}) # Use extra_body to pass additional parameters, here to disable thinking mode\nresponse = model.invoke([HumanMessage(\"Hello\")])\nprint(response)\n</code></pre> <ul> <li>Supports passing multimodal data</li> </ul> <p>Supports passing multimodal data, you can use OpenAI compatible multimodal data formats or directly use <code>content_block</code> in <code>langchain</code>.</p> Passing Multimodal Data <p>Passing image type data:</p> <pre><code>from langchain_core.messages import HumanMessage\nmessages = [\n    HumanMessage(\n        content_blocks=[\n            {\n                \"type\": \"image\",\n                \"url\": \"https://example.com/image.png\",\n            },\n            {\"type\": \"text\", \"text\": \"Describe this image\"},\n        ]\n    )\n]\n\nmodel = ChatVLLM(\"qwen3-vl-2b\")\nresponse = model.invoke(messages)\nprint(response)\n</code></pre> <p>Passing video type data:</p> <pre><code>from langchain_core.messages import HumanMessage\n\nmessages = [\n    HumanMessage(\n        content_blocks=[\n            {\n                \"type\": \"video\",\n                \"url\": \"https://example.com/video.mp4\",\n            },\n            {\"type\": \"text\", \"text\": \"Describe this video\"},\n        ]\n    )\n]\n\nmodel = ChatVLLM(\"qwen3-vl-2b\")\nresponse = model.invoke(messages)\nprint(response)\n</code></pre> <p>Additional Information</p> <p>vllm also supports deploying multimodal models, such as <code>qwen3-vl-2b</code>: <pre><code>vllm serve Qwen/Qwen3-VL-2B-Instruct \\\n--trust-remote-code \\\n--host 0.0.0.0 --port 8000 \\\n--served-model-name qwen3-vl-2b\n</code></pre></p> <ul> <li>Supports OpenAI's latest <code>responses api</code> (not yet fully guaranteed to be supported, can be used for simple testing, but not for production environments)</li> </ul> <p>This model class also supports OpenAI's latest <code>responses_api</code>. However, currently only a few providers support this API style. If your model provider supports this API style, you can pass in the <code>use_responses_api</code> parameter as <code>True</code>.     For example, vllm supports <code>responses_api</code>, so you can use it like this:</p> OpenAI's latest <code>responses_api</code> <pre><code>from langchain_core.messages import HumanMessage\n\nmodel = ChatVLLM(\"qwen3-4b\", use_responses_api=True)\nresponse = model.invoke([HumanMessage(content=\"Hello\")])\nprint(response)\n</code></pre>"},{"location":"adavance-guide/openai-compatible/#using-embedding-model-class","title":"Using Embedding Model Class","text":"<p>We use the previously created <code>VLLMEmbeddings</code> class to initialize an embedding model instance.</p> <ul> <li>Vectorize query</li> </ul> Vectorize Query <pre><code>embedding = VLLMEmbeddings(model=\"qwen3-embedding-4b\")\nprint(embedding.embed_query(\"Hello\"))\n</code></pre> <p>Asynchronous version</p> <pre><code>embedding = VLLMEmbeddings(model=\"qwen3-embedding-4b\")\nres = await embedding.aembed_query(\"Hello\")\nprint(res)\n</code></pre> <ul> <li>Vectorize string list</li> </ul> Vectorize String List <pre><code>documents = [\"Hello\", \"Hello, I'm Zhang San\"]\nembedding = VLLMEmbeddings(model=\"qwen3-embedding-4b\")\nprint(embedding.embed_documents(documents))\n</code></pre> <p>Asynchronous version</p> <pre><code>documents = [\"Hello\", \"Hello, I'm Zhang San\"]\nembedding = VLLMEmbeddings(model=\"qwen3-embedding-4b\")\nres = await embedding.aembed_documents(documents)\nprint(res)\n</code></pre> <p>Note: The chat model class and embedding model class created using this feature support passing any parameters of <code>BaseChatOpenAI</code> and <code>OpenAIEmbeddings</code>, such as <code>temperature</code>, <code>extra_body</code>, <code>dimensions</code>, etc.</p>"},{"location":"adavance-guide/openai-compatible/#integration-with-model-management-functionality","title":"Integration with Model Management Functionality","text":"<p>This library has seamlessly integrated this functionality into the model management feature. When registering a chat model, simply set <code>chat_model</code> to <code>\"openai-compatible\"</code>; when registering an embedding model, set <code>embeddings_model</code> to <code>\"openai-compatible\"</code>.</p>"},{"location":"adavance-guide/openai-compatible/#chat-model-class-registration","title":"Chat Model Class Registration","text":"<p>Specific code is as follows:</p> <p>Method 1: Explicit Parameter Passing</p> <pre><code>from langchain_dev_utils.chat_models import register_model_provider\n\nregister_model_provider(\n    provider_name=\"vllm\",\n    chat_model=\"openai-compatible\",\n    base_url=\"http://localhost:8000/v1\"\n)\n</code></pre> <p>Method 2: Through Environment Variables (Recommended for Configuration Management)</p> <pre><code>from langchain_dev_utils.chat_models import register_model_provider\n\nregister_model_provider(\n    provider_name=\"vllm\",\n    chat_model=\"openai-compatible\"\n    # Automatically reads VLLM_API_BASE\n)\n</code></pre> <p>At the same time, the <code>base_url</code>, <code>compatibility_options</code>, and <code>model_profiles</code> parameters in the <code>create_openai_compatible_model</code> function also support being passed in. Just pass in the corresponding parameters in the <code>register_model_provider</code> function.</p>"},{"location":"adavance-guide/openai-compatible/#embedding-model-class-registration","title":"Embedding Model Class Registration","text":"<p>Similar to chat model class registration:</p> <p>Method 1: Explicit Parameter Passing</p> <pre><code>from langchain_dev_utils.embeddings import register_embeddings_provider\n\nregister_embeddings_provider(\n    provider_name=\"vllm\",\n    embeddings_model=\"openai-compatible\",\n    base_url=\"http://localhost:8000/v1\",\n)\n</code></pre> <p>Method 2: Environment Variables (Recommended)</p> <pre><code>export VLLM_API_BASE=http://localhost:8000/v1\n</code></pre> <pre><code>from langchain_dev_utils.embeddings import register_embeddings_provider\n\nregister_embeddings_provider(\n    provider_name=\"vllm\",\n    embeddings_model=\"openai-compatible\"\n)\n</code></pre>"},{"location":"adavance-guide/pipeline/","title":"State Graph Orchestration","text":""},{"location":"adavance-guide/pipeline/#overview","title":"Overview","text":"<p>In LangGraph, state graph orchestration is a key technology for building complex AI applications. By combining multiple state graphs according to specific patterns, you can create powerful workflows with clear logic.</p> <p>This library provides the following two orchestration methods:</p> Orchestration Method Function Description Applicable Scenarios Sequential Orchestration Combines multiple state graphs in a sequential manner to form a sequential workflow Tasks need to be executed step by step, with each step depending on the output of the previous step Parallel Orchestration Combines multiple state graphs in a parallel manner to form a parallel workflow Multiple tasks are independent of each other and can be executed simultaneously to improve efficiency"},{"location":"adavance-guide/pipeline/#sequential-orchestration","title":"Sequential Orchestration","text":"<p>Sequential Orchestration (Sequential Pipeline) is a work pattern that decomposes complex tasks into a series of continuous, ordered sub-tasks, which are then processed sequentially by different specialized agents.</p> <p>Multiple state graphs can be combined in a sequential orchestration manner through <code>create_sequential_pipeline</code>.</p>"},{"location":"adavance-guide/pipeline/#typical-application-scenarios","title":"Typical Application Scenarios","text":"<p>Taking the software development process as an example, it usually follows a strict linear process:</p> Phase Responsible Role Input Output 1. Requirements Analysis Product Manager User Requirements Product Requirements Document (PRD) 2. Architecture Design Architect PRD System Architecture Diagram and Technical Solution 3. Code Writing Development Engineer Architecture Solution Executable Source Code 4. Testing &amp; Quality Assurance Test Engineer Source Code Test Report and Optimization Suggestions <p>This process is interconnected and the order cannot be reversed. Through the <code>create_sequential_pipeline</code> function, these four agents can be seamlessly connected to form a highly automated software development pipeline with clear responsibilities.</p>"},{"location":"adavance-guide/pipeline/#basic-example","title":"Basic Example","text":"<p>The following code shows how to use <code>create_sequential_pipeline</code> to build a software development pipeline:</p> <pre><code>from langchain.agents import AgentState\nfrom langchain_core.messages import HumanMessage\nfrom langchain_dev_utils.agents import create_agent\nfrom langchain_dev_utils.pipeline import create_sequential_pipeline\nfrom langchain_core.tools import tool\nfrom langchain_dev_utils.chat_models import register_model_provider\n\nregister_model_provider(\n    provider_name=\"vllm\",\n    chat_model=\"openai-compatible\",\n    base_url=\"http://localhost:8000/v1\",\n)\n\n\n@tool\ndef analyze_requirements(user_request: str) -&gt; str:\n    \"\"\"Analyze user requirements and generate detailed product requirements document\"\"\"\n    return f\"Based on the user request '{user_request}', a detailed product requirements document has been generated, including feature list, user stories, and acceptance criteria.\"\n\n\n@tool\ndef design_architecture(requirements: str) -&gt; str:\n    \"\"\"Design system architecture based on requirements document\"\"\"\n    return \"Based on the requirements document, system architecture has been designed, including microservice division, data flow diagram, and technology stack selection.\"\n\n\n@tool\ndef generate_code(architecture: str) -&gt; str:\n    \"\"\"Generate core code based on architecture design\"\"\"\n    return \"Based on the architecture design, core business code has been generated, including API interfaces, data models, and business logic implementation.\"\n\n\n@tool\ndef create_tests(code: str) -&gt; str:\n    \"\"\"Create test cases for the generated code\"\"\"\n    return \"Unit tests, integration tests, and end-to-end test cases have been created for the generated code.\"\n\n\n# Product Manager Agent\nrequirements_agent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    tools=[analyze_requirements],\n    system_prompt=\"You are a product manager responsible for analyzing user requirements and generating detailed product requirements documents.\",\n    name=\"requirements_agent\",\n)\n\n# System Architect Agent\narchitecture_agent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    tools=[design_architecture],\n    system_prompt=\"You are a system architect responsible for designing system architecture based on requirements documents.\",\n    name=\"architecture_agent\",\n)\n\n# Senior Development Engineer Agent\ncoding_agent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    tools=[generate_code],\n    system_prompt=\"You are a senior development engineer responsible for generating core code based on architecture design.\",\n    name=\"coding_agent\",\n)\n\n# Test Engineer Agent\ntesting_agent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    tools=[create_tests],\n    system_prompt=\"You are a test engineer responsible for creating comprehensive test cases for the generated code.\",\n    name=\"testing_agent\",\n)\n\n# Build an automated software development sequential workflow (pipeline)\ngraph = create_sequential_pipeline(\n    sub_graphs=[\n        requirements_agent,\n        architecture_agent,\n        coding_agent,\n        testing_agent,\n    ],\n    state_schema=AgentState,\n)\n\nresponse = graph.invoke(\n    {\"messages\": [HumanMessage(\"Develop an e-commerce website with user registration, product browsing, and shopping cart functionality\")]}\n)\nprint(response)\n</code></pre>"},{"location":"adavance-guide/pipeline/#execution-flow-diagram","title":"Execution Flow Diagram","text":"<p>The generated diagram is as follows:</p> <p></p>"},{"location":"adavance-guide/pipeline/#context-engineering-optimization","title":"Context Engineering Optimization","text":"<p>The basic example above is for reference only. In practice, this example passes the complete context of all previous agents to the current agent, which may lead to context inflation, affecting performance and effectiveness.</p> <p>It is recommended to adopt either of the following solutions to streamline the context:</p> Solution Description Advantages Using Middleware Use <code>create_agent</code> with middleware to extract and pass only necessary information Simple implementation, minimal code changes Custom State Graph Completely customize the state graph based on <code>LangGraph</code>, explicitly control state fields and message flow High flexibility, precise control Click to view reference code for solving with middleware <pre><code>from typing import Any\n\nfrom langchain.agents import AgentState\nfrom langchain.agents.middleware import AgentMiddleware\nfrom langchain_core.messages import HumanMessage, RemoveMessage\nfrom langgraph.runtime import Runtime\n\nfrom langchain_dev_utils.agents import create_agent\nfrom langchain_dev_utils.agents.middleware import format_prompt\nfrom langchain_dev_utils.pipeline import create_sequential_pipeline\n\n\nclass DeveloperState(AgentState, total=False):\n    requirement: str\n    architecture: str\n    code: str\n    tests: str\n\nclass ClearAgentContextMiddleware(AgentMiddleware):\n    state_schema = DeveloperState\n\n    def __init__(self, result_save_key: str) -&gt; None:\n        super().__init__()\n        self.result_save_key = result_save_key\n\n    def after_agent(\n        self, state: DeveloperState, runtime: Runtime\n    ) -&gt; dict[str, Any] | None:\n        final_message = state[\"messages\"][-1]\n        update_key = self.result_save_key\n        return {\n            \"messages\": [\n                RemoveMessage(id=msg.id or \"\") for msg in state[\"messages\"][1:]\n            ],\n            update_key: final_message.content,\n        }\n\n# Product Manager Agent\nrequirements_agent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    tools=[analyze_requirements],\n    system_prompt=\"You are a product manager responsible for analyzing user requirements and generating detailed product requirements documents.\",\n    name=\"requirements_agent\",\n    state_schema=DeveloperState,\n    middleware=[format_prompt, ClearAgentContextMiddleware(\"requirement\")],\n)\n\n# System Architect Agent\narchitecture_agent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    tools=[design_architecture],\n    system_prompt=\"You are a system architect responsible for designing system architecture based on requirements documents.\",\n    name=\"architecture_agent\",\n    state_schema=DeveloperState,\n    middleware=[format_prompt, ClearAgentContextMiddleware(\"architecture\")],\n)\n\n# Senior Development Engineer Agent\ncoding_agent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    tools=[generate_code],\n    system_prompt=\"You are a senior development engineer responsible for generating core code based on architecture design.\",\n    name=\"coding_agent\",\n    state_schema=DeveloperState,\n    middleware=[format_prompt, ClearAgentContextMiddleware(\"code\")],\n)\n\n# Test Engineer Agent\ntesting_agent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    tools=[create_tests],\n    system_prompt=\"You are a test engineer responsible for creating comprehensive test cases for the generated code.\",\n    name=\"testing_agent\",\n    state_schema=DeveloperState,\n    middleware=[format_prompt, ClearAgentContextMiddleware(\"tests\")],\n)\n\n# Build an automated software development sequential workflow (pipeline)\ngraph = create_sequential_pipeline(\n    sub_graphs=[\n        requirements_agent,\n        architecture_agent,\n        coding_agent,\n        testing_agent,\n    ],\n    state_schema=DeveloperState,\n)\n\nresponse = graph.invoke(\n    {\"messages\": [HumanMessage(\"Develop an e-commerce website with user registration, product browsing, and shopping cart functionality\")]}\n)\nprint(response)\n</code></pre> <p>Implementation Notes:</p> <ol> <li> <p>Extended State Schema: Added four fields <code>requirement</code>, <code>architecture</code>, <code>code</code>, and <code>tests</code> to the agent's State Schema to store the final output results of corresponding agents.</p> </li> <li> <p>Custom Middleware: Created the <code>ClearAgentContextMiddleware</code> middleware, which after each agent finishes:</p> </li> <li>Clears the current execution context (using <code>RemoveMessage</code>)</li> <li> <p>Saves the final result (<code>final_message.content</code>) to the corresponding field</p> </li> <li> <p>Dynamic Prompt Formatting: Uses the built-in <code>format_prompt</code> middleware to dynamically concatenate the output of previous agents into the <code>system_prompt</code> at runtime</p> </li> </ol> <p>Tip</p> <p>For serially combined graphs, LangGraph's <code>StateGraph</code> provides the <code>add_sequence</code> method as a convenient syntax. This method is most suitable when nodes are functions (rather than subgraphs).</p> <pre><code>graph = StateGraph(AgentState)\ngraph.add_sequence([(\"graph1\", graph1), (\"graph2\", graph2), (\"graph3\", graph3)])\ngraph.add_edge(\"__start__\", \"graph1\")\ngraph = graph.compile()\n</code></pre> <p>However, the above syntax is still somewhat cumbersome. Therefore, it is more recommended to use the <code>create_sequential_pipeline</code> function, which can quickly build a serial execution graph with one line of code, making it more concise and efficient.</p>"},{"location":"adavance-guide/pipeline/#parallel-orchestration","title":"Parallel Orchestration","text":"<p>Parallel Orchestration (Parallel Pipeline) improves task execution efficiency by combining multiple state graphs in parallel and executing tasks concurrently on each state graph.</p> <p>Through the <code>create_parallel_pipeline</code> function, multiple state graphs can be combined in a parallel orchestration manner to achieve the effect of parallel task execution.</p>"},{"location":"adavance-guide/pipeline/#typical-application-scenarios_1","title":"Typical Application Scenarios","text":"<p>In software development, after the system architecture design is completed, different functional modules can often be developed simultaneously by different teams or engineers because they are relatively independent of each other. This is a typical scenario for parallel work.</p> <p>Suppose we want to develop an e-commerce website, whose core functions can be divided into three independent modules:</p> Module Function Development Content User Module User Management Registration, Login, Personal Center Product Module Product Management Display, Search, Classification Order Module Order Management Placing Orders, Payment, Status Query <p>If developed serially, the time required would be the sum of all three. However, if developed in parallel, the total time will be approximately equal to the development time of the longest module, greatly improving efficiency.</p>"},{"location":"adavance-guide/pipeline/#basic-example_1","title":"Basic Example","text":"<pre><code>from langchain_dev_utils.pipeline import create_parallel_pipeline\n\n\n@tool\ndef develop_user_module():\n    \"\"\"Develop user module functionality\"\"\"\n    return \"User module development completed, including registration, login, and personal profile management functions.\"\n\n\n@tool\ndef develop_product_module():\n    \"\"\"Develop product module functionality\"\"\"\n    return \"Product module development completed, including product display, search, and classification functions.\"\n\n\n@tool\ndef develop_order_module():\n    \"\"\"Develop order module functionality\"\"\"\n    return \"Order module development completed, including order placement, payment, and order query functions.\"\n\n\n# User Module Development Agent\nuser_module_agent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    tools=[develop_user_module],\n    system_prompt=\"You are a frontend development engineer responsible for developing user-related modules.\",\n    name=\"user_module_agent\",\n)\n\n# Product Module Development Agent\nproduct_module_agent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    tools=[develop_product_module],\n    system_prompt=\"You are a frontend development engineer responsible for developing product-related modules.\",\n    name=\"product_module_agent\",\n)\n\n# Order Module Development Agent\norder_module_agent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    tools=[develop_order_module],\n    system_prompt=\"You are a frontend development engineer responsible for developing order-related modules.\",\n    name=\"order_module_agent\",\n)\n\n# Build a parallel workflow (pipeline) for frontend module development\ngraph = create_parallel_pipeline(\n    sub_graphs=[\n        user_module_agent,\n        product_module_agent,\n        order_module_agent,\n    ],\n    state_schema=AgentState,\n)\nresponse = graph.invoke({\"messages\": [HumanMessage(\"Parallel development of three core modules of an e-commerce website\")]})\nprint(response)\n</code></pre>"},{"location":"adavance-guide/pipeline/#execution-flow-diagram_1","title":"Execution Flow Diagram","text":"<p>The generated diagram is as follows:</p> <p></p>"},{"location":"adavance-guide/pipeline/#using-branch-functions-to-specify-parallelly-executed-subgraphs","title":"Using Branch Functions to Specify Parallelly Executed Subgraphs","text":"<p>Sometimes it is necessary to specify which subgraphs are executed in parallel based on conditions. In this case, branch functions can be used. The branch function needs to return a list of <code>Send</code>.</p>"},{"location":"adavance-guide/pipeline/#application-scenario","title":"Application Scenario","text":"<p>For example, in the above case, assuming the modules to be developed are specified by the user, only the specified modules will be executed in parallel.</p> <pre><code># Build a parallel pipeline (select subgraphs to execute in parallel based on conditions)\nfrom langgraph.types import Send\n\n\nclass DevAgentState(AgentState):\n    \"\"\"Development Agent State\"\"\"\n\n    selected_modules: list[tuple[str, str]]\n\n\n# Specify modules selected by the user\nselect_modules = [(\"user_module\", \"Develop user module\"), (\"product_module\", \"Develop product module\")]\n\nuser_module_agent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    tools=[develop_user_module],\n    system_prompt=\"You are a frontend development engineer responsible for developing user-related modules.\",\n    name=\"user_module_agent\",\n)\n\nproduct_module_agent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    tools=[develop_product_module],\n    system_prompt=\"You are a frontend development engineer responsible for developing product-related modules.\",\n    name=\"product_module_agent\",\n)\n\n\norder_module_agent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    tools=[develop_order_module],\n    system_prompt=\"You are a frontend development engineer responsible for developing order-related modules.\",\n    name=\"order_module_agent\",\n)\n\n\ngraph = create_parallel_pipeline(\n    sub_graphs=[\n        user_module_agent,\n        product_module_agent,\n        order_module_agent,\n    ],\n    state_schema=DevAgentState,\n    branches_fn=lambda state: [\n        Send(module_name + \"_agent\", {\"messages\": [HumanMessage(task_name)]})\n        for module_name, task_name in state[\"selected_modules\"]\n    ],\n)\n\nresponse = graph.invoke(\n    {\n        \"messages\": [HumanMessage(\"Develop some modules of an e-commerce website\")],\n        \"selected_modules\": select_modules,\n    }\n)\nprint(response)\n</code></pre> <p>Tip</p> <ul> <li>When not passing the <code>branches_fn</code> parameter: All subgraphs will be executed in parallel</li> <li>When passing the <code>branches_fn</code> parameter: Which subgraphs are executed is determined by the return value of this function</li> </ul>"},{"location":"api-reference/agent/","title":"Agent Module API Reference Documentation","text":""},{"location":"api-reference/agent/#create_agent","title":"create_agent","text":"<p>Creates an agent that provides the same functionality as the official <code>create_agent</code> from langchain, but with expanded string model specification.</p>"},{"location":"api-reference/agent/#function-signature","title":"Function Signature","text":"<pre><code>def create_agent(  # noqa: PLR0915\n    model: str,\n    tools: Sequence[BaseTool | Callable | dict[str, Any]] | None = None,\n    *,\n    system_prompt: str | SystemMessage | None = None,\n    response_format: ResponseFormat[ResponseT] | type[ResponseT] | None = None,\n    middleware: Sequence[AgentMiddleware[StateT_co, ContextT]] = (),\n    state_schema: type[AgentState[ResponseT]] | None = None,\n    context_schema: type[ContextT] | None = None,\n    checkpointer: Checkpointer | None = None,\n    store: BaseStore | None = None,\n    interrupt_before: list[str] | None = None,\n    interrupt_after: list[str] | None = None,\n    debug: bool = False,\n    name: str | None = None,\n    cache: BaseCache | None = None,\n) -&gt; CompiledStateGraph[\n    AgentState[ResponseT], ContextT, _InputAgentState, _OutputAgentState[ResponseT]\n]:\n</code></pre>"},{"location":"api-reference/agent/#parameters","title":"Parameters","text":"Parameter Type Required Default Description model str Yes - Model identifier string that can be loaded by <code>load_chat_model</code>. Can be specified in \"provider:model-name\" format tools Sequence[BaseTool | Callable | dict[str, Any]] | None No None List of tools available to the agent system_prompt str | SystemMessage | None No None Custom system prompt for the agent middleware Sequence[AgentMiddleware[AgentState[ResponseT], ContextT]] No () Agent middleware response_format ResponseFormat[ResponseT] | type[ResponseT] | None No None Response format for the agent state_schema type[AgentState[ResponseT]] | None No None State schema for the agent context_schema type[ContextT] | None No None Context schema for the agent checkpointer Checkpointer | None No None Checkpoint for state persistence store BaseStore | None No None Storage for data persistence interrupt_before list[str] | None No None Nodes to interrupt before execution interrupt_after list[str] | None No None Nodes to interrupt after execution debug bool No False Enable debug mode name str | None No None Agent name cache BaseCache | None No None Cache"},{"location":"api-reference/agent/#notes","title":"Notes","text":"<p>This function provides the same functionality as the official <code>create_agent</code> from <code>langchain</code>, but with expanded model selection. The main difference is that the <code>model</code> parameter must be a string that can be loaded by the <code>load_chat_model</code> function, allowing for more flexible model selection using registered model providers.</p>"},{"location":"api-reference/agent/#example","title":"Example","text":"<pre><code>agent = create_agent(model=\"vllm:qwen3-4b\", tools=[get_current_time])\n</code></pre>"},{"location":"api-reference/agent/#wrap_agent_as_tool","title":"wrap_agent_as_tool","text":"<p>Wraps an agent as a tool.</p>"},{"location":"api-reference/agent/#function-signature_1","title":"Function Signature","text":"<pre><code>def wrap_agent_as_tool(\n    agent: CompiledStateGraph,\n    tool_name: Optional[str] = None,\n    tool_description: Optional[str] = None,\n    pre_input_hooks: Optional[\n        tuple[\n            Callable[[str, ToolRuntime], str],\n            Callable[[str, ToolRuntime], Awaitable[str]],\n        ]\n        | Callable[[str, ToolRuntime], str]\n    ] = None,\n    post_output_hooks: Optional[\n        tuple[\n            Callable[[str, list[AnyMessage], ToolRuntime], Any],\n            Callable[[str, list[AnyMessage], ToolRuntime], Awaitable[Any]],\n        ]\n        | Callable[[str, list[AnyMessage], ToolRuntime], Any]\n    ] = None,\n) -&gt; BaseTool\n</code></pre>"},{"location":"api-reference/agent/#parameters_1","title":"Parameters","text":"Parameter Type Required Default Description agent CompiledStateGraph Yes - Agent tool_name Optional[str] No None Tool name tool_description Optional[str] No None Tool description pre_input_hooks Optional[tuple[Callable[[str, ToolRuntime], str], Callable[[str, ToolRuntime], Awaitable[str]]] | Callable[[str, ToolRuntime], str]] No None Agent input preprocessing function post_output_hooks Optional[tuple[Callable[[str, list[AnyMessage], ToolRuntime], Any], Callable[[str, list[AnyMessage], ToolRuntime], Awaitable[Any]]] | Callable[[str, list[AnyMessage], ToolRuntime], Any]] No None Agent output postprocessing function"},{"location":"api-reference/agent/#example_1","title":"Example","text":"<pre><code>tool = wrap_agent_as_tool(agent)\n</code></pre>"},{"location":"api-reference/agent/#wrap_all_agents_as_tool","title":"wrap_all_agents_as_tool","text":"<p>Wraps all agents as a single tool.</p>"},{"location":"api-reference/agent/#function-signature_2","title":"Function Signature","text":"<pre><code>def wrap_all_agents_as_tool(\n    agents: list[CompiledStateGraph],\n    tool_name: Optional[str] = None,\n    tool_description: Optional[str] = None,\n    pre_input_hooks: Optional[\n        tuple[\n            Callable[[str, ToolRuntime], str],\n            Callable[[str, ToolRuntime], Awaitable[str]],\n        ]\n        | Callable[[str, ToolRuntime], str]\n    ] = None,\n    post_output_hooks: Optional[\n        tuple[\n            Callable[[str, list[AnyMessage], ToolRuntime], Any],\n            Callable[[str, list[AnyMessage], ToolRuntime], Awaitable[Any]],\n        ]\n        | Callable[[str, list[AnyMessage], ToolRuntime], Any]\n    ] = None,\n) -&gt; BaseTool:\n</code></pre>"},{"location":"api-reference/agent/#parameters_2","title":"Parameters","text":"Parameter Type Required Default Description agents list[CompiledStateGraph] Yes - List of agents (must contain at least 2, and each agent must have a unique name) tool_name Optional[str] No None Tool name tool_description Optional[str] No None Tool description pre_input_hooks Optional[tuple[Callable[[str, ToolRuntime], str], Callable[[str, ToolRuntime], Awaitable[str]]] | Callable[[str, ToolRuntime], str]] No None Agent input preprocessing function post_output_hooks Optional[tuple[Callable[[str, list[AnyMessage], ToolRuntime], Any], Callable[[str, list[AnyMessage], ToolRuntime], Awaitable[Any]]] | Callable[[str, list[AnyMessage], ToolRuntime], Any]] No None Agent output postprocessing function"},{"location":"api-reference/agent/#example_2","title":"Example","text":"<pre><code>tool = wrap_all_agents_as_tool([time_agent, weather_agent])\n</code></pre>"},{"location":"api-reference/agent/#summarizationmiddleware","title":"SummarizationMiddleware","text":"<p>Middleware for agent context summarization.</p>"},{"location":"api-reference/agent/#class-definition","title":"Class Definition","text":"<pre><code>class SummarizationMiddleware(_SummarizationMiddleware):\n    def __init__(\n        self,\n        model: str,\n        *,\n        trigger: ContextSize | list[ContextSize] | None = None,\n        keep: ContextSize = (\"messages\", _DEFAULT_MESSAGES_TO_KEEP),\n        token_counter: TokenCounter = count_tokens_approximately,\n        summary_prompt: str = DEFAULT_SUMMARY_PROMPT,\n        trim_tokens_to_summarize: int | None = _DEFAULT_TRIM_TOKEN_LIMIT,\n        **deprecated_kwargs: Any,\n    ) -&gt; None\n</code></pre>"},{"location":"api-reference/agent/#parameters_3","title":"Parameters","text":"Parameter Type Required Default Description model str Yes - Model identifier string that can be loaded by <code>load_chat_model</code>. Can be specified in \"provider:model-name\" format trigger ContextSize | list[ContextSize] | None No None Context size that triggers summarization keep ContextSize No (\"messages\", _DEFAULT_MESSAGES_TO_KEEP) Context size to keep token_counter TokenCounter No count_tokens_approximately Token counter summary_prompt str No DEFAULT_SUMMARY_PROMPT Summary prompt trim_tokens_to_summarize int | None No _DEFAULT_TRIM_TOKEN_LIMIT Number of tokens to trim before summarizing"},{"location":"api-reference/agent/#example_3","title":"Example","text":"<pre><code>summarization_middleware = SummarizationMiddleware(model=\"vllm:qwen3-4b\")\n</code></pre>"},{"location":"api-reference/agent/#llmtoolselectormiddleware","title":"LLMToolSelectorMiddleware","text":"<p>Middleware for agent tool selection.</p>"},{"location":"api-reference/agent/#class-definition_1","title":"Class Definition","text":"<pre><code>class LLMToolSelectorMiddleware(_LLMToolSelectorMiddleware):\n    def __init__(\n        self,\n        *,\n        model: str,\n        system_prompt: Optional[str] = None,\n        max_tools: Optional[int] = None,\n        always_include: Optional[list[str]] = None,\n    ) -&gt; None\n</code></pre>"},{"location":"api-reference/agent/#parameters_4","title":"Parameters","text":"Parameter Type Required Default Description model str Yes - Model identifier string that can be loaded by <code>load_chat_model</code>. Can be specified in \"provider:model-name\" format system_prompt Optional[str] No None System prompt max_tools Optional[int] No None Maximum number of tools always_include Optional[list[str]] No None Tools to always include"},{"location":"api-reference/agent/#example_4","title":"Example","text":"<pre><code>llm_tool_selector_middleware = LLMToolSelectorMiddleware(model=\"vllm:qwen3-4b\")\n</code></pre>"},{"location":"api-reference/agent/#planmiddleware","title":"PlanMiddleware","text":"<p>Middleware for agent plan management.</p>"},{"location":"api-reference/agent/#class-definition_2","title":"Class Definition","text":"<pre><code>class PlanMiddleware(AgentMiddleware):\n    state_schema = PlanState\n    def __init__(\n        self,\n        *,\n        system_prompt: Optional[str] = None,\n        custom_plan_tool_descriptions: Optional[PlanToolDescription] = None,\n        use_read_plan_tool: bool = True,\n    ) -&gt; None:\n</code></pre>"},{"location":"api-reference/agent/#parameters_5","title":"Parameters","text":"Parameter Type Required Default Description system_prompt Optional[str] No None System prompt custom_plan_tool_descriptions Optional[PlanToolDescription] No None Custom descriptions for plan tools use_read_plan_tool bool No True Whether to use the read plan tool"},{"location":"api-reference/agent/#example_5","title":"Example","text":"<pre><code>plan_middleware = PlanMiddleware()\n</code></pre>"},{"location":"api-reference/agent/#modelfallbackmiddleware","title":"ModelFallbackMiddleware","text":"<p>Middleware for agent model fallback.</p>"},{"location":"api-reference/agent/#class-definition_3","title":"Class Definition","text":"<pre><code>class ModelFallbackMiddleware(_ModelFallbackMiddleware):\n    def __init__(\n        self,\n        first_model: str,\n        *additional_models: str,\n    ) -&gt; None\n</code></pre>"},{"location":"api-reference/agent/#parameters_6","title":"Parameters","text":"Parameter Type Required Default Description first_model str Yes - Model identifier string that can be loaded by <code>load_chat_model</code>. Can be specified in \"provider:model-name\" format additional_models str No - List of backup models"},{"location":"api-reference/agent/#example_6","title":"Example","text":"<pre><code>model_fallback_middleware = ModelFallbackMiddleware(\n    \"vllm:qwen3-4b\",\n    \"vllm:qwen3-8b\"\n)\n</code></pre>"},{"location":"api-reference/agent/#llmtoolemulator","title":"LLMToolEmulator","text":"<p>Middleware for using large models to simulate tool calls.</p>"},{"location":"api-reference/agent/#class-definition_4","title":"Class Definition","text":"<pre><code>class LLMToolEmulator(_LLMToolEmulator):\n    def __init__(\n        self,\n        *,\n        model: str,\n        tools: list[str | BaseTool] | None = None,\n    ) -&gt; None\n</code></pre>"},{"location":"api-reference/agent/#parameters_7","title":"Parameters","text":"Parameter Type Required Default Description model str Yes - Model identifier string that can be loaded by <code>load_chat_model</code>. Can be specified in \"provider:model-name\" format tools list[str | BaseTool] | None No None List of tools"},{"location":"api-reference/agent/#example_7","title":"Example","text":"<pre><code>llm_tool_emulator = LLMToolEmulator(model=\"vllm:qwen3-4b\", tools=[get_current_time])\n</code></pre>"},{"location":"api-reference/agent/#modelroutermiddleware","title":"ModelRouterMiddleware","text":"<p>Middleware for dynamically routing to appropriate models based on input content.</p>"},{"location":"api-reference/agent/#class-definition_5","title":"Class Definition","text":"<pre><code>class ModelRouterMiddleware(AgentMiddleware):\n    state_schema = ModelRouterState\n    def __init__(\n        self,\n        router_model: str | BaseChatModel,\n        model_list: list[ModelDict],\n        router_prompt: Optional[str] = None,\n    ) -&gt; None\n</code></pre>"},{"location":"api-reference/agent/#parameters_8","title":"Parameters","text":"Parameter Type Required Default Description router_model str | BaseChatModel Yes - Model for routing, accepts string type (loaded with <code>load_chat_model</code>) or directly passed ChatModel model_list list[ModelDict] Yes - List of models, each model needs to contain <code>model_name</code> and <code>model_description</code> keys, and can optionally include <code>tools</code>, <code>model_kwargs</code>, <code>model_instance</code>, <code>model_system_prompt</code> these four keys router_prompt Optional[str] No None Prompt for the routing model, if None then the default prompt is used"},{"location":"api-reference/agent/#example_8","title":"Example","text":"<pre><code>model_router_middleware = ModelRouterMiddleware(\n    router_model=\"vllm:qwen3-4b\",\n    model_list=[\n        {\n            \"model_name\": \"vllm:qwen3-4b\",\n            \"model_description\": \"Suitable for ordinary tasks, such as dialogue, text generation, etc.\"\n        },\n        {\n            \"model_name\": \"vllm:qwen3-8b\",\n            \"model_description\": \"Suitable for complex tasks, such as code generation, data analysis, etc.\",\n        },\n    ]\n)\n</code></pre>"},{"location":"api-reference/agent/#handoffagentmiddleware","title":"HandoffAgentMiddleware","text":"<p>Middleware for implementing multi-agent switching (handoffs).</p>"},{"location":"api-reference/agent/#class-definition_6","title":"Class Definition","text":"<pre><code>class HandoffAgentMiddleware(AgentMiddleware):\n    state_schema = MultiAgentState\n    def __init__(\n        self,\n        agents_config: dict[str, AgentConfig],\n        custom_handoffs_tool_descriptions: Optional[dict[str, str]] = None,\n    ) -&gt; None:\n</code></pre>"},{"location":"api-reference/agent/#parameters_9","title":"Parameters","text":"Parameter Type Required Default Description agents_config dict[str, AgentConfig] Yes - Dictionary of agent configurations, keys are agent names, values are agent configurations custom_handoffs_tool_descriptions Optional[dict[str, str]] No None Custom descriptions for tools to handoff to other agents"},{"location":"api-reference/agent/#example_9","title":"Example","text":"<pre><code>handoffs_agent_middleware = HandoffsAgentMiddleware({\n    \"time_agent\":{\n        \"model\":\"vllm:qwen3-4b\",\n        \"prompt\":\"You are a time agent, responsible for answering time-related questions.\",\n        \"tools\":[get_current_time, transfer_to_default_agent],\n        \"handoffs\":[\"default_agent\"]\n    },\n    \"default_agent\":{\n        \"model\":\"vllm:qwen3-8b\",\n        \"prompt\":\"You are a complex task agent, responsible for answering complex task-related questions.\",\n        \"default\":True,\n        \"handoffs\":[\"time_agent\"]\n    }\n})\n</code></pre>"},{"location":"api-reference/agent/#toolcallrepairmiddleware","title":"ToolCallRepairMiddleware","text":"<p>Middleware for repairing invalid tool calls.</p>"},{"location":"api-reference/agent/#class-definition_7","title":"Class Definition","text":"<pre><code>class ToolCallRepairMiddleware(AgentMiddleware):\n</code></pre>"},{"location":"api-reference/agent/#example_10","title":"Example","text":"<pre><code>tool_call_repair_middleware = ToolCallRepairMiddleware()\n</code></pre>"},{"location":"api-reference/agent/#format_prompt","title":"format_prompt","text":"<p>Middleware for formatting prompts.</p>"},{"location":"api-reference/agent/#function-signature_3","title":"Function Signature","text":"<pre><code>@dynamic_prompt\ndef format_prompt(request: ModelRequest) -&gt; str\n</code></pre>"},{"location":"api-reference/agent/#planstate","title":"PlanState","text":"<p>State Schema for Plan.</p>"},{"location":"api-reference/agent/#class-definition_8","title":"Class Definition","text":"<pre><code>class Plan(TypedDict):\n    content: str\n    status: Literal[\"pending\", \"in_progress\", \"done\"]\n\n\nclass PlanState(AgentState):\n    plan: NotRequired[list[Plan]]\n</code></pre>"},{"location":"api-reference/agent/#properties","title":"Properties","text":"Property Type Description plan NotRequired[list[Plan]] List of plans plan.content str Plan content plan.status Literal[\"pending\", \"in_progress\", \"done\"] Plan status, values are <code>pending</code>, <code>in_progress</code>, <code>done</code>"},{"location":"api-reference/agent/#modeldict","title":"ModelDict","text":"<p>Type for model list.</p>"},{"location":"api-reference/agent/#class-definition_9","title":"Class Definition","text":"<pre><code>class ModelDict(TypedDict):\n    model_name: str\n    model_description: str\n    tools: NotRequired[list[BaseTool | dict[str, Any]]]\n    model_kwargs: NotRequired[dict[str, Any]]\n    model_instance: NotRequired[BaseChatModel]\n    model_system_prompt: NotRequired[str]\n</code></pre>"},{"location":"api-reference/agent/#properties_1","title":"Properties","text":"Property Type Required Description model_name str Yes Model name model_description str Yes Model description tools NotRequired[list[BaseTool | dict[str, Any]]] No Tools available to the model model_kwargs NotRequired[dict[str, Any]] No Additional parameters to pass to the model model_instance NotRequired[BaseChatModel] No Model instance model_system_prompt NotRequired[str] No System prompt for the model"},{"location":"api-reference/agent/#selectmodel","title":"SelectModel","text":"<p>Tool class for model selection.</p>"},{"location":"api-reference/agent/#class-definition_10","title":"Class Definition","text":"<pre><code>class SelectModel(BaseModel):\n    \"\"\"Tool for model selection - Must call this tool to return the finally selected model\"\"\"\n\n    model_name: str = Field(\n        ...,\n        description=\"Selected model name (must be the full model name, for example, openai:gpt-4o)\",\n    )\n</code></pre>"},{"location":"api-reference/agent/#properties_2","title":"Properties","text":"Property Type Required Description model_name str Yes Selected model name (must be the full model name, for example, openai:gpt-4o)"},{"location":"api-reference/agent/#multiagentstate","title":"MultiAgentState","text":"<p>State Schema for multi-agent switching.</p>"},{"location":"api-reference/agent/#class-definition_11","title":"Class Definition","text":"<pre><code>class MultiAgentState(AgentState):\n    active_agent: NotRequired[str]\n</code></pre>"},{"location":"api-reference/agent/#properties_3","title":"Properties","text":"Property Type Description active_agent NotRequired[str] Name of the currently active agent"},{"location":"api-reference/agent/#agentconfig","title":"AgentConfig","text":"<p>Type for agent configuration.</p>"},{"location":"api-reference/agent/#class-definition_12","title":"Class Definition","text":"<pre><code>class AgentConfig(TypedDict):\n    model: NotRequired[str | BaseChatModel]\n    prompt: str | SystemMessage\n    tools: NotRequired[list[BaseTool | dict[str, Any]]]\n    default: NotRequired[bool]\n    handoffs: list[str] | Literal[\"all\"]\n</code></pre>"},{"location":"api-reference/agent/#properties_4","title":"Properties","text":"Property Type Required Description model NotRequired[str | BaseChatModel] No Model name or model instance prompt str | SystemMessage Yes Prompt for the agent tools list[BaseTool | dict[str, Any]] Yes Tools available to the agent default NotRequired[bool] No Whether this is the default agent handoffs list[str] | Literal[\"all\"] Yes List of agent names that can be handed off to, or \"all\" for all agents"},{"location":"api-reference/chat_model/","title":"ChatModel Module API Reference Documentation","text":""},{"location":"api-reference/chat_model/#register_model_provider","title":"register_model_provider","text":"<p>Register a provider for chat models.</p>"},{"location":"api-reference/chat_model/#function-signature","title":"Function Signature","text":"<pre><code>def register_model_provider(\n    provider_name: str,\n    chat_model: ChatModelType,\n    base_url: Optional[str] = None,\n    model_profiles: Optional[dict[str, dict[str, Any]]] = None,\n    compatibility_options: Optional[CompatibilityOptions] = None,\n) -&gt; None:\n</code></pre>"},{"location":"api-reference/chat_model/#parameters","title":"Parameters","text":"Parameter Type Required Default Description provider_name str Yes - Custom provider name chat_model ChatModelType Yes - ChatModel class or supported provider string type base_url Optional[str] No None BaseURL of the provider model_profiles Optional[dict[str, dict[str, Any]]] No None Profiles of models supported by the provider, format: <code>{model_name: model_profile}</code> compatibility_options Optional[CompatibilityOptions] No None Compatibility options"},{"location":"api-reference/chat_model/#example","title":"Example","text":"<pre><code>register_model_provider(\"fakechat\", FakeChatModel)\nregister_model_provider(\"vllm\", \"openai-compatible\", base_url=\"http://localhost:8000/v1\")\n</code></pre>"},{"location":"api-reference/chat_model/#batch_register_model_provider","title":"batch_register_model_provider","text":"<p>Batch register model providers.</p>"},{"location":"api-reference/chat_model/#function-signature_1","title":"Function Signature","text":"<pre><code>def batch_register_model_provider(\n    providers: list[ChatModelProvider],\n) -&gt; None:\n</code></pre>"},{"location":"api-reference/chat_model/#parameters_1","title":"Parameters","text":"Parameter Type Required Default Description providers list[ChatModelProvider] Yes - List of provider configurations"},{"location":"api-reference/chat_model/#example_1","title":"Example","text":"<pre><code>batch_register_model_provider([\n    {\"provider_name\": \"fakechat\", \"chat_model\": FakeChatModel},\n    {\"provider_name\": \"vllm\", \"chat_model\": \"openai-compatible\", \"base_url\": \"http://localhost:8000/v1\"},\n])\n</code></pre>"},{"location":"api-reference/chat_model/#load_chat_model","title":"load_chat_model","text":"<p>Load a chat model from registered providers.</p>"},{"location":"api-reference/chat_model/#function-signature_2","title":"Function Signature","text":"<pre><code>def load_chat_model(\n    model: str,\n    *,\n    model_provider: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; BaseChatModel:\n</code></pre>"},{"location":"api-reference/chat_model/#parameters_2","title":"Parameters","text":"Parameter Type Required Default Description model str Yes - Model name, format: <code>model_name</code> or <code>provider_name:model_name</code> model_provider Optional[str] No None Model provider name **kwargs Any No - Additional model parameters"},{"location":"api-reference/chat_model/#example_2","title":"Example","text":"<pre><code>model = load_chat_model(\"vllm:qwen3-4b\")\n</code></pre>"},{"location":"api-reference/chat_model/#create_openai_compatible_model","title":"create_openai_compatible_model","text":"<p>Create an OpenAI compatible chat model class.</p>"},{"location":"api-reference/chat_model/#function-signature_3","title":"Function Signature","text":"<pre><code>def create_openai_compatible_model(\n    model_provider: str,\n    base_url: Optional[str] = None,\n    compatibility_options: Optional[CompatibilityOptions] = None,\n    model_profiles: Optional[dict[str, dict[str, Any]]] = None,\n    chat_model_cls_name: Optional[str] = None,\n) -&gt; type[BaseChatModel]:\n</code></pre>"},{"location":"api-reference/chat_model/#parameters_3","title":"Parameters","text":"Parameter Type Required Default Description model_provider str Yes - Model provider name base_url Optional[str] No None BaseURL of the model provider compatibility_options Optional[CompatibilityOptions] No None Compatibility options model_profiles Optional[dict[str, dict[str, Any]]] No None Profiles of models supported by the provider, format: <code>{model_name: model_profile}</code> chat_model_cls_name Optional[str] No None Custom chat model class name"},{"location":"api-reference/chat_model/#return-value","title":"Return Value","text":"Type Description type[BaseChatModel] Dynamically created OpenAI compatible chat model class"},{"location":"api-reference/chat_model/#example_3","title":"Example","text":"<pre><code>ChatVLLM = create_openai_compatible_model(\n    model_provider=\"vllm\",\n    base_url=\"http://localhost:8000/v1\",\n    chat_model_cls_name=\"ChatVLLM\",\n)\n</code></pre>"},{"location":"api-reference/chat_model/#chatmodeltype","title":"ChatModelType","text":"<p>Types supported by the <code>chat_model</code> parameter when registering model providers.</p>"},{"location":"api-reference/chat_model/#type-definition","title":"Type Definition","text":"<pre><code>ChatModelType = Union[type[BaseChatModel], Literal[\"openai-compatible\"]]\n</code></pre>"},{"location":"api-reference/chat_model/#toolchoicetype","title":"ToolChoiceType","text":"<p>Types supported by the <code>tool_choice</code> parameter.</p>"},{"location":"api-reference/chat_model/#type-definition_1","title":"Type Definition","text":"<pre><code>ToolChoiceType = list[Literal[\"auto\", \"none\", \"required\", \"specific\"]]\n</code></pre>"},{"location":"api-reference/chat_model/#responseformattype","title":"ResponseFormatType","text":"<p>Types supported by <code>response_format</code>.</p>"},{"location":"api-reference/chat_model/#type-definition_2","title":"Type Definition","text":"<pre><code>ResponseFormatType = list[Literal[\"json_schema\", \"json_mode\"]]\n</code></pre>"},{"location":"api-reference/chat_model/#reasoningkeeppolicy","title":"ReasoningKeepPolicy","text":"<p>Retention policy for the reasoning_content field in the messages list.</p>"},{"location":"api-reference/chat_model/#type-definition_3","title":"Type Definition","text":"<pre><code>ReasoningKeepPolicy = Literal[\"never\", \"current\", \"all\"]\n</code></pre>"},{"location":"api-reference/chat_model/#compatibilityoptions","title":"CompatibilityOptions","text":"<p>Compatibility options for model providers.</p>"},{"location":"api-reference/chat_model/#class-definition","title":"Class Definition","text":"<pre><code>class CompatibilityOptions(TypedDict):\n    supported_tool_choice: NotRequired[ToolChoiceType]\n    supported_response_format: NotRequired[ResponseFormatType]\n    reasoning_keep_policy: NotRequired[ReasoningKeepPolicy]\n    include_usage: NotRequired[bool]\n</code></pre>"},{"location":"api-reference/chat_model/#field-description","title":"Field Description","text":"Field Type Required Description supported_tool_choice NotRequired[ToolChoiceType] No List of supported <code>tool_choice</code> strategies supported_response_format NotRequired[ResponseFormatType] No List of supported <code>response_format</code> methods reasoning_keep_policy NotRequired[ReasoningKeepPolicy] No Retention policy for the <code>reasoning_content</code> field in historical messages (messages) passed to the model. Optional values are <code>never</code>, <code>current</code>, <code>all</code> include_usage NotRequired[bool] No Whether to include <code>usage</code> information in the last streaming response result"},{"location":"api-reference/chat_model/#chatmodelprovider","title":"ChatModelProvider","text":"<p>Chat model provider configuration type.</p>"},{"location":"api-reference/chat_model/#class-definition_1","title":"Class Definition","text":"<pre><code>class ChatModelProvider(TypedDict):\n    provider_name: str\n    chat_model: ChatModelType\n    base_url: NotRequired[str]\n    model_profiles: NotRequired[dict[str, dict[str, Any]]]\n    compatibility_options: NotRequired[CompatibilityOptions]\n</code></pre>"},{"location":"api-reference/chat_model/#field-description_1","title":"Field Description","text":"Field Type Required Description provider_name str Yes Provider name chat_model ChatModelType Yes Support passing chat model class or string (currently only supports <code>openai-compatible</code>) base_url NotRequired[str] No Base URL model_profiles NotRequired[dict[str, dict[str, Any]]] No Profiles of models supported by the provider, format: <code>{model_name: model_profile}</code> compatibility_options NotRequired[CompatibilityOptions] No Model provider compatibility options"},{"location":"api-reference/embeddings/","title":"Embeddings Module API Reference Documentation","text":""},{"location":"api-reference/embeddings/#register_embeddings_provider","title":"register_embeddings_provider","text":"<p>Register a provider for embedding models.</p>"},{"location":"api-reference/embeddings/#function-signature","title":"Function Signature","text":"<pre><code>def register_embeddings_provider(\n    provider_name: str,\n    embeddings_model: EmbeddingsType,\n    base_url: Optional[str] = None,\n) -&gt; None:\n</code></pre>"},{"location":"api-reference/embeddings/#parameters","title":"Parameters","text":"Parameter Type Required Default Description provider_name str Yes - Custom provider name embeddings_model EmbeddingsType Yes - Embedding model class or supported provider string type base_url Optional[str] No None BaseURL of the provider"},{"location":"api-reference/embeddings/#example","title":"Example","text":"<pre><code>register_embeddings_provider(\"fakeembeddings\", FakeEmbeddings)\nregister_embeddings_provider(\"vllm\", \"openai-compatible\", base_url=\"http://localhost:8000/v1\")\n</code></pre>"},{"location":"api-reference/embeddings/#batch_register_embeddings_provider","title":"batch_register_embeddings_provider","text":"<p>Batch register embedding model providers.</p>"},{"location":"api-reference/embeddings/#function-signature_1","title":"Function Signature","text":"<pre><code>def batch_register_embeddings_provider(\n    providers: list[EmbeddingProvider]\n) -&gt; None:\n</code></pre>"},{"location":"api-reference/embeddings/#parameters_1","title":"Parameters","text":"Parameter Type Required Default Description providers list[EmbeddingProvider] Yes - List of provider configurations"},{"location":"api-reference/embeddings/#example_1","title":"Example","text":"<pre><code>batch_register_embeddings_provider([\n    {\"provider_name\": \"fakeembeddings\", \"embeddings_model\": FakeEmbeddings},\n    {\"provider_name\": \"vllm\", \"embeddings_model\": \"openai-compatible\", \"base_url\": \"http://localhost:8000/v1\"},\n])\n</code></pre>"},{"location":"api-reference/embeddings/#load_embeddings","title":"load_embeddings","text":"<p>Load an embedding model from registered providers.</p>"},{"location":"api-reference/embeddings/#function-signature_2","title":"Function Signature","text":"<pre><code>def load_embeddings(\n    model: str,\n    *,\n    provider: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; Embeddings:\n</code></pre>"},{"location":"api-reference/embeddings/#parameters_2","title":"Parameters","text":"Parameter Type Required Default Description model str Yes - Model name, format: <code>model_name</code> or <code>provider_name:model_name</code> provider Optional[str] No None Model provider name **kwargs Any No - Additional model parameters"},{"location":"api-reference/embeddings/#example_2","title":"Example","text":"<pre><code>embeddings = load_embeddings(\"vllm:qwen3-embedding-4b\")\n</code></pre>"},{"location":"api-reference/embeddings/#create_openai_compatible_embedding","title":"create_openai_compatible_embedding","text":"<p>Create an OpenAI compatible embedding model class.</p>"},{"location":"api-reference/embeddings/#function-signature_3","title":"Function Signature","text":"<pre><code>def create_openai_compatible_embedding(\n    embedding_provider: str,\n    base_url: Optional[str] = None,\n    embedding_model_cls_name: Optional[str] = None,\n) -&gt; type[Embeddings]:\n</code></pre>"},{"location":"api-reference/embeddings/#parameters_3","title":"Parameters","text":"Parameter Type Required Default Description embedding_provider str Yes - Embedding model provider name base_url Optional[str] No None BaseURL of the model provider embedding_model_cls_name Optional[str] No None Custom embedding model class name"},{"location":"api-reference/embeddings/#return-value","title":"Return Value","text":"Type Description type[Embeddings] Dynamically created OpenAI compatible embedding model class"},{"location":"api-reference/embeddings/#example_3","title":"Example","text":""},{"location":"api-reference/embeddings/#vllmembeddings-create_openai_compatible_embedding-embedding_providervllm-base_urlhttplocalhost8000v1-embedding_model_cls_namevllmembeddings","title":"<pre><code>VLLMEmbeddings = create_openai_compatible_embedding(\n    embedding_provider=\"vllm\",\n    base_url=\"http://localhost:8000/v1\",\n    embedding_model_cls_name=\"VLLMEmbeddings\",\n)\n</code></pre>","text":""},{"location":"api-reference/embeddings/#embeddingstype","title":"EmbeddingsType","text":"<p>Types supported by the <code>embeddings_model</code> parameter when registering embedding providers.</p>"},{"location":"api-reference/embeddings/#type-definition","title":"Type Definition","text":"<pre><code>EmbeddingsType = Union[type[Embeddings], Literal[\"openai-compatible\"]]\n</code></pre>"},{"location":"api-reference/embeddings/#embeddingprovider","title":"EmbeddingProvider","text":"<p>Embedding model provider configuration type.</p>"},{"location":"api-reference/embeddings/#class-definition","title":"Class Definition","text":"<pre><code>class EmbeddingProvider(TypedDict):\n    provider_name: str\n    embeddings_model: EmbeddingsType\n    base_url: NotRequired[str]\n</code></pre>"},{"location":"api-reference/embeddings/#field-description","title":"Field Description","text":"Field Type Required Description provider_name str Yes Provider name embeddings_model EmbeddingsType Yes Embedding model class or string base_url NotRequired[str] No Base URL"},{"location":"api-reference/message_convert/","title":"Message Convert Module API Reference Documentation","text":""},{"location":"api-reference/message_convert/#convert_reasoning_content_for_ai_message","title":"convert_reasoning_content_for_ai_message","text":"<p>Merges the chain of thought into the final response.</p>"},{"location":"api-reference/message_convert/#function-signature","title":"Function Signature","text":"<pre><code>def convert_reasoning_content_for_ai_message(\n    model_response: AIMessage,\n    think_tag: Tuple[str, str] = (\"&lt;think&gt;\", \"&lt;/think&gt;\"),\n) -&gt; AIMessage\n</code></pre>"},{"location":"api-reference/message_convert/#parameters","title":"Parameters","text":"Parameter Type Required Default Description model_response AIMessage Yes - AI message containing reasoning content think_tag Tuple[str, str] No <code>(\"&lt;think&gt;\",\"&lt;/think&gt;\")</code> Start and end tags for reasoning content"},{"location":"api-reference/message_convert/#example","title":"Example","text":"<pre><code>response = convert_reasoning_content_for_ai_message(\n    response, think_tag=(\"&lt;start&gt;\", \"&lt;end&gt;\")\n)\n</code></pre>"},{"location":"api-reference/message_convert/#convert_reasoning_content_for_chunk_iterator","title":"convert_reasoning_content_for_chunk_iterator","text":"<p>Merges reasoning content for streaming message chunks.</p>"},{"location":"api-reference/message_convert/#function-signature_1","title":"Function Signature","text":"<pre><code>def convert_reasoning_content_for_chunk_iterator(\n    model_response: Iterator[AIMessageChunk | AIMessage],\n    think_tag: Tuple[str, str] = (\"think\", \"think\"),\n) -&gt; Iterator[AIMessageChunk | AIMessage]\n</code></pre>"},{"location":"api-reference/message_convert/#parameters_1","title":"Parameters","text":"Parameter Type Required Default Description model_response Iterator[AIMessageChunk | AIMessage] Yes - Iterator of message chunks think_tag Tuple[str, str] No <code>(\"&lt;think&gt;\",\"&lt;/think&gt;\")</code> Start and end tags for reasoning content"},{"location":"api-reference/message_convert/#example_1","title":"Example","text":"<pre><code>for chunk in convert_reasoning_content_for_chunk_iterator(\n    model.stream(\"Hello\"), think_tag=(\"&lt;think&gt;\", \"&lt;/think&gt;\")\n):\n    print(chunk.content, end=\"\", flush=True)\n</code></pre>"},{"location":"api-reference/message_convert/#aconvert_reasoning_content_for_chunk_iterator","title":"aconvert_reasoning_content_for_chunk_iterator","text":"<p>Asynchronous version of <code>convert_reasoning_content_for_chunk_iterator</code>.</p>"},{"location":"api-reference/message_convert/#function-signature_2","title":"Function Signature","text":"<pre><code>async def aconvert_reasoning_content_for_chunk_iterator(\n    model_response: AsyncIterator[AIMessageChunk | AIMessage],\n    think_tag: Tuple[str, str] = (\"think\", \"think\"),\n) -&gt; AsyncIterator[AIMessageChunk | AIMessage]\n</code></pre>"},{"location":"api-reference/message_convert/#parameters_2","title":"Parameters","text":"Parameter Type Required Default Description model_response AsyncIterator[AIMessageChunk | AIMessage] Yes - Async iterator of message chunks think_tag Tuple[str, str] No <code>(\"&lt;think&gt;\",\"&lt;/think&gt;\")</code> Start and end tags for reasoning content"},{"location":"api-reference/message_convert/#example_2","title":"Example","text":"<pre><code>async for chunk in aconvert_reasoning_content_for_chunk_iterator(\n    model.astream(\"Hello\"), think_tag=(\"&lt;think&gt;\", \"&lt;/think&gt;\")\n):\n    print(chunk.content, end=\"\", flush=True)\n</code></pre>"},{"location":"api-reference/message_convert/#merge_ai_message_chunk","title":"merge_ai_message_chunk","text":"<p>Merges streaming output chunks into a single AIMessage.</p>"},{"location":"api-reference/message_convert/#function-signature_3","title":"Function Signature","text":"<pre><code>def merge_ai_message_chunk(\n    chunks: Sequence[AIMessageChunk]\n) -&gt; AIMessage\n</code></pre>"},{"location":"api-reference/message_convert/#parameters_3","title":"Parameters","text":"Parameter Type Required Default Description chunks Sequence[AIMessageChunk] Yes - List of message chunks to merge"},{"location":"api-reference/message_convert/#example_3","title":"Example","text":"<pre><code>chunks = list(model.stream(\"Hello\"))\nmerged = merge_ai_message_chunk(chunks)\n</code></pre>"},{"location":"api-reference/message_convert/#format_sequence","title":"format_sequence","text":"<p>Formats a list of BaseMessage, Document, or strings into a single string.</p>"},{"location":"api-reference/message_convert/#function-signature_4","title":"Function Signature","text":"<pre><code>def format_sequence(\n    inputs: List[Union[BaseMessage, Document, str]],\n    separator: str = \"-\",\n    with_num: bool = False\n) -&gt; str\n</code></pre>"},{"location":"api-reference/message_convert/#parameters_4","title":"Parameters","text":"Parameter Type Required Default Description inputs List[Union[BaseMessage, Document, str]] Yes - List of items to format separator str No \"-\" Separator string with_num bool No False Whether to add number prefix"},{"location":"api-reference/message_convert/#example_4","title":"Example","text":"<pre><code>formatted = format_sequence(messages, separator=\"\\n\", with_num=True)\n</code></pre>"},{"location":"api-reference/pipeline/","title":"Pipeline Module API Reference Documentation","text":""},{"location":"api-reference/pipeline/#create_sequential_pipeline","title":"create_sequential_pipeline","text":"<p>Combines multiple subgraphs with the same state in a sequential manner.</p>"},{"location":"api-reference/pipeline/#function-signature","title":"Function Signature","text":"<pre><code>def create_sequential_pipeline(\n    sub_graphs: list[SubGraph],\n    state_schema: type[StateT],\n    graph_name: Optional[str] = None,\n    context_schema: type[ContextT] | None = None,\n    input_schema: type[InputT] | None = None,\n    output_schema: type[OutputT] | None = None,\n    checkpointer: Checkpointer | None = None,\n    store: BaseStore | None = None,\n    cache: BaseCache | None = None,\n) -&gt; CompiledStateGraph[StateT, ContextT, InputT, OutputT]:\n</code></pre>"},{"location":"api-reference/pipeline/#parameters","title":"Parameters","text":"Parameter Type Required Default Description sub_graphs list[SubGraph] Yes - List of state graphs to combine state_schema type[StateT] Yes - State Schema of the final generated graph graph_name Optional[str] No None Name of the final generated graph context_schema type[ContextT] | None No None Context Schema of the final generated graph input_schema type[InputT] | None No None Input Schema of the final generated graph output_schema type[OutputT] | None No None Output Schema of the final generated graph checkpointer Checkpointer | None No None Checkpointer of the final generated graph store BaseStore | None No None Store of the final generated graph cache BaseCache | None No None Cache of the final generated graph"},{"location":"api-reference/pipeline/#example","title":"Example","text":"<pre><code>create_sequential_pipeline(\n    sub_graphs=[graph1, graph2],\n    state_schema=State,\n    graph_name=\"sequential_pipeline\",\n    context_schema=Context,\n    input_schema=Input,\n    output_schema=Output,\n)\n</code></pre>"},{"location":"api-reference/pipeline/#create_parallel_pipeline","title":"create_parallel_pipeline","text":"<p>Combines multiple subgraphs with the same state in a parallel manner.</p>"},{"location":"api-reference/pipeline/#function-signature_1","title":"Function Signature","text":"<pre><code>def create_parallel_pipeline(\n    sub_graphs: list[SubGraph],\n    state_schema: type[StateT],\n    graph_name: Optional[str] = None,\n    branches_fn: Optional[\n        Union[\n            Callable[..., list[Send]],\n            Callable[..., Awaitable[list[Send]]],\n        ]\n    ] = None,\n    context_schema: type[ContextT] | None = None,\n    input_schema: type[InputT] | None = None,\n    output_schema: type[OutputT] | None = None,\n    checkpointer: Checkpointer | None = None,\n    store: BaseStore | None = None,\n    cache: BaseCache | None = None,\n) -&gt; CompiledStateGraph[StateT, ContextT, InputT, OutputT]:\n</code></pre>"},{"location":"api-reference/pipeline/#parameters_1","title":"Parameters","text":"Parameter Type Required Default Description sub_graphs list[SubGraph] Yes - List of state graphs to combine state_schema type[StateT] Yes - State Schema of the final generated graph graph_name Optional[str] No None Name of the final generated graph branches_fn Optional[Union[Callable[..., list[Send]], Callable[..., Awaitable[list[Send]]]]] No None Parallel branch function, returns a list of <code>Send</code> objects to control parallel execution context_schema type[ContextT] | None No None Context Schema of the final generated graph input_schema type[InputT] | None No None Input Schema of the final generated graph output_schema type[OutputT] | None No None Output Schema of the final generated graph checkpointer Checkpointer | None No None Checkpointer of the final generated graph store BaseStore | None No None Store of the final generated graph cache BaseCache | None No None Cache of the final generated graph"},{"location":"api-reference/pipeline/#example_1","title":"Example","text":"<pre><code>create_parallel_pipeline(\n    sub_graphs=[graph1, graph2],\n    state_schema=State,\n    graph_name=\"parallel_pipeline\",\n    branches_fn=lambda state: [Send(\"graph1\", state), Send(\"graph2\", state)],\n    context_schema=Context,\n    input_schema=Input,\n    output_schema=Output,\n)\n</code></pre>"},{"location":"api-reference/tool_calling/","title":"Tool Calling Module API Reference Documentation","text":""},{"location":"api-reference/tool_calling/#has_tool_calling","title":"has_tool_calling","text":"<p>Checks if a message contains a tool call.</p>"},{"location":"api-reference/tool_calling/#function-signature","title":"Function Signature","text":"<pre><code>def has_tool_calling(\n    message: AIMessage\n) -&gt; bool\n</code></pre>"},{"location":"api-reference/tool_calling/#parameters","title":"Parameters","text":"Parameter Type Required Default Description message AIMessage Yes - The message to check"},{"location":"api-reference/tool_calling/#example","title":"Example","text":"<pre><code>if has_tool_calling(response):\n    # Handle tool call\n    pass\n</code></pre>"},{"location":"api-reference/tool_calling/#parse_tool_calling","title":"parse_tool_calling","text":"<p>Parses tool call arguments from a message.</p>"},{"location":"api-reference/tool_calling/#function-signature_1","title":"Function Signature","text":"<pre><code>def parse_tool_calling(\n    message: AIMessage, first_tool_call_only: bool = False\n) -&gt; Union[tuple[str, dict], list[tuple[str, dict]]]\n</code></pre>"},{"location":"api-reference/tool_calling/#parameters_1","title":"Parameters","text":"Parameter Type Required Default Description message AIMessage Yes - The message to parse first_tool_call_only bool No False Whether to return only the first tool call"},{"location":"api-reference/tool_calling/#example_1","title":"Example","text":"<pre><code># Get all tool calls\ntool_calls = parse_tool_calling(response)\n\n# Get only the first tool call\nname, args = parse_tool_calling(response, first_tool_call_only=True)\n</code></pre>"},{"location":"api-reference/tool_calling/#human_in_the_loop","title":"human_in_the_loop","text":"<p>A decorator to add \"human-in-the-loop\" manual review capability to synchronous tool functions.</p>"},{"location":"api-reference/tool_calling/#function-signature_2","title":"Function Signature","text":"<pre><code>def human_in_the_loop(\n    func: Optional[Callable] = None,\n    *,\n    handler: Optional[HumanInterruptHandler] = None\n) -&gt; Union[Callable[[Callable], BaseTool], BaseTool]\n</code></pre>"},{"location":"api-reference/tool_calling/#parameters_2","title":"Parameters","text":"Parameter Type Required Default Description func Optional[Callable] No None The synchronous function to be decorated (decorator syntactic sugar) handler Optional[HumanInterruptHandler] No None Custom interrupt handler function"},{"location":"api-reference/tool_calling/#example_2","title":"Example","text":"<pre><code>@human_in_the_loop\ndef get_current_time():\n    \"\"\"Get the current time\"\"\"\n    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n</code></pre>"},{"location":"api-reference/tool_calling/#human_in_the_loop_async","title":"human_in_the_loop_async","text":"<p>A decorator to add \"human-in-the-loop\" manual review capability to asynchronous tool functions.</p>"},{"location":"api-reference/tool_calling/#function-signature_3","title":"Function Signature","text":"<pre><code>def human_in_the_loop_async(\n    func: Optional[Callable] = None,\n    *,\n    handler: Optional[HumanInterruptHandler] = None\n) -&gt; Union[Callable[[Callable], BaseTool], BaseTool]\n</code></pre>"},{"location":"api-reference/tool_calling/#parameters_3","title":"Parameters","text":"Parameter Type Required Default Description func Optional[Callable] No None The asynchronous function to be decorated (decorator syntactic sugar) handler Optional[HumanInterruptHandler] No None Custom interrupt handler function"},{"location":"api-reference/tool_calling/#example_3","title":"Example","text":"<pre><code>@human_in_the_loop_async\nasync def get_current_time():\n    \"\"\"Get the current time\"\"\"\n    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n</code></pre>"},{"location":"api-reference/tool_calling/#interruptparams","title":"InterruptParams","text":"<p>The type of parameters passed to the interrupt handler function.</p>"},{"location":"api-reference/tool_calling/#class-definition","title":"Class Definition","text":"<pre><code>class InterruptParams(TypedDict):\n    tool_call_name: str\n    tool_call_args: Dict[str, Any]\n    tool: BaseTool\n</code></pre>"},{"location":"api-reference/tool_calling/#field-description","title":"Field Description","text":"Field Type Required Description tool_call_name str Yes The name of the tool call tool_call_args Dict[str, Any] Yes The arguments of the tool call tool BaseTool Yes The tool instance"},{"location":"api-reference/tool_calling/#humaninterrupthandler","title":"HumanInterruptHandler","text":"<p>Type alias for the interrupt handler function.</p>"},{"location":"api-reference/tool_calling/#type-definition","title":"Type Definition","text":"<pre><code>HumanInterruptHandler = Callable[[InterruptParams], Any]\n</code></pre>"},{"location":"getting-started-guide/chat/","title":"Chat Model Management","text":""},{"location":"getting-started-guide/chat/#overview","title":"Overview","text":"<p>LangChain's <code>init_chat_model</code> function only supports a limited number of model providers. This library offers a more flexible chat model management solution that supports custom model providers, particularly suitable for scenarios where you need to integrate model services not natively supported (such as vLLM).</p>"},{"location":"getting-started-guide/chat/#registering-model-providers","title":"Registering Model Providers","text":"<p>To register a chat model provider, call <code>register_model_provider</code>. The registration steps vary slightly for different situations.</p>"},{"location":"getting-started-guide/chat/#existing-langchain-chat-model-class","title":"Existing LangChain Chat Model Class","text":"<p>If the model provider already has a suitable LangChain integration (see Chat Model Class Integration), pass the corresponding integrated chat model class as the chat_model parameter.</p>"},{"location":"getting-started-guide/chat/#parameter-description","title":"Parameter Description","text":"Parameter Description <code>provider_name</code> Model provider name, used for subsequent reference in <code>load_chat_model</code>.Type: <code>str</code>Required: Yes <code>chat_model</code> LangChain chat model class.Type: <code>type[BaseChatModel]</code>Required: Yes <code>base_url</code> API base URL, usually no need to set manually.Type: <code>str</code>Required: No <code>model_profiles</code> Dictionary of model configuration information.Type: <code>dict</code>Required: No"},{"location":"getting-started-guide/chat/#code-example","title":"Code Example","text":"<pre><code>from langchain_core.language_models.fake_chat_models import FakeChatModel\nfrom langchain_dev_utils.chat_models import register_model_provider\n\nregister_model_provider(\n    provider_name=\"fake_provider\",\n    chat_model=FakeChatModel,\n)\n</code></pre>"},{"location":"getting-started-guide/chat/#usage-instructions","title":"Usage Instructions","text":"<ul> <li><code>FakeChatModel</code> is only for testing. In actual use, you must pass a <code>ChatModel</code> class with real functionality.</li> <li><code>provider_name</code> represents the name of the model provider, used for subsequent reference in <code>load_chat_model</code>. The name can be customized, but should not contain special characters such as <code>:</code>, <code>-</code>, etc.</li> </ul>"},{"location":"getting-started-guide/chat/#optional-parameter-description","title":"Optional Parameter Description","text":"<p>base_url</p> <p>This parameter usually doesn't need to be set (since the model class generally has a default API address defined internally). It should only be passed when you need to override the model class's default address, and it only affects attributes with field names <code>api_base</code> or <code>base_url</code> (including aliases).</p> <p>model_profiles</p> <p>If your LangChain integrated chat model class fully supports the <code>profile</code> parameter (i.e., you can directly access model-related properties through <code>model.profile</code>, such as <code>max_input_tokens</code>, <code>tool_calling</code>, etc.), there's no need to set <code>model_profiles</code> additionally.</p> <p>If accessing through <code>model.profile</code> returns an empty dictionary <code>{}</code>, it indicates that the LangChain chat model class may not support the <code>profile</code> parameter yet, in which case you can manually provide <code>model_profiles</code>.</p> <p><code>model_profiles</code> is a dictionary where each key is a model name, and the value is the profile configuration for the corresponding model:</p> <pre><code>{\n    \"model_name_1\": {\n        \"max_input_tokens\": 100_000,\n        \"tool_calling\": True,\n        \"structured_output\": True,\n        # ... other optional fields\n    },\n    \"model_name_2\": {\n        \"max_input_tokens\": 32768,\n        \"image_inputs\": True,\n        \"tool_calling\": False,\n        # ... other optional fields\n    },\n    # you can have any number of model configurations\n}\n</code></pre> <p>Tip</p> <p>It's recommended to use the <code>langchain-model-profiles</code> library to get profiles for your model provider.</p>"},{"location":"getting-started-guide/chat/#no-langchain-chat-model-class-but-provider-supports-openai-compatible-api","title":"No LangChain Chat Model Class, but Provider Supports OpenAI Compatible API","text":"<p>The parameter description for this situation is as follows:</p>"},{"location":"getting-started-guide/chat/#parameter-description_1","title":"Parameter Description","text":"Parameter Description <code>provider_name</code> Model provider name.Type: <code>str</code>Required: Yes <code>chat_model</code> Fixed value <code>\"openai-compatible\"</code>.Type: <code>str</code>Required: Yes <code>base_url</code> API base URL.Type: <code>str</code>Required: No <code>model_profiles</code> Dictionary of model configuration information.Type: <code>dict</code>Required: No <code>compatibility_options</code> Compatibility options configuration.Type: <code>dict</code>Required: No"},{"location":"getting-started-guide/chat/#code-example_1","title":"Code Example","text":"<p>Method 1: Explicit Parameters</p> <pre><code>register_model_provider(\n    provider_name=\"vllm\",\n    chat_model=\"openai-compatible\",\n    base_url=\"http://localhost:8000/v1\"\n)\n</code></pre> <p>Method 2: Through Environment Variables (Recommended for Configuration Management)</p> <pre><code>export VLLM_API_BASE=http://localhost:8000/v1\n</code></pre> <pre><code>register_model_provider(\n    provider_name=\"vllm\",\n    chat_model=\"openai-compatible\"\n    # Automatically reads VLLM_API_BASE\n)\n</code></pre> <p>Note: For more details on this part, please refer to OpenAI Compatible API Integration.</p>"},{"location":"getting-started-guide/chat/#batch-registration","title":"Batch Registration","text":"<p>If you need to register multiple providers, you can use <code>batch_register_model_provider</code> to avoid repeated calls.</p>"},{"location":"getting-started-guide/chat/#parameter-description_2","title":"Parameter Description","text":"Parameter Description <code>providers</code> List of provider configurations, each dictionary contains registration parameters.Type: <code>list[dict]</code>Required: Yes"},{"location":"getting-started-guide/chat/#code-example_2","title":"Code Example","text":"<pre><code>from langchain_dev_utils.chat_models import batch_register_model_provider\nfrom langchain_core.language_models.fake_chat_models import FakeChatModel\n\nbatch_register_model_provider(\n    providers=[\n        {\n            \"provider_name\": \"fake_provider\",\n            \"chat_model\": FakeChatModel,\n        },\n        {\n            \"provider_name\": \"vllm\",\n            \"chat_model\": \"openai-compatible\",\n            \"base_url\": \"http://localhost:8000/v1\",\n        },\n    ]\n)\n</code></pre> <p>Note</p> <p>Both registration functions are implemented based on a global dictionary. To avoid multi-threading issues, all registrations must be completed during the application startup phase, and dynamic registration during runtime is prohibited.</p> <p>Additionally, when registering with <code>chat_model</code> set to <code>openai-compatible</code>, the system internally uses <code>pydantic.create_model</code> to dynamically create new model classes (with <code>BaseChatOpenAICompatible</code> as the base class, generating corresponding chat model integration classes). This process involves Python metaclass operations and pydantic validation logic initialization, which has certain performance overhead, so please avoid frequent registration during runtime.</p>"},{"location":"getting-started-guide/chat/#loading-chat-models","title":"Loading Chat Models","text":"<p>Use the <code>load_chat_model</code> function to load chat models (initialize chat model instances).</p>"},{"location":"getting-started-guide/chat/#parameter-description_3","title":"Parameter Description","text":"Parameter Description <code>model</code> Model name.Type: <code>str</code>Required: Yes <code>model_provider</code> Model provider name.Type: <code>str</code>Required: No <p>In addition, any number of keyword arguments can be passed to provide additional parameters for the chat model class.</p>"},{"location":"getting-started-guide/chat/#parameter-rules","title":"Parameter Rules","text":"<ul> <li>If <code>model_provider</code> is not passed, then <code>model</code> must be in the format <code>provider_name:model_name</code>;</li> <li>If <code>model_provider</code> is passed, then <code>model</code> must only be <code>model_name</code>.</li> </ul>"},{"location":"getting-started-guide/chat/#code-example_3","title":"Code Example","text":"<pre><code># Method 1: model includes provider information\nmodel = load_chat_model(\"vllm:qwen3-4b\")\n\n# Method 2: specify provider separately\nmodel = load_chat_model(\"qwen3-4b\", model_provider=\"vllm\")\n</code></pre>"},{"location":"getting-started-guide/chat/#model-methods-and-parameters","title":"Model Methods and Parameters","text":"<p>For supported model methods and parameters, refer to the usage instructions of the corresponding chat model class. If you're using the second situation, all methods and parameters of the <code>BaseChatOpenAI</code> class are supported.</p>"},{"location":"getting-started-guide/chat/#compatibility-with-official-providers","title":"Compatibility with Official Providers","text":"<p>For providers already supported by LangChain (such as <code>openai</code>), you can directly use <code>load_chat_model</code> without registration:</p> <pre><code>model = load_chat_model(\"openai:gpt-4o-mini\")\n# or\nmodel = load_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n</code></pre> <p>Best Practice</p> <p>For the use of this module, you can choose based on the following three situations:</p> <ol> <li> <p>If all model providers you're integrating are supported by the official <code>init_chat_model</code>, please use the official function directly for the best compatibility and stability.</p> </li> <li> <p>If some model providers you're integrating are not officially supported, you can use the functionality of this module. First, use <code>register_model_provider</code> to register the model provider, then use <code>load_chat_model</code> to load the model.</p> </li> <li> <p>If the model provider you're integrating doesn't have a suitable integration yet, but the provider offers an OpenAI-compatible API (such as vLLM), it's recommended to use the functionality of this module. First, use <code>register_model_provider</code> to register the model provider (passing <code>openai-compatible</code> for chat_model), then use <code>load_chat_model</code> to load the model.</p> </li> </ol>"},{"location":"getting-started-guide/embedding/","title":"Embedding Model Management","text":""},{"location":"getting-started-guide/embedding/#overview","title":"Overview","text":"<p>LangChain's <code>init_embeddings</code> function only supports a limited number of embedding model providers. This library provides a more flexible embedding model management solution, particularly suitable for scenarios where you need to integrate embedding services not natively supported (such as vLLM).</p>"},{"location":"getting-started-guide/embedding/#registering-embedding-model-providers","title":"Registering Embedding Model Providers","text":"<p>To register an embedding model provider, call <code>register_embeddings_provider</code>. The registration method varies slightly depending on the type of <code>embeddings_model</code>.</p>"},{"location":"getting-started-guide/embedding/#existing-langchain-embedding-model-class","title":"Existing LangChain Embedding Model Class","text":"<p>If the embedding model provider already has a ready and suitable LangChain integration (see Embedding Model Integration List), directly pass the corresponding embedding model class to the <code>embeddings_model</code> parameter.</p>"},{"location":"getting-started-guide/embedding/#parameter-description","title":"Parameter Description","text":"Parameter Description <code>provider_name</code> Model provider name, used for subsequent reference in <code>load_embeddings</code>.Type: <code>str</code>Required: Yes <code>embeddings_model</code> LangChain embedding model class.Type: <code>type[Embeddings]</code>Required: Yes <code>base_url</code> API base URL, usually no need to set manually.Type: <code>str</code>Required: No"},{"location":"getting-started-guide/embedding/#code-example","title":"Code Example","text":"<pre><code>from langchain_core.embeddings.fake import FakeEmbeddings\nfrom langchain_dev_utils.embeddings import register_embeddings_provider\n\nregister_embeddings_provider(\n    provider_name=\"fake_provider\",\n    embeddings_model=FakeEmbeddings,\n)\n</code></pre>"},{"location":"getting-started-guide/embedding/#usage-instructions","title":"Usage Instructions","text":"<ul> <li><code>FakeEmbeddings</code> is only for testing. In actual use, you must pass an <code>Embeddings</code> class with real functionality.</li> <li><code>provider_name</code> represents the name of the model provider, used for subsequent reference in <code>load_embeddings</code>. The name can be customized, but should not contain special characters such as <code>:</code>, <code>-</code>, etc.</li> <li>The <code>base_url</code> parameter usually doesn't need to be set manually (since the embedding model class has already defined the API address internally). Only when you need to override the default address should you explicitly pass <code>base_url</code>; the override scope is limited to attributes with field names <code>api_base</code> or <code>base_url</code> (including aliases) in the model class.</li> </ul>"},{"location":"getting-started-guide/embedding/#no-langchain-embedding-model-class-but-provider-supports-openai-compatible-api","title":"No LangChain Embedding Model Class, but Provider Supports OpenAI Compatible API","text":"<p>The parameter description for this situation is as follows:</p>"},{"location":"getting-started-guide/embedding/#parameter-description_1","title":"Parameter Description","text":"Parameter Description <code>provider_name</code> Model provider name, used for subsequent reference in <code>load_embeddings</code>.Type: <code>str</code>Required: Yes <code>embeddings_model</code> Fixed value <code>\"openai-compatible\"</code>.Type: <code>str</code>Required: Yes <code>base_url</code> API base URL.Type: <code>str</code>Required: No"},{"location":"getting-started-guide/embedding/#code-example_1","title":"Code Example","text":"<p>Method 1: Explicit Parameters</p> <pre><code>register_embeddings_provider(\n    provider_name=\"vllm\",\n    embeddings_model=\"openai-compatible\",\n    base_url=\"http://localhost:8000/v1\"\n)\n</code></pre> <p>Method 2: Environment Variables (Recommended)</p> <pre><code>export VLLM_API_BASE=http://localhost:8000/v1\n</code></pre> <pre><code>register_embeddings_provider(\n    provider_name=\"vllm\",\n    embeddings_model=\"openai-compatible\"\n    # Automatically reads VLLM_API_BASE\n)\n</code></pre> <p>Note: For more details on this part, please refer to OpenAI Compatible API Integration.</p>"},{"location":"getting-started-guide/embedding/#batch-registration","title":"Batch Registration","text":"<p>If you need to register multiple providers, you can use <code>batch_register_embeddings_provider</code>.</p>"},{"location":"getting-started-guide/embedding/#parameter-description_2","title":"Parameter Description","text":"Parameter Description <code>providers</code> List of provider configurations, each dictionary contains registration parameters.Type: <code>list[dict]</code>Required: Yes"},{"location":"getting-started-guide/embedding/#code-example_2","title":"Code Example","text":"<pre><code>from langchain_dev_utils.embeddings import batch_register_embeddings_provider\nfrom langchain_core.embeddings.fake import FakeEmbeddings\n\nbatch_register_embeddings_provider(\n    providers=[\n        {\n            \"provider_name\": \"fake_provider\",\n            \"embeddings_model\": FakeEmbeddings,\n        },\n        {\n            \"provider_name\": \"vllm\",\n            \"embeddings_model\": \"openai-compatible\",\n            \"base_url\": \"http://localhost:8000/v1\",\n        },\n    ]\n)\n</code></pre> <p>Note</p> <p>Both registration functions are implemented based on a global dictionary. All registrations must be completed during the application startup phase, and dynamic registration during runtime is prohibited to avoid multi-threading issues.</p> <p>Additionally, when registering with <code>embeddings_model</code> set to <code>openai-compatible</code>, the system internally uses <code>pydantic.create_model</code> to dynamically create new model classes (with <code>BaseEmbeddingOpenAICompatible</code> as the base class, generating corresponding embedding model integration classes). This process involves Python metaclass operations and pydantic validation logic initialization, which has certain performance overhead, so please avoid frequent registration during runtime.</p>"},{"location":"getting-started-guide/embedding/#loading-embedding-models","title":"Loading Embedding Models","text":"<p>Use <code>load_embeddings</code> to initialize embedding model instances.</p>"},{"location":"getting-started-guide/embedding/#parameter-description_3","title":"Parameter Description","text":"Parameter Type Required Default Description <code>model</code> <code>str</code> Yes - Model name <code>provider</code> <code>str</code> No <code>None</code> Model provider name <p>In addition, any number of keyword arguments can be passed to provide additional parameters for the embedding model class.</p>"},{"location":"getting-started-guide/embedding/#parameter-rules","title":"Parameter Rules","text":"<ul> <li>If <code>provider</code> is not passed, then <code>model</code> must be in the format <code>provider_name:embeddings_name</code>;</li> <li>If <code>provider</code> is passed, then <code>model</code> is only <code>embeddings_name</code>.</li> </ul>"},{"location":"getting-started-guide/embedding/#code-example_3","title":"Code Example","text":"<pre><code># Method 1: model includes provider information\nembedding = load_embeddings(\"vllm:qwen3-embedding-4b\")\n\n# Method 2: specify provider separately\nembedding = load_embeddings(\"qwen3-embedding-4b\", provider=\"vllm\")\n</code></pre>"},{"location":"getting-started-guide/embedding/#model-methods-and-parameters","title":"Model Methods and Parameters","text":"<p>For supported model methods and parameters, refer to the usage instructions of the corresponding embedding model class. If you're using the second situation, all methods and parameters of the <code>OpenAIEmbeddings</code> class are supported.</p>"},{"location":"getting-started-guide/embedding/#compatibility-with-official-providers","title":"Compatibility with Official Providers","text":"<p>For providers already supported by LangChain (such as <code>openai</code>), you can directly use <code>load_embeddings</code> without registration:</p> <pre><code>model = load_embeddings(\"openai:text-embedding-3-large\")\n# or\nmodel = load_embeddings(\"text-embedding-3-large\", provider=\"openai\")\n</code></pre> <p>Best Practice</p> <p>For the use of this module, you can choose based on the following three situations:</p> <ol> <li> <p>If all embedding model providers you're integrating are supported by the official <code>init_embeddings</code>, please use the official function directly for the best compatibility.</p> </li> <li> <p>If some embedding model providers you're integrating are not officially supported, you can use the registration and loading mechanism of this module. First, use <code>register_embeddings_provider</code> to register the model provider, then use <code>load_embeddings</code> to load the model.</p> </li> <li> <p>If the embedding model provider you're integrating doesn't have a suitable integration yet, but the provider offers an OpenAI-compatible API (such as vLLM), it's recommended to use the functionality of this module. First, use <code>register_embeddings_provider</code> to register the model provider (passing <code>openai-compatible</code> for embeddings_model), then use <code>load_embeddings</code> to load the model.</p> </li> </ol>"},{"location":"getting-started-guide/format/","title":"Formatting Sequence","text":""},{"location":"getting-started-guide/format/#overview","title":"Overview","text":"<p>Used to format a list consisting of Documents, Messages, or strings into a single text string. The specific function is <code>format_sequence</code>.</p>"},{"location":"getting-started-guide/format/#usage-examples","title":"Usage Examples","text":""},{"location":"getting-started-guide/format/#message","title":"Message","text":""},{"location":"getting-started-guide/format/#code-example","title":"Code Example","text":"<pre><code>from langchain_core.documents import Document\nfrom langchain_core.messages import AIMessage\nfrom langchain_dev_utils.message_convert import format_sequence\n\nformated1 = format_sequence(\n    [\n        AIMessage(content=\"Hello1\"),\n        AIMessage(content=\"Hello2\"),\n        AIMessage(content=\"Hello3\"),\n    ]\n)\nprint(formated1)\n</code></pre>"},{"location":"getting-started-guide/format/#output-result","title":"Output Result","text":"<pre><code>-Hello1\n-Hello2\n-Hello3\n</code></pre>"},{"location":"getting-started-guide/format/#document","title":"Document","text":""},{"location":"getting-started-guide/format/#code-example_1","title":"Code Example","text":"<pre><code>format2 = format_sequence(\n    [\n        Document(page_content=\"content1\"),\n        Document(page_content=\"content2\"),\n        Document(page_content=\"content3\"),\n    ],\n    separator=\"&gt;\",\n)\nprint(format2)\n</code></pre>"},{"location":"getting-started-guide/format/#output-result_1","title":"Output Result","text":"<pre><code>&gt;content1\n&gt;content2\n&gt;content3\n</code></pre>"},{"location":"getting-started-guide/format/#string","title":"String","text":""},{"location":"getting-started-guide/format/#code-example_2","title":"Code Example","text":"<pre><code>format3 = format_sequence(\n    [\n        \"str1\",\n        \"str2\",\n        \"str3\",\n    ],\n    separator=\"&gt;\",\n    with_num=True,\n)\nprint(format3)\n</code></pre>"},{"location":"getting-started-guide/format/#output-result_2","title":"Output Result","text":"<pre><code>&gt;1. str1\n&gt;2. str2\n&gt;3. str3\n</code></pre>"},{"location":"getting-started-guide/installation/","title":"Installation","text":"<p><code>langchain-dev-utils</code> supports installation via multiple package managers, including <code>pip</code>, <code>poetry</code>, and <code>uv</code>.</p> <p>To install the base version of <code>langchain-dev-utils</code>:</p> pippoetryuv <pre><code>pip install -U langchain-dev-utils\n</code></pre> <pre><code>poetry add langchain-dev-utils\n</code></pre> <pre><code>uv add langchain-dev-utils\n</code></pre> <p>To install the full-featured version of <code>langchain-dev-utils</code>:</p> pippoetryuv <pre><code>pip install -U \"langchain-dev-utils[standard]\"\n</code></pre> <pre><code>poetry add \"langchain-dev-utils[standard]\"\n</code></pre> <pre><code>uv add langchain-dev-utils[standard]\n</code></pre>"},{"location":"getting-started-guide/installation/#verifying-the-installation","title":"Verifying the Installation","text":"<p>After installation, verify that the package is correctly installed:</p> <pre><code>import langchain_dev_utils\nprint(langchain_dev_utils.__version__)\n</code></pre>"},{"location":"getting-started-guide/installation/#dependencies","title":"Dependencies","text":"<p>The package automatically installs the following dependencies:</p> <ul> <li><code>langchain</code></li> <li><code>langgraph</code> (installed alongside <code>langchain</code>)</li> </ul> <p>If you install the <code>standard</code> version, the following additional dependencies will also be installed:</p> <ul> <li><code>langchain-openai</code> (for model management)</li> <li><code>json-repair</code> (for fixing tool-call errors in middleware)</li> </ul>"},{"location":"getting-started-guide/message/","title":"Message Processing","text":""},{"location":"getting-started-guide/message/#overview","title":"Overview","text":"<p>Main features include:</p> <ul> <li>Merge reasoning content into final responses</li> <li>Merge streaming output Chunks</li> </ul>"},{"location":"getting-started-guide/message/#merge-reasoning-content-into-final-response","title":"Merge Reasoning Content into Final Response","text":"<p>Used to merge reasoning content (<code>reasoning_content</code>) into the final response (<code>content</code>).</p>"},{"location":"getting-started-guide/message/#function-description","title":"Function Description","text":"Function Description <code>convert_reasoning_content_for_ai_message</code> Merge reasoning content in AIMessage into the content field (for model's invoke and ainvoke) <code>convert_reasoning_content_for_chunk_iterator</code> Merge reasoning content in streaming responses into the content field (for model's stream) <code>aconvert_reasoning_content_for_chunk_iterator</code> Async version of <code>convert_reasoning_content_for_chunk_iterator</code>, for async streaming processing (for model's astream)"},{"location":"getting-started-guide/message/#code-example","title":"Code Example","text":"<pre><code>from langchain_dev_utils.message_convert import (\n    convert_reasoning_content_for_ai_message,\n    convert_reasoning_content_for_chunk_iterator,\n)\n\nresponse = model.invoke(\"\u4f60\u597d\")\nconverted_response = convert_reasoning_content_for_ai_message(\n    response, think_tag=(\"&lt;start&gt;\", \"&lt;end&gt;\")\n)\nprint(converted_response.content)\n\nfor chunk in convert_reasoning_content_for_chunk_iterator(\n    model.stream(\"\u4f60\u597d\"), think_tag=(\"&lt;start&gt;\", \"&lt;end&gt;\")\n):\n    print(chunk.content, end=\"\", flush=True)\n</code></pre>"},{"location":"getting-started-guide/message/#merge-streaming-output-chunks","title":"Merge Streaming Output Chunks","text":"<p>Provides utility functions to merge multiple AIMessageChunks generated due to streaming output into a single AIMessage.</p>"},{"location":"getting-started-guide/message/#core-functions","title":"Core Functions","text":"Function Description <code>merge_ai_message_chunk</code> Merge AI message chunks"},{"location":"getting-started-guide/message/#code-example_1","title":"Code Example","text":"<pre><code>from langchain_dev_utils.message_convert import merge_ai_message_chunk\n\nchunks = []\nfor chunk in model.stream(\"\u4f60\u597d\"):\n    chunks.append(chunk)\n\nmerged_message = merge_ai_message_chunk(chunks)\nprint(merged_message)\n</code></pre>"},{"location":"getting-started-guide/tool/","title":"Tool Call Processing","text":""},{"location":"getting-started-guide/tool/#overview","title":"Overview","text":"<p>Provides utilities for detecting and parsing tool call arguments.</p>"},{"location":"getting-started-guide/tool/#detect-tool-calls","title":"Detect Tool Calls","text":"<p>Detects whether a message contains a tool call. The core function is <code>has_tool_calling</code>.</p>"},{"location":"getting-started-guide/tool/#code-example","title":"Code Example","text":"<pre><code>import datetime\nfrom langchain_core.tools import tool\nfrom langchain_dev_utils.tool_calling import has_tool_calling\n\n@tool\ndef get_current_time() -&gt; str:\n    \"\"\"Get the current timestamp\"\"\"\n    return str(datetime.datetime.now().timestamp())\n\nresponse = model.bind_tools([get_current_time]).invoke(\"What time is it now?\")\nprint(has_tool_calling(response))\n</code></pre>"},{"location":"getting-started-guide/tool/#parse-tool-call-arguments","title":"Parse Tool Call Arguments","text":"<p>Provides a utility function to parse tool call arguments, extracting parameter information from a message. The core function is <code>parse_tool_calling</code>.</p>"},{"location":"getting-started-guide/tool/#code-example_1","title":"Code Example","text":"<pre><code>import datetime\nfrom langchain_core.tools import tool\nfrom langchain_dev_utils.tool_calling import has_tool_calling, parse_tool_calling\n\n@tool\ndef get_current_time() -&gt; str:\n    \"\"\"Get the current timestamp\"\"\"\n    return str(datetime.datetime.now().timestamp())\n\nresponse = model.bind_tools([get_current_time]).invoke(\"What time is it now?\")\n\nif has_tool_calling(response):\n    name, args = parse_tool_calling(\n        response, first_tool_call_only=True\n    )\n    print(name, args)\n</code></pre>"},{"location":"zh/","title":"\ud83e\udd9c\ufe0f\ud83e\uddf0 langchain-dev-utils","text":"<p> \ud83d\ude80 \u4e13\u4e3a LangChain \u548c LangGraph \u5f00\u53d1\u8005\u6253\u9020\u7684\u9ad8\u6548\u5de5\u5177\u5e93 </p> <p> </p>"},{"location":"zh/#langchain-dev-utils_1","title":"\u4e3a\u4ec0\u4e48\u9009\u62e9 langchain-dev-utils\uff1f","text":"<p>\u538c\u5026\u4e86\u5728 LangChain \u5f00\u53d1\u4e2d\u7f16\u5199\u91cd\u590d\u4ee3\u7801\uff1f<code>langchain-dev-utils</code> \u6b63\u662f\u60a8\u9700\u8981\u7684\u89e3\u51b3\u65b9\u6848\uff01\u8fd9\u4e2a\u8f7b\u91cf\u4f46\u529f\u80fd\u5f3a\u5927\u7684\u5de5\u5177\u5e93\u4e13\u4e3a\u63d0\u5347 LangChain \u548c LangGraph \u5f00\u53d1\u4f53\u9a8c\u800c\u8bbe\u8ba1\uff0c\u5e2e\u52a9\u60a8\uff1a</p> <ul> <li>\u63d0\u5347\u5f00\u53d1\u6548\u7387 - \u51cf\u5c11\u6837\u677f\u4ee3\u7801\uff0c\u8ba9\u60a8\u4e13\u6ce8\u4e8e\u6838\u5fc3\u529f\u80fd</li> <li>\u7b80\u5316\u590d\u6742\u6d41\u7a0b - \u8f7b\u677e\u7ba1\u7406\u591a\u6a21\u578b\u3001\u591a\u5de5\u5177\u548c\u591a\u667a\u80fd\u4f53\u5e94\u7528</li> <li>\u589e\u5f3a\u4ee3\u7801\u8d28\u91cf - \u63d0\u9ad8\u4e00\u81f4\u6027\u548c\u53ef\u8bfb\u6027\uff0c\u51cf\u5c11\u7ef4\u62a4\u6210\u672c</li> <li>\u52a0\u901f\u539f\u578b\u5f00\u53d1 - \u5feb\u901f\u5b9e\u73b0\u60f3\u6cd5\uff0c\u66f4\u5feb\u8fed\u4ee3\u9a8c\u8bc1</li> </ul>"},{"location":"zh/#_1","title":"\u6838\u5fc3\u529f\u80fd","text":"<ul> <li> <p> \u7edf\u4e00\u7684\u6a21\u578b\u7ba1\u7406</p> <p>\u901a\u8fc7\u5b57\u7b26\u4e32\u6307\u5b9a\u6a21\u578b\u63d0\u4f9b\u5546\uff0c\u8f7b\u677e\u5207\u6362\u548c\u7ec4\u5408\u4e0d\u540c\u6a21\u578b</p> </li> <li> <p> \u7075\u6d3b\u7684\u6d88\u606f\u5904\u7406</p> <p>\u652f\u6301\u601d\u7ef4\u94fe\u62fc\u63a5\u3001\u6d41\u5f0f\u5904\u7406\u548c\u6d88\u606f\u683c\u5f0f\u5316</p> </li> <li> <p> \u5f3a\u5927\u7684\u5de5\u5177\u8c03\u7528</p> <p>\u5185\u7f6e\u5de5\u5177\u8c03\u7528\u68c0\u6d4b\u3001\u53c2\u6570\u89e3\u6790\u548c\u4eba\u5de5\u5ba1\u6838\u529f\u80fd</p> </li> <li> <p> \u9ad8\u6548\u7684 Agent \u5f00\u53d1</p> <p>\u7b80\u5316\u667a\u80fd\u4f53\u521b\u5efa\u6d41\u7a0b\uff0c\u6269\u5145\u66f4\u591a\u7684\u5e38\u7528\u4e2d\u95f4\u4ef6</p> </li> <li> <p> \u7075\u6d3b\u7684\u72b6\u6001\u56fe\u7ec4\u5408</p> <p>\u652f\u6301\u4e32\u884c\u548c\u5e76\u884c\u65b9\u5f0f\u7ec4\u5408\u591a\u4e2a StateGraph</p> </li> </ul>"},{"location":"zh/#_2","title":"\u5feb\u901f\u5f00\u59cb","text":"<p>1. \u5b89\u88c5 <code>langchain-dev-utils</code></p> <pre><code>pip install -U \"langchain-dev-utils[standard]\"\n</code></pre> <p>2. \u5f00\u59cb\u4f7f\u7528</p> <pre><code>from langchain.tools import tool\nfrom langchain_core.messages import HumanMessage\nfrom langchain_dev_utils.chat_models import register_model_provider, load_chat_model\nfrom langchain_dev_utils.agents import create_agent\n\n# \u6ce8\u518c\u6a21\u578b\u63d0\u4f9b\u5546\nregister_model_provider(\"vllm\", \"openai-compatible\", base_url=\"http://localhost:8000/v1\")\n\n@tool\ndef get_current_weather(location: str) -&gt; str:\n    \"\"\"\u83b7\u53d6\u6307\u5b9a\u5730\u70b9\u7684\u5f53\u524d\u5929\u6c14\"\"\"\n    return f\"25\u5ea6\uff0c{location}\"\n\n# \u4f7f\u7528\u5b57\u7b26\u4e32\u52a8\u6001\u52a0\u8f7d\u6a21\u578b\nmodel = load_chat_model(\"vllm:qwen3-4b\")\nresponse = model.invoke(\"\u4f60\u597d\")\nprint(response)\n\n# \u521b\u5efa\u667a\u80fd\u4f53\nagent = create_agent(\"vllm:qwen3-4b\", tools=[get_current_weather])\nresponse = agent.invoke({\"messages\": [HumanMessage(content=\"\u4eca\u5929\u7ebd\u7ea6\u7684\u5929\u6c14\u5982\u4f55\uff1f\")]})\nprint(response)\n</code></pre>"},{"location":"zh/#github","title":"GitHub \u4ed3\u5e93","text":"<p>\u8bbf\u95ee GitHub \u4ed3\u5e93 \u67e5\u770b\u6e90\u4ee3\u7801\u548c\u95ee\u9898\u3002</p>"},{"location":"zh/example-project/","title":"Langchain-dev-utils Example Project","text":"<p>\u8be5\u4ed3\u5e93\u63d0\u4f9b\u4e86\u4e00\u4e2a\u793a\u4f8b\u9879\u76ee<code>langchain-dev-utils-example</code>\uff0c\u76ee\u7684\u662f\u4e3a\u4e86\u5e2e\u52a9\u5f00\u53d1\u8005\u5feb\u901f\u4e86\u89e3\u5982\u4f55\u5229\u7528 <code>langchain-dev-utils</code> \u63d0\u4f9b\u7684\u5de5\u5177\u51fd\u6570\uff0c\u9ad8\u6548\u6784\u5efa\u4e24\u79cd\u5178\u578b\u7684\u667a\u80fd\u4f53\uff08agent\uff09\u7cfb\u7edf\uff1a</p> <ul> <li>\u5355\u667a\u80fd\u4f53\uff08Single Agent\uff09\uff1a\u9002\u7528\u4e8e\u6267\u884c\u7b80\u5355\u4efb\u52a1\u4ee5\u53ca\u957f\u671f\u8bb0\u5fc6\u5b58\u50a8\u76f8\u5173\u7684\u4efb\u52a1\u3002</li> <li>\u76d1\u7763\u8005-\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff08Supervisor-Multi-Agent Architecture\uff09\uff1a\u901a\u8fc7\u4e00\u4e2a\u4e2d\u592e\u76d1\u7763\u8005\u534f\u8c03\u591a\u4e2a\u4e13\u4e1a\u5316\u667a\u80fd\u4f53\uff0c\u9002\u7528\u4e8e\u9700\u8981\u4efb\u52a1\u5206\u89e3\u3001\u89c4\u5212\u548c\u8fed\u4ee3\u4f18\u5316\u7684\u590d\u6742\u573a\u666f\u3002</li> </ul> <p> </p>"},{"location":"zh/example-project/#_1","title":"\u5feb\u901f\u5f00\u59cb","text":"<ol> <li>\u514b\u9686\u672c\u4ed3\u5e93\uff1a <pre><code>git clone https://github.com/TBice123123/langchain-dev-utils-example.git  \ncd langchain-dev-utils-example\n</code></pre></li> <li>\u4f7f\u7528 uv \u5b89\u88c5\u4f9d\u8d56\uff1a <pre><code>uv sync\n</code></pre></li> <li>\u521b\u5efa.env\u6587\u4ef6 <pre><code>cp .env.example .env\n</code></pre></li> <li> <p>\u7f16\u8f91 <code>.env</code> \u6587\u4ef6\uff0c\u586b\u5165\u4f60\u7684 API \u5bc6\u94a5\uff08\u9700\u8981 <code>ZhipuAI</code> \u548c <code>Tavily</code> \u7684 API \u5bc6\u94a5\uff09\u3002</p> </li> <li> <p>\u542f\u52a8\u9879\u76ee <pre><code>langgraph dev\n</code></pre></p> </li> </ol>"},{"location":"zh/example-project/#_2","title":"\u4f7f\u7528\u7684\u529f\u80fd","text":"<p>\u5355\u667a\u80fd\u4f53\uff08Simple Agent\uff09\uff1a</p> <p>\u4f7f\u7528\u7684\u672c\u5e93\u7684\u529f\u80fd\uff1a</p> <ul> <li>\u5bf9\u8bdd\u6a21\u578b\u7ba1\u7406\uff1a<code>register_model_provider</code>\u3001<code>load_chat_model</code></li> <li>\u5d4c\u5165\u6a21\u578b\u7ba1\u7406\uff1a<code>register_embeddings_provider</code>\u3001<code>load_embeddings</code></li> <li>\u683c\u5f0f\u5316\u5e8f\u5217\uff1a<code>format_sequence</code></li> <li>\u4e2d\u95f4\u4ef6\uff1a<code>format_prompt</code></li> </ul> <p>\u76d1\u7763\u8005-\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff08Supervisor-Multi-Agent Architecture\uff09\uff1a</p> <p>\u4f7f\u7528\u7684\u672c\u5e93\u7684\u529f\u80fd\uff1a</p> <ul> <li>\u5bf9\u8bdd\u6a21\u578b\u7ba1\u7406\uff1a<code>register_model_provider</code>\u3001<code>load_chat_model</code></li> <li>\u591a\u667a\u80fd\u4f53\u6784\u5efa\uff1a<code>wrap_agent_as_tool</code></li> </ul>"},{"location":"zh/example-project/#_3","title":"\u5982\u4f55\u81ea\u5b9a\u4e49","text":"<p>\u53ef\u4ee5\u6839\u636e\u5b9e\u9645\u7684\u9700\u6c42\uff0c\u5bf9\u672c\u9879\u76ee\u8fdb\u884c\u81ea\u5b9a\u4e49\u4fee\u6539\u3002</p>"},{"location":"zh/example-project/#1","title":"1. \u66ff\u6362\u5bf9\u8bdd\u6a21\u578b\u63d0\u4f9b\u5546","text":"<p>\u672c\u9879\u76ee\u9ed8\u8ba4\u4f7f\u7528\u667a\u8c31AI\u7684GLM\u7cfb\u5217\u4f5c\u4e3a\u6838\u5fc3\u6a21\u578b\uff0c\u5177\u4f53\u5982\u4e0b\uff1a</p> <ul> <li><code>GLM-4.7</code>\uff1a\u7528\u4e8e<code>simple-agent</code></li> <li><code>GLM-4.6</code>\uff1a\u7528\u4e8e<code>supervisor-agent</code>\u7684<code>supervisor</code></li> <li><code>GLM-4.5</code>\uff1a\u7528\u4e8e<code>supervisor-agent</code>\u7684<code>subagent</code></li> </ul> <p>\u5982\u679c\u4f60\u60f3\u81ea\u5b9a\u4e49\u4f60\u7684\u6a21\u578b\u63d0\u4f9b\u5546\uff0c\u9700\u8981\u4fee\u6539<code>src/utils/providers/chat_models/register.py</code>\u4e2d\u7684\u5185\u5bb9\uff0c\u5728<code>register_all_model_providers</code>\u51fd\u6570\u4e2d\u4f7f\u7528<code>register_model_provider</code>\u51fd\u6570\u6ce8\u518c\u4f60\u7684\u6a21\u578b\u63d0\u4f9b\u5546\u3002 \u540c\u65f6\u4e5f\u5efa\u8bae\u4fee\u6539<code>src/utils/providers/chat_models/load.py</code>\u4e2d\u7684\u5185\u5bb9\uff0c\u5728<code>load_chat_model</code>\u51fd\u6570\u4e2d\u6dfb\u52a0\u4f60\u7684\u6a21\u578b\u63d0\u4f9b\u5546\u7684\u52a0\u8f7d\u903b\u8f91\u3002</p> <p>\u5bf9\u8bdd\u6a21\u578b\u7ba1\u7406\u6700\u4f73\u5b9e\u8df5</p> <p>\u5bf9\u4e8e\u4e0d\u540c\u5bf9\u8bdd\u6a21\u578b\u7c7b\u7684\u989d\u5916\u53c2\u6570\uff0c<code>load_chat_model</code>\u51fd\u6570\u91c7\u7528\u5173\u952e\u5b57\u53c2\u6570\u65b9\u5f0f\u8fdb\u884c\u63a5\u6536\uff08LangChain\u5bf9\u5e94\u7684\u51fd\u6570\u4e5f\u91c7\u7528\u6b64\u65b9\u5f0f\uff09\u3002\u867d\u7136\u6b64\u65b9\u5f0f\u63d0\u5347\u4e86\u901a\u7528\u6027\uff0c\u4f46\u4f1a\u524a\u5f31IDE\u7c7b\u578b\u63d0\u793a\uff0c\u589e\u52a0\u53c2\u6570\u8bef\u7528\u98ce\u9669\u3002\u56e0\u6b64\uff0c\u82e5\u5df2\u786e\u5b9a\u5177\u4f53\u63d0\u4f9b\u5546\uff0c\u53ef\u4ee5\u9488\u5bf9\u5176\u96c6\u6210\u5bf9\u8bdd\u6a21\u578b\u7c7b\uff08\u6216\u5d4c\u5165\u6a21\u578b\u7c7b\uff09\u6269\u5c55\u53c2\u6570\u7b7e\u540d\u4ee5\u6062\u590d\u7c7b\u578b\u63d0\u793a\uff0c\u53ef\u4ee5\u53c2\u8003<code>src\\utils\\providers\\chat_models\\load.py</code>\u4e2d\u5185\u5bb9\u8fdb\u884c\u9488\u5bf9\u6027\u4fee\u6539\u3002</p>"},{"location":"zh/example-project/#2","title":"2. \u6ce8\u518c\u5d4c\u5165\u6a21\u578b\u63d0\u4f9b\u5546","text":"<p>\u4e0e\u5bf9\u8bdd\u6a21\u578b\u63d0\u4f9b\u5546\u7c7b\u4f3c\uff0c\u4f60\u4e5f\u53ef\u4ee5\u6839\u636e\u9700\u8981\u6ce8\u518c\u81ea\u5b9a\u4e49\u7684\u5d4c\u5165\u6a21\u578b\u63d0\u4f9b\u5546\u3002\u9700\u8981\u4fee\u6539<code>src/utils/providers/embeddings/register.py</code>\u4e2d\u7684\u5185\u5bb9\uff0c\u5728<code>register_all_embeddings_providers</code>\u51fd\u6570\u4e2d\u4f7f\u7528<code>register_embeddings_provider</code>\u51fd\u6570\u6ce8\u518c\u4f60\u7684\u5d4c\u5165\u6a21\u578b\u63d0\u4f9b\u5546\u3002</p> <p>\u5982\u679c\u9700\u8981\uff0c\u4e5f\u53ef\u4ee5\u6839\u636e\u5b9e\u9645\u60c5\u51b5\uff0c\u4fee\u6539<code>src/utils/providers/embeddings/load.py</code>\u4e2d\u7684\u5185\u5bb9\uff0c\u5728<code>load_embeddings</code>\u51fd\u6570\u4e2d\u6dfb\u52a0\u4f60\u7684\u5d4c\u5165\u6a21\u578b\u63d0\u4f9b\u5546\u7684\u52a0\u8f7d\u903b\u8f91\u3002</p>"},{"location":"zh/example-project/#3","title":"3. \u81ea\u5b9a\u4e49\u5de5\u5177","text":"<p>\u5355\u667a\u80fd\u4f53\uff08simple-agent\uff09 \u5de5\u5177\u5b9e\u73b0\u4f4d\u4e8e <code>src/agents/simple_agent/tools.py</code>\uff0c\u5df2\u5185\u7f6e\uff1a - <code>save_user_memory</code> \u2014\u2014 \u6301\u4e45\u5316\u7528\u6237\u8bb0\u5fc6 - <code>get_user_memory</code> \u2014\u2014 \u8bfb\u53d6\u7528\u6237\u8bb0\u5fc6  </p> <p>\u5982\u9700\u6269\u5c55\uff0c\u76f4\u63a5\u5728\u8be5\u6587\u4ef6\u5185\u65b0\u589e\u5bf9\u5e94\u7684\u5de5\u5177\u5b9e\u73b0\u5373\u53ef\u3002</p> <p>\u76d1\u7763\u8005-\u591a\u667a\u80fd\u4f53\uff08supervisor-agent\uff09 \u5de5\u5177\u5b9e\u73b0\u4f4d\u4e8e <code>src/agents/supervisor/subagent/tools.py</code>\u3002\u662f\u5b50\u667a\u80fd\u4f53\u7684\u5de5\u5177\u5b9e\u73b0\uff0c\u5982\u9700\u4e3a\u5b50\u667a\u80fd\u4f53\u6dfb\u52a0\u81ea\u5b9a\u4e49\u5de5\u5177\uff0c\u76f4\u63a5\u5728\u8be5\u6587\u4ef6\u5185\u65b0\u589e\u5bf9\u5e94\u7684\u5de5\u5177\u5b9e\u73b0\u5373\u53ef\u3002</p> <p>\u6ce8\u610f\uff1a<code>supervisor</code> \u9ed8\u8ba4\u4ec5\u6301\u6709\u201c\u8c03\u7528\u5b50\u667a\u80fd\u4f53\u201d\u7684\u4e24\u4e2a\u5de5\u5177\u3002\u82e5\u9700\u4e3a <code>supervisor</code> \u8ffd\u52a0\u81ea\u5b9a\u4e49\u5de5\u5177\uff0c\u5efa\u8bae\u5728 <code>src/agents/supervisor/</code> \u4e0b\u65b0\u5efa <code>tools.py</code>\uff0c\u7f16\u5199\u5b8c\u6210\u540e\u5728 <code>src/agents/supervisor/agent.py</code> \u4e2d\u5bfc\u5165\u5e76\u4f20\u9012\u7ed9 <code>create_agent</code> \u51fd\u6570\u5373\u53ef\u3002</p>"},{"location":"zh/adavance-guide/human-in-the-loop/","title":"\u4e3a\u5de5\u5177\u8c03\u7528\u6dfb\u52a0\u4eba\u5de5\u5ba1\u6838","text":""},{"location":"zh/adavance-guide/human-in-the-loop/#_2","title":"\u6982\u8ff0","text":"<p>\u672c\u5e93\u63d0\u4f9b\u4e86\u88c5\u9970\u5668\u51fd\u6570\uff0c\u7528\u4e8e\u4e3a\u5de5\u5177\u8c03\u7528\u6dfb\u52a0\"\u4eba\u5728\u56de\u8def\"\u5ba1\u6838\u652f\u6301\uff0c\u5728\u5de5\u5177\u6267\u884c\u671f\u95f4\u542f\u7528\u4eba\u5de5\u5ba1\u6838\u3002</p> \u88c5\u9970\u5668 \u9002\u7528\u573a\u666f <code>human_in_the_loop</code> \u7528\u4e8e\u540c\u6b65\u5de5\u5177\u51fd\u6570 <code>human_in_the_loop_async</code> \u7528\u4e8e\u5f02\u6b65\u5de5\u5177\u51fd\u6570"},{"location":"zh/adavance-guide/human-in-the-loop/#_3","title":"\u53c2\u6570\u8bf4\u660e","text":"\u53c2\u6570 \u8bf4\u660e <code>handler</code> \u81ea\u5b9a\u4e49\u5904\u7406\u51fd\u6570\uff0c\u82e5\u4e3a <code>None</code> \u5219\u4f7f\u7528\u9ed8\u8ba4\u5904\u7406\u51fd\u6570\u3002\u7c7b\u578b: <code>Callable</code>\u5fc5\u586b: \u5426"},{"location":"zh/adavance-guide/human-in-the-loop/#_4","title":"\u4f7f\u7528\u793a\u4f8b","text":""},{"location":"zh/adavance-guide/human-in-the-loop/#handler","title":"\u4f7f\u7528\u9ed8\u8ba4\u7684 handler","text":"<pre><code>from langchain_dev_utils.tool_calling import human_in_the_loop\nimport datetime\n\n\n@human_in_the_loop\ndef get_current_time() -&gt; str:\n    \"\"\"\u83b7\u53d6\u5f53\u524d\u65f6\u95f4\u6233\"\"\"\n    return str(datetime.datetime.now().timestamp())\n</code></pre>"},{"location":"zh/adavance-guide/human-in-the-loop/#_5","title":"\u5f02\u6b65\u5de5\u5177\u793a\u4f8b","text":"<pre><code>from langchain_dev_utils.tool_calling import human_in_the_loop_async\nimport asyncio\nimport datetime\n\n\n@human_in_the_loop_async\nasync def async_get_current_time() -&gt; str:\n    \"\"\"\u5f02\u6b65\u83b7\u53d6\u5f53\u524d\u65f6\u95f4\u6233\"\"\"\n    await asyncio.sleep(1)\n    return str(datetime.datetime.now().timestamp())\n</code></pre>"},{"location":"zh/adavance-guide/human-in-the-loop/#handler_1","title":"\u9ed8\u8ba4 handler \u7684\u5b9e\u73b0","text":"<p>\u9ed8\u8ba4 handler \u7684\u5b9e\u73b0\u5982\u4e0b\uff1a</p> <pre><code>def _get_human_in_the_loop_request(params: InterruptParams) -&gt; dict[str, Any]:\n    return {\n        \"action_request\": {\n            \"action\": params[\"tool_call_name\"],\n            \"args\": params[\"tool_call_args\"],\n        },\n        \"config\": {\n            \"allow_accept\": True,\n            \"allow_edit\": True,\n            \"allow_respond\": True,\n        },\n        \"description\": f\"Please review tool call: {params['tool_call_name']}\",\n    }\n\n\ndef default_handler(params: InterruptParams) -&gt; Any:\n    request = _get_human_in_the_loop_request(params)\n    response = interrupt(request)\n\n    if response[\"type\"] == \"accept\":\n        return params[\"tool\"].invoke(params[\"tool_call_args\"])\n    elif response[\"type\"] == \"edit\":\n        updated_args = response[\"args\"]\n        return params[\"tool\"].invoke(updated_args)\n    elif response[\"type\"] == \"response\":\n        return response[\"args\"]\n    else:\n        raise ValueError(f\"Unsupported interrupt response type: {response['type']}\")\n</code></pre>"},{"location":"zh/adavance-guide/human-in-the-loop/#_6","title":"\u4e2d\u65ad\u8bf7\u6c42\u683c\u5f0f","text":"<p>\u4e2d\u65ad\u65f6\u4f1a\u53d1\u9001\u5982\u4e0b JSON Schema \u683c\u5f0f\u7684\u8bf7\u6c42\uff1a</p> \u5b57\u6bb5 \u8bf4\u660e <code>action_request.action</code> \u5de5\u5177\u8c03\u7528\u540d\u79f0\u3002\u7c7b\u578b: <code>str</code> <code>action_request.args</code> \u5de5\u5177\u8c03\u7528\u53c2\u6570\u3002\u7c7b\u578b: <code>dict</code> <code>config.allow_accept</code> \u662f\u5426\u5141\u8bb8\u63a5\u53d7\u64cd\u4f5c\u3002\u7c7b\u578b: <code>bool</code> <code>config.allow_edit</code> \u662f\u5426\u5141\u8bb8\u7f16\u8f91\u53c2\u6570\u3002\u7c7b\u578b: <code>bool</code> <code>config.allow_respond</code> \u662f\u5426\u5141\u8bb8\u76f4\u63a5\u54cd\u5e94\u3002\u7c7b\u578b: <code>bool</code> <code>description</code> \u64cd\u4f5c\u63cf\u8ff0\u3002\u7c7b\u578b: <code>str</code>"},{"location":"zh/adavance-guide/human-in-the-loop/#_7","title":"\u4e2d\u65ad\u54cd\u5e94\u683c\u5f0f","text":"<p>\u54cd\u5e94\u65f6\u9700\u8981\u8fd4\u56de\u5982\u4e0b JSON Schema \u683c\u5f0f\u7684\u6570\u636e\uff1a</p> \u5b57\u6bb5 \u8bf4\u660e <code>type</code> \u54cd\u5e94\u7c7b\u578b\uff0c\u53ef\u9009\u503c\u4e3a <code>accept</code>\u3001<code>edit</code>\u3001<code>response</code>\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u662f <code>args</code> \u5f53 <code>type</code> \u4e3a <code>edit</code> \u6216 <code>response</code> \u65f6\uff0c\u5305\u542b\u66f4\u65b0\u540e\u7684\u53c2\u6570\u6216\u54cd\u5e94\u5185\u5bb9\u3002\u7c7b\u578b: <code>dict</code>\u5fc5\u586b: \u5426"},{"location":"zh/adavance-guide/human-in-the-loop/#handler_2","title":"\u81ea\u5b9a\u4e49 Handler \u793a\u4f8b","text":"<p>\u4f60\u53ef\u4ee5\u5b8c\u5168\u63a7\u5236\u4e2d\u65ad\u884c\u4e3a\uff0c\u4f8b\u5982\u53ea\u5141\u8bb8\"\u63a5\u53d7/\u62d2\u7edd\"\uff0c\u6216\u81ea\u5b9a\u4e49\u63d0\u793a\u8bed\uff1a</p> <pre><code>from typing import Any\nfrom langchain_dev_utils.tool_calling import human_in_the_loop_async, InterruptParams\nfrom langgraph.types import interrupt\n\n\nasync def custom_handler(params: InterruptParams) -&gt; Any:\n    response = interrupt(\n        f\"\u6211\u8981\u8c03\u7528\u5de5\u5177 {params['tool_call_name']}\uff0c\u53c2\u6570\u4e3a {params['tool_call_args']}\uff0c\u8bf7\u786e\u8ba4\u662f\u5426\u8c03\u7528\"\n    )\n    if response[\"type\"] == \"accept\":\n        return await params[\"tool\"].ainvoke(params[\"tool_call_args\"])\n    elif response[\"type\"] == \"reject\":\n        return \"\u7528\u6237\u62d2\u7edd\u8c03\u7528\u8be5\u5de5\u5177\"\n    else:\n        raise ValueError(f\"\u4e0d\u652f\u6301\u7684\u54cd\u5e94\u7c7b\u578b: {response['type']}\")\n\n\n@human_in_the_loop_async(handler=custom_handler)\nasync def get_weather(city: str) -&gt; str:\n    \"\"\"\u83b7\u53d6\u5929\u6c14\u4fe1\u606f\"\"\"\n    return f\"{city}\u5929\u6c14\u6674\u6717\"\n</code></pre> <p>\u6700\u4f73\u5b9e\u8df5</p> <p>\u8be5\u88c5\u9970\u5668\u5728\u5b9e\u73b0\u81ea\u5b9a\u4e49\u4eba\u5728\u56de\u8def\u903b\u8f91\u65f6\uff0c\u9700\u8981\u4f20\u5165 <code>handler</code> \u53c2\u6570\u3002\u6b64 <code>handler</code> \u53c2\u6570\u662f\u4e00\u4e2a\u51fd\u6570\uff0c\u5185\u90e8\u5fc5\u987b\u4f7f\u7528 LangGraph \u7684 <code>interrupt</code> \u51fd\u6570\u6765\u6267\u884c\u4e2d\u65ad\u64cd\u4f5c\u3002\u56e0\u6b64\uff0c\u5982\u679c\u4ec5\u4e3a\u5355\u4e2a\u5de5\u5177\u6dfb\u52a0\u81ea\u5b9a\u4e49\u7684\u4eba\u5728\u56de\u8def\u903b\u8f91\uff0c\u5efa\u8bae\u76f4\u63a5\u4f7f\u7528 LangGraph \u7684 <code>interrupt</code> \u51fd\u6570\u3002\u5f53\u591a\u4e2a\u5de5\u5177\u9700\u8981\u76f8\u540c\u81ea\u5b9a\u4e49\u4eba\u5728\u56de\u8def\u903b\u8f91\u65f6\uff0c\u4f7f\u7528\u672c\u88c5\u9970\u5668\u53ef\u4ee5\u6709\u6548\u907f\u514d\u4ee3\u7801\u91cd\u590d\u3002</p>"},{"location":"zh/adavance-guide/middleware/","title":"\u4e2d\u95f4\u4ef6","text":""},{"location":"zh/adavance-guide/middleware/#_2","title":"\u6982\u8ff0","text":"<p>\u4e2d\u95f4\u4ef6\u662f\u4e13\u95e8\u9488\u5bf9 LangChain \u9884\u6784\u5efa\u7684 Agent \u800c\u6784\u5efa\u7684\u7ec4\u4ef6\u3002\u5b98\u65b9\u63d0\u4f9b\u4e86\u4e00\u4e9b\u5185\u7f6e\u7684\u4e2d\u95f4\u4ef6\uff0c\u672c\u5e93\u5219\u6839\u636e\u5b9e\u9645\u4f7f\u7528\u573a\u666f\u63d0\u4f9b\u4e86\u66f4\u591a\u5b9e\u7528\u7684\u4e2d\u95f4\u4ef6\u3002</p> <p>\u672c\u5e93\u63d0\u4f9b\u7684\u4e2d\u95f4\u4ef6\u5305\u62ec\uff1a</p> <ul> <li><code>PlanMiddleware</code>\uff1a\u4efb\u52a1\u89c4\u5212\uff0c\u5c06\u590d\u6742\u4efb\u52a1\u62c6\u89e3\u4e3a\u6709\u5e8f\u5b50\u4efb\u52a1</li> <li><code>ModelRouterMiddleware</code>\uff1a\u6839\u636e\u8f93\u5165\u5185\u5bb9\u52a8\u6001\u8def\u7531\u5230\u6700\u9002\u914d\u7684\u6a21\u578b</li> <li><code>HandoffAgentMiddleware</code>\uff1a\u5728\u591a\u4e2a\u5b50 Agent \u4e4b\u95f4\u7075\u6d3b\u5207\u6362</li> <li><code>ToolCallRepairMiddleware</code>\uff1a\u81ea\u52a8\u4fee\u590d\u5927\u6a21\u578b\u65e0\u6548\u5de5\u5177\u8c03\u7528</li> <li><code>format_prompt</code>\uff1a\u52a8\u6001\u683c\u5f0f\u5316\u7cfb\u7edf\u63d0\u793a\u8bcd\u4e2d\u7684\u5360\u4f4d\u7b26</li> </ul> <p>\u6b64\u5916\uff0c\u672c\u5e93\u8fd8\u6269\u5145\u4e86\u5b98\u65b9\u4e2d\u95f4\u4ef6\u7684\u529f\u80fd\uff0c\u652f\u6301\u901a\u8fc7\u5b57\u7b26\u4e32\u53c2\u6570\u6307\u5b9a\u6a21\u578b\uff1a</p> <ul> <li>SummarizationMiddleware</li> <li>LLMToolSelectorMiddleware</li> <li>ModelFallbackMiddleware</li> <li>LLMToolEmulator</li> </ul>"},{"location":"zh/adavance-guide/middleware/#_3","title":"\u4efb\u52a1\u89c4\u5212","text":"<p><code>PlanMiddleware</code> \u662f\u4e00\u4e2a\u7528\u4e8e\u5728\u6267\u884c\u590d\u6742\u4efb\u52a1\u524d\u8fdb\u884c\u7ed3\u6784\u5316\u5206\u89e3\u4e0e\u8fc7\u7a0b\u7ba1\u7406\u7684\u4e2d\u95f4\u4ef6\u3002</p> <p>\u8865\u5145\u8bf4\u660e</p> <p>\u4efb\u52a1\u89c4\u5212\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u4e0a\u4e0b\u6587\u5de5\u7a0b\u7ba1\u7406\u7b56\u7565\u3002\u5728\u6267\u884c\u4efb\u52a1\u4e4b\u524d\uff0c\u5927\u6a21\u578b\u9996\u5148\u5c06\u6574\u4f53\u4efb\u52a1\u62c6\u89e3\u4e3a\u591a\u4e2a\u6709\u5e8f\u7684\u5b50\u4efb\u52a1\uff0c\u5f62\u6210\u4efb\u52a1\u89c4\u5212\u5217\u8868\uff08\u5728\u672c\u5e93\u4e2d\u79f0\u4e3a plan\uff09\u3002\u968f\u540e\u6309\u987a\u5e8f\u6267\u884c\u5404\u5b50\u4efb\u52a1\uff0c\u5e76\u5728\u6bcf\u5b8c\u6210\u4e00\u4e2a\u6b65\u9aa4\u540e\u52a8\u6001\u66f4\u65b0\u4efb\u52a1\u72b6\u6001\uff0c\u76f4\u81f3\u6240\u6709\u5b50\u4efb\u52a1\u6267\u884c\u5b8c\u6bd5\u3002</p>"},{"location":"zh/adavance-guide/middleware/#_4","title":"\u53c2\u6570\u8bf4\u660e","text":"\u53c2\u6570 \u8bf4\u660e <code>system_prompt</code> \u7cfb\u7edf\u63d0\u793a\u8bcd\uff0c\u82e5\u4e3a <code>None</code> \u5219\u4f7f\u7528\u9ed8\u8ba4\u63d0\u793a\u8bcd\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u5426 <code>custom_plan_tool_descriptions</code> \u81ea\u5b9a\u4e49\u8ba1\u5212\u76f8\u5173\u5de5\u5177\u7684\u63cf\u8ff0\u3002\u7c7b\u578b: <code>dict</code>\u5fc5\u586b: \u5426 <code>use_read_plan_tool</code> \u662f\u5426\u542f\u7528\u8bfb\u8ba1\u5212\u5de5\u5177\u3002\u7c7b\u578b: <code>bool</code>\u5fc5\u586b: \u5426\u9ed8\u8ba4\u503c: <code>True</code> <p><code>custom_plan_tool_descriptions</code> \u5b57\u5178\u7684\u952e\u53ef\u53d6\u4ee5\u4e0b\u4e09\u4e2a\u503c\uff1a</p> \u952e \u8bf4\u660e <code>write_plan</code> \u5199\u8ba1\u5212\u5de5\u5177\u7684\u63cf\u8ff0 <code>finish_sub_plan</code> \u5b8c\u6210\u5b50\u8ba1\u5212\u5de5\u5177\u7684\u63cf\u8ff0 <code>read_plan</code> \u8bfb\u8ba1\u5212\u5de5\u5177\u7684\u63cf\u8ff0"},{"location":"zh/adavance-guide/middleware/#_5","title":"\u4f7f\u7528\u793a\u4f8b","text":"<pre><code>from langchain_dev_utils.agents.middleware import PlanMiddleware\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    middleware=[\n        PlanMiddleware(\n            custom_plan_tool_descriptions={\n                \"write_plan\": \"\u7528\u4e8e\u5199\u8ba1\u5212\uff0c\u5c06\u4efb\u52a1\u62c6\u89e3\u4e3a\u591a\u4e2a\u6709\u5e8f\u7684\u5b50\u4efb\u52a1\u3002\",\n                \"finish_sub_plan\": \"\u7528\u4e8e\u5b8c\u6210\u5b50\u4efb\u52a1\uff0c\u66f4\u65b0\u5b50\u4efb\u52a1\u72b6\u6001\u4e3a\u5df2\u5b8c\u6210\u3002\",\n                \"read_plan\": \"\u7528\u4e8e\u67e5\u8be2\u5f53\u524d\u7684\u4efb\u52a1\u89c4\u5212\u5217\u8868\u3002\"\n            },\n            use_read_plan_tool=True,  # \u5982\u679c\u4e0d\u4f7f\u7528\u8bfb\u8ba1\u5212\u5de5\u5177\uff0c\u53ef\u4ee5\u8bbe\u7f6e\u6b64\u53c2\u6570\u4e3a False\n        )\n    ],\n)\n\nresponse = agent.invoke(\n    {\"messages\": [HumanMessage(content=\"\u6211\u8981\u53bbNew York\u73a9\u51e0\u5929\uff0c\u5e2e\u6211\u89c4\u5212\u884c\u7a0b\")]}\n)\nprint(response)\n</code></pre>"},{"location":"zh/adavance-guide/middleware/#_6","title":"\u5de5\u5177\u8bf4\u660e","text":"<p><code>PlanMiddleware</code> \u8981\u6c42\u5fc5\u987b\u4f7f\u7528 <code>write_plan</code> \u548c <code>finish_sub_plan</code> \u4e24\u4e2a\u5de5\u5177\uff0c\u800c <code>read_plan</code> \u5de5\u5177\u9ed8\u8ba4\u542f\u7528\uff1b\u82e5\u4e0d\u9700\u8981\u4f7f\u7528\uff0c\u53ef\u5c06 <code>use_read_plan_tool</code> \u53c2\u6570\u8bbe\u4e3a <code>False</code>\u3002</p>"},{"location":"zh/adavance-guide/middleware/#to-do-list","title":"\u4e0e\u5b98\u65b9 To-do list \u4e2d\u95f4\u4ef6\u7684\u5bf9\u6bd4","text":"<p>\u672c\u4e2d\u95f4\u4ef6\u4e0e LangChain \u5b98\u65b9\u63d0\u4f9b\u7684 To-do list \u4e2d\u95f4\u4ef6 \u529f\u80fd\u5b9a\u4f4d\u76f8\u4f3c\uff0c\u4f46\u5728\u5de5\u5177\u8bbe\u8ba1\u4e0a\u5b58\u5728\u5dee\u5f02\uff1a</p> \u7279\u6027 \u5b98\u65b9 To-do list \u4e2d\u95f4\u4ef6 \u672c\u5e93 PlanMiddleware \u5de5\u5177\u6570\u91cf 1 \u4e2a\uff08<code>write_todo</code>\uff09 3 \u4e2a\uff08<code>write_plan</code>\u3001<code>finish_sub_plan</code>\u3001<code>read_plan</code>\uff09 \u529f\u80fd\u5b9a\u4f4d \u9762\u5411\u5f85\u529e\u6e05\u5355\uff08todo list\uff09 \u4e13\u95e8\u7528\u4e8e\u89c4\u5212\u5217\u8868\uff08plan list\uff09 \u64cd\u4f5c\u65b9\u5f0f \u6dfb\u52a0\u548c\u4fee\u6539\u901a\u8fc7\u4e00\u4e2a\u5de5\u5177\u5b8c\u6210 \u5199\u5165\u3001\u4fee\u6539\u3001\u67e5\u8be2\u5206\u522b\u7531\u4e0d\u540c\u5de5\u5177\u5b8c\u6210 <p>\u65e0\u8bba\u662f <code>todo</code> \u8fd8\u662f <code>plan</code>\uff0c\u5176\u672c\u8d28\u90fd\u662f\u540c\u4e00\u4e2a\u6982\u5ff5\u3002\u672c\u4e2d\u95f4\u4ef6\u533a\u522b\u4e8e\u5b98\u65b9\u7684\u5173\u952e\u70b9\u5728\u4e8e\u63d0\u4f9b\u4e86\u4e09\u4e2a\u4e13\u7528\u5de5\u5177\uff1a</p> <ul> <li><code>write_plan</code>\uff1a\u7528\u4e8e\u5199\u5165\u8ba1\u5212\u6216\u66f4\u65b0\u8ba1\u5212\u5185\u5bb9</li> <li><code>finish_sub_plan</code>\uff1a\u7528\u4e8e\u5728\u5b8c\u6210\u67d0\u4e2a\u5b50\u4efb\u52a1\u540e\u66f4\u65b0\u5176\u72b6\u6001</li> <li><code>read_plan</code>\uff1a\u7528\u4e8e\u67e5\u8be2\u8ba1\u5212\u5185\u5bb9</li> </ul>"},{"location":"zh/adavance-guide/middleware/#_7","title":"\u6a21\u578b\u8def\u7531","text":"<p><code>ModelRouterMiddleware</code> \u662f\u4e00\u4e2a\u7528\u4e8e\u6839\u636e\u8f93\u5165\u5185\u5bb9\u52a8\u6001\u8def\u7531\u5230\u6700\u9002\u914d\u6a21\u578b\u7684\u4e2d\u95f4\u4ef6\u3002\u5b83\u901a\u8fc7\u4e00\u4e2a\"\u8def\u7531\u6a21\u578b\"\u5206\u6790\u7528\u6237\u8bf7\u6c42\uff0c\u4ece\u9884\u5b9a\u4e49\u7684\u6a21\u578b\u5217\u8868\u4e2d\u9009\u62e9\u6700\u9002\u5408\u5f53\u524d\u4efb\u52a1\u7684\u6a21\u578b\u8fdb\u884c\u5904\u7406\u3002</p>"},{"location":"zh/adavance-guide/middleware/#_8","title":"\u53c2\u6570\u8bf4\u660e","text":"\u53c2\u6570 \u8bf4\u660e <code>router_model</code> \u7528\u4e8e\u6267\u884c\u8def\u7531\u51b3\u7b56\u7684\u6a21\u578b\u3002\u7c7b\u578b: <code>str</code> | <code>BaseChatModel</code>\u5fc5\u586b: \u662f <code>model_list</code> \u6a21\u578b\u914d\u7f6e\u5217\u8868\u3002\u7c7b\u578b: <code>list[ModelDict]</code>\u5fc5\u586b: \u662f <code>router_prompt</code> \u81ea\u5b9a\u4e49\u8def\u7531\u6a21\u578b\u7684\u63d0\u793a\u8bcd\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u5426"},{"location":"zh/adavance-guide/middleware/#model_list","title":"<code>model_list</code> \u914d\u7f6e\u8bf4\u660e","text":"<p>\u6bcf\u4e2a\u6a21\u578b\u914d\u7f6e\u4e3a\u4e00\u4e2a\u5b57\u5178\uff0c\u5305\u542b\u4ee5\u4e0b\u5b57\u6bb5\uff1a</p> \u5b57\u6bb5 \u8bf4\u660e <code>model_name</code> \u6a21\u578b\u7684\u552f\u4e00\u6807\u8bc6\uff0c\u4f7f\u7528 <code>provider:model-name</code> \u683c\u5f0f\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u662f <code>model_description</code> \u6a21\u578b\u80fd\u529b\u6216\u9002\u7528\u573a\u666f\u7684\u7b80\u8981\u63cf\u8ff0\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u662f <code>tools</code> \u8be5\u6a21\u578b\u53ef\u8c03\u7528\u7684\u5de5\u5177\u767d\u540d\u5355\u3002\u7c7b\u578b: <code>list[BaseTool]</code>\u5fc5\u586b: \u5426 <code>model_kwargs</code> \u6a21\u578b\u52a0\u8f7d\u65f6\u7684\u989d\u5916\u53c2\u6570\u3002\u7c7b\u578b: <code>dict</code>\u5fc5\u586b: \u5426 <code>model_system_prompt</code> \u6a21\u578b\u7684\u7cfb\u7edf\u7ea7\u63d0\u793a\u8bcd\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u5426 <code>model_instance</code> \u5df2\u5b9e\u4f8b\u5316\u7684\u6a21\u578b\u5bf9\u8c61\u3002\u7c7b\u578b: <code>BaseChatModel</code>\u5fc5\u586b: \u5426 <p>model_instance \u5b57\u6bb5\u8bf4\u660e</p> <ul> <li>\u82e5\u63d0\u4f9b\uff1a\u76f4\u63a5\u4f7f\u7528\u8be5\u5b9e\u4f8b\uff0c<code>model_name</code> \u4ec5\u4f5c\u6807\u8bc6\uff0c<code>model_kwargs</code> \u88ab\u5ffd\u7565\uff1b\u9002\u7528\u4e8e\u4e0d\u4f7f\u7528\u672c\u5e93\u7684\u5bf9\u8bdd\u6a21\u578b\u7ba1\u7406\u529f\u80fd\u7684\u60c5\u51b5\u3002</li> <li>\u82e5\u672a\u63d0\u4f9b\uff1a\u6839\u636e <code>model_name</code> \u548c <code>model_kwargs</code> \u4f7f\u7528 <code>load_chat_model</code> \u52a0\u8f7d\u6a21\u578b\u3002</li> <li>\u547d\u540d\u683c\u5f0f\uff1a\u65e0\u8bba\u54ea\u79cd\u60c5\u51b5\uff0c<code>model_name</code> \u7684\u547d\u540d\u90fd\u63a8\u8350\u91c7\u7528 <code>provider:model-name</code> \u683c\u5f0f\u3002</li> </ul>"},{"location":"zh/adavance-guide/middleware/#_9","title":"\u4f7f\u7528\u793a\u4f8b","text":""},{"location":"zh/adavance-guide/middleware/#_10","title":"\u6b65\u9aa4\u4e00\uff1a\u5b9a\u4e49\u6a21\u578b\u5217\u8868","text":"<pre><code>from langchain_dev_utils.agents.middleware.model_router import ModelDict\n\nmodel_list: list[ModelDict] = [\n    {\n        \"model_name\": \"vllm:qwen3-8b\",\n        \"model_description\": \"\u9002\u5408\u666e\u901a\u4efb\u52a1\uff0c\u5982\u5bf9\u8bdd\u3001\u6587\u672c\u751f\u6210\u7b49\",\n        \"model_kwargs\": {\n            \"temperature\": 0.7,\n            \"extra_body\": {\"chat_template_kwargs\": {\"enable_thinking\": False}},\n        },\n        \"model_system_prompt\": \"\u4f60\u662f\u4e00\u4e2a\u52a9\u624b\uff0c\u64c5\u957f\u5904\u7406\u666e\u901a\u4efb\u52a1\uff0c\u5982\u5bf9\u8bdd\u3001\u6587\u672c\u751f\u6210\u7b49\u3002\",\n    },\n    {\n        \"model_name\": \"vllm:qwen3-vl-2b\",\n        \"model_description\": \"\u9002\u5408\u89c6\u89c9\u4efb\u52a1\",\n        \"tools\": [],  # \u5982\u679c\u8be5\u6a21\u578b\u4e0d\u9700\u8981\u4efb\u4f55\u5de5\u5177\uff0c\u8bf7\u5c06\u6b64\u5b57\u6bb5\u8bbe\u7f6e\u4e3a\u7a7a\u5217\u8868 []\n    },\n    {\n        \"model_name\": \"vllm:qwen3-coder-flash\",\n        \"model_description\": \"\u9002\u5408\u4ee3\u7801\u751f\u6210\u4efb\u52a1\",\n        \"tools\": [run_python_code],  # \u4ec5\u5141\u8bb8\u4f7f\u7528 run_python_code \u5de5\u5177\n    },\n    {\n        \"model_name\": \"openai:gpt-4o\",\n        \"model_description\": \"\u9002\u5408\u7efc\u5408\u7c7b\u9ad8\u96be\u5ea6\u4efb\u52a1\",\n        \"model_system_prompt\": \"\u4f60\u662f\u4e00\u4e2a\u52a9\u624b\uff0c\u64c5\u957f\u5904\u7406\u7efc\u5408\u7c7b\u7684\u9ad8\u96be\u5ea6\u4efb\u52a1\",\n        \"model_instance\": ChatOpenAI(\n            model_name=\"gpt-4o\"\n        ),  # \u76f4\u63a5\u4f20\u5165\u5b9e\u4f8b\uff0c\u6b64\u65f6 model_name \u4ec5\u4f5c\u6807\u8bc6\uff0cmodel_kwargs \u88ab\u5ffd\u7565\n    },\n]\n</code></pre>"},{"location":"zh/adavance-guide/middleware/#agent","title":"\u6b65\u9aa4\u4e8c\uff1a\u521b\u5efa Agent \u5e76\u542f\u7528\u4e2d\u95f4\u4ef6","text":"<pre><code>from langchain_dev_utils.agents.middleware import ModelRouterMiddleware\nfrom langchain_core.messages import HumanMessage\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",  # \u6b64\u6a21\u578b\u4ec5\u4f5c\u5360\u4f4d\uff0c\u5b9e\u9645\u7531\u4e2d\u95f4\u4ef6\u52a8\u6001\u66ff\u6362\n    tools=[run_python_code, get_current_time],\n    middleware=[\n        ModelRouterMiddleware(\n            router_model=\"vllm:qwen3-4b\",\n            model_list=model_list,\n        )\n    ],\n)\n\n# \u8def\u7531\u4e2d\u95f4\u4ef6\u4f1a\u6839\u636e\u8f93\u5165\u5185\u5bb9\u81ea\u52a8\u9009\u62e9\u6700\u5408\u9002\u7684\u6a21\u578b\nresponse = agent.invoke({\"messages\": [HumanMessage(content=\"\u5e2e\u6211\u5199\u4e00\u4e2a\u5192\u6ce1\u6392\u5e8f\u4ee3\u7801\")]})\nprint(response)\n</code></pre> <p>\u901a\u8fc7 <code>ModelRouterMiddleware</code>\uff0c\u4f60\u53ef\u4ee5\u8f7b\u677e\u6784\u5efa\u4e00\u4e2a\u591a\u6a21\u578b\u3001\u591a\u80fd\u529b\u7684 Agent\uff0c\u6839\u636e\u4efb\u52a1\u7c7b\u578b\u81ea\u52a8\u9009\u62e9\u6700\u4f18\u6a21\u578b\uff0c\u63d0\u5347\u54cd\u5e94\u8d28\u91cf\u4e0e\u6548\u7387\u3002</p>"},{"location":"zh/adavance-guide/middleware/#_11","title":"\u667a\u80fd\u4f53\u5207\u6362","text":"<p><code>HandoffAgentMiddleware</code> \u662f\u4e00\u4e2a\u7528\u4e8e\u5728\u591a\u4e2a\u5b50 Agent \u4e4b\u95f4\u7075\u6d3b\u5207\u6362\u7684\u4e2d\u95f4\u4ef6\uff0c\u5b8c\u6574\u5b9e\u73b0\u4e86 LangChain \u5b98\u65b9\u7684 <code>handoffs</code> \u591a\u667a\u80fd\u4f53\u534f\u4f5c\u65b9\u6848\u3002</p>"},{"location":"zh/adavance-guide/middleware/#_12","title":"\u53c2\u6570\u8bf4\u660e","text":"\u53c2\u6570 \u8bf4\u660e <code>agents_config</code> \u667a\u80fd\u4f53\u914d\u7f6e\u5b57\u5178\uff0c\u952e\u4e3a\u667a\u80fd\u4f53\u540d\u79f0\uff0c\u503c\u4e3a\u667a\u80fd\u4f53\u914d\u7f6e\u5b57\u5178\u3002\u7c7b\u578b: <code>dict[str, AgentConfig]</code>\u5fc5\u586b: \u662f <code>custom_handoffs_tool_descriptions</code> \u81ea\u5b9a\u4e49\u4ea4\u63a5\u5de5\u5177\u7684\u63cf\u8ff0\uff0c\u952e\u4e3a\u667a\u80fd\u4f53\u540d\u79f0\uff0c\u503c\u4e3a\u5bf9\u5e94\u7684\u4ea4\u63a5\u5de5\u5177\u63cf\u8ff0\u3002\u7c7b\u578b: <code>dict[str, str]</code>\u5fc5\u586b: \u5426"},{"location":"zh/adavance-guide/middleware/#agents_config","title":"<code>agents_config</code> \u914d\u7f6e\u8bf4\u660e","text":"<p>\u6bcf\u4e2a\u667a\u80fd\u4f53\u914d\u7f6e\u4e3a\u4e00\u4e2a\u5b57\u5178\uff0c\u5305\u542b\u4ee5\u4e0b\u5b57\u6bb5\uff1a</p> \u5b57\u6bb5 \u8bf4\u660e <code>model</code> \u6307\u5b9a\u8be5\u667a\u80fd\u4f53\u4f7f\u7528\u7684\u6a21\u578b\uff1b\u82e5\u4e0d\u4f20\uff0c\u5219\u6cbf\u7528 <code>create_agent</code> \u7684 <code>model</code> \u53c2\u6570\u5bf9\u5e94\u7684\u6a21\u578b\u3002\u652f\u6301\u5b57\u7b26\u4e32\uff08\u987b\u4e3a <code>provider:model-name</code> \u683c\u5f0f\uff0c\u5982 <code>vllm:qwen3-4b</code>\uff09\u6216 <code>BaseChatModel</code> \u5b9e\u4f8b\u3002\u7c7b\u578b: <code>str</code> | <code>BaseChatModel</code>\u5fc5\u586b: \u5426 <code>prompt</code> \u667a\u80fd\u4f53\u7684\u7cfb\u7edf\u63d0\u793a\u8bcd\u3002\u7c7b\u578b: <code>str</code> | <code>SystemMessage</code>\u5fc5\u586b: \u662f <code>tools</code> \u667a\u80fd\u4f53\u53ef\u8c03\u7528\u7684\u5de5\u5177\u5217\u8868\u3002\u7c7b\u578b: <code>list[BaseTool]</code>\u5fc5\u586b: \u5426 <code>default</code> \u662f\u5426\u8bbe\u4e3a\u9ed8\u8ba4\u667a\u80fd\u4f53\uff1b\u7f3a\u7701\u4e3a <code>False</code>\u3002\u5168\u90e8\u914d\u7f6e\u4e2d\u5fc5\u987b\u4e14\u53ea\u80fd\u6709\u4e00\u4e2a\u667a\u80fd\u4f53\u8bbe\u4e3a <code>True</code>\u3002\u7c7b\u578b: <code>bool</code>\u5fc5\u586b: \u5426 <code>handoffs</code> \u8be5\u667a\u80fd\u4f53\u53ef\u4ea4\u63a5\u7ed9\u7684\u5176\u5b83\u667a\u80fd\u4f53\u540d\u79f0\u5217\u8868\u3002\u82e5\u8bbe\u4e3a <code>\"all\"</code>\uff0c\u5219\u8868\u793a\u8be5\u667a\u80fd\u4f53\u53ef\u4ea4\u63a5\u7ed9\u6240\u6709\u5176\u5b83\u667a\u80fd\u4f53\u3002\u7c7b\u578b: <code>list[str]</code> | <code>str</code>\u5fc5\u586b: \u662f <p>\u5bf9\u4e8e\u8fd9\u79cd\u8303\u5f0f\u7684\u591a\u667a\u80fd\u4f53\u5b9e\u73b0\uff0c\u5f80\u5f80\u9700\u8981\u4e00\u4e2a\u7528\u4e8e\u4ea4\u63a5\uff08handoffs\uff09\u7684\u5de5\u5177\u3002\u672c\u4e2d\u95f4\u4ef6\u5229\u7528\u6bcf\u4e2a\u667a\u80fd\u4f53\u7684 <code>handoffs</code> \u914d\u7f6e\uff0c\u81ea\u52a8\u4e3a\u6bcf\u4e2a\u667a\u80fd\u4f53\u521b\u5efa\u5bf9\u5e94\u7684\u4ea4\u63a5\u5de5\u5177\u3002\u5982\u679c\u8981\u81ea\u5b9a\u4e49\u4ea4\u63a5\u5de5\u5177\u7684\u63cf\u8ff0\uff0c\u5219\u53ef\u4ee5\u901a\u8fc7 <code>custom_handoffs_tool_descriptions</code> \u53c2\u6570\u5b9e\u73b0\u3002</p> <p>\u4f7f\u7528\u793a\u4f8b</p> <p>\u672c\u793a\u4f8b\u4e2d\uff0c\u5c06\u4f7f\u7528\u56db\u4e2a\u667a\u80fd\u4f53\uff1a<code>time_agent</code>\u3001<code>weather_agent</code>\u3001<code>code_agent</code> \u548c <code>default_agent</code>\u3002</p> <p>\u63a5\u4e0b\u6765\u8981\u521b\u5efa\u5bf9\u5e94\u667a\u80fd\u4f53\u7684\u914d\u7f6e\u5b57\u5178 <code>agent_config</code>\u3002</p> <pre><code>from langchain_dev_utils.agents.middleware.handoffs import AgentConfig\n\nagent_config: dict[str, AgentConfig] = {\n    \"time_agent\": {\n        \"model\": \"vllm:qwen3-8b\",\n        \"prompt\": \"\u4f60\u662f\u4e00\u4e2a\u65f6\u95f4\u52a9\u624b\",\n        \"tools\": [get_current_time],\n        \"handoffs\": [\"default_agent\"],  # \u8be5\u667a\u80fd\u4f53\u53ea\u80fd\u4ea4\u63a5\u5230default_agent\n    },\n    \"weather_agent\": {\n        \"prompt\": \"\u4f60\u662f\u4e00\u4e2a\u5929\u6c14\u52a9\u624b\",\n        \"tools\": [get_current_weather, get_current_city],\n        \"handoffs\": [\"default_agent\"],\n    },\n    \"code_agent\": {\n        \"model\": load_chat_model(\"vllm:qwen3-coder-flash\"),\n        \"prompt\": \"\u4f60\u662f\u4e00\u4e2a\u4ee3\u7801\u52a9\u624b\",\n        \"tools\": [\n            run_code,\n        ],\n        \"handoffs\": [\"default_agent\"],\n    },\n    \"default_agent\": {\n        \"prompt\": \"\u4f60\u662f\u4e00\u4e2a\u52a9\u624b\",\n        \"default\": True, # \u8bbe\u4e3a\u9ed8\u8ba4\u667a\u80fd\u4f53\n        \"handoffs\": \"all\",  # \u8be5\u667a\u80fd\u4f53\u53ef\u4ee5\u4ea4\u63a5\u5230\u6240\u6709\u5176\u5b83\u667a\u80fd\u4f53\n    },\n}\n</code></pre> <p>\u6700\u7ec8\u5c06\u8fd9\u4e2a\u914d\u7f6e\u4f20\u9012\u7ed9 <code>HandoffAgentMiddleware</code> \u5373\u53ef\u3002</p> <pre><code>from langchain_dev_utils.agents.middleware import HandoffAgentMiddleware\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    tools=[\n        get_current_time,\n        get_current_weather,\n        get_current_city,\n        run_code,\n    ],\n    middleware=[HandoffAgentMiddleware(agents_config=agent_config)],\n)\n\nresponse = agent.invoke({\"messages\": [HumanMessage(content=\"\u5f53\u524d\u65f6\u95f4\u662f\u591a\u5c11\uff1f\")]})\nprint(response)\n</code></pre> <p>\u5982\u679c\u60f3\u8981\u81ea\u5b9a\u4e49\u4ea4\u63a5\u5de5\u5177\u7684\u63cf\u8ff0\uff0c\u53ef\u4ee5\u4f20\u9012\u7b2c\u4e8c\u4e2a\u53c2\u6570 <code>custom_handoffs_tool_descriptions</code>\u3002</p> <pre><code>from langchain_dev_utils.agents.middleware import HandoffAgentMiddleware\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    tools=[\n        get_current_time,\n        get_current_weather,\n        get_current_city,\n        run_code,\n    ],\n    middleware=[\n        HandoffAgentMiddleware(\n            agents_config=agent_config,\n            custom_handoffs_tool_descriptions={\n                \"time_agent\": \"\u6b64\u5de5\u5177\u7528\u4e8e\u4ea4\u63a5\u5230\u65f6\u95f4\u52a9\u624b\u53bb\u89e3\u51b3\u65f6\u95f4\u67e5\u8be2\u95ee\u9898\",\n                \"weather_agent\": \"\u6b64\u5de5\u5177\u7528\u4e8e\u4ea4\u63a5\u5230\u5929\u6c14\u52a9\u624b\u53bb\u89e3\u51b3\u5929\u6c14\u67e5\u8be2\u95ee\u9898\",\n                \"code_agent\": \"\u6b64\u5de5\u5177\u7528\u4e8e\u4ea4\u63a5\u5230\u4ee3\u7801\u52a9\u624b\u53bb\u89e3\u51b3\u4ee3\u7801\u95ee\u9898\",\n                \"default_agent\": \"\u6b64\u5de5\u5177\u7528\u4e8e\u4ea4\u63a5\u5230\u9ed8\u8ba4\u7684\u52a9\u624b\",\n            },\n        )\n    ],\n)\n</code></pre>"},{"location":"zh/adavance-guide/middleware/#_13","title":"\u5de5\u5177\u8c03\u7528\u4fee\u590d","text":"<p><code>ToolCallRepairMiddleware</code> \u662f\u4e00\u4e2a\u81ea\u52a8\u4fee\u590d\u5927\u6a21\u578b\u65e0\u6548\u5de5\u5177\u8c03\u7528\uff08<code>invalid_tool_calls</code>\uff09\u7684\u4e2d\u95f4\u4ef6\u3002</p> <p>\u5927\u6a21\u578b\u5728\u8f93\u51fa\u5de5\u5177\u8c03\u7528\u7684 JSON Schema \u65f6\uff0c\u53ef\u80fd\u56e0\u6a21\u578b\u81ea\u8eab\u539f\u56e0\u751f\u6210 JSON \u683c\u5f0f\u9519\u8bef\u7684\u5185\u5bb9\uff08\u9519\u8bef\u7684\u5185\u5bb9\u5e38\u89c1\u4e8e <code>arguments</code> \u5b57\u6bb5\uff09\uff0c\u5bfc\u81f4 JSON \u89e3\u6790\u5931\u8d25\u3002\u8fd9\u7c7b\u8c03\u7528\u4f1a\u88ab\u5b58\u5230 <code>invalid_tool_calls</code> \u5b57\u6bb5\u4e2d\u3002<code>ToolCallRepairMiddleware</code> \u4f1a\u5728\u6a21\u578b\u8fd4\u56de\u7ed3\u679c\u540e\u81ea\u52a8\u68c0\u6d4b <code>invalid_tool_calls</code>\uff0c\u5e76\u5c1d\u8bd5\u8c03\u7528 <code>json-repair</code> \u8fdb\u884c\u4fee\u590d\uff0c\u4f7f\u5de5\u5177\u8c03\u7528\u5f97\u4ee5\u6b63\u5e38\u6267\u884c\u3002</p> <p>\u8bf7\u786e\u4fdd\u5df2\u5b89\u88c5 <code>langchain-dev-utils[standard]</code>\uff0c\u8be6\u89c1\u5b89\u88c5\u6307\u5357\u3002</p>"},{"location":"zh/adavance-guide/middleware/#_14","title":"\u53c2\u6570\u8bf4\u660e","text":"<p>\u8be5\u4e2d\u95f4\u4ef6\u96f6\u914d\u7f6e\u5f00\u7bb1\u5373\u7528\uff0c\u65e0\u9700\u989d\u5916\u53c2\u6570\u3002</p>"},{"location":"zh/adavance-guide/middleware/#_15","title":"\u4f7f\u7528\u793a\u4f8b","text":"<pre><code>from langchain_dev_utils.agents.middleware import ToolCallRepairMiddleware\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    tools=[run_python_code, get_current_time],\n    middleware=[\n        ToolCallRepairMiddleware()\n    ],\n)\n</code></pre> <p>\u6ce8\u610f</p> <p>\u672c\u4e2d\u95f4\u4ef6\u65e0\u6cd5\u4fdd\u8bc1 100% \u4fee\u590d\u6240\u6709\u65e0\u6548\u5de5\u5177\u8c03\u7528\uff0c\u5b9e\u9645\u6548\u679c\u53d6\u51b3\u4e8e <code>json-repair</code> \u7684\u4fee\u590d\u80fd\u529b\uff1b\u6b64\u5916\uff0c\u5b83\u4ec5\u4f5c\u7528\u4e8e <code>invalid_tool_calls</code> \u5b57\u6bb5\u4e2d\u7684\u65e0\u6548\u5de5\u5177\u8c03\u7528\u5185\u5bb9\u3002</p>"},{"location":"zh/adavance-guide/middleware/#_16","title":"\u683c\u5f0f\u5316\u7cfb\u7edf\u63d0\u793a\u8bcd","text":"<p><code>format_prompt</code> \u662f\u4e00\u4e2a\u88c5\u9970\u5668\uff0c\u5141\u8bb8\u60a8\u5728 <code>system_prompt</code> \u4e2d\u4f7f\u7528 <code>f-string</code> \u98ce\u683c\u7684\u5360\u4f4d\u7b26\uff08\u5982 <code>{name}</code>\uff09\uff0c\u5e76\u5728\u8fd0\u884c\u65f6\u52a8\u6001\u5730\u7528\u5b9e\u9645\u503c\u66ff\u6362\u5b83\u4eec\u3002</p>"},{"location":"zh/adavance-guide/middleware/#_17","title":"\u53c2\u6570\u8bf4\u660e","text":"<p>\u5360\u4f4d\u7b26\u4e2d\u7684\u53d8\u91cf\u503c\u9075\u5faa\u4e00\u4e2a\u660e\u786e\u7684\u89e3\u6790\u987a\u5e8f\uff1a</p> <ol> <li>\u4f18\u5148\u4ece <code>state</code> \u4e2d\u67e5\u627e\uff1a\u4f1a\u5148\u4ece <code>state</code> \u5b57\u5178\u4e2d\u67e5\u627e\u4e0e\u5360\u4f4d\u7b26\u540c\u540d\u7684\u5b57\u6bb5</li> <li>\u5176\u6b21\u4ece <code>context</code> \u4e2d\u67e5\u627e\uff1a\u5982\u679c\u5728 <code>state</code> \u4e2d\u672a\u627e\u5230\u8be5\u5b57\u6bb5\uff0c\u5219\u4f1a\u7ee7\u7eed\u5728 <code>context</code> \u5bf9\u8c61\u4e2d\u67e5\u627e</li> </ol> <p>\u8fd9\u4e2a\u987a\u5e8f\u610f\u5473\u7740 <code>state</code> \u4e2d\u7684\u503c\u62e5\u6709\u66f4\u9ad8\u7684\u4f18\u5148\u7ea7\uff0c\u53ef\u4ee5\u8986\u76d6 <code>context</code> \u4e2d\u540c\u540d\u7684\u503c\u3002</p>"},{"location":"zh/adavance-guide/middleware/#_18","title":"\u4f7f\u7528\u793a\u4f8b","text":""},{"location":"zh/adavance-guide/middleware/#state","title":"\u4ec5\u4ece <code>state</code> \u4e2d\u83b7\u53d6\u53d8\u91cf","text":"<p>\u8fd9\u662f\u6700\u57fa\u7840\u7684\u7528\u6cd5\uff0c\u6240\u6709\u5360\u4f4d\u7b26\u53d8\u91cf\u90fd\u7531 <code>state</code> \u63d0\u4f9b\u3002</p> <pre><code>from langchain_dev_utils.agents.middleware import format_prompt\nfrom langchain.agents import AgentState\n\nclass AssistantState(AgentState):\n    name: str\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    system_prompt=\"\u4f60\u662f\u4e00\u4e2a\u667a\u80fd\u52a9\u624b\uff0c\u4f60\u7684\u540d\u5b57\u53eb\u505a{name}\u3002\",\n    middleware=[format_prompt],\n    state_schema=AssistantState,\n)\n\n# \u5728\u8c03\u7528\u65f6\uff0c\u5fc5\u987b\u4e3a state \u63d0\u4f9b 'name' \u7684\u503c\nresponse = agent.invoke(\n    {\"messages\": [HumanMessage(content=\"\u4f60\u597d\u554a\")], \"name\": \"assistant\"}\n)\nprint(response)\n</code></pre>"},{"location":"zh/adavance-guide/middleware/#state-context","title":"\u540c\u65f6\u4ece <code>state</code> \u548c <code>context</code> \u4e2d\u83b7\u53d6\u53d8\u91cf","text":"<p>\u540c\u65f6\u4f7f\u7528 <code>state</code> \u548c <code>context</code>\uff1a</p> <pre><code>from dataclasses import dataclass\n\n@dataclass\nclass Context:\n    user: str\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    # {name} \u5c06\u4ece state \u83b7\u53d6\uff0c{user} \u5c06\u4ece context \u83b7\u53d6\n    system_prompt=\"\u4f60\u662f\u4e00\u4e2a\u667a\u80fd\u52a9\u624b\uff0c\u4f60\u7684\u540d\u5b57\u53eb\u505a{name}\u3002\u4f60\u7684\u4f7f\u7528\u8005\u53eb\u505a{user}\u3002\",\n    middleware=[format_prompt],\n    state_schema=AssistantState,\n    context_schema=Context,\n)\n\n# \u5728\u8c03\u7528\u65f6\uff0c\u4e3a state \u63d0\u4f9b 'name'\uff0c\u4e3a context \u63d0\u4f9b 'user'\nresponse = agent.invoke(\n    {\n        \"messages\": [HumanMessage(content=\"\u6211\u8981\u53bbNew York\u73a9\u51e0\u5929\uff0c\u5e2e\u6211\u89c4\u5212\u884c\u7a0b\")],\n        \"name\": \"assistant\",\n    },\n    context=Context(user=\"\u5f20\u4e09\"),\n)\nprint(response)\n</code></pre>"},{"location":"zh/adavance-guide/middleware/#_19","title":"\u53d8\u91cf\u8986\u76d6\u793a\u4f8b","text":"<p>\u6b64\u793a\u4f8b\u5c55\u793a\u4e86\u5f53 <code>state</code> \u548c <code>context</code> \u4e2d\u5b58\u5728\u540c\u540d\u53d8\u91cf\u65f6\uff0c<code>state</code> \u7684\u503c\u4f1a\u4f18\u5148\u751f\u6548\u3002</p> <pre><code>from dataclasses import dataclass\n\n@dataclass\nclass Context:\n    # context \u4e2d\u5b9a\u4e49\u4e86 'name'\n    name: str\n    user: str\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    system_prompt=\"\u4f60\u662f\u4e00\u4e2a\u667a\u80fd\u52a9\u624b\uff0c\u4f60\u7684\u540d\u5b57\u53eb\u505a{name}\u3002\u4f60\u7684\u4f7f\u7528\u8005\u53eb\u505a{user}\u3002\",\n    middleware=[format_prompt],\n    state_schema=AssistantState, # state \u4e2d\u4e5f\u5b9a\u4e49\u4e86 'name'\n    context_schema=Context,\n)\n\n# \u5728\u8c03\u7528\u65f6\uff0cstate \u548c context \u90fd\u63d0\u4f9b\u4e86 'name' \u7684\u503c\nresponse = agent.invoke(\n    {\n        \"messages\": [HumanMessage(content=\"\u4f60\u53eb\u4ec0\u4e48\u540d\u5b57\uff1f\")],\n        \"name\": \"assistant-1\",\n    },\n    context=Context(name=\"assistant-2\", user=\"\u5f20\u4e09\"),\n)\n\n# \u6700\u7ec8\u7684\u7cfb\u7edf\u63d0\u793a\u8bcd\u4f1a\u662f \"\u4f60\u662f\u4e00\u4e2a\u667a\u80fd\u52a9\u624b\uff0c\u4f60\u7684\u540d\u5b57\u53eb\u505aassistant-1\u3002\u4f60\u7684\u4f7f\u7528\u8005\u53eb\u505a\u5f20\u4e09\u3002\"\n# \u56e0\u4e3a state \u7684\u4f18\u5148\u7ea7\u66f4\u9ad8\nprint(response)\n</code></pre> <p>\u6ce8\u610f</p> <p>\u81ea\u5b9a\u4e49\u4e2d\u95f4\u4ef6\u6709\u4e24\u79cd\u5b9e\u73b0\u65b9\u5f0f\uff1a\u88c5\u9970\u5668\u6216\u7ee7\u627f\u7c7b\u3002 - \u7ee7\u627f\u7c7b\u5b9e\u73b0\uff1a<code>PlanMiddleware</code>\u3001<code>ModelMiddleware</code>\u3001<code>HandoffAgentMiddleware</code>\u3001<code>ToolCallRepairMiddleware</code> - \u88c5\u9970\u5668\u5b9e\u73b0\uff1a<code>format_prompt</code>\uff08\u88c5\u9970\u5668\u4f1a\u628a\u51fd\u6570\u76f4\u63a5\u53d8\u6210\u4e2d\u95f4\u4ef6\u5b9e\u4f8b\uff0c\u56e0\u6b64\u65e0\u9700\u624b\u52a8\u5b9e\u4f8b\u5316\u5373\u53ef\u4f7f\u7528\uff09</p> <p>\u5b98\u65b9\u4e2d\u95f4\u4ef6\u6269\u5145</p> <p>\u672c\u5e93\u6269\u5145\u4e86\u4ee5\u4e0b\u5b98\u65b9\u4e2d\u95f4\u4ef6\uff0c\u652f\u6301\u901a\u8fc7\u5b57\u7b26\u4e32\u53c2\u6570\u6307\u5b9a\u5df2\u88ab <code>register_model_provider</code> \u6ce8\u518c\u7684\u6a21\u578b\uff1a</p> <p>\u4f60\u53ea\u9700\u8981\u5bfc\u5165\u672c\u5e93\u4e2d\u7684\u8fd9\u4e9b\u4e2d\u95f4\u4ef6\uff0c\u5373\u53ef\u4f7f\u7528\u5b57\u7b26\u4e32\u6307\u5b9a\u5df2\u7ecf\u88ab<code>register_model_provider</code>\u6ce8\u518c\u7684\u6a21\u578b\u3002\u4e2d\u95f4\u4ef6\u4f7f\u7528\u65b9\u6cd5\u548c\u5b98\u65b9\u4e2d\u95f4\u4ef6\u4fdd\u6301\u4e00\u81f4\uff0c\u4f8b\u5982\uff1a <pre><code>from langchain_core.messages import AIMessage\nfrom langchain_dev_utils.agents.middleware import SummarizationMiddleware\nfrom langchain_dev_utils.chat_models import register_model_provider\n\nregister_model_provider(\n    provider_name=\"vllm\",\n    chat_model=\"openai-compatible\",\n    base_url=\"http://localhost:8000/v1\",\n)\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    middleware=[\n        SummarizationMiddleware(\n            model=\"vllm:qwen3-4b\",\n            trigger=(\"tokens\", 50),\n            keep=(\"messages\", 1),\n        )\n    ],\n    system_prompt=\"\u4f60\u662f\u4e00\u4e2a\u667a\u80fd\u7684AI\u52a9\u624b\uff0c\u53ef\u4ee5\u89e3\u51b3\u7528\u6237\u7684\u95ee\u9898\",\n)\n# big_text \u662f\u4e00\u4e2a\u5305\u542b\u5927\u91cf\u5185\u5bb9\u7684\u6587\u672c\uff0c\u8fd9\u91cc\u7701\u7565\nbig_messages = [\n    HumanMessage(content=\"\u4f60\u597d\uff0c\u4f60\u662f\u8c01\"),\n    AIMessage(content=\"\u6211\u662f\u4f60\u7684AI\u52a9\u624b\"),\n    HumanMessage(content=\"\u5199\u4e00\u6bb5\u4f18\u7f8e\u7684\u957f\u6587\u672c\"),\n    AIMessage(content=f\"\u597d\u7684\uff0c\u6211\u4f1a\u5199\u4e00\u6bb5\u4f18\u7f8e\u7684\u957f\u6587\u672c\uff0c\u5185\u5bb9\u662f\uff1a{big_text}\"),\n    HumanMessage(content=\"\u4f60\u4e3a\u5565\u8981\u5199\u8fd9\u6bb5\u957f\u6587\u672c\u5462\uff1f\"),\n]\nresponse = agent.invoke({\"messages\": big_messages})\nprint(response)\n</code></pre></p>"},{"location":"zh/adavance-guide/multi-agent/","title":"\u591a\u667a\u80fd\u4f53\u6784\u5efa","text":""},{"location":"zh/adavance-guide/multi-agent/#_2","title":"\u6982\u8ff0","text":"<p>\u5728\u6784\u5efa\u590d\u6742\u7684 AI \u5e94\u7528\u65f6\uff0c\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u662f\u4e00\u79cd\u5f3a\u5927\u7684\u67b6\u6784\u6a21\u5f0f\u3002\u901a\u8fc7\u5c06\u4e0d\u540c\u804c\u8d23\u5206\u914d\u7ed9\u4e13\u95e8\u7684\u667a\u80fd\u4f53\uff0c\u53ef\u4ee5\u5b9e\u73b0\u4efb\u52a1\u7684\u4e13\u4e1a\u5316\u5206\u5de5\u548c\u9ad8\u6548\u534f\u4f5c\u3002</p> <p>\u5b9e\u73b0\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6709\u591a\u79cd\u65b9\u5f0f\uff0c\u5176\u4e2d\u5de5\u5177\u8c03\u7528\u662f\u4e00\u79cd\u5e38\u7528\u4e14\u7075\u6d3b\u7684\u5b9e\u73b0\u65b9\u5f0f\u3002\u901a\u8fc7\u5c06\u5b50\u667a\u80fd\u4f53\uff08subagents\uff09\u5c01\u88c5\u4e3a\u5de5\u5177\uff0c\u4e3b\u667a\u80fd\u4f53\u53ef\u4ee5\u6839\u636e\u4efb\u52a1\u9700\u6c42\u52a8\u6001\u59d4\u6d3e\u7ed9\u4e13\u95e8\u7684\u5b50\u667a\u80fd\u4f53\u5904\u7406\u3002</p> <p>\u672c\u5e93\u63d0\u4f9b\u4e86\u4e24\u4e2a\u9884\u6784\u5efa\u51fd\u6570\u6765\u7b80\u5316\u8fd9\u79cd\u5b9e\u73b0\u65b9\u5f0f\uff1a</p> \u51fd\u6570\u540d \u529f\u80fd\u63cf\u8ff0 <code>wrap_agent_as_tool</code> \u5c06\u5355\u4e2a\u667a\u80fd\u4f53\u5b9e\u4f8b\u5c01\u88c5\u4e3a\u4e00\u4e2a\u72ec\u7acb\u5de5\u5177 <code>wrap_all_agents_as_tool</code> \u5c06\u591a\u4e2a\u667a\u80fd\u4f53\u5b9e\u4f8b\u5c01\u88c5\u4e3a\u4e00\u4e2a\u7edf\u4e00\u5de5\u5177\uff0c\u901a\u8fc7\u53c2\u6570\u6307\u5b9a\u8c03\u7528\u54ea\u4e2a\u5b50\u667a\u80fd\u4f53"},{"location":"zh/adavance-guide/multi-agent/#_3","title":"\u5c01\u88c5\u5355\u4e2a\u667a\u80fd\u4f53\u4e3a\u5de5\u5177","text":"<p>\u5c01\u88c5\u5355\u4e2a\u667a\u80fd\u4f53\u53ea\u9700\u4e09\u6b65\uff1a</p> <ol> <li>\u5bfc\u5165 <code>wrap_agent_as_tool</code></li> <li>\u628a\u667a\u80fd\u4f53\u5b9e\u4f8b\u4f5c\u4e3a\u53c2\u6570\u4f20\u5165</li> <li>\u83b7\u5f97\u53ef\u76f4\u63a5\u88ab\u5176\u4ed6\u667a\u80fd\u4f53\u8c03\u7528\u7684\u5de5\u5177\u5bf9\u8c61</li> </ol>"},{"location":"zh/adavance-guide/multi-agent/#_4","title":"\u51fd\u6570\u53c2\u6570\u8bf4\u660e","text":"\u53c2\u6570 \u8bf4\u660e <code>agent</code> \u667a\u80fd\u4f53\u5b9e\u4f8b\uff0c\u5fc5\u987b\u5df2\u5b9a\u4e49 <code>name</code> \u5c5e\u6027\u3002\u7c7b\u578b: <code>CompiledStateGraph</code>\u5fc5\u586b: \u662f <code>tool_name</code> \u5de5\u5177\u540d\u79f0\uff0c\u9ed8\u8ba4\u4e3a <code>transfer_to_{agent_name}</code>\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u5426 <code>tool_description</code> \u5de5\u5177\u63cf\u8ff0\uff0c\u9ed8\u8ba4\u4e3a <code>This tool transforms input to {agent_name}</code>\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u5426 <code>pre_input_hooks</code> \u667a\u80fd\u4f53\u8fd0\u884c\u524d\u7684\u94a9\u5b50\u51fd\u6570\u3002\u7c7b\u578b: <code>tuple</code>\u5fc5\u586b: \u5426 <code>post_output_hooks</code> \u667a\u80fd\u4f53\u8fd0\u884c\u540e\u7684\u94a9\u5b50\u51fd\u6570\u3002\u7c7b\u578b: <code>tuple</code>\u5fc5\u586b: \u5426"},{"location":"zh/adavance-guide/multi-agent/#_5","title":"\u4f7f\u7528\u793a\u4f8b","text":"<p>\u4e0b\u9762\u6211\u4eec\u4ee5 <code>supervisor</code> \u667a\u80fd\u4f53\u4e3a\u4f8b\uff0c\u4ecb\u7ecd\u5982\u4f55\u901a\u8fc7 <code>wrap_agent_as_tool</code> \u5c06\u5b50\u667a\u80fd\u4f53\u5c01\u88c5\u4e3a\u5de5\u5177\u3002</p> <p>\u9996\u5148\u5b9e\u73b0\u4e24\u4e2a\u5b50\u667a\u80fd\u4f53\uff0c\u4e00\u4e2a\u7528\u4e8e\u53d1\u9001\u90ae\u4ef6\uff0c\u4e00\u4e2a\u7528\u4e8e\u65e5\u7a0b\u67e5\u8be2\u548c\u5b89\u6392\u3002</p>"},{"location":"zh/adavance-guide/multi-agent/#_6","title":"\u90ae\u4ef6\u667a\u80fd\u4f53","text":"<pre><code>from langchain_core.tools import tool\nfrom langchain_dev_utils.chat_models import register_model_provider\nfrom langchain_dev_utils.agents import create_agent, wrap_agent_as_tool \n\nregister_model_provider(\n    \"vllm\",\n    \"openai-compatible\",\n    base_url=\"http://localhost:8000/v1\",\n)\n\n\n@tool\ndef send_email(\n    to: list[str],  # \u7535\u5b50\u90ae\u4ef6\u5730\u5740\n    subject: str,\n    body: str,\n    cc: list[str] = [],\n) -&gt; str:\n    \"\"\"\u901a\u8fc7\u7535\u5b50\u90ae\u4ef6API\u53d1\u9001\u90ae\u4ef6\u3002\u8981\u6c42\u6b63\u786e\u683c\u5f0f\u7684\u5730\u5740\u3002\"\"\"\n    # \u5b58\u6839\uff1a\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u8fd9\u91cc\u4f1a\u8c03\u7528SendGrid\u3001Gmail API\u7b49\n    return f\"\u90ae\u4ef6\u5df2\u53d1\u9001\u81f3 {', '.join(to)} - \u4e3b\u9898: {subject}\"\n\n\nEMAIL_AGENT_PROMPT = (\n    \"\u4f60\u662f\u4e00\u4e2a\u7535\u5b50\u90ae\u4ef6\u52a9\u624b\u3002\"\n    \"\u6839\u636e\u81ea\u7136\u8bed\u8a00\u8bf7\u6c42\u64b0\u5199\u4e13\u4e1a\u90ae\u4ef6\u3002\"\n    \"\u63d0\u53d6\u6536\u4ef6\u4eba\u4fe1\u606f\u5e76\u5236\u4f5c\u6070\u5f53\u7684\u4e3b\u9898\u884c\u548c\u6b63\u6587\u5185\u5bb9\u3002\"\n    \"\u4f7f\u7528 send_email \u6765\u53d1\u9001\u90ae\u4ef6\u3002\"\n    \"\u59cb\u7ec8\u5728\u6700\u7ec8\u56de\u590d\u4e2d\u786e\u8ba4\u5df2\u53d1\u9001\u7684\u5185\u5bb9\u3002\"\n)\n\nemail_agent = create_agent(\n    \"vllm:qwen3-4b\",\n    tools=[send_email],\n    system_prompt=EMAIL_AGENT_PROMPT,\n    name=\"email_agent\",\n)\n</code></pre>"},{"location":"zh/adavance-guide/multi-agent/#_7","title":"\u65e5\u7a0b\u667a\u80fd\u4f53","text":"<pre><code>@tool\ndef create_calendar_event(\n    title: str,\n    start_time: str,  # ISO\u683c\u5f0f: \"2024-01-15T14:00:00\"\n    end_time: str,  # ISO\u683c\u5f0f: \"2024-01-15T15:00:00\"\n    attendees: list[str],  # \u7535\u5b50\u90ae\u4ef6\u5730\u5740\n    location: str = \"\",\n) -&gt; str:\n    \"\"\"\u521b\u5efa\u65e5\u5386\u4e8b\u4ef6\u3002\u8981\u6c42\u7cbe\u786e\u7684ISO\u65e5\u671f\u65f6\u95f4\u683c\u5f0f\u3002\"\"\"\n    # \u5b58\u6839\uff1a\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u8fd9\u91cc\u4f1a\u8c03\u7528Google Calendar API\u3001Outlook API\u7b49\n    return f\"\u4e8b\u4ef6\u5df2\u521b\u5efa\uff1a{title} \u4ece {start_time} \u5230 {end_time}\uff0c\u5171\u6709 {len(attendees)} \u4f4d\u53c2\u4e0e\u8005\"\n\n\n@tool\ndef get_available_time_slots(\n    attendees: list[str],\n    date: str,  # ISO\u683c\u5f0f: \"2024-01-15\"\n    duration_minutes: int,\n) -&gt; list[str]:\n    \"\"\"\u5728\u7279\u5b9a\u65e5\u671f\u67e5\u8be2\u53c2\u4e0e\u8005\u7684\u65e5\u5386\u53ef\u7528\u65f6\u95f4\u3002\"\"\"\n    # \u5b58\u6839\uff1a\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u8fd9\u91cc\u4f1a\u67e5\u8be2\u65e5\u5386API\n    return [\"09:00\", \"14:00\", \"16:00\"]\n\n\nCALENDAR_AGENT_PROMPT = (\n    \"\u4f60\u662f\u4e00\u4e2a\u65e5\u5386\u65e5\u7a0b\u5b89\u6392\u52a9\u624b\u3002\"\n    \"\u5c06\u81ea\u7136\u8bed\u8a00\u7684\u65e5\u7a0b\u5b89\u6392\u8bf7\u6c42\uff08\u4f8b\u5982'\u4e0b\u5468\u4e8c\u4e0b\u53482\u70b9'\uff09\u89e3\u6790\u4e3a\u6b63\u786e\u7684ISO\u65e5\u671f\u65f6\u95f4\u683c\u5f0f\u3002\"\n    \"\u9700\u8981\u65f6\u4f7f\u7528 get_available_time_slots \u6765\u68c0\u67e5\u53ef\u7528\u65f6\u95f4\u3002\"\n    \"\u4f7f\u7528 create_calendar_event \u6765\u5b89\u6392\u4e8b\u4ef6\u3002\"\n    \"\u59cb\u7ec8\u5728\u6700\u7ec8\u56de\u590d\u4e2d\u786e\u8ba4\u5df2\u5b89\u6392\u7684\u5185\u5bb9\u3002\"\n)\n\ncalendar_agent = create_agent(\n    \"vllm:qwen3-4b\",\n    tools=[create_calendar_event, get_available_time_slots],\n    system_prompt=CALENDAR_AGENT_PROMPT,\n    name=\"calendar_agent\",\n)\n</code></pre> <p>\u63a5\u4e0b\u6765\uff0c\u4f7f\u7528 <code>wrap_agent_as_tool</code> \u5c06\u8fd9\u4e24\u4e2a\u5b50\u667a\u80fd\u4f53\u5c01\u88c5\u4e3a\u5de5\u5177\u3002</p> <pre><code>schedule_event = wrap_agent_as_tool(\n    calendar_agent,\n    tool_name=\"schedule_event\",\n    tool_description=(\n        \"\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u5b89\u6392\u65e5\u5386\u4e8b\u4ef6\u3002\"\n        \"\u5728\u7528\u6237\u60f3\u8981\u521b\u5efa\u3001\u4fee\u6539\u6216\u68c0\u67e5\u65e5\u5386\u7ea6\u4f1a\u65f6\u4f7f\u7528\u6b64\u529f\u80fd\u3002\"\n        \"\u80fd\u591f\u5904\u7406\u65e5\u671f/\u65f6\u95f4\u89e3\u6790\u3001\u67e5\u8be2\u53ef\u7528\u65f6\u95f4\u548c\u521b\u5efa\u4e8b\u4ef6\u3002\"\n        \"\u8f93\u5165\uff1a\u81ea\u7136\u8bed\u8a00\u65e5\u5386\u5b89\u6392\u8bf7\u6c42\uff08\u4f8b\u5982'\u4e0e\u8bbe\u8ba1\u56e2\u961f\u4e0b\u4e2a\u661f\u671f\u4e8c\u4e0b\u53482\u70b9\u7684\u4f1a\u8bae'\uff09\"\n    ),\n)\nmanage_email = wrap_agent_as_tool(\n    email_agent,\n    tool_name=\"manage_email\",\n    tool_description=(\n        \"\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u53d1\u9001\u7535\u5b50\u90ae\u4ef6\u3002\"\n        \"\u5728\u7528\u6237\u60f3\u8981\u53d1\u9001\u901a\u77e5\u3001\u63d0\u9192\u6216\u4efb\u4f55\u7535\u5b50\u90ae\u4ef6\u901a\u4fe1\u65f6\u4f7f\u7528\u6b64\u529f\u80fd\u3002\"\n        \"\u80fd\u591f\u63d0\u53d6\u6536\u4ef6\u4eba\u4fe1\u606f\u3001\u4e3b\u9898\u751f\u6210\u548c\u7535\u5b50\u90ae\u4ef6\u64b0\u5199\u3002\"\n        \"\u8f93\u5165\uff1a\u81ea\u7136\u8bed\u8a00\u7535\u5b50\u90ae\u4ef6\u8bf7\u6c42\uff08\u4f8b\u5982'\u5411\u4ed6\u4eec\u53d1\u9001\u4f1a\u8bae\u63d0\u9192'\uff09\"\n    ),\n)\n</code></pre> <p>\u6700\u7ec8\u521b\u5efa\u4e00\u4e2a <code>supervisor_agent</code>\uff0c\u5b83\u53ef\u4ee5\u8c03\u7528\u8fd9\u4e24\u4e2a\u5de5\u5177\u3002</p> <pre><code>SUPERVISOR_PROMPT = (\n    \"\u4f60\u662f\u4e00\u4e2a\u6709\u7528\u7684\u4e2a\u4eba\u52a9\u624b\u3002\"\n    \"\u4f60\u53ef\u4ee5\u5b89\u6392\u65e5\u5386\u4e8b\u4ef6\u5e76\u53d1\u9001\u7535\u5b50\u90ae\u4ef6\u3002\"\n    \"\u5c06\u7528\u6237\u8bf7\u6c42\u5206\u89e3\u4e3a\u9002\u5f53\u7684\u5de5\u5177\u8c03\u7528\uff0c\u5e76\u534f\u8c03\u7ed3\u679c\u3002\"\n    \"\u5f53\u8bf7\u6c42\u6d89\u53ca\u591a\u4e2a\u64cd\u4f5c\u65f6\uff0c\u8bf7\u4f7f\u7528\u591a\u4e2a\u5de5\u5177\u6309\u987a\u5e8f\u64cd\u4f5c\u3002\"\n)\n\n\nsupervisor_agent = create_agent(\n    \"vllm:qwen3-4b\",\n    tools=[schedule_event, manage_email],\n    system_prompt=SUPERVISOR_PROMPT,\n)\n\nprint(\n    supervisor_agent.invoke({\"messages\": [HumanMessage(content=\"\u67e5\u8be2\u660e\u5929\u7684\u7a7a\u95f2\u65f6\u95f4\")]})\n)\nprint(\n    supervisor_agent.invoke(\n        {\"messages\": [HumanMessage(content=\"\u7ed9test@123.com\u53d1\u9001\u90ae\u4ef6\u4f1a\u8bae\u63d0\u9192\")]}\n    )\n)\n</code></pre> <p>\u63d0\u793a</p> <p>\u4e0a\u8ff0\u793a\u4f8b\u4e2d\uff0c\u6211\u4eec\u662f\u4ece <code>langchain_dev_utils.agents</code> \u4e2d\u5bfc\u5165\u4e86 <code>create_agent</code> \u51fd\u6570\uff0c\u800c\u4e0d\u662f <code>langchain.agents</code>\u3002\u8fd9\u662f\u56e0\u4e3a\u672c\u5e93\u4e5f\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e0e\u5b98\u65b9 <code>create_agent</code> \u51fd\u6570\u529f\u80fd\u5b8c\u5168\u76f8\u540c\u7684\u51fd\u6570\uff0c\u53ea\u662f\u6269\u5145\u4e86\u901a\u8fc7\u5b57\u7b26\u4e32\u6307\u5b9a\u6a21\u578b\u7684\u529f\u80fd\u3002\u8fd9\u4f7f\u5f97\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 <code>register_model_provider</code> \u6ce8\u518c\u7684\u6a21\u578b\uff0c\u800c\u65e0\u9700\u521d\u59cb\u5316\u6a21\u578b\u5b9e\u4f8b\u540e\u4f20\u5165\u3002</p>"},{"location":"zh/adavance-guide/multi-agent/#_8","title":"\u5c01\u88c5\u591a\u4e2a\u667a\u80fd\u4f53\u4e3a\u5355\u4e00\u5de5\u5177","text":"<p>\u5c06\u591a\u4e2a\u667a\u80fd\u4f53\u5c01\u88c5\u4e3a\u5355\u4e00\u5de5\u5177\u53ea\u9700\u4e09\u6b65\uff1a</p> <ol> <li>\u5bfc\u5165 <code>wrap_all_agents_as_tool</code></li> <li>\u628a\u591a\u4e2a\u667a\u80fd\u4f53\u5b9e\u4f8b\u4f5c\u4e3a\u5217\u8868\u4e00\u6b21\u6027\u4f20\u5165</li> <li>\u83b7\u5f97\u53ef\u76f4\u63a5\u88ab\u5176\u4ed6\u667a\u80fd\u4f53\u8c03\u7528\u7684\u7edf\u4e00\u5de5\u5177\u5bf9\u8c61</li> </ol>"},{"location":"zh/adavance-guide/multi-agent/#_9","title":"\u51fd\u6570\u53c2\u6570\u8bf4\u660e","text":"\u53c2\u6570 \u8bf4\u660e <code>agents</code> \u667a\u80fd\u4f53\u5b9e\u4f8b\u5217\u8868\u3002\u7c7b\u578b: <code>list[CompiledStateGraph]</code>\u5fc5\u586b: \u662f <code>tool_name</code> \u5de5\u5177\u540d\u79f0\uff0c\u9ed8\u8ba4\u4e3a <code>task</code>\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u5426 <code>tool_description</code> \u5de5\u5177\u63cf\u8ff0\uff0c\u9ed8\u8ba4\u5305\u542b\u6240\u6709\u53ef\u7528\u667a\u80fd\u4f53\u4fe1\u606f\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u5426 <code>pre_input_hooks</code> \u667a\u80fd\u4f53\u8fd0\u884c\u524d\u7684\u94a9\u5b50\u51fd\u6570\u3002\u7c7b\u578b: <code>tuple</code>\u5fc5\u586b: \u5426 <code>post_output_hooks</code> \u667a\u80fd\u4f53\u8fd0\u884c\u540e\u7684\u94a9\u5b50\u51fd\u6570\u3002\u7c7b\u578b: <code>tuple</code>\u5fc5\u586b: \u5426"},{"location":"zh/adavance-guide/multi-agent/#_10","title":"\u4f7f\u7528\u793a\u4f8b","text":"<p>\u5bf9\u4e8e\u4e0a\u4e00\u4e2a\u793a\u4f8b\u7684 <code>calendar_agent</code> \u548c <code>email_agent</code>\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06\u5b83\u4eec\u5c01\u88c5\u4e3a\u4e00\u4e2a\u5de5\u5177 <code>call_subagent</code>\uff1a</p> <pre><code>call_subagent_tool = wrap_all_agents_as_tool(\n    [calendar_agent, email_agent],\n    tool_name=\"call_subagent\",\n    tool_description=(\n        \"\u8c03\u7528\u5b50\u667a\u80fd\u4f53\u6267\u884c\u4efb\u52a1\u3002\"\n        \"\u53ef\u4ee5\u4f7f\u7528\u7684\u667a\u80fd\u4f53\u6709\uff1a\"\n        \"- calendar_agent\uff1a\u7528\u4e8e\u5b89\u6392\u65e5\u5386\u4e8b\u4ef6\"\n        \"- email_agent\uff1a\u7528\u4e8e\u53d1\u9001\u7535\u5b50\u90ae\u4ef6\"\n    ),\n)\n\nMAIN_AGENT_PROMPT = (\n    \"\u4f60\u662f\u4e00\u4e2a\u6709\u7528\u7684\u4e2a\u4eba\u52a9\u624b\u3002\"\n    \"\u4f60\u53ef\u4ee5\u4f7f\u7528**call_subagent**\u5de5\u5177\u8c03\u7528\u5b50\u667a\u80fd\u4f53\u6267\u884c\u4efb\u52a1\u3002\"\n    \"\u5c06\u7528\u6237\u8bf7\u6c42\u5206\u89e3\u4e3a\u9002\u5f53\u7684\u5de5\u5177\u8c03\u7528\uff0c\u5e76\u534f\u8c03\u7ed3\u679c\u3002\"\n    \"\u5f53\u8bf7\u6c42\u6d89\u53ca\u591a\u4e2a\u64cd\u4f5c\u65f6\uff0c\u8bf7\u4f7f\u7528\u591a\u4e2a\u5de5\u5177\u6309\u987a\u5e8f\u64cd\u4f5c\u3002\"\n)\n\nmain_agent = create_agent(\n    \"vllm:qwen3-4b\",\n    tools=[call_subagent_tool],\n    system_prompt=MAIN_AGENT_PROMPT,\n)\n</code></pre> <p>\u63d0\u793a</p> <p>\u9664\u4e86\u4f7f\u7528\u672c\u5e93\u63d0\u4f9b\u7684 <code>wrap_all_agents_as_tool</code> \u5c06\u591a\u4e2a\u667a\u80fd\u4f53\u5c01\u88c5\u4e3a\u5355\u4e00\u5de5\u5177\u5916\uff0c\u4f60\u8fd8\u53ef\u4ee5\u4f7f\u7528 <code>deepagents</code> \u5e93\u63d0\u4f9b\u7684 <code>SubAgentMiddleware</code> \u4e2d\u95f4\u4ef6\u5b9e\u73b0\u7c7b\u4f3c\u7684\u6548\u679c\u3002</p>"},{"location":"zh/adavance-guide/multi-agent/#_11","title":"\u94a9\u5b50\u51fd\u6570","text":"<p>\u672c\u5e93\u5185\u7f6e\u4e86\u7075\u6d3b\u7684\u94a9\u5b50\uff08hook\uff09\u673a\u5236\uff0c\u5141\u8bb8\u5728\u5b50\u667a\u80fd\u4f53\u8fd0\u884c\u524d\u540e\u63d2\u5165\u81ea\u5b9a\u4e49\u903b\u8f91\u3002\u8be5\u673a\u5236\u540c\u65f6\u9002\u7528\u4e8e <code>wrap_agent_as_tool</code> \u4e0e <code>wrap_all_agents_as_tool</code>\uff0c\u4e0b\u6587\u4ee5 <code>wrap_agent_as_tool</code> \u4e3a\u4f8b\u8fdb\u884c\u8bf4\u660e\u3002</p>"},{"location":"zh/adavance-guide/multi-agent/#1-pre_input_hooks","title":"1. pre_input_hooks","text":"<p>\u5728\u667a\u80fd\u4f53\u8fd0\u884c\u524d\u5bf9\u8f93\u5165\u8fdb\u884c\u9884\u5904\u7406\u3002\u53ef\u7528\u4e8e\u8f93\u5165\u589e\u5f3a\u3001\u4e0a\u4e0b\u6587\u6ce8\u5165\u3001\u683c\u5f0f\u6821\u9a8c\u3001\u6743\u9650\u68c0\u67e5\u7b49\u3002</p>"},{"location":"zh/adavance-guide/multi-agent/#_12","title":"\u652f\u6301\u7684\u4f20\u5165\u7c7b\u578b","text":"\u7c7b\u578b \u8bf4\u660e \u5355\u4e2a\u540c\u6b65\u51fd\u6570 \u540c\u65f6\u7528\u4e8e\u540c\u6b65\uff08<code>invoke</code>\uff09\u548c\u5f02\u6b65\uff08<code>ainvoke</code>\uff09\u8c03\u7528\u8def\u5f84\uff08\u5f02\u6b65\u8def\u5f84\u4e2d\u4e0d\u4f1a <code>await</code>\uff0c\u76f4\u63a5\u8c03\u7528\uff09 \u4e8c\u5143\u7ec4 <code>(sync_func, async_func)</code> \u7b2c\u4e00\u4e2a\u51fd\u6570\u7528\u4e8e\u540c\u6b65\u8c03\u7528\u8def\u5f84\uff1b\u7b2c\u4e8c\u4e2a\u51fd\u6570\uff08\u5fc5\u987b\u662f <code>async def</code>\uff09\u7528\u4e8e\u5f02\u6b65\u8c03\u7528\u8def\u5f84\uff0c\u5e76\u4f1a\u88ab <code>await</code>"},{"location":"zh/adavance-guide/multi-agent/#_13","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def pre_input_hook(request: str, runtime: ToolRuntime) -&gt; str:\n    \"\"\"\n    \u53c2\u6570:\n        request: \u539f\u59cb\u5de5\u5177\u8c03\u7528\u8f93\u5165\n        runtime: langchain \u7684 ToolRuntime\n\n    \u8fd4\u56de:\n        \u5904\u7406\u540e\u7684 str\uff0c\u4f5c\u4e3a agent \u7684\u5b9e\u9645\u8f93\u5165\n    \"\"\"\n</code></pre>"},{"location":"zh/adavance-guide/multi-agent/#_14","title":"\u4f7f\u7528\u793a\u4f8b","text":"<pre><code>def process_input(request: str, runtime: ToolRuntime) -&gt; str:\n    return \"&lt;task_description&gt;\" + request + \"&lt;/task_description&gt;\"\n\n# \u6216\u652f\u6301\u5f02\u6b65\nasync def process_input_async(request: str, runtime: ToolRuntime) -&gt; str:\n    return \"&lt;task_description&gt;\" + request + \"&lt;/task_description&gt;\"\n\n# \u4f7f\u7528\ncall_agent_tool = wrap_agent_as_tool(\n    agent,\n    pre_input_hooks=(process_input, process_input_async)\n)\n</code></pre> <p>\u63d0\u793a</p> <p>\u4e0a\u8ff0\u7684\u4f8b\u5b50\u6bd4\u8f83\u7b80\u5355\uff0c\u5b9e\u9645\u4e0a\u4f60\u53ef\u4ee5\u6839\u636e <code>runtime</code> \u91cc\u9762\u7684 <code>state</code> \u6216\u8005 <code>context</code> \u6dfb\u52a0\u66f4\u590d\u6742\u7684\u903b\u8f91\u3002</p>"},{"location":"zh/adavance-guide/multi-agent/#2-post_output_hooks","title":"2. post_output_hooks","text":"<p>\u5728\u667a\u80fd\u4f53\u8fd0\u884c\u5b8c\u6210\u540e\uff0c\u5bf9\u5176\u8fd4\u56de\u7684\u5b8c\u6574\u6d88\u606f\u5217\u8868\u8fdb\u884c\u540e\u5904\u7406\uff0c\u4ee5\u751f\u6210\u5de5\u5177\u7684\u6700\u7ec8\u8fd4\u56de\u503c\u3002\u53ef\u7528\u4e8e\u7ed3\u679c\u63d0\u53d6\u3001\u7ed3\u6784\u5316\u8f6c\u6362\u7b49\u3002</p>"},{"location":"zh/adavance-guide/multi-agent/#_15","title":"\u652f\u6301\u7684\u4f20\u5165\u7c7b\u578b","text":"\u7c7b\u578b \u8bf4\u660e \u5355\u4e2a\u51fd\u6570 \u540c\u65f6\u7528\u4e8e\u540c\u6b65\u548c\u5f02\u6b65\u8def\u5f84\uff08\u5f02\u6b65\u8def\u5f84\u4e2d\u4e0d <code>await</code>\uff09 \u4e8c\u5143\u7ec4 <code>(sync_func, async_func)</code> \u7b2c\u4e00\u4e2a\u7528\u4e8e\u540c\u6b65\u8def\u5f84\uff1b\u7b2c\u4e8c\u4e2a\uff08<code>async def</code>\uff09\u7528\u4e8e\u5f02\u6b65\u8def\u5f84\uff0c\u5e76\u4f1a\u88ab <code>await</code>"},{"location":"zh/adavance-guide/multi-agent/#_16","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def post_output_hook(request: str, messages: list, runtime: ToolRuntime) -&gt; Union[str, Command]:\n    \"\"\"\n    \u53c2\u6570:\n        request: \uff08\u53ef\u80fd\u5df2\u5904\u7406\u7684\uff09\u539f\u59cb\u8f93\u5165\n        messages: agent \u8fd4\u56de\u7684\u5b8c\u6574\u6d88\u606f\u5386\u53f2\uff08\u6765\u81ea response[\"messages\"]\uff09\n        runtime: langchain \u7684 ToolRuntime\n\n    \u8fd4\u56de:\n        \u80fd\u591f\u88ab\u5e8f\u5217\u5316\u4e3a\u5b57\u7b26\u4e32\u7684\u503c\uff0c\u6216\u8005\u662f Command \u5bf9\u8c61\n    \"\"\"\n</code></pre>"},{"location":"zh/adavance-guide/multi-agent/#_17","title":"\u4f7f\u7528\u793a\u4f8b","text":"<pre><code>from langgraph.types import Command\n\ndef process_output_sync(request: str, messages: list, runtime: ToolRuntime) -&gt; Command:\n    return Command(update={\n        \"messages\":[ToolMessage(content=messages[-1].content, tool_call_id=runtime.tool_call_id)]\n    })\n\nasync def process_output_async(request: str, messages: list, runtime: ToolRuntime) -&gt; Command:\n    return Command(update={\n        \"messages\":[ToolMessage(content=messages[-1].content, tool_call_id=runtime.tool_call_id)]\n    })\n\n# \u4f7f\u7528\ncall_agent_tool = wrap_agent_as_tool(\n    agent,\n    post_output_hooks=(process_output_sync, process_output_async)\n)\n</code></pre> <p>\u63d0\u793a</p> <p>\u4e0a\u8ff0\u7684\u4f8b\u5b50\u6bd4\u8f83\u7b80\u5355\uff0c\u5b9e\u9645\u4e0a\u4f60\u53ef\u4ee5\u6839\u636e <code>runtime</code> \u91cc\u9762\u7684 <code>state</code> \u6216\u8005 <code>context</code> \u6dfb\u52a0\u66f4\u590d\u6742\u7684\u903b\u8f91\u3002</p>"},{"location":"zh/adavance-guide/multi-agent/#_18","title":"\u9ed8\u8ba4\u884c\u4e3a","text":"<ul> <li>\u82e5\u672a\u63d0\u4f9b <code>pre_input_hooks</code>\uff0c\u8f93\u5165\u539f\u6837\u4f20\u9012</li> <li>\u82e5\u672a\u63d0\u4f9b <code>post_output_hooks</code>\uff0c\u9ed8\u8ba4\u8fd4\u56de <code>response[\"messages\"][-1].content</code>\uff08\u5373\u6700\u540e\u4e00\u6761\u6d88\u606f\u7684\u6587\u672c\u5185\u5bb9\uff09</li> </ul>"},{"location":"zh/adavance-guide/openai-compatible/","title":"OpenAI \u517c\u5bb9 API \u6a21\u578b\u63d0\u4f9b\u5546\u96c6\u6210","text":""},{"location":"zh/adavance-guide/openai-compatible/#_1","title":"\u6982\u8ff0","text":"<p>\u5f88\u591a\u6a21\u578b\u63d0\u4f9b\u5546\u90fd\u652f\u6301OpenAI \u517c\u5bb9\u7684API\u670d\u52a1\uff0c\u4f8b\u5982\uff1avLLM\u3001OpenRouter\u3001Together AI\u7b49\u3002\u672c\u5e93\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684OpenAI\u517c\u5bb9API\u96c6\u6210\u65b9\u6848\uff0c\u652f\u6301\u5bf9\u8bdd\u6a21\u578b\u548c\u5d4c\u5165\u6a21\u578b\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6682\u65f6\u6ca1\u6709\u5bf9\u5e94\u7684LangChain\u96c6\u6210\u800c\u63d0\u4f9b\u5546\u63d0\u4f9bOpenAI\u517c\u5bb9API\u7684\u573a\u666f\u3002</p> <p>\u63d0\u793a</p> <p>\u63a5\u5165 OpenAI \u517c\u5bb9 API \u7684\u5e38\u89c1\u505a\u6cd5\u662f\u76f4\u63a5\u4f7f\u7528 <code>langchain-openai</code> \u4e2d\u7684 <code>ChatOpenAI</code> \u6216 <code>OpenAIEmbeddings</code>\uff0c\u53ea\u9700\u4f20\u5165 <code>base_url</code> \u4e0e <code>api_key</code> \u5373\u53ef\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u65b9\u5f0f\u4ec5\u9002\u7528\u4e8e\u7b80\u5355\u573a\u666f\uff0c\u5b58\u5728\u8bf8\u591a\u517c\u5bb9\u6027\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5bf9\u8bdd\u6a21\u578b\uff0c\u5177\u4f53\u5305\u62ec\uff1a</p> <ol> <li>\u65e0\u6cd5\u663e\u793a\u975e OpenAI \u5b98\u65b9\u63a8\u7406\u6a21\u578b\u7684\u601d\u7ef4\u94fe\uff08<code>reasoning_content</code>\uff09</li> <li>\u4e0d\u652f\u6301\u89c6\u9891\u7c7b\u578b\u7684 content_block</li> <li>\u7ed3\u6784\u5316\u8f93\u51fa\u9ed8\u8ba4\u7b56\u7565\u8986\u76d6\u7387\u4f4e</li> </ol> <p>\u672c\u5e93\u63d0\u4f9b\u6b64\u529f\u80fd\u6b63\u662f\u4e3a\u4e86\u89e3\u51b3\u4e0a\u8ff0\u517c\u5bb9\u6027\u95ee\u9898\u3002\u5bf9\u4e8e\u7b80\u5355\u573a\u666f\uff08\u5c24\u5176\u662f\u5bf9\u517c\u5bb9\u6027\u8981\u6c42\u4e0d\u9ad8\u7684\u573a\u666f\uff09\uff0c\u53ef\u76f4\u63a5\u4f7f\u7528 <code>ChatOpenAI</code>\uff0c\u65e0\u9700\u4f7f\u7528\u672c\u529f\u80fd\u3002<code>OpenAIEmbeddings</code> \u517c\u5bb9\u6027\u8f83\u597d\uff0c\u53ea\u9700\u5c06 <code>check_embedding_ctx_length</code> \u8bbe\u4e3a <code>False</code> \u5373\u53ef\u3002\u6b64\u5916\uff0c\u4e3a\u65b9\u4fbf\u5f00\u53d1\u8005\uff0c\u6211\u4eec\u4e5f\u63d0\u4f9b\u4e86\u5d4c\u5165\u6a21\u578b\u7684 OpenAI \u517c\u5bb9\u96c6\u6210\u7c7b\u529f\u80fd\u3002</p>"},{"location":"zh/adavance-guide/openai-compatible/#_2","title":"\u521b\u5efa\u5bf9\u5e94\u7684\u96c6\u6210\u7c7b","text":"<p>\u672c\u5e93\u63d0\u4f9b\u4e86\u4e24\u4e2a\u5de5\u5177\u51fd\u6570\uff0c\u7528\u4e8e\u521b\u5efa\u5bf9\u5e94\u7684\u5bf9\u8bdd\u6a21\u578b\u96c6\u6210\u7c7b\u548c\u5d4c\u5165\u6a21\u578b\u96c6\u6210\u7c7b\u3002\u5177\u4f53\u4e3a\uff1a</p> \u51fd\u6570\u540d \u8bf4\u660e <code>create_openai_compatible_model</code> \u521b\u5efa\u5bf9\u8bdd\u6a21\u578b\u96c6\u6210\u7c7b <code>create_openai_compatible_embedding</code> \u521b\u5efa\u5d4c\u5165\u6a21\u578b\u96c6\u6210\u7c7b"},{"location":"zh/adavance-guide/openai-compatible/#_3","title":"\u521b\u5efa\u5bf9\u8bdd\u6a21\u578b\u7c7b","text":"<p>\u4f7f\u7528 <code>create_openai_compatible_model</code> \u51fd\u6570\u53ef\u4ee5\u521b\u5efa\u5bf9\u8bdd\u6a21\u578b\u96c6\u6210\u7c7b\u3002\u8be5\u51fd\u6570\u63a5\u53d7\u4ee5\u4e0b\u53c2\u6570\uff1a</p> \u53c2\u6570 \u8bf4\u660e <code>model_provider</code> \u6a21\u578b\u63d0\u4f9b\u5546\u540d\u79f0\uff0c\u4f8b\u5982 <code>vllm</code>\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u662f <code>base_url</code> \u6a21\u578b\u63d0\u4f9b\u5546\u7684\u9ed8\u8ba4API\u5730\u5740\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u5426 <code>compatibility_options</code> \u517c\u5bb9\u6027\u9009\u9879\u914d\u7f6e\u3002\u7c7b\u578b: <code>dict</code>\u5fc5\u586b: \u5426 <code>model_profiles</code> \u8be5\u6a21\u578b\u63d0\u4f9b\u5546\u6240\u63d0\u4f9b\u7684\u6a21\u578b\u5bf9\u5e94\u7684profiles\u3002\u7c7b\u578b: <code>dict</code>\u5fc5\u586b: \u5426 <code>chat_model_cls_name</code> \u5bf9\u8bdd\u6a21\u578b\u7c7b\u540d\uff0c\u9ed8\u8ba4\u503c\u4e3a <code>Chat{model_provider}</code>\uff08\u5176\u4e2d <code>{model_provider}</code> \u9996\u5b57\u6bcd\u5927\u5199\uff09\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u5426 <p>\u672c\u5e93\u4f1a\u6839\u636e\u7528\u6237\u4f20\u5165\u7684\u4e0a\u8ff0\u53c2\u6570\uff0c\u4f7f\u7528\u5185\u7f6e <code>BaseChatOpenAICompatible</code> \u7c7b\u6784\u5efa\u5bf9\u5e94\u4e8e\u7279\u5b9a\u63d0\u4f9b\u5546\u7684\u5bf9\u8bdd\u6a21\u578b\u7c7b\u3002\u8be5\u7c7b\u7ee7\u627f\u81ea <code>langchain-openai</code> \u7684 <code>BaseChatOpenAI</code>\uff0c\u5e76\u589e\u5f3a\u4ee5\u4e0b\u80fd\u529b\uff1a</p> <ul> <li>\u652f\u6301\u66f4\u591a\u683c\u5f0f\u7684\u63a8\u7406\u5185\u5bb9\uff1a\u76f8\u8f83\u4e8e <code>ChatOpenAI</code> \u53ea\u80fd\u8f93\u51fa\u5b98\u65b9\u7684\u63a8\u7406\u5185\u5bb9\uff0c\u672c\u7c7b\u8fd8\u652f\u6301\u8f93\u51fa\u66f4\u591a\u683c\u5f0f\u7684\u63a8\u7406\u5185\u5bb9\uff08\u4f8b\u5982 <code>vLLM</code>\uff09\u3002</li> <li>\u652f\u6301 <code>video</code> \u7c7b\u578b content_block\uff1a<code>ChatOpenAI</code> \u65e0\u6cd5\u8f6c\u6362 <code>type=video</code> \u7684 <code>content_block</code>\uff0c\u672c\u5b9e\u73b0\u5df2\u5b8c\u6210\u652f\u6301\u3002</li> <li>\u52a8\u6001\u9002\u914d\u5e76\u9009\u62e9\u6700\u5408\u9002\u7684\u7ed3\u6784\u5316\u8f93\u51fa\u65b9\u6cd5\uff1a\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u80fd\u591f\u6839\u636e\u6a21\u578b\u63d0\u4f9b\u5546\u7684\u5b9e\u9645\u652f\u6301\u60c5\u51b5\uff0c\u81ea\u52a8\u9009\u62e9\u6700\u4f18\u7684\u7ed3\u6784\u5316\u8f93\u51fa\u65b9\u6cd5\uff08<code>function_calling</code> \u6216 <code>json_schema</code>\uff09\u3002</li> <li>\u901a\u8fc7 compatibility_options \u7cbe\u7ec6\u9002\u914d\u5dee\u5f02\uff1a\u901a\u8fc7\u914d\u7f6e\u63d0\u4f9b\u5546\u517c\u5bb9\u6027\u9009\u9879\uff0c\u89e3\u51b3 <code>tool_choice</code>\u3001<code>response_format</code> \u7b49\u53c2\u6570\u7684\u652f\u6301\u5dee\u5f02\u3002</li> </ul> <p>\u6ce8\u610f</p> <p>\u4f7f\u7528\u6b64\u529f\u80fd\u65f6\uff0c\u5fc5\u987b\u5b89\u88c5 standard \u7248\u672c\u7684 <code>langchain-dev-utils</code> \u5e93\u3002\u5177\u4f53\u53ef\u4ee5\u53c2\u8003\u5b89\u88c5\u90e8\u5206\u7684\u4ecb\u7ecd\u3002</p>"},{"location":"zh/adavance-guide/openai-compatible/#_4","title":"\u4ee3\u7801\u793a\u4f8b","text":"<p>\u6211\u4eec\u4ee5\u96c6\u6210 vLLM \u4e3a\u4f8b\uff0c\u5c55\u793a\u5982\u4f55\u4f7f\u7528 <code>create_openai_compatible_model</code> \u51fd\u6570\u521b\u5efa\u5bf9\u8bdd\u6a21\u578b\u96c6\u6210\u7c7b\u3002</p> <p>\u8865\u5145</p> <p>vLLM \u662f\u5e38\u7528\u7684\u5927\u6a21\u578b\u63a8\u7406\u6846\u67b6\uff0c\u5176\u53ef\u4ee5\u5c06\u5927\u6a21\u578b\u90e8\u7f72\u4e3a OpenAI \u517c\u5bb9\u7684 API\uff0c\u4f8b\u5982\u672c\u4f8b\u5b50\u4e2d\u7684 Qwen3-4B\uff1a</p> <p><pre><code>vllm serve Qwen/Qwen3-4B \\\n--reasoning-parser qwen3 \\\n--enable-auto-tool-choice --tool-call-parser hermes \\\n--host 0.0.0.0 --port 8000 \\\n--served-model-name qwen3-4b\n</code></pre> \u670d\u52a1\u5730\u5740\u4e3a <code>http://localhost:8000/v1</code>\u3002</p> <p>\u4f7f\u7528\u5982\u4e0b\u4ee3\u7801\u521b\u5efa\u5bf9\u8bdd\u6a21\u578b\u7c7b\uff1a</p> <pre><code>from langchain_dev_utils.chat_models.adapters import create_openai_compatible_model\n\nChatVLLM = create_openai_compatible_model(\n    model_provider=\"vllm\",\n    base_url=\"http://localhost:8000/v1\",\n    chat_model_cls_name=\"ChatVLLM\",\n)\n\nmodel = ChatVLLM(model=\"qwen3-4b\")\nprint(model.invoke(\"\u4f60\u597d\"))\n</code></pre> <p>\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5728\u521b\u5efa\u5bf9\u8bdd\u6a21\u578b\u7c7b\u65f6\uff0c<code>base_url</code>\u53c2\u6570\u53ef\u4ee5\u7701\u7565\uff0c\u82e5\u672a\u4f20\u5165\uff0c\u672c\u5e93\u4f1a\u9ed8\u8ba4\u8bfb\u53d6\u5bf9\u5e94\u7684\u73af\u5883\u53d8\u91cf\u3002\u4f8b\u5982</p> <pre><code>export VLLM_API_BASE=http://localhost:8000/v1\n</code></pre> <p>\u6b64\u65f6\u4ee3\u7801\u53ef\u4ee5\u7701\u7565 <code>base_url</code> \u53c2\u6570\uff1a</p> <pre><code>from langchain_dev_utils.chat_models.adapters import create_openai_compatible_model\n\nChatVLLM = create_openai_compatible_model(\n    model_provider=\"vllm\",\n    chat_model_cls_name=\"ChatVLLM\",\n)\n\nmodel = ChatVLLM(model=\"qwen3-4b\")\nprint(model.invoke(\"\u4f60\u597d\"))\n</code></pre> <p>\u6ce8\u610f\uff1a\u4e0a\u8ff0\u4ee3\u7801\u80fd\u591f\u6210\u529f\u8fd0\u884c\u7684\u524d\u63d0\u662f\uff0c\u4f60\u8bbe\u7f6e\u4e86<code>VLLM_API_KEY</code>\u7684\u73af\u5883\u53d8\u91cf\u3002\u867d\u7136vLLM\u4e0d\u8981\u6c42\u4f20\u5165API Key\uff0c\u4f46\u662f\u7531\u4e8e\u5bf9\u8bdd\u6a21\u578b\u7c7b\u521d\u59cb\u5316\u65f6\u9700\u8981API Key\u3002</p> <p>\u63d0\u793a</p> <p>\u521b\u5efa\u7684\u5bf9\u8bdd\u6a21\u578b\u7c7b\u73af\u5883\u53d8\u91cf\u7684\u547d\u540d\u89c4\u5219\uff1a</p> <ul> <li> <p>API\u5730\u5740\uff1a<code>${PROVIDER_NAME}_API_BASE</code>\uff08\u5168\u5927\u5199\uff0c\u4e0b\u5212\u7ebf\u5206\u9694\uff09\u3002</p> </li> <li> <p>API Key\uff1a<code>${PROVIDER_NAME}_API_KEY</code>\uff08\u5168\u5927\u5199\uff0c\u4e0b\u5212\u7ebf\u5206\u9694\uff09\u3002</p> </li> </ul>"},{"location":"zh/adavance-guide/openai-compatible/#_5","title":"\u517c\u5bb9\u6027\u53c2\u6570","text":"<p><code>compatibility_options</code> \u662f\u4e00\u4e2a\u5b57\u5178\uff0c\u7528\u4e8e\u58f0\u660e\u8be5\u63d0\u4f9b\u5546\u5bf9 OpenAI API \u7684\u90e8\u5206\u7279\u6027\u7684\u652f\u6301\u60c5\u51b5\uff0c\u4ee5\u63d0\u9ad8\u517c\u5bb9\u6027\u548c\u7a33\u5b9a\u6027\u3002</p> <p>\u76ee\u524d\u652f\u6301\u4ee5\u4e0b\u914d\u7f6e\u9879\uff1a</p> \u914d\u7f6e\u9879 \u8bf4\u660e <code>supported_tool_choice</code> \u652f\u6301\u7684 <code>tool_choice</code> \u7b56\u7565\u5217\u8868\u3002\u7c7b\u578b: <code>list[str]</code>\u9ed8\u8ba4\u503c: <code>[\"auto\"]</code> <code>supported_response_format</code> \u652f\u6301\u7684 <code>response_format</code> \u683c\u5f0f\u5217\u8868(<code>json_schema</code>\u3001<code>json_object</code>)\u3002\u7c7b\u578b: <code>list[str]</code>\u9ed8\u8ba4\u503c: <code>[]</code> <code>reasoning_keep_policy</code> \u5386\u53f2\u6d88\u606f\u4e2d <code>reasoning_content</code> \u5b57\u6bb5\u7684\u4fdd\u7559\u7b56\u7565\u3002\u7c7b\u578b: <code>str</code>\u9ed8\u8ba4\u503c: <code>\"never\"</code> <code>include_usage</code> \u662f\u5426\u5728\u6d41\u5f0f\u8fd4\u56de\u7ed3\u679c\u4e2d\u5305\u542b <code>usage</code> \u4fe1\u606f\u3002\u7c7b\u578b: <code>bool</code>\u9ed8\u8ba4\u503c: <code>True</code> <p>\u8865\u5145</p> <p>\u7531\u4e8e\u540c\u4e00\u6a21\u578b\u63d0\u4f9b\u5546\u7684\u4e0d\u540c\u6a21\u578b\u5bf9 <code>tool_choice</code>\u3001<code>response_format</code> \u7b49\u53c2\u6570\u7684\u652f\u6301\u60c5\u51b5\u5b58\u5728\u5dee\u5f02\uff0c\u8fd9\u56db\u4e2a\u517c\u5bb9\u6027\u9009\u9879\u4e3a\u7c7b\u7684\u5b9e\u4f8b\u5c5e\u6027\u3002\u56e0\u6b64\uff0c\u521b\u5efa\u5bf9\u8bdd\u6a21\u578b\u7c7b\u65f6\u53ef\u4ee5\u4f20\u5165\u503c\u4f5c\u4e3a\u5168\u5c40\u9ed8\u8ba4\u503c\uff08\u4ee3\u8868\u8be5\u63d0\u4f9b\u5546\u5927\u90e8\u5206\u6a21\u578b\u652f\u6301\u7684\u914d\u7f6e\uff09\uff0c\u540e\u7eed\u5982\u9700\u9488\u5bf9\u7279\u5b9a\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u53ef\u5728\u5b9e\u4f8b\u5316\u65f6\u8986\u76d6\u540c\u540d\u53c2\u6570\u3002</p> <p>\u5bf9\u4e8e\u8fd9\u4e9b\u914d\u7f6e\u9879\u7684\u8be6\u7ec6\u4ecb\u7ecd\u5982\u4e0b\uff1a</p> 1. supported_tool_choice <p><code>tool_choice</code> \u7528\u4e8e\u63a7\u5236\u5927\u6a21\u578b\u5728\u54cd\u5e94\u65f6\u662f\u5426\u4ee5\u53ca\u8c03\u7528\u54ea\u4e2a\u5916\u90e8\u5de5\u5177\uff0c\u4ee5\u63d0\u5347\u51c6\u786e\u6027\u3001\u53ef\u9760\u6027\u548c\u53ef\u63a7\u6027\u3002\u5e38\u89c1\u7684\u53d6\u503c\u6709\uff1a</p> <ul> <li><code>\"auto\"</code>\uff1a\u6a21\u578b\u81ea\u4e3b\u51b3\u5b9a\u662f\u5426\u8c03\u7528\u5de5\u5177(\u9ed8\u8ba4\u884c\u4e3a)\uff1b</li> <li><code>\"none\"</code>\uff1a\u7981\u6b62\u8c03\u7528\u5de5\u5177\uff1b</li> <li><code>\"required\"</code>\uff1a\u5f3a\u5236\u8c03\u7528\u81f3\u5c11\u4e00\u4e2a\u5de5\u5177\uff1b</li> <li>\u6307\u5b9a\u5177\u4f53\u5de5\u5177\uff08\u5728 OpenAI \u517c\u5bb9 API \u4e2d\uff0c\u5177\u4f53\u4e3a <code>{\"type\": \"function\", \"function\": {\"name\": \"xxx\"}}</code>\uff09\u3002</li> </ul> <p>\u4e0d\u540c\u63d0\u4f9b\u5546\u652f\u6301\u8303\u56f4\u4e0d\u540c\u3002\u4e3a\u907f\u514d\u9519\u8bef\uff0c\u672c\u5e93\u9ed8\u8ba4\u7684<code>supported_tool_choice</code>\u4e3a<code>[\"auto\"]</code>\uff0c\u5219\u5728<code>bind_tools</code>\u65f6\uff0c<code>tool_choice</code>\u53c2\u6570\u53ea\u80fd\u4f20\u9012<code>auto</code>\uff0c\u5982\u679c\u4f20\u9012\u5176\u5b83\u53d6\u503c\u5747\u4f1a\u88ab\u8fc7\u6ee4\u3002</p> <p>\u82e5\u9700\u652f\u6301\u4f20\u9012\u5176\u5b83<code>tool_choice</code>\u53d6\u503c\uff0c\u5fc5\u987b\u914d\u7f6e\u652f\u6301\u9879\u3002\u914d\u7f6e\u503c\u4e3a\u5b57\u7b26\u4e32\u5217\u8868\uff0c\u6bcf\u4e2a\u5b57\u7b26\u4e32\u7684\u53ef\u9009\u503c\uff1a</p> <ul> <li><code>\"auto\"</code>, <code>\"none\"</code>, <code>\"required\"</code>\uff1a\u5bf9\u5e94\u6807\u51c6\u7b56\u7565\uff1b</li> <li><code>\"specific\"</code>\uff1a\u672c\u5e93\u7279\u6709\u6807\u8bc6\uff0c\u8868\u793a\u652f\u6301\u6307\u5b9a\u5177\u4f53\u5de5\u5177\u3002</li> </ul> <p>\u4f8b\u5982 vLLM \u652f\u6301\u5168\u90e8\u7b56\u7565\uff1a</p> <pre><code>from langchain_dev_utils.chat_models.adapters import create_openai_compatible_model\n\nChatVLLM = create_openai_compatible_model(\n    model_provider=\"vllm\",\n    chat_model_cls_name=\"ChatVLLM\",\n    compatibility_options={\n        \"supported_tool_choice\": [\"auto\", \"required\", \"none\", \"specific\"]\n    },\n)\n\nmodel = ChatVLLM(model=\"qwen3-4b\")\n</code></pre> <p>\u63d0\u793a</p> <p>\u5982\u65e0\u7279\u6b8a\u9700\u6c42\uff0c\u53ef\u4fdd\u6301\u9ed8\u8ba4\uff08\u5373<code>[\"auto\"]</code>\uff09\u3002\u82e5\u4e1a\u52a1\u573a\u666f\u8981\u6c42\u6a21\u578b\u5fc5\u987b\u8c03\u7528\u7279\u5b9a\u5de5\u5177\u6216\u4ece\u7ed9\u5b9a\u5217\u8868\u4e2d\u4efb\u9009\u5176\u4e00\uff0c\u4e14\u6a21\u578b\u63d0\u4f9b\u5546\u652f\u6301\u5bf9\u5e94\u7b56\u7565\uff0c\u518d\u6309\u9700\u5f00\u542f\uff1a</p> <ol> <li> <p>\u5982\u679c\u8981\u6c42\u81f3\u5c11\u8c03\u7528\u4e00\u4e2a\u5de5\u5177\uff0c\u4e14\u6a21\u578b\u63d0\u4f9b\u5546\u652f\u6301<code>required</code>\uff0c\u5219\u53ef\u4ee5\u8bbe\u4e3a <code>[\"required\"]</code>  \uff08\u540c\u65f6\u5728\u8c03\u7528<code>bind_tools</code>\u65f6\uff0c\u9700\u8981\u663e\u793a\u4f20\u9012<code>tool_choice=\"required\"</code>\uff09</p> </li> <li> <p>\u5982\u679c\u8981\u6c42\u8c03\u7528\u6307\u5b9a\u5de5\u5177\uff0c\u4e14\u6a21\u578b\u63d0\u4f9b\u5546\u652f\u6301\u6307\u5b9a\u67d0\u4e2a\u5177\u4f53\u7684\u5de5\u5177\u8c03\u7528\uff0c\u5219\u53ef\u4ee5\u8bbe\u4e3a <code>[\"specific\"]</code>\uff08\u5728 <code>function_calling</code> \u7ed3\u6784\u5316\u8f93\u51fa\u4e2d\uff0c\u6b64\u914d\u7f6e\u975e\u5e38\u6709\u7528\uff0c\u53ef\u4ee5\u786e\u4fdd\u6a21\u578b\u8c03\u7528\u6307\u5b9a\u7684\u7ed3\u6784\u5316\u8f93\u51fa\u5de5\u5177\uff0c\u4ee5\u4fdd\u8bc1\u7ed3\u6784\u5316\u8f93\u51fa\u7684\u7a33\u5b9a\u6027\u3002\u56e0\u4e3a\u5728 <code>with_structured_output</code> \u65b9\u6cd5\u4e2d\uff0c\u5176\u5185\u90e8\u5b9e\u73b0\u4f1a\u5728\u8c03\u7528<code>bind_tools</code> \u65f6\u4f20\u5165\u80fd\u591f\u5f3a\u5236\u8c03\u7528\u6307\u5b9a\u5de5\u5177\u7684 <code>tool_choice</code> \u53d6\u503c\uff0c\u4f46\u5982\u679c <code>supported_tool_choice</code> \u4e2d\u6ca1\u6709 <code>\"specific\"</code>\uff0c\u8be5\u53c2\u6570\u5c06\u4f1a\u88ab\u8fc7\u6ee4\u3002\u6545\u5982\u679c\u60f3\u8981\u4fdd\u8bc1\u80fd\u591f\u6b63\u5e38\u4f20\u5165 <code>tool_choice</code>\uff0c\u5fc5\u987b\u5728 <code>supported_tool_choice</code> \u4e2d\u6dfb\u52a0 <code>\"specific\"</code>\u3002\uff09</p> </li> </ol> <p>\u8be5\u53c2\u6570\u65e2\u53ef\u5728\u521b\u5efa\u65f6\u7edf\u4e00\u8bbe\u7f6e\uff0c\u4e5f\u53ef\u5728\u5b9e\u4f8b\u5316\u65f6\u9488\u5bf9\u5355\u6a21\u578b\u52a8\u6001\u8986\u76d6\uff1b\u63a8\u8350\u5728\u521b\u5efa\u65f6\u4e00\u6b21\u6027\u58f0\u660e\u8be5\u63d0\u4f9b\u5546\u7684\u5927\u591a\u6570\u6a21\u578b\u7684<code>tool_choice</code>\u652f\u6301\u60c5\u51b5\uff0c\u800c\u5bf9\u4e8e\u90e8\u5206\u652f\u6301\u60c5\u51b5\u4e0d\u540c\u7684\u6a21\u578b\uff0c\u5219\u5728\u5b9e\u4f8b\u5316\u65f6\u5355\u72ec\u6307\u5b9a\u3002</p> 2. supported_response_format <p>\u76ee\u524d\u5e38\u89c1\u7684\u7ed3\u6784\u5316\u8f93\u51fa\u65b9\u6cd5\u6709\u4e09\u79cd\u3002</p> <ul> <li><code>function_calling</code>\uff1a\u901a\u8fc7\u8c03\u7528\u4e00\u4e2a\u7b26\u5408\u6307\u5b9a schema \u7684\u5de5\u5177\u6765\u751f\u6210\u7ed3\u6784\u5316\u8f93\u51fa\u3002</li> <li><code>json_schema</code>\uff1a\u7531\u6a21\u578b\u63d0\u4f9b\u5546\u63d0\u4f9b\u7684\u4e13\u95e8\u7528\u4e8e\u751f\u6210\u7ed3\u6784\u5316\u8f93\u51fa\u7684\u529f\u80fd\uff0c\u5728OpenAI\u517c\u5bb9API\u4e2d\uff0c\u5177\u4f53\u4e3a<code>response_format={\"type\": \"json_schema\", \"json_schema\": {...}}</code>\u3002</li> <li><code>json_mode</code>\uff1a\u662f\u67d0\u4e9b\u63d0\u4f9b\u5546\u5728\u63a8\u51fa<code>json_schema</code>\u4e4b\u524d\u63d0\u4f9b\u7684\u4e00\u79cd\u529f\u80fd\uff0c\u5b83\u80fd\u751f\u6210\u6709\u6548\u7684 JSON\uff0c\u4f46 schema \u5fc5\u987b\u5728\u63d0\u793a\uff08prompt\uff09\u4e2d\u8fdb\u884c\u63cf\u8ff0\u3002\u5728 OpenAI \u517c\u5bb9 API \u4e2d\uff0c\u5177\u4f53\u4e3a <code>response_format={\"type\": \"json_object\"}</code>\uff09\u3002</li> </ul> <p>\u5176\u4e2d\uff0c<code>json_schema</code> \u4ec5\u5c11\u6570 OpenAI \u517c\u5bb9 API \u63d0\u4f9b\u5546\u652f\u6301\uff08\u5982 <code>OpenRouter</code>\u3001<code>TogetherAI</code>\uff09\uff1b<code>json_mode</code> \u652f\u6301\u5ea6\u66f4\u9ad8\uff0c\u591a\u6570\u63d0\u4f9b\u5546\u5df2\u517c\u5bb9\uff1b\u800c <code>function_calling</code> \u6700\u4e3a\u901a\u7528\uff0c\u53ea\u8981\u6a21\u578b\u652f\u6301\u5de5\u5177\u8c03\u7528\u5373\u53ef\u4f7f\u7528\u3002</p> <p>\u672c\u53c2\u6570\u7528\u4e8e\u58f0\u660e\u6a21\u578b\u63d0\u4f9b\u5546\u5bf9\u4e8e<code>response_format</code>\u7684\u652f\u6301\u60c5\u51b5\u3002\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u4e3a<code>[]</code>\uff0c\u4ee3\u8868\u6a21\u578b\u63d0\u4f9b\u5546\u65e2\u4e0d\u652f\u6301<code>json_mode</code>\u4e5f\u4e0d\u652f\u6301<code>json_schema</code>\u3002\u6b64\u65f6<code>with_structured_output</code>\u65b9\u6cd5\u4e2d\u7684<code>method</code>\u53c2\u6570\u53ea\u80fd\u4f20\u9012<code>function_calling</code>\uff0c\u5982\u679c\u4f20\u9012\u4e86<code>json_mode</code>\u6216<code>json_schema</code>\uff0c\u5219\u4f1a\u81ea\u52a8\u88ab\u8f6c\u5316\u4e3a<code>function_calling</code>\u3002\u5982\u679c\u60f3\u8981\u542f\u7528<code>json_mode</code>\u6216\u8005<code>json_schema</code>\u7684\u7ed3\u6784\u5316\u8f93\u51fa\u5b9e\u73b0\u65b9\u5f0f\uff0c\u5219\u9700\u8981\u663e\u5f0f\u8bbe\u7f6e\u8be5\u53c2\u6570\u3002</p> <p>\u4f8b\u5982vLLM\u90e8\u7f72\u7684\u6a21\u578b\u652f\u6301<code>json_schema</code>\u7684\u7ed3\u6784\u5316\u8f93\u51fa\u65b9\u6cd5\uff0c\u5219\u53ef\u4ee5\u6ce8\u518c\u7684\u65f6\u5019\u8fdb\u884c\u58f0\u660e\uff1a</p> <pre><code>from langchain_dev_utils.chat_models.adapters import create_openai_compatible_model\n\nChatVLLM = create_openai_compatible_model(\n    model_provider=\"vllm\",\n    chat_model_cls_name=\"ChatVLLM\",\n    compatibility_options={\"supported_response_format\": [\"json_schema\"]},\n)\n\nmodel = ChatVLLM(model=\"qwen3-4b\")\n</code></pre> <p>\u63d0\u793a</p> <p>\u901a\u5e38\u4e00\u822c\u60c5\u51b5\u4e0b\u4e5f\u65e0\u9700\u914d\u7f6e\u3002\u4ec5\u5728\u9700\u8981\u4f7f\u7528<code>with_structured_output</code>\u65b9\u6cd5\u65f6\u9700\u8981\u8003\u8651\u8fdb\u884c\u914d\u7f6e\uff0c\u6b64\u65f6\uff0c\u5982\u679c\u6a21\u578b\u63d0\u4f9b\u5546\u652f\u6301<code>json_schema</code>\uff0c\u5219\u53ef\u4ee5\u8003\u8651\u914d\u7f6e\u672c\u53c2\u6570\uff08\u56e0\u4e3a<code>json_schema</code>\u7684\u7ed3\u6784\u5316\u8f93\u51fa\u7a33\u5b9a\u6027\u8981\u4f18\u4e8e<code>function_calling</code>\uff09\u3002\u4ee5\u4fdd\u8bc1\u7ed3\u6784\u5316\u8f93\u51fa\u7684\u7a33\u5b9a\u6027\u3002\u5bf9\u4e8e<code>json_mode</code>\uff0c\u56e0\u4e3a\u5176\u53ea\u80fd\u4fdd\u8bc1\u8f93\u51faJSON\uff0c\u56e0\u6b64\u4e00\u822c\u6ca1\u6709\u5fc5\u8981\u8bbe\u7f6e\u3002\u4ec5\u5f53\u6a21\u578b\u4e0d\u652f\u6301\u5de5\u5177\u8c03\u7528\u4e14\u4ec5\u652f\u6301\u8bbe\u7f6e<code>response_format={\"type\":\"json_object\"}</code>\u65f6\uff0c\u624d\u9700\u8981\u914d\u7f6e\u672c\u53c2\u6570\u5305\u542b<code>json_mode</code>\u3002</p> <p>\u540c\u6837\uff0c\u8be5\u53c2\u6570\u65e2\u53ef\u5728\u521b\u5efa\u65f6\u7edf\u4e00\u8bbe\u7f6e\uff0c\u4e5f\u53ef\u5728\u5b9e\u4f8b\u5316\u65f6\u9488\u5bf9\u5355\u6a21\u578b\u52a8\u6001\u8986\u76d6\uff1b\u63a8\u8350\u5728\u521b\u5efa\u65f6\u4e00\u6b21\u6027\u58f0\u660e\u8be5\u63d0\u4f9b\u5546\u7684\u5927\u591a\u6570\u6a21\u578b\u7684<code>response_format</code>\u652f\u6301\u60c5\u51b5\uff0c\u800c\u5bf9\u4e8e\u90e8\u5206\u652f\u6301\u60c5\u51b5\u4e0d\u540c\u7684\u6a21\u578b\uff0c\u5219\u5728\u5b9e\u4f8b\u5316\u65f6\u5355\u72ec\u6307\u5b9a\u3002</p> <p>\u6ce8\u610f</p> <p>\u672c\u53c2\u6570\u76ee\u524d\u4ec5\u5f71\u54cd<code>model.with_structured_output</code>\u65b9\u6cd5\u3002\u5bf9\u4e8e<code>create_agent</code>\u4e2d\u7684\u7ed3\u6784\u5316\u8f93\u51fa\uff0c\u82e5\u9700\u8981\u4f7f\u7528<code>json_schema</code>\u7684\u5b9e\u73b0\u65b9\u5f0f\uff0c\u4f60\u9700\u8981\u786e\u4fdd\u5bf9\u5e94\u6a21\u578b\u7684<code>profile</code>\u4e2d\u5305\u542b<code>structured_output</code>\u5b57\u6bb5\uff0c\u4e14\u503c\u4e3a<code>True</code>\u3002</p> 3. reasoning_keep_policy <p>\u7528\u4e8e\u63a7\u5236\u5386\u53f2\u6d88\u606f\uff08messages\uff09\u4e2d<code>reasoning_content</code> \u5b57\u6bb5\u7684\u4fdd\u7559\u7b56\u7565\uff0c\u4e3b\u8981\u9002\u914d\u4e8e\u4e0d\u540c\u6a21\u578b\u63d0\u4f9b\u5546\u7684\u6a21\u578b\u7684\u4e0d\u540c\u7684\u601d\u8003\u6a21\u5f0f\u3002</p> <p>\u652f\u6301\u4ee5\u4e0b\u53d6\u503c\uff1a</p> <ul> <li> <p><code>never</code>\uff1a\u5728\u5386\u53f2\u6d88\u606f\u4e2d\u4e0d\u4fdd\u7559\u4efb\u4f55\u63a8\u7406\u5185\u5bb9\uff08\u9ed8\u8ba4)\uff1b</p> </li> <li> <p><code>current</code>\uff1a\u4ec5\u4fdd\u7559\u5f53\u524d\u5bf9\u8bdd\u4e2d\u7684 <code>reasoning_content</code> \u5b57\u6bb5\uff1b</p> </li> <li> <p><code>all</code>\uff1a\u4fdd\u7559\u6240\u6709\u5bf9\u8bdd\u4e2d\u7684 <code>reasoning_content</code> \u5b57\u6bb5\u3002</p> </li> </ul> <p>\u4f8b\u5982\uff1a \u4f8b\u5982\uff0c\u7528\u6237\u5148\u63d0\u95ee\u201c\u7ebd\u7ea6\u5929\u6c14\u5982\u4f55\uff1f\u201d\uff0c\u968f\u540e\u8ffd\u95ee\u201c\u4f26\u6566\u5929\u6c14\u5982\u4f55\uff1f\u201d\uff0c\u5f53\u524d\u6b63\u8981\u8fdb\u884c\u7b2c\u4e8c\u8f6e\u5bf9\u8bdd\uff0c\u4e14\u5373\u5c06\u8fdb\u884c\u6700\u540e\u4e00\u6b21\u6a21\u578b\u8c03\u7528\u3002</p> <ul> <li>\u53d6\u503c\u4e3a<code>never</code>\u65f6</li> </ul> <p>\u5f53\u53d6\u503c\u4e3a<code>never</code>\u65f6\uff0c\u5219\u6700\u7ec8\u4f20\u9012\u7ed9\u6a21\u578b\u7684 messages \u4e2d\u4e0d\u4f1a\u6709\u4efb\u4f55\u7684 <code>reasoning_content</code> \u5b57\u6bb5\u3002\u6700\u7ec8\u6a21\u578b\u6536\u5230\u7684 messages \u4e3a\uff1a</p> <pre><code>messages = [\n    {\"content\": \"\u67e5\u7ebd\u7ea6\u5929\u6c14\u5982\u4f55\uff1f\", \"role\": \"user\"},\n    {\"content\": \"\", \"role\": \"assistant\", \"tool_calls\": [...]},\n    {\"content\": \"\u591a\u4e91 7~13\u00b0C\", \"role\": \"tool\", \"tool_call_id\": \"...\"},\n    {\"content\": \"\u7ebd\u7ea6\u4eca\u5929\u5929\u6c14\u4e3a\u591a\u4e91\uff0c7~13\u00b0C\u3002\", \"role\": \"assistant\"},\n    {\"content\": \"\u67e5\u4f26\u6566\u5929\u6c14\u5982\u4f55\uff1f\", \"role\": \"user\"},\n    {\"content\": \"\", \"role\": \"assistant\", \"tool_calls\": [...]},\n    {\"content\": \"\u96e8\u5929\uff0c14~20\u00b0C\", \"role\": \"tool\", \"tool_call_id\": \"...\"},\n]\n</code></pre> <ul> <li>\u53d6\u503c\u4e3a<code>current</code>\u65f6</li> </ul> <p>\u5f53\u53d6\u503c\u4e3a<code>current</code>\u65f6\uff0c\u4ec5\u4fdd\u7559\u5f53\u524d\u5bf9\u8bdd\u4e2d\u7684 <code>reasoning_content</code> \u5b57\u6bb5\u3002\u6700\u7ec8\u6a21\u578b\u6536\u5230\u7684 messages \u4e3a\uff1a <pre><code>messages = [\n    {\"content\": \"\u67e5\u7ebd\u7ea6\u5929\u6c14\u5982\u4f55\uff1f\", \"role\": \"user\"},\n    {\"content\": \"\", \"role\": \"assistant\", \"tool_calls\": [...]},\n    {\"content\": \"\u591a\u4e91 7~13\u00b0C\", \"role\": \"tool\", \"tool_call_id\": \"...\"},\n    {\"content\": \"\u7ebd\u7ea6\u4eca\u5929\u5929\u6c14\u4e3a\u591a\u4e91\uff0c7~13\u00b0C\u3002\", \"role\": \"assistant\"},\n    {\"content\": \"\u67e5\u4f26\u6566\u5929\u6c14\u5982\u4f55\uff1f\", \"role\": \"user\"},\n    {\n        \"content\": \"\",\n        \"reasoning_content\": \"\u67e5\u4f26\u6566\u5929\u6c14\uff0c\u9700\u8981\u76f4\u63a5\u8c03\u7528\u5929\u6c14\u5de5\u5177\u3002\",  # \u4ec5\u4fdd\u7559\u672c\u8f6e\u5bf9\u8bdd\u7684 reasoning_content\n        \"role\": \"assistant\",\n        \"tool_calls\": [...],\n    },\n    {\"content\": \"\u96e8\u5929\uff0c14~20\u00b0C\", \"role\": \"tool\", \"tool_call_id\": \"...\"},\n]\n</code></pre></p> <ul> <li>\u53d6\u503c\u4e3a<code>all</code>\u65f6</li> </ul> <p>\u5f53\u53d6\u503c\u4e3a<code>all</code>\u65f6\uff0c\u4fdd\u7559\u6240\u6709\u5bf9\u8bdd\u4e2d\u7684 <code>reasoning_content</code> \u5b57\u6bb5\u3002\u6700\u7ec8\u6a21\u578b\u6536\u5230\u7684 messages \u4e3a\uff1a <pre><code>messages = [\n    {\"content\": \"\u67e5\u7ebd\u7ea6\u5929\u6c14\u5982\u4f55\uff1f\", \"role\": \"user\"},\n    {\n        \"content\": \"\",\n        \"reasoning_content\": \"\u67e5\u7ebd\u7ea6\u5929\u6c14\uff0c\u9700\u8981\u76f4\u63a5\u8c03\u7528\u5929\u6c14\u5de5\u5177\u3002\",  # \u4fdd\u7559 reasoning_content\n        \"role\": \"assistant\",\n        \"tool_calls\": [...],\n    },\n    {\"content\": \"\u591a\u4e91 7~13\u00b0C\", \"role\": \"tool\", \"tool_call_id\": \"...\"},\n    {\n        \"content\": \"\u7ebd\u7ea6\u4eca\u5929\u5929\u6c14\u4e3a\u591a\u4e91\uff0c7~13\u00b0C\u3002\",\n        \"reasoning_content\": \"\u76f4\u63a5\u8fd4\u56de\u7ebd\u7ea6\u5929\u6c14\u7ed3\u679c\u3002\",  # \u4fdd\u7559 reasoning_content\n        \"role\": \"assistant\",\n    },\n    {\"content\": \"\u67e5\u4f26\u6566\u5929\u6c14\u5982\u4f55\uff1f\", \"role\": \"user\"},\n    {\n        \"content\": \"\",\n        \"reasoning_content\": \"\u67e5\u4f26\u6566\u5929\u6c14\uff0c\u9700\u8981\u76f4\u63a5\u8c03\u7528\u5929\u6c14\u5de5\u5177\u3002\",  # \u4fdd\u7559 reasoning_content\n        \"role\": \"assistant\",\n        \"tool_calls\": [...],\n    },\n    {\"content\": \"\u96e8\u5929\uff0c14~20\u00b0C\", \"role\": \"tool\", \"tool_call_id\": \"...\"},\n]\n</code></pre></p> <p>\u6ce8\u610f\uff1a\u5982\u679c\u672c\u8f6e\u5bf9\u8bdd\u4e0d\u6d89\u53ca\u5de5\u5177\u8c03\u7528\uff0c\u5219<code>current</code>\u6548\u679c\u548c<code>never</code>\u6548\u679c\u76f8\u540c\u3002</p> <p>\u63d0\u793a</p> <p>\u6839\u636e\u6a21\u578b\u63d0\u4f9b\u5546\u5bf9 <code>reasoning_content</code> \u7684\u4fdd\u7559\u8981\u6c42\u7075\u6d3b\u914d\u7f6e\uff1a</p> <ul> <li>\u82e5\u63d0\u4f9b\u5546\u8981\u6c42\u5168\u7a0b\u4fdd\u7559\u63a8\u7406\u5185\u5bb9\uff0c\u8bbe\u4e3a <code>all</code>\uff1b  </li> <li>\u82e5\u4ec5\u8981\u6c42\u5728\u672c\u8f6e\u5de5\u5177\u8c03\u7528\u4e2d\u4fdd\u7559\uff0c\u8bbe\u4e3a <code>current</code>\uff1b  </li> <li>\u82e5\u65e0\u7279\u6b8a\u8981\u6c42\uff0c\u4fdd\u6301\u9ed8\u8ba4 <code>never</code> \u5373\u53ef\u3002  </li> </ul> <p>\u540c\u6837\uff0c\u8be5\u53c2\u6570\u65e2\u53ef\u5728\u521b\u5efa\u65f6\u7edf\u4e00\u8bbe\u7f6e\uff0c\u4e5f\u53ef\u5728\u5b9e\u4f8b\u5316\u65f6\u9488\u5bf9\u5355\u6a21\u578b\u52a8\u6001\u8986\u76d6\uff1b\u4e00\u822c\u63a8\u8350\u5728\u5b9e\u4f8b\u5316\u65f6\u5355\u72ec\u6307\u5b9a\uff0c\u6b64\u65f6\u521b\u5efa\u65f6\u65e0\u9700\u8bbe\u7f6e\u3002</p> 4. include_usage <p><code>include_usage</code> \u662f OpenAI \u517c\u5bb9 API \u4e2d\u7684\u4e00\u4e2a\u53c2\u6570\uff0c\u7528\u4e8e\u63a7\u5236\u662f\u5426\u5728\u6d41\u5f0f\u54cd\u5e94\u7684\u672b\u5c3e\u9644\u52a0\u4e00\u6761\u5305\u542b token \u4f7f\u7528\u60c5\u51b5\uff08\u5982 <code>prompt_tokens</code> \u548c <code>completion_tokens</code>\uff09\u7684\u6d88\u606f\u3002\u7531\u4e8e\u6807\u51c6\u6d41\u5f0f\u54cd\u5e94\u9ed8\u8ba4\u4e0d\u8fd4\u56de\u7528\u91cf\u4fe1\u606f\uff0c\u542f\u7528\u8be5\u9009\u9879\u540e\uff0c\u5ba2\u6237\u7aef\u53ef\u76f4\u63a5\u83b7\u53d6\u5b8c\u6574\u7684 token \u6d88\u8017\u6570\u636e\uff0c\u4fbf\u4e8e\u8ba1\u8d39\u3001\u76d1\u63a7\u6216\u65e5\u5fd7\u8bb0\u5f55\u3002</p> <p>\u901a\u5e38\u901a\u8fc7 <code>stream_options={\"include_usage\": true}</code> \u542f\u7528\u3002\u8003\u8651\u5230\u6709\u4e9b\u6a21\u578b\u63d0\u4f9b\u5546\u4e0d\u652f\u6301\u8be5\u53c2\u6570\uff0c\u56e0\u6b64\u672c\u5e93\u5c06\u5176\u8bbe\u4e3a\u517c\u5bb9\u6027\u9009\u9879\uff0c\u9ed8\u8ba4\u503c\u4e3a <code>True</code>\uff0c\u56e0\u4e3a\u7edd\u5927\u591a\u6570\u6a21\u578b\u63d0\u4f9b\u5546\u5747\u652f\u6301\u8be5\u53c2\u6570\uff0c\u5982\u679c\u4e0d\u652f\u6301\uff0c\u5219\u53ef\u4ee5\u663e\u5f0f\u8bbe\u4e3a <code>False</code>\u3002</p> <p>\u63d0\u793a</p> <p>\u6b64\u53c2\u6570\u4e00\u822c\u65e0\u9700\u8bbe\u7f6e\uff0c\u4fdd\u6301\u9ed8\u8ba4\u503c\u5373\u53ef\u3002\u53ea\u6709\u5728\u6a21\u578b\u63d0\u4f9b\u5546\u4e0d\u652f\u6301\u65f6\uff0c\u624d\u9700\u8981\u8bbe\u7f6e\u4e3a <code>False</code>\u3002</p>"},{"location":"zh/adavance-guide/openai-compatible/#model_profiles","title":"model_profiles\u53c2\u6570\u8bbe\u7f6e","text":"<p>\u5982\u679c\u8981\u4f7f\u7528 <code>model.profile</code> \u53c2\u6570\uff0c\u5219\u5fc5\u987b\u5728\u521b\u5efa\u65f6\u663e\u5f0f\u4f20\u5165\u3002</p> <p>\u4f8b\u5982\uff1a</p> <pre><code>from langchain_dev_utils.chat_models.adapters import create_openai_compatible_model\n\nmodel_profiles = {\n    \"qwen3-4b\": {\n        \"max_input_tokens\": 131072,\n        \"max_output_tokens\": 8192,\n        \"image_inputs\": False,\n        \"audio_inputs\": False,\n        \"video_inputs\": False,\n        \"image_outputs\": False,\n        \"audio_outputs\": False,\n        \"video_outputs\": False,\n        \"reasoning_output\": True,\n        \"tool_calling\": True,\n    }\n    # \u6b64\u5904\u8fd8\u53ef\u4ee5\u5199\u66f4\u591a\u7684model profile\n}\n\nChatVLLM = create_openai_compatible_model(\n    model_provider=\"vllm\",\n    base_url=\"http://localhost:8000/v1\",\n    model_profiles=model_profiles,\n)\n\nmodel = ChatVLLM(\n    model=\"qwen3-4b\",\n)\nprint(model.profile)\n</code></pre> <p>\u6ce8\u610f</p> <p>\u5c3d\u7ba1\u5df2\u63d0\u4f9b\u4e0a\u8ff0\u517c\u5bb9\u6027\u914d\u7f6e\uff0c\u672c\u5e93\u4ecd\u65e0\u6cd5\u4fdd\u8bc1 100% \u9002\u914d\u6240\u6709 OpenAI \u517c\u5bb9\u63a5\u53e3\u3002\u82e5\u6a21\u578b\u63d0\u4f9b\u5546\u5df2\u6709\u5b98\u65b9\u6216\u793e\u533a\u96c6\u6210\u7c7b\uff0c\u8bf7\u4f18\u5148\u91c7\u7528\u8be5\u96c6\u6210\u7c7b\u3002\u5982\u9047\u5230\u4efb\u4f55\u517c\u5bb9\u6027\u95ee\u9898\uff0c\u6b22\u8fce\u5728\u672c\u5e93 GitHub \u4ed3\u5e93\u63d0\u4ea4 issue\u3002</p>"},{"location":"zh/adavance-guide/openai-compatible/#_6","title":"\u521b\u5efa\u5d4c\u5165\u6a21\u578b\u7c7b","text":"<p>\u4e0e\u5bf9\u8bdd\u6a21\u578b\u7c7b\u7c7b\u4f3c\uff0c\u53ef\u4ee5\u4f7f\u7528<code>create_openai_compatible_embedding</code>\u6765\u521b\u5efa\u5d4c\u5165\u6a21\u578b\u7c7b\u3002</p>"},{"location":"zh/adavance-guide/openai-compatible/#_7","title":"\u793a\u4f8b\u4ee3\u7801","text":"<p>\u540c\u6837\uff0c\u6211\u4eec\u4f7f\u7528<code>create_openai_compatible_embedding</code>\u6765\u96c6\u6210vLLM\u7684\u5d4c\u5165\u6a21\u578b\u3002</p> <p>\u8865\u5145</p> <p>vLLM \u53ef\u90e8\u7f72\u5d4c\u5165\u6a21\u578b\u5e76\u66b4\u9732 OpenAI \u517c\u5bb9\u63a5\u53e3\uff0c\u4f8b\u5982\uff1a</p> <pre><code>vllm serve Qwen/Qwen3-Embedding-4B \\\n--task embed \\\n--served-model-name qwen3-embedding-4b \\\n--host 0.0.0.0 --port 8000\n</code></pre> <p>\u670d\u52a1\u5730\u5740\u4e3a <code>http://localhost:8000/v1</code>\u3002</p> <pre><code>from langchain_dev_utils.embeddings.adapters import create_openai_compatible_embedding\n\nVLLMEmbedding = create_openai_compatible_embedding(\n    embedding_provider=\"vllm\",\n    base_url=\"http://localhost:8000/v1\",\n    embedding_model_cls_name=\"VLLMEmbedding\",\n)\n\nembedding = VLLMEmbedding(model=\"qwen3-embedding-8b\")\n\nprint(embedding.embed_query(\"\u4f60\u597d\"))\n</code></pre> <p>\u540c\u6837\uff0c<code>base_url</code>\u53ef\u4ee5\u7701\u7565\uff0c\u6b64\u65f6\u9700\u8981\u8bbe\u7f6e\u73af\u5883\u53d8\u91cf <code>VLLM_API_BASE</code>\u3002</p> <pre><code>export VLLM_API_BASE=\"http://localhost:8000/v1\"\n</code></pre> <p>\u4ee3\u7801\u5904\u53ef\u4ee5\u7701\u7565<code>base_url</code>\u3002</p> <pre><code>from langchain_dev_utils.embeddings.adapters import create_openai_compatible_embedding\n\nVLLMEmbedding = create_openai_compatible_embedding(\n    embedding_provider=\"vllm\",\n    embedding_model_cls_name=\"VLLMEmbedding\",\n)\n\nembedding = VLLMEmbedding(model=\"qwen3-embedding-8b\")\n\nprint(embedding.embed_query(\"\u4f60\u597d\"))\n</code></pre>"},{"location":"zh/adavance-guide/openai-compatible/#_8","title":"\u96c6\u6210\u7c7b\u7684\u4f7f\u7528","text":""},{"location":"zh/adavance-guide/openai-compatible/#_9","title":"\u5bf9\u8bdd\u6a21\u578b\u7c7b\u7684\u4f7f\u7528","text":"<p>\u9996\u5148\uff0c\u6211\u4eec\u9700\u8981\u521b\u5efa\u5bf9\u8bdd\u6a21\u578b\u7c7b\u3002\u6211\u4eec\u4f7f\u7528\u4e4b\u524d\u521b\u5efa\u597d\u7684<code>ChatVLLM</code>\u7c7b\u3002</p> <ul> <li>\u652f\u6301<code>invoke</code>\u3001<code>ainvoke</code>\u3001<code>stream</code>\u3001<code>astream</code>\u7b49\u65b9\u6cd5\u3002</li> </ul> \u666e\u901a\u8c03\u7528 <p>\u652f\u6301<code>invoke</code>\u8fdb\u884c\u7b80\u5355\u7684\u8c03\u7528\uff1a</p> <pre><code>from langchain_core.messages import HumanMessage\n\nmodel = ChatVLLM(\"qwen3-4b\")\nresponse = model.invoke([HumanMessage(\"Hello\")])\nprint(response)\n</code></pre> <p>\u540c\u65f6\u4e5f\u652f\u6301<code>ainvoke</code>\u8fdb\u884c\u5f02\u6b65\u8c03\u7528\uff1a</p> <pre><code>from langchain_core.messages import HumanMessage\n\nmodel = ChatVLLM(\"qwen3-4b\")\nresponse = await model.ainvoke([HumanMessage(\"Hello\")])\nprint(response)\n</code></pre> \u6d41\u5f0f\u8f93\u51fa <p>\u652f\u6301<code>stream</code>\u8fdb\u884c\u6d41\u5f0f\u8f93\u51fa\uff1a</p> <pre><code>from langchain_core.messages import HumanMessage\n\nmodel = ChatVLLM(\"qwen3-4b\")\nfor chunk in model.stream([HumanMessage(\"Hello\")]):\n    print(chunk)\n</code></pre> <p>\u4ee5\u53ca<code>astream</code>\u8fdb\u884c\u5f02\u6b65\u6d41\u5f0f\u8c03\u7528\uff1a</p> <pre><code>from langchain_core.messages import HumanMessage\n\nmodel = ChatVLLM(\"qwen3-4b\")\nasync for chunk in model.astream([HumanMessage(\"Hello\")]):\n    print(chunk)\n</code></pre> <ul> <li>\u652f\u6301<code>bind_tools</code>\u65b9\u6cd5\uff0c\u8fdb\u884c\u5de5\u5177\u8c03\u7528\u3002</li> </ul> <p>\u5982\u679c\u6a21\u578b\u672c\u8eab\u652f\u6301\u5de5\u5177\u8c03\u7528\uff0c\u90a3\u4e48\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528<code>bind_tools</code>\u65b9\u6cd5\u8fdb\u884c\u5de5\u5177\u8c03\u7528\uff1a</p> \u5de5\u5177\u8c03\u7528 <pre><code>from langchain_core.messages import HumanMessage\nfrom langchain_core.tools import tool\nimport datetime\n\n@tool\ndef get_current_time() -&gt; str:\n    \"\"\"\u83b7\u53d6\u5f53\u524d\u65f6\u95f4\u6233\"\"\"\n    return str(datetime.datetime.now().timestamp())\n\nmodel = ChatVLLM(\"qwen3-4b\").bind_tools([get_current_time])\nresponse = model.invoke([HumanMessage(\"\u83b7\u53d6\u5f53\u524d\u65f6\u95f4\u6233\")])\nprint(response)\n</code></pre> <ul> <li>\u652f\u6301<code>with_structured_output</code>\u65b9\u6cd5\uff0c\u8fdb\u884c\u7ed3\u6784\u5316\u8f93\u51fa\u3002</li> </ul> <p>\u5982\u679c\u8be5\u6a21\u578b\u7c7b\u7684<code>supported_response_format</code>\u53c2\u6570\u4e2d\u5305\u542b<code>json_schema</code>\uff0c\u5219<code>with_structured_output</code> \u4f18\u5148\u4f7f\u7528 <code>json_schema</code>\u8fdb\u884c\u7ed3\u6784\u5316\u8f93\u51fa\uff0c\u5426\u5219\u56de\u9000 <code>function_calling</code>\uff1b\u5982\u9700 <code>json_mode</code>\uff0c\u663e\u5f0f\u6307\u5b9a <code>method=\"json_mode\"</code> \u5e76\u786e\u4fdd\u6ce8\u518c\u65f6\u5305\u542b <code>json_mode</code>\u3002</p> \u7ed3\u6784\u5316\u8f93\u51fa <pre><code>from langchain_core.messages import HumanMessage\nfrom langchain_core.tools import tool\nfrom pydantic import BaseModel\n\nclass User(BaseModel):\n    name: str\n    age: int\n\nmodel = ChatVLLM(\"qwen3-4b\").with_structured_output(User)\nresponse = model.invoke([HumanMessage(\"\u4f60\u597d\uff0c\u6211\u53eb\u5f20\u4e09\uff0c\u4eca\u5e7425\u5c81\")])\nprint(response)\n</code></pre> <ul> <li>\u652f\u6301\u4f20\u9012<code>BaseChatOpenAI</code>\u7684\u53c2\u6570\uff0c\u4f8b\u5982<code>temperature</code>\u3001<code>top_p</code>\u3001<code>max_tokens</code>\u7b49\u3002</li> </ul> <p>\u9664\u6b64\u4e4b\u5916\uff0c\u7531\u4e8e\u8be5\u7c7b\u7ee7\u627f\u4e86<code>BaseChatOpenAI</code>,\u56e0\u6b64\u652f\u6301\u4f20\u9012<code>BaseChatOpenAI</code>\u7684\u6a21\u578b\u53c2\u6570\uff0c\u4f8b\u5982<code>temperature</code>, <code>extra_body</code>\u7b49\uff1a</p> \u4f20\u9012\u6a21\u578b\u53c2\u6570 <pre><code>from langchain_core.messages import HumanMessage\n\nmodel = ChatVLLM(\"qwen3-4b\",extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}}) #\u5229\u7528extra_body\u4f20\u9012\u989d\u5916\u53c2\u6570\uff0c\u8fd9\u91cc\u662f\u5173\u95ed\u601d\u8003\u6a21\u5f0f\nresponse = model.invoke([HumanMessage(\"Hello\")])\nprint(response)\n</code></pre> <ul> <li>\u652f\u6301\u4f20\u9012\u591a\u6a21\u6001\u6570\u636e</li> </ul> <p>\u652f\u6301\u4f20\u9012\u591a\u6a21\u6001\u6570\u636e\uff0c\u4f60\u53ef\u4ee5\u4f7f\u7528 OpenAI \u517c\u5bb9\u7684\u591a\u6a21\u6001\u6570\u636e\u683c\u5f0f\u6216\u8005\u76f4\u63a5\u4f7f\u7528<code>langchain</code>\u4e2d\u7684<code>content_block</code>\u3002</p> \u4f20\u9012\u591a\u6a21\u6001\u6570\u636e <p>\u4f20\u9012\u56fe\u7247\u7c7b\u6570\u636e\uff1a</p> <pre><code>from langchain_core.messages import HumanMessage\nmessages = [\n    HumanMessage(\n        content_blocks=[\n            {\n                \"type\": \"image\",\n                \"url\": \"https://example.com/image.png\",\n            },\n            {\"type\": \"text\", \"text\": \"\u63cf\u8ff0\u8fd9\u5f20\u56fe\u7247\"},\n        ]\n    )\n]\n\nmodel = ChatVLLM(\"qwen3-vl-2b\")\nresponse = model.invoke(messages)\nprint(response)\n</code></pre> <p>\u4f20\u9012\u89c6\u9891\u7c7b\u6570\u636e\uff1a</p> <pre><code>from langchain_core.messages import HumanMessage\n\nmessages = [\n    HumanMessage(\n        content_blocks=[\n            {\n                \"type\": \"video\",\n                \"url\": \"https://example.com/video.mp4\",\n            },\n            {\"type\": \"text\", \"text\": \"\u63cf\u8ff0\u8fd9\u89c6\u9891\"},\n        ]\n    )\n]\n\nmodel = ChatVLLM(\"qwen3-vl-2b\")\nresponse = model.invoke(messages)\nprint(response)\n</code></pre> <p>\u8865\u5145</p> <p>vllm \u4e5f\u652f\u6301\u90e8\u7f72\u591a\u6a21\u6001\u6a21\u578b\uff0c\u4f8b\u5982 <code>qwen3-vl-2b</code>\uff1a <pre><code>vllm serve Qwen/Qwen3-VL-2B-Instruct \\\n--trust-remote-code \\\n--host 0.0.0.0 --port 8000 \\\n--served-model-name qwen3-vl-2b\n</code></pre></p> <ul> <li>\u652f\u6301 OpenAI \u6700\u65b0\u7684<code>responses api</code> (\u6682\u672a\u4fdd\u8bc1\u5b8c\u5168\u652f\u6301\uff0c\u53ef\u4ee5\u7528\u4e8e\u7b80\u5355\u6d4b\u8bd5\uff0c\u4f46\u4e0d\u8981\u7528\u4e8e\u751f\u4ea7\u73af\u5883)</li> </ul> <p>\u8be5\u6a21\u578b\u7c7b\u4e5f\u652f\u6301 OpenAI \u6700\u65b0\u7684<code>responses_api</code>\u3002\u4f46\u662f\u76ee\u524d\u4ec5\u6709\u5c11\u91cf\u7684\u63d0\u4f9b\u5546\u652f\u6301\u8be5\u98ce\u683c\u7684 API\u3002\u5982\u679c\u4f60\u7684\u6a21\u578b\u63d0\u4f9b\u5546\u652f\u6301\u8be5 API \u98ce\u683c\uff0c\u5219\u53ef\u4ee5\u5728\u4f20\u5165<code>use_responses_api</code>\u53c2\u6570\u4e3a<code>True</code>\u3002     \u4f8b\u5982 vllm \u652f\u6301<code>responses_api</code>\uff0c\u5219\u53ef\u4ee5\u8fd9\u6837\u4f7f\u7528\uff1a</p> OpenAI \u6700\u65b0\u7684<code>responses_api</code> <pre><code>from langchain_core.messages import HumanMessage\n\nmodel = ChatVLLM(\"qwen3-4b\", use_responses_api=True)\nresponse = model.invoke([HumanMessage(content=\"\u4f60\u597d\")])\nprint(response)\n</code></pre>"},{"location":"zh/adavance-guide/openai-compatible/#_10","title":"\u5d4c\u5165\u6a21\u578b\u7c7b\u7684\u4f7f\u7528","text":"<p>\u6211\u4eec\u4f7f\u7528\u524d\u9762\u521b\u5efa\u7684<code>VLLMEmbeddings</code>\u7c7b\u6765\u521d\u59cb\u5316\u5d4c\u5165\u6a21\u578b\u5b9e\u4f8b\u3002</p> <ul> <li>\u5411\u91cf\u5316\u67e5\u8be2</li> </ul> \u5411\u91cf\u5316\u67e5\u8be2 <pre><code>embedding = VLLMEmbeddings(model=\"qwen3-embedding-4b\")\nprint(embedding.embed_query(\"\u4f60\u597d\"))\n</code></pre> <p>\u5f02\u6b65\u5199\u6cd5</p> <pre><code>embedding = VLLMEmbeddings(model=\"qwen3-embedding-4b\")\nres = await embedding.aembed_query(\"\u4f60\u597d\")\nprint(res)\n</code></pre> <ul> <li>\u5411\u91cf\u5316\u5b57\u7b26\u4e32\u5217\u8868</li> </ul> \u5411\u91cf\u5316\u5b57\u7b26\u4e32\u5217\u8868 <pre><code>documents = [\"\u4f60\u597d\", \"\u4f60\u597d\uff0c\u6211\u662f\u5f20\u4e09\"]\nembedding = VLLMEmbeddings(model=\"qwen3-embedding-4b\")\nprint(embedding.embed_documents(documents))\n</code></pre> <p>\u5f02\u6b65\u5199\u6cd5</p> <pre><code>documents = [\"\u4f60\u597d\", \"\u4f60\u597d\uff0c\u6211\u662f\u5f20\u4e09\"]\nembedding = VLLMEmbeddings(model=\"qwen3-embedding-4b\")\nres = await embedding.aembed_documents(documents)\nprint(res)\n</code></pre> <p>\u6ce8\u610f\uff1a\u4f7f\u7528\u672c\u529f\u80fd\u521b\u5efa\u7684\u5bf9\u8bdd\u6a21\u578b\u7c7b\u548c\u5d4c\u5165\u6a21\u578b\u7c7b\u652f\u6301\u4f20\u5165\u4efb\u4f55<code>BaseChatOpenAI</code>\u548c<code>OpenAIEmbeddings</code>\u7684\u53c2\u6570\uff0c\u4f8b\u5982<code>temperature</code>, <code>extra_body</code>,<code>dimensions</code>\u7b49\u3002</p>"},{"location":"zh/adavance-guide/openai-compatible/#_11","title":"\u4e0e\u6a21\u578b\u7ba1\u7406\u529f\u80fd\u96c6\u6210","text":"<p>\u672c\u5e93\u5df2\u5c06\u6b64\u529f\u80fd\u65e0\u7f1d\u63a5\u5165\u6a21\u578b\u7ba1\u7406\u529f\u80fd\u3002\u6ce8\u518c\u5bf9\u8bdd\u6a21\u578b\u65f6\uff0c\u53ea\u9700\u5c06 <code>chat_model</code> \u8bbe\u4e3a <code>\"openai-compatible\"</code>\uff1b\u6ce8\u518c\u5d4c\u5165\u6a21\u578b\u65f6\uff0c\u5c06 <code>embeddings_model</code> \u8bbe\u4e3a <code>\"openai-compatible\"</code> \u5373\u53ef\u3002</p>"},{"location":"zh/adavance-guide/openai-compatible/#_12","title":"\u5bf9\u8bdd\u6a21\u578b\u7c7b\u6ce8\u518c","text":"<p>\u5177\u4f53\u4ee3\u7801\u5982\u4e0b\uff1a</p> <p>\u65b9\u5f0f\u4e00\uff1a\u663e\u5f0f\u4f20\u53c2</p> <pre><code>from langchain_dev_utils.chat_models import register_model_provider\n\nregister_model_provider(\n    provider_name=\"vllm\",\n    chat_model=\"openai-compatible\",\n    base_url=\"http://localhost:8000/v1\"\n)\n</code></pre> <p>\u65b9\u5f0f\u4e8c\uff1a\u901a\u8fc7\u73af\u5883\u53d8\u91cf\uff08\u63a8\u8350\u7528\u4e8e\u914d\u7f6e\u7ba1\u7406\uff09</p> <pre><code>from langchain_dev_utils.chat_models import register_model_provider\n\nregister_model_provider(\n    provider_name=\"vllm\",\n    chat_model=\"openai-compatible\"\n    # \u81ea\u52a8\u8bfb\u53d6 VLLM_API_BASE\n)\n</code></pre> <p>\u540c\u65f6\uff0c<code>create_openai_compatible_model</code>\u51fd\u6570\u4e2d\u7684<code>base_url</code>\u3001<code>compatibility_options</code>\u3001<code>model_profiles</code>\u53c2\u6570\u4e5f\u652f\u6301\u4f20\u5165\u3002\u53ea\u9700\u8981\u5728<code>register_model_provider</code>\u51fd\u6570\u4e2d\u4f20\u5165\u5bf9\u5e94\u7684\u53c2\u6570\u5373\u53ef\u3002</p>"},{"location":"zh/adavance-guide/openai-compatible/#_13","title":"\u5d4c\u5165\u6a21\u578b\u7c7b\u6ce8\u518c","text":"<p>\u4e0e\u5bf9\u8bdd\u6a21\u578b\u7c7b\u6ce8\u518c\u7c7b\u4f3c\uff1a</p> <p>\u65b9\u5f0f\u4e00\uff1a\u663e\u5f0f\u4f20\u53c2</p> <pre><code>from langchain_dev_utils.embeddings import register_embeddings_provider\n\nregister_embeddings_provider(\n    provider_name=\"vllm\",\n    embeddings_model=\"openai-compatible\",\n    base_url=\"http://localhost:8000/v1\",\n)\n</code></pre> <p>\u65b9\u5f0f\u4e8c\uff1a\u73af\u5883\u53d8\u91cf\uff08\u63a8\u8350\uff09</p> <pre><code>export VLLM_API_BASE=http://localhost:8000/v1\n</code></pre> <pre><code>from langchain_dev_utils.embeddings import register_embeddings_provider\n\nregister_embeddings_provider(\n    provider_name=\"vllm\",\n    embeddings_model=\"openai-compatible\"\n)\n</code></pre>"},{"location":"zh/adavance-guide/pipeline/","title":"\u72b6\u6001\u56fe\u7f16\u6392","text":""},{"location":"zh/adavance-guide/pipeline/#_2","title":"\u6982\u8ff0","text":"<p>\u5728LangGraph\u4e2d\uff0c\u72b6\u6001\u56fe\u7f16\u6392\u662f\u6784\u5efa\u590d\u6742 AI \u5e94\u7528\u7684\u5173\u952e\u6280\u672f\u3002\u901a\u8fc7\u5c06\u591a\u4e2a\u72b6\u6001\u56fe\u6309\u7167\u7279\u5b9a\u6a21\u5f0f\u8fdb\u884c\u7ec4\u5408\uff0c\u53ef\u4ee5\u6784\u5efa\u51fa\u529f\u80fd\u5f3a\u5927\u3001\u903b\u8f91\u6e05\u6670\u7684\u5de5\u4f5c\u6d41\u3002</p> <p>\u672c\u5e93\u63d0\u4f9b\u4e86\u4ee5\u4e0b\u4e24\u79cd\u7f16\u6392\u65b9\u5f0f\uff1a</p> \u7f16\u6392\u65b9\u5f0f \u529f\u80fd\u63cf\u8ff0 \u9002\u7528\u573a\u666f \u987a\u5e8f\u7f16\u6392 \u5c06\u591a\u4e2a\u72b6\u6001\u56fe\u6309\u7167\u987a\u5e8f\u65b9\u5f0f\u8fdb\u884c\u7f16\u6392\uff0c\u5f62\u6210\u987a\u5e8f\u5de5\u4f5c\u6d41 \u4efb\u52a1\u9700\u8981\u6309\u6b65\u9aa4\u4f9d\u6b21\u6267\u884c\uff0c\u6bcf\u4e2a\u6b65\u9aa4\u4f9d\u8d56\u524d\u4e00\u6b65\u9aa4\u7684\u8f93\u51fa \u5e76\u884c\u7f16\u6392 \u5c06\u591a\u4e2a\u72b6\u6001\u56fe\u6309\u7167\u5e76\u884c\u65b9\u5f0f\u8fdb\u884c\u7f16\u6392\uff0c\u5f62\u6210\u5e76\u884c\u5de5\u4f5c\u6d41 \u591a\u4e2a\u4efb\u52a1\u76f8\u4e92\u72ec\u7acb\uff0c\u53ef\u4ee5\u540c\u65f6\u6267\u884c\u4ee5\u63d0\u9ad8\u6548\u7387"},{"location":"zh/adavance-guide/pipeline/#_3","title":"\u987a\u5e8f\u7f16\u6392","text":"<p>\u987a\u5e8f\u7f16\u6392\uff08Sequential Pipeline\uff09\u662f\u4e00\u79cd\u5c06\u590d\u6742\u4efb\u52a1\u5206\u89e3\u4e3a\u4e00\u7cfb\u5217\u8fde\u7eed\u3001\u6709\u5e8f\u7684\u5b50\u4efb\u52a1\uff0c\u5e76\u4ea4\u7531\u4e0d\u540c\u7684\u4e13\u95e8\u5316\u667a\u80fd\u4f53\u4f9d\u6b21\u5904\u7406\u7684\u5de5\u4f5c\u6a21\u5f0f\u3002</p> <p>\u901a\u8fc7 <code>create_sequential_pipeline</code> \u53ef\u5c06\u591a\u4e2a\u72b6\u6001\u56fe\u4ee5\u987a\u5e8f\u7f16\u6392\u65b9\u5f0f\u8fdb\u884c\u7ec4\u5408\u3002</p>"},{"location":"zh/adavance-guide/pipeline/#_4","title":"\u5178\u578b\u5e94\u7528\u573a\u666f","text":"<p>\u4ee5\u8f6f\u4ef6\u5f00\u53d1\u6d41\u7a0b\u4e3a\u4f8b\uff0c\u901a\u5e38\u9075\u5faa\u4e00\u4e2a\u4e25\u683c\u7684\u7ebf\u6027\u6d41\u7a0b\uff1a</p> \u9636\u6bb5 \u8d1f\u8d23\u89d2\u8272 \u8f93\u5165 \u8f93\u51fa 1. \u9700\u6c42\u5206\u6790 \u4ea7\u54c1\u7ecf\u7406 \u7528\u6237\u9700\u6c42 \u4ea7\u54c1\u9700\u6c42\u6587\u6863\uff08PRD\uff09 2. \u67b6\u6784\u8bbe\u8ba1 \u67b6\u6784\u5e08 PRD \u7cfb\u7edf\u67b6\u6784\u56fe\u548c\u6280\u672f\u65b9\u6848 3. \u4ee3\u7801\u7f16\u5199 \u5f00\u53d1\u5de5\u7a0b\u5e08 \u67b6\u6784\u65b9\u6848 \u53ef\u6267\u884c\u7684\u6e90\u4ee3\u7801 4. \u6d4b\u8bd5\u4e0e\u8d28\u4fdd \u6d4b\u8bd5\u5de5\u7a0b\u5e08 \u6e90\u4ee3\u7801 \u6d4b\u8bd5\u62a5\u544a\u548c\u4f18\u5316\u5efa\u8bae <p>\u8fd9\u4e2a\u6d41\u7a0b\u73af\u73af\u76f8\u6263\uff0c\u987a\u5e8f\u4e0d\u53ef\u98a0\u5012\u3002\u901a\u8fc7 <code>create_sequential_pipeline</code> \u51fd\u6570\uff0c\u53ef\u4ee5\u5c06\u8fd9\u56db\u4e2a\u667a\u80fd\u4f53\u65e0\u7f1d\u5730\u4e32\u8054\u8d77\u6765\uff0c\u5f62\u6210\u4e00\u4e2a\u9ad8\u5ea6\u81ea\u52a8\u5316\u3001\u804c\u8d23\u5206\u660e\u7684\u8f6f\u4ef6\u5f00\u53d1\u6d41\u6c34\u7ebf\u3002</p>"},{"location":"zh/adavance-guide/pipeline/#_5","title":"\u57fa\u7840\u793a\u4f8b","text":"<p>\u4ee5\u4e0b\u4ee3\u7801\u5c55\u793a\u4e86\u5982\u4f55\u4f7f\u7528 <code>create_sequential_pipeline</code> \u6784\u5efa\u8f6f\u4ef6\u5f00\u53d1\u6d41\u6c34\u7ebf\uff1a</p> <pre><code>from langchain.agents import AgentState\nfrom langchain_core.messages import HumanMessage\nfrom langchain_dev_utils.agents import create_agent\nfrom langchain_dev_utils.pipeline import create_sequential_pipeline\nfrom langchain_core.tools import tool\nfrom langchain_dev_utils.chat_models import register_model_provider\n\nregister_model_provider(\n    provider_name=\"vllm\",\n    chat_model=\"openai-compatible\",\n    base_url=\"http://localhost:8000/v1\",\n)\n\n\n@tool\ndef analyze_requirements(user_request: str) -&gt; str:\n    \"\"\"\u5206\u6790\u7528\u6237\u9700\u6c42\u5e76\u751f\u6210\u8be6\u7ec6\u7684\u4ea7\u54c1\u9700\u6c42\u6587\u6863\"\"\"\n    return f\"\u6839\u636e\u7528\u6237\u8bf7\u6c42'{user_request}'\uff0c\u5df2\u751f\u6210\u8be6\u7ec6\u7684\u4ea7\u54c1\u9700\u6c42\u6587\u6863\uff0c\u5305\u542b\u529f\u80fd\u5217\u8868\u3001\u7528\u6237\u6545\u4e8b\u548c\u9a8c\u6536\u6807\u51c6\u3002\"\n\n\n@tool\ndef design_architecture(requirements: str) -&gt; str:\n    \"\"\"\u6839\u636e\u9700\u6c42\u6587\u6863\u8bbe\u8ba1\u7cfb\u7edf\u67b6\u6784\"\"\"\n    return \"\u57fa\u4e8e\u9700\u6c42\u6587\u6863\uff0c\u5df2\u8bbe\u8ba1\u7cfb\u7edf\u67b6\u6784\uff0c\u5305\u542b\u5fae\u670d\u52a1\u5212\u5206\u3001\u6570\u636e\u6d41\u56fe\u548c\u6280\u672f\u6808\u9009\u62e9\u3002\"\n\n\n@tool\ndef generate_code(architecture: str) -&gt; str:\n    \"\"\"\u6839\u636e\u67b6\u6784\u8bbe\u8ba1\u751f\u6210\u6838\u5fc3\u4ee3\u7801\"\"\"\n    return \"\u57fa\u4e8e\u67b6\u6784\u8bbe\u8ba1\uff0c\u5df2\u751f\u6210\u6838\u5fc3\u4e1a\u52a1\u4ee3\u7801\uff0c\u5305\u542bAPI\u63a5\u53e3\u3001\u6570\u636e\u6a21\u578b\u548c\u4e1a\u52a1\u903b\u8f91\u5b9e\u73b0\u3002\"\n\n\n@tool\ndef create_tests(code: str) -&gt; str:\n    \"\"\"\u4e3a\u751f\u6210\u7684\u4ee3\u7801\u521b\u5efa\u6d4b\u8bd5\u7528\u4f8b\"\"\"\n    return \"\u4e3a\u751f\u6210\u7684\u4ee3\u7801\u521b\u5efa\u4e86\u5355\u5143\u6d4b\u8bd5\u3001\u96c6\u6210\u6d4b\u8bd5\u548c\u7aef\u5230\u7aef\u6d4b\u8bd5\u7528\u4f8b\u3002\"\n\n\n# \u4ea7\u54c1\u7ecf\u7406\u667a\u80fd\u4f53\nrequirements_agent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    tools=[analyze_requirements],\n    system_prompt=\"\u4f60\u662f\u4e00\u4e2a\u4ea7\u54c1\u7ecf\u7406\uff0c\u8d1f\u8d23\u5206\u6790\u7528\u6237\u9700\u6c42\u5e76\u751f\u6210\u8be6\u7ec6\u7684\u4ea7\u54c1\u9700\u6c42\u6587\u6863\u3002\",\n    name=\"requirements_agent\",\n)\n\n# \u7cfb\u7edf\u67b6\u6784\u5e08\u667a\u80fd\u4f53\narchitecture_agent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    tools=[design_architecture],\n    system_prompt=\"\u4f60\u662f\u4e00\u4e2a\u7cfb\u7edf\u67b6\u6784\u5e08\uff0c\u8d1f\u8d23\u6839\u636e\u9700\u6c42\u6587\u6863\u8bbe\u8ba1\u7cfb\u7edf\u67b6\u6784\u3002\",\n    name=\"architecture_agent\",\n)\n\n# \u9ad8\u7ea7\u5f00\u53d1\u5de5\u7a0b\u5e08\u667a\u80fd\u4f53\ncoding_agent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    tools=[generate_code],\n    system_prompt=\"\u4f60\u662f\u4e00\u4e2a\u9ad8\u7ea7\u5f00\u53d1\u5de5\u7a0b\u5e08\uff0c\u8d1f\u8d23\u6839\u636e\u67b6\u6784\u8bbe\u8ba1\u751f\u6210\u6838\u5fc3\u4ee3\u7801\u3002\",\n    name=\"coding_agent\",\n)\n\n# \u6d4b\u8bd5\u5de5\u7a0b\u5e08\u667a\u80fd\u4f53\ntesting_agent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    tools=[create_tests],\n    system_prompt=\"\u4f60\u662f\u4e00\u4e2a\u6d4b\u8bd5\u5de5\u7a0b\u5e08\uff0c\u8d1f\u8d23\u4e3a\u751f\u6210\u7684\u4ee3\u7801\u521b\u5efa\u5168\u9762\u7684\u6d4b\u8bd5\u7528\u4f8b\u3002\",\n    name=\"testing_agent\",\n)\n\n# \u6784\u5efa\u81ea\u52a8\u5316\u8f6f\u4ef6\u5f00\u53d1\u987a\u5e8f\u5de5\u4f5c\u6d41\uff08\u7ba1\u9053\uff09\ngraph = create_sequential_pipeline(\n    sub_graphs=[\n        requirements_agent,\n        architecture_agent,\n        coding_agent,\n        testing_agent,\n    ],\n    state_schema=AgentState,\n)\n\nresponse = graph.invoke(\n    {\"messages\": [HumanMessage(\"\u5f00\u53d1\u4e00\u4e2a\u7535\u5546\u7f51\u7ad9\uff0c\u5305\u542b\u7528\u6237\u6ce8\u518c\u3001\u5546\u54c1\u6d4f\u89c8\u548c\u8d2d\u7269\u8f66\u529f\u80fd\")]}\n)\nprint(response)\n</code></pre>"},{"location":"zh/adavance-guide/pipeline/#_6","title":"\u6267\u884c\u6d41\u7a0b\u56fe","text":"<p>\u6700\u7ec8\u751f\u6210\u7684\u56fe\u5982\u4e0b\uff1a</p> <p></p>"},{"location":"zh/adavance-guide/pipeline/#_7","title":"\u4e0a\u4e0b\u6587\u5de5\u7a0b\u4f18\u5316","text":"<p>\u4e0a\u8ff0\u57fa\u7840\u793a\u4f8b\u4ec5\u4f5c\u53c2\u8003\u3002\u5b9e\u9645\u4e0a\uff0c\u8be5\u4f8b\u5b50\u5728\u8fd0\u884c\u65f6\u4f1a\u5c06\u524d\u9762\u6240\u6709\u667a\u80fd\u4f53\u7684\u5b8c\u6574\u4e0a\u4e0b\u6587\u4f9d\u6b21\u4f20\u9012\u7ed9\u5f53\u524d\u667a\u80fd\u4f53\uff0c\u53ef\u80fd\u5bfc\u81f4\u4e0a\u4e0b\u6587\u81a8\u80c0\uff0c\u5f71\u54cd\u6027\u80fd\u4e0e\u6548\u679c\u3002</p> <p>\u63a8\u8350\u91c7\u7528\u4ee5\u4e0b\u4efb\u4e00\u65b9\u6848\u7cbe\u7b80\u4e0a\u4e0b\u6587\uff1a</p> \u65b9\u6848 \u63cf\u8ff0 \u4f18\u70b9 \u4f7f\u7528\u4e2d\u95f4\u4ef6 \u4f7f\u7528 <code>create_agent</code> \u914d\u5408\u4e2d\u95f4\u4ef6\uff0c\u4ec5\u63d0\u53d6\u5e76\u4f20\u9012\u5fc5\u8981\u4fe1\u606f \u5b9e\u73b0\u7b80\u5355\uff0c\u4ee3\u7801\u6539\u52a8\u5c0f \u81ea\u5b9a\u4e49\u72b6\u6001\u56fe \u57fa\u4e8e <code>LangGraph</code> \u5b8c\u5168\u81ea\u5b9a\u4e49\u72b6\u6001\u56fe\uff0c\u663e\u5f0f\u63a7\u5236\u72b6\u6001\u5b57\u6bb5\u4e0e\u6d88\u606f\u6d41\u52a8 \u7075\u6d3b\u6027\u9ad8\uff0c\u53ef\u7cbe\u786e\u63a7\u5236 \u70b9\u51fb\u67e5\u770b\u5229\u7528\u4e2d\u95f4\u4ef6\u89e3\u51b3\u7684\u53c2\u8003\u4ee3\u7801 <pre><code>from typing import Any\n\nfrom langchain.agents import AgentState\nfrom langchain.agents.middleware import AgentMiddleware\nfrom langchain_core.messages import HumanMessage, RemoveMessage\nfrom langgraph.runtime import Runtime\n\nfrom langchain_dev_utils.agents import create_agent\nfrom langchain_dev_utils.agents.middleware import format_prompt\nfrom langchain_dev_utils.pipeline import create_sequential_pipeline\n\n\nclass DeveloperState(AgentState, total=False):\n    requirement: str\n    architecture: str\n    code: str\n    tests: str\n\nclass ClearAgentContextMiddleware(AgentMiddleware):\n    state_schema = DeveloperState\n\n    def __init__(self, result_save_key: str) -&gt; None:\n        super().__init__()\n        self.result_save_key = result_save_key\n\n    def after_agent(\n        self, state: DeveloperState, runtime: Runtime\n    ) -&gt; dict[str, Any] | None:\n        final_message = state[\"messages\"][-1]\n        update_key = self.result_save_key\n        return {\n            \"messages\": [\n                RemoveMessage(id=msg.id or \"\") for msg in state[\"messages\"][1:]\n            ],\n            update_key: final_message.content,\n        }\n\n# \u4ea7\u54c1\u7ecf\u7406\u667a\u80fd\u4f53\nrequirements_agent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    tools=[analyze_requirements],\n    system_prompt=\"\u4f60\u662f\u4e00\u4e2a\u4ea7\u54c1\u7ecf\u7406\uff0c\u8d1f\u8d23\u5206\u6790\u7528\u6237\u9700\u6c42\u5e76\u751f\u6210\u8be6\u7ec6\u7684\u4ea7\u54c1\u9700\u6c42\u6587\u6863\u3002\",\n    name=\"requirements_agent\",\n    state_schema=DeveloperState,\n    middleware=[format_prompt, ClearAgentContextMiddleware(\"requirement\")],\n)\n\n# \u7cfb\u7edf\u67b6\u6784\u5e08\u667a\u80fd\u4f53\narchitecture_agent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    tools=[design_architecture],\n    system_prompt=\"\u4f60\u662f\u4e00\u4e2a\u7cfb\u7edf\u67b6\u6784\u5e08\uff0c\u8d1f\u8d23\u6839\u636e\u9700\u6c42\u6587\u6863\u8bbe\u8ba1\u7cfb\u7edf\u67b6\u6784\u3002\",\n    name=\"architecture_agent\",\n    state_schema=DeveloperState,\n    middleware=[format_prompt, ClearAgentContextMiddleware(\"architecture\")],\n)\n\n# \u9ad8\u7ea7\u5f00\u53d1\u5de5\u7a0b\u5e08\u667a\u80fd\u4f53\ncoding_agent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    tools=[generate_code],\n    system_prompt=\"\u4f60\u662f\u4e00\u4e2a\u9ad8\u7ea7\u5f00\u53d1\u5de5\u7a0b\u5e08\uff0c\u8d1f\u8d23\u6839\u636e\u67b6\u6784\u8bbe\u8ba1\u751f\u6210\u6838\u5fc3\u4ee3\u7801\u3002\",\n    name=\"coding_agent\",\n    state_schema=DeveloperState,\n    middleware=[format_prompt, ClearAgentContextMiddleware(\"code\")],\n)\n\n# \u6d4b\u8bd5\u5de5\u7a0b\u5e08\u667a\u80fd\u4f53\ntesting_agent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    tools=[create_tests],\n    system_prompt=\"\u4f60\u662f\u4e00\u4e2a\u6d4b\u8bd5\u5de5\u7a0b\u5e08\uff0c\u8d1f\u8d23\u4e3a\u751f\u6210\u7684\u4ee3\u7801\u521b\u5efa\u5168\u9762\u7684\u6d4b\u8bd5\u7528\u4f8b\u3002\",\n    name=\"testing_agent\",\n    state_schema=DeveloperState,\n    middleware=[format_prompt, ClearAgentContextMiddleware(\"tests\")],\n)\n\n# \u6784\u5efa\u81ea\u52a8\u5316\u8f6f\u4ef6\u5f00\u53d1\u987a\u5e8f\u5de5\u4f5c\u6d41\uff08\u7ba1\u9053\uff09\ngraph = create_sequential_pipeline(\n    sub_graphs=[\n        requirements_agent,\n        architecture_agent,\n        coding_agent,\n        testing_agent,\n    ],\n    state_schema=DeveloperState,\n)\n\nresponse = graph.invoke(\n    {\"messages\": [HumanMessage(\"\u5f00\u53d1\u4e00\u4e2a\u7535\u5546\u7f51\u7ad9\uff0c\u5305\u542b\u7528\u6237\u6ce8\u518c\u3001\u5546\u54c1\u6d4f\u89c8\u548c\u8d2d\u7269\u8f66\u529f\u80fd\")]}\n)\nprint(response)\n</code></pre> <p>\u5b9e\u73b0\u8bf4\u660e\uff1a</p> <ol> <li> <p>\u6269\u5c55\u72b6\u6001\u6a21\u5f0f\uff1a\u5728\u667a\u80fd\u4f53\u7684 State Schema \u4e2d\u6dfb\u52a0\u4e86 <code>requirement</code>\u3001<code>architecture</code>\u3001<code>code</code>\u3001<code>tests</code> \u56db\u4e2a\u5b57\u6bb5\uff0c\u7528\u4e8e\u5b58\u50a8\u5bf9\u5e94\u667a\u80fd\u4f53\u7684\u6700\u7ec8\u8f93\u51fa\u7ed3\u679c\u3002</p> </li> <li> <p>\u81ea\u5b9a\u4e49\u4e2d\u95f4\u4ef6\uff1a\u521b\u5efa\u4e86 <code>ClearAgentContextMiddleware</code> \u4e2d\u95f4\u4ef6\uff0c\u5728\u6bcf\u4e2a\u667a\u80fd\u4f53\u7ed3\u675f\u540e\uff1a</p> </li> <li>\u6e05\u9664\u5f53\u524d\u7684\u8fd0\u884c\u4e0a\u4e0b\u6587\uff08\u4f7f\u7528 <code>RemoveMessage</code>\uff09</li> <li> <p>\u5c06\u6700\u7ec8\u7ed3\u679c\uff08<code>final_message.content</code>\uff09\u4fdd\u5b58\u5230\u5bf9\u5e94\u7684\u5b57\u6bb5\u4e2d</p> </li> <li> <p>\u52a8\u6001\u63d0\u793a\u683c\u5f0f\u5316\uff1a\u4f7f\u7528\u672c\u5e93\u5185\u7f6e\u7684 <code>format_prompt</code> \u4e2d\u95f4\u4ef6\uff0c\u5728\u8fd0\u884c\u65f6\u5c06\u524d\u7f6e\u667a\u80fd\u4f53\u7684\u8f93\u51fa\u6309\u9700\u52a8\u6001\u62fc\u5165 <code>system_prompt</code></p> </li> </ol> <p>\u63d0\u793a</p> <p>\u5bf9\u4e8e\u4e32\u884c\u7ec4\u5408\u7684\u56fe\uff0cLangGraph \u7684 <code>StateGraph</code> \u63d0\u4f9b\u4e86 <code>add_sequence</code> \u65b9\u6cd5\u4f5c\u4e3a\u7b80\u4fbf\u5199\u6cd5\u3002\u8be5\u65b9\u6cd5\u6700\u9002\u5408\u5728\u8282\u70b9\u4e3a\u51fd\u6570\uff08\u800c\u975e\u5b50\u56fe\uff09\u65f6\u4f7f\u7528\u3002</p> <pre><code>graph = StateGraph(AgentState)\ngraph.add_sequence([(\"graph1\", graph1), (\"graph2\", graph2), (\"graph3\", graph3)])\ngraph.add_edge(\"__start__\", \"graph1\")\ngraph = graph.compile()\n</code></pre> <p>\u4e0d\u8fc7\uff0c\u4e0a\u8ff0\u5199\u6cd5\u4ecd\u663e\u7e41\u7410\u3002\u56e0\u6b64\uff0c\u66f4\u63a8\u8350\u4f7f\u7528 <code>create_sequential_pipeline</code> \u51fd\u6570\uff0c\u5b83\u80fd\u901a\u8fc7\u4e00\u884c\u4ee3\u7801\u5feb\u901f\u6784\u5efa\u4e32\u884c\u6267\u884c\u56fe\uff0c\u66f4\u4e3a\u7b80\u6d01\u9ad8\u6548\u3002</p>"},{"location":"zh/adavance-guide/pipeline/#_8","title":"\u5e76\u884c\u7f16\u6392","text":"<p>\u5e76\u884c\u7f16\u6392\uff08Parallel Pipeline\uff09\u901a\u8fc7\u5c06\u591a\u4e2a\u72b6\u6001\u56fe\u5e76\u884c\u7ec4\u5408\uff0c\u5bf9\u6bcf\u4e2a\u72b6\u6001\u56fe\u5e76\u53d1\u5730\u6267\u884c\u4efb\u52a1\uff0c\u4ece\u800c\u63d0\u9ad8\u4efb\u52a1\u7684\u6267\u884c\u6548\u7387\u3002</p> <p>\u901a\u8fc7 <code>create_parallel_pipeline</code> \u51fd\u6570\uff0c\u53ef\u5c06\u591a\u4e2a\u72b6\u6001\u56fe\u4ee5\u5e76\u884c\u7f16\u6392\u65b9\u5f0f\u8fdb\u884c\u7ec4\u5408\uff0c\u5b9e\u73b0\u5e76\u884c\u6267\u884c\u4efb\u52a1\u7684\u6548\u679c\u3002</p>"},{"location":"zh/adavance-guide/pipeline/#_9","title":"\u5178\u578b\u5e94\u7528\u573a\u666f","text":"<p>\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\uff0c\u5f53\u7cfb\u7edf\u67b6\u6784\u8bbe\u8ba1\u5b8c\u6210\u540e\uff0c\u4e0d\u540c\u7684\u529f\u80fd\u6a21\u5757\u5f80\u5f80\u53ef\u4ee5\u7531\u4e0d\u540c\u7684\u56e2\u961f\u6216\u5de5\u7a0b\u5e08\u540c\u65f6\u8fdb\u884c\u5f00\u53d1\uff0c\u56e0\u4e3a\u5b83\u4eec\u4e4b\u95f4\u662f\u76f8\u5bf9\u72ec\u7acb\u7684\u3002\u8fd9\u5c31\u662f\u5e76\u884c\u5de5\u4f5c\u7684\u5178\u578b\u573a\u666f\u3002</p> <p>\u5047\u8bbe\u8981\u5f00\u53d1\u4e00\u4e2a\u7535\u5546\u7f51\u7ad9\uff0c\u5176\u6838\u5fc3\u529f\u80fd\u53ef\u4ee5\u5206\u4e3a\u4e09\u4e2a\u72ec\u7acb\u6a21\u5757\uff1a</p> \u6a21\u5757 \u529f\u80fd \u5f00\u53d1\u5185\u5bb9 \u7528\u6237\u6a21\u5757 \u7528\u6237\u7ba1\u7406 \u6ce8\u518c\u3001\u767b\u5f55\u3001\u4e2a\u4eba\u4e2d\u5fc3 \u5546\u54c1\u6a21\u5757 \u5546\u54c1\u7ba1\u7406 \u5c55\u793a\u3001\u641c\u7d22\u3001\u5206\u7c7b \u8ba2\u5355\u6a21\u5757 \u8ba2\u5355\u7ba1\u7406 \u4e0b\u5355\u3001\u652f\u4ed8\u3001\u72b6\u6001\u67e5\u8be2 <p>\u5982\u679c\u4e32\u884c\u5f00\u53d1\uff0c\u8017\u65f6\u5c06\u662f\u4e09\u8005\u4e4b\u548c\u3002\u4f46\u5982\u679c\u5e76\u884c\u5f00\u53d1\uff0c\u603b\u8017\u65f6\u5c06\u7ea6\u7b49\u4e8e\u8017\u65f6\u6700\u957f\u7684\u90a3\u4e00\u4e2a\u6a21\u5757\u7684\u5f00\u53d1\u65f6\u95f4\uff0c\u6548\u7387\u5927\u5927\u63d0\u5347\u3002</p>"},{"location":"zh/adavance-guide/pipeline/#_10","title":"\u57fa\u7840\u793a\u4f8b","text":"<pre><code>from langchain_dev_utils.pipeline import create_parallel_pipeline\n\n\n@tool\ndef develop_user_module():\n    \"\"\"\u5f00\u53d1\u7528\u6237\u6a21\u5757\u529f\u80fd\"\"\"\n    return \"\u7528\u6237\u6a21\u5757\u5f00\u53d1\u5b8c\u6210\uff0c\u5305\u542b\u6ce8\u518c\u3001\u767b\u5f55\u548c\u4e2a\u4eba\u8d44\u6599\u7ba1\u7406\u529f\u80fd\u3002\"\n\n\n@tool\ndef develop_product_module():\n    \"\"\"\u5f00\u53d1\u5546\u54c1\u6a21\u5757\u529f\u80fd\"\"\"\n    return \"\u5546\u54c1\u6a21\u5757\u5f00\u53d1\u5b8c\u6210\uff0c\u5305\u542b\u5546\u54c1\u5c55\u793a\u3001\u641c\u7d22\u548c\u5206\u7c7b\u529f\u80fd\u3002\"\n\n\n@tool\ndef develop_order_module():\n    \"\"\"\u5f00\u53d1\u8ba2\u5355\u6a21\u5757\u529f\u80fd\"\"\"\n    return \"\u8ba2\u5355\u6a21\u5757\u5f00\u53d1\u5b8c\u6210\uff0c\u5305\u542b\u4e0b\u5355\u3001\u652f\u4ed8\u548c\u8ba2\u5355\u67e5\u8be2\u529f\u80fd\u3002\"\n\n\n# \u7528\u6237\u6a21\u5757\u5f00\u53d1\u667a\u80fd\u4f53\nuser_module_agent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    tools=[develop_user_module],\n    system_prompt=\"\u4f60\u662f\u4e00\u4e2a\u524d\u7aef\u5f00\u53d1\u5de5\u7a0b\u5e08\uff0c\u8d1f\u8d23\u5f00\u53d1\u7528\u6237\u76f8\u5173\u6a21\u5757\u3002\",\n    name=\"user_module_agent\",\n)\n\n# \u5546\u54c1\u6a21\u5757\u5f00\u53d1\u667a\u80fd\u4f53\nproduct_module_agent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    tools=[develop_product_module],\n    system_prompt=\"\u4f60\u662f\u4e00\u4e2a\u524d\u7aef\u5f00\u53d1\u5de5\u7a0b\u5e08\uff0c\u8d1f\u8d23\u5f00\u53d1\u5546\u54c1\u76f8\u5173\u6a21\u5757\u3002\",\n    name=\"product_module_agent\",\n)\n\n# \u8ba2\u5355\u6a21\u5757\u5f00\u53d1\u667a\u80fd\u4f53\norder_module_agent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    tools=[develop_order_module],\n    system_prompt=\"\u4f60\u662f\u4e00\u4e2a\u524d\u7aef\u5f00\u53d1\u5de5\u7a0b\u5e08\uff0c\u8d1f\u8d23\u5f00\u53d1\u8ba2\u5355\u76f8\u5173\u6a21\u5757\u3002\",\n    name=\"order_module_agent\",\n)\n\n# \u6784\u5efa\u524d\u7aef\u6a21\u5757\u5f00\u53d1\u7684\u5e76\u884c\u5de5\u4f5c\u6d41\uff08\u7ba1\u9053\uff09\ngraph = create_parallel_pipeline(\n    sub_graphs=[\n        user_module_agent,\n        product_module_agent,\n        order_module_agent,\n    ],\n    state_schema=AgentState,\n)\nresponse = graph.invoke({\"messages\": [HumanMessage(\"\u5e76\u884c\u5f00\u53d1\u7535\u5546\u7f51\u7ad9\u7684\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\")]})\nprint(response)\n</code></pre>"},{"location":"zh/adavance-guide/pipeline/#_11","title":"\u6267\u884c\u6d41\u7a0b\u56fe","text":"<p>\u6700\u7ec8\u751f\u6210\u7684\u56fe\u5982\u4e0b\uff1a</p> <p></p>"},{"location":"zh/adavance-guide/pipeline/#_12","title":"\u4f7f\u7528\u5206\u652f\u51fd\u6570\u6307\u5b9a\u5e76\u884c\u6267\u884c\u7684\u5b50\u56fe","text":"<p>\u6709\u4e9b\u65f6\u5019\u9700\u8981\u6839\u636e\u6761\u4ef6\u6307\u5b9a\u5e76\u884c\u6267\u884c\u54ea\u4e9b\u5b50\u56fe\uff0c\u8fd9\u65f6\u53ef\u4ee5\u4f7f\u7528\u5206\u652f\u51fd\u6570\u3002\u5206\u652f\u51fd\u6570\u9700\u8981\u8fd4\u56de <code>Send</code> \u5217\u8868\u3002</p>"},{"location":"zh/adavance-guide/pipeline/#_13","title":"\u5e94\u7528\u573a\u666f","text":"<p>\u4f8b\u5982\u4e0a\u8ff0\u4f8b\u5b50\uff0c\u5047\u8bbe\u5f00\u53d1\u7684\u6a21\u5757\u7531\u7528\u6237\u6307\u5b9a\uff0c\u5219\u53ea\u6709\u88ab\u6307\u5b9a\u7684\u6a21\u5757\u624d\u4f1a\u88ab\u5e76\u884c\u6267\u884c\u3002</p> <pre><code># \u6784\u5efa\u5e76\u884c\u7ba1\u9053\uff08\u6839\u636e\u6761\u4ef6\u9009\u62e9\u5e76\u884c\u6267\u884c\u7684\u5b50\u56fe\uff09\nfrom langgraph.types import Send\n\n\nclass DevAgentState(AgentState):\n    \"\"\"\u5f00\u53d1\u4ee3\u7406\u72b6\u6001\"\"\"\n\n    selected_modules: list[tuple[str, str]]\n\n\n# \u6307\u5b9a\u7528\u6237\u9009\u62e9\u7684\u6a21\u5757\nselect_modules = [(\"user_module\", \"\u5f00\u53d1\u7528\u6237\u6a21\u5757\"), (\"product_module\", \"\u5f00\u53d1\u5546\u54c1\u6a21\u5757\")]\n\nuser_module_agent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    tools=[develop_user_module],\n    system_prompt=\"\u4f60\u662f\u4e00\u4e2a\u524d\u7aef\u5f00\u53d1\u5de5\u7a0b\u5e08\uff0c\u8d1f\u8d23\u5f00\u53d1\u7528\u6237\u76f8\u5173\u6a21\u5757\u3002\",\n    name=\"user_module_agent\",\n)\n\nproduct_module_agent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    tools=[develop_product_module],\n    system_prompt=\"\u4f60\u662f\u4e00\u4e2a\u524d\u7aef\u5f00\u53d1\u5de5\u7a0b\u5e08\uff0c\u8d1f\u8d23\u5f00\u53d1\u5546\u54c1\u76f8\u5173\u6a21\u5757\u3002\",\n    name=\"product_module_agent\",\n)\n\n\norder_module_agent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    tools=[develop_order_module],\n    system_prompt=\"\u4f60\u662f\u4e00\u4e2a\u524d\u7aef\u5f00\u53d1\u5de5\u7a0b\u5e08\uff0c\u8d1f\u8d23\u5f00\u53d1\u8ba2\u5355\u76f8\u5173\u6a21\u5757\u3002\",\n    name=\"order_module_agent\",\n)\n\n\ngraph = create_parallel_pipeline(\n    sub_graphs=[\n        user_module_agent,\n        product_module_agent,\n        order_module_agent,\n    ],\n    state_schema=DevAgentState,\n    branches_fn=lambda state: [\n        Send(module_name + \"_agent\", {\"messages\": [HumanMessage(task_name)]})\n        for module_name, task_name in state[\"selected_modules\"]\n    ],\n)\n\nresponse = graph.invoke(\n    {\n        \"messages\": [HumanMessage(\"\u5f00\u53d1\u7535\u5546\u7f51\u7ad9\u7684\u90e8\u5206\u6a21\u5757\")],\n        \"selected_modules\": select_modules,\n    }\n)\nprint(response)\n</code></pre> <p>\u63d0\u793a</p> <ul> <li>\u4e0d\u4f20\u5165 <code>branches_fn</code> \u53c2\u6570\u65f6\uff1a\u6240\u6709\u5b50\u56fe\u90fd\u4f1a\u5e76\u884c\u6267\u884c</li> <li>\u4f20\u5165 <code>branches_fn</code> \u53c2\u6570\u65f6\uff1a\u6267\u884c\u54ea\u4e9b\u5b50\u56fe\u7531\u8be5\u51fd\u6570\u7684\u8fd4\u56de\u503c\u51b3\u5b9a</li> </ul>"},{"location":"zh/api-reference/agent/","title":"Agent \u6a21\u5757 API \u53c2\u8003\u6587\u6863","text":""},{"location":"zh/api-reference/agent/#create_agent","title":"create_agent","text":"<p>\u521b\u5efa\u4e00\u4e2a\u667a\u80fd\u4f53\uff0c\u63d0\u4f9b\u4e0e langchain \u5b98\u65b9 <code>create_agent</code> \u5b8c\u5168\u76f8\u540c\u7684\u529f\u80fd\uff0c\u4f46\u62d3\u5c55\u4e86\u5b57\u7b26\u4e32\u6307\u5b9a\u6a21\u578b\u3002</p>"},{"location":"zh/api-reference/agent/#_1","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def create_agent(  # noqa: PLR0915\n    model: str,\n    tools: Sequence[BaseTool | Callable | dict[str, Any]] | None = None,\n    *,\n    system_prompt: str | SystemMessage | None = None,\n    response_format: ResponseFormat[ResponseT] | type[ResponseT] | None = None,\n    middleware: Sequence[AgentMiddleware[StateT_co, ContextT]] = (),\n    state_schema: type[AgentState[ResponseT]] | None = None,\n    context_schema: type[ContextT] | None = None,\n    checkpointer: Checkpointer | None = None,\n    store: BaseStore | None = None,\n    interrupt_before: list[str] | None = None,\n    interrupt_after: list[str] | None = None,\n    debug: bool = False,\n    name: str | None = None,\n    cache: BaseCache | None = None,\n) -&gt; CompiledStateGraph[\n    AgentState[ResponseT], ContextT, _InputAgentState, _OutputAgentState[ResponseT]\n]:\n</code></pre>"},{"location":"zh/api-reference/agent/#_2","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 model str \u662f - \u53ef\u7531 <code>load_chat_model</code> \u52a0\u8f7d\u7684\u6a21\u578b\u6807\u8bc6\u7b26\u5b57\u7b26\u4e32\u3002\u53ef\u6307\u5b9a\u4e3a \"provider:model-name\" \u683c\u5f0f tools Sequence[BaseTool | Callable | dict[str, Any]] | None \u5426 None \u667a\u80fd\u4f53\u53ef\u7528\u7684\u5de5\u5177\u5217\u8868 system_prompt str | SystemMessage | None \u5426 None \u667a\u80fd\u4f53\u7684\u81ea\u5b9a\u4e49\u7cfb\u7edf\u63d0\u793a\u8bcd middleware Sequence[AgentMiddleware[AgentState[ResponseT], ContextT]] \u5426 () \u667a\u80fd\u4f53\u7684\u4e2d\u95f4\u4ef6 response_format ResponseFormat[ResponseT] | type[ResponseT] | None \u5426 None \u667a\u80fd\u4f53\u7684\u54cd\u5e94\u683c\u5f0f state_schema type[AgentState[ResponseT]] | None \u5426 None \u667a\u80fd\u4f53\u7684\u72b6\u6001\u6a21\u5f0f context_schema type[ContextT] | None \u5426 None \u667a\u80fd\u4f53\u7684\u4e0a\u4e0b\u6587\u6a21\u5f0f checkpointer Checkpointer | None \u5426 None \u72b6\u6001\u6301\u4e45\u5316\u7684\u68c0\u67e5\u70b9 store BaseStore | None \u5426 None \u6570\u636e\u6301\u4e45\u5316\u7684\u5b58\u50a8 interrupt_before list[str] | None \u5426 None \u6267\u884c\u524d\u8981\u4e2d\u65ad\u7684\u8282\u70b9 interrupt_after list[str] | None \u5426 None \u6267\u884c\u540e\u8981\u4e2d\u65ad\u7684\u8282\u70b9 debug bool \u5426 False \u542f\u7528\u8c03\u8bd5\u6a21\u5f0f name str | None \u5426 None \u667a\u80fd\u4f53\u540d\u79f0 cache BaseCache | None \u5426 None \u7f13\u5b58"},{"location":"zh/api-reference/agent/#_3","title":"\u6ce8\u610f\u4e8b\u9879","text":"<p>\u6b64\u51fd\u6570\u63d0\u4f9b\u4e0e <code>langchain</code> \u5b98\u65b9 <code>create_agent</code> \u5b8c\u5168\u76f8\u540c\u7684\u529f\u80fd\uff0c\u4f46\u62d3\u5c55\u4e86\u6a21\u578b\u9009\u62e9\u3002\u4e3b\u8981\u533a\u522b\u5728\u4e8e <code>model</code> \u53c2\u6570\u5fc5\u987b\u662f\u53ef\u7531 <code>load_chat_model</code> \u51fd\u6570\u52a0\u8f7d\u7684\u5b57\u7b26\u4e32\uff0c\u5141\u8bb8\u4f7f\u7528\u6ce8\u518c\u7684\u6a21\u578b\u63d0\u4f9b\u8005\u8fdb\u884c\u66f4\u7075\u6d3b\u7684\u6a21\u578b\u9009\u62e9\u3002</p>"},{"location":"zh/api-reference/agent/#_4","title":"\u793a\u4f8b","text":"<pre><code>agent = create_agent(model=\"vllm:qwen3-4b\", tools=[get_current_time])\n</code></pre>"},{"location":"zh/api-reference/agent/#wrap_agent_as_tool","title":"wrap_agent_as_tool","text":"<p>\u5c06\u667a\u80fd\u4f53\u5305\u88c5\u4e3a\u5de5\u5177\u3002</p>"},{"location":"zh/api-reference/agent/#_5","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def wrap_agent_as_tool(\n    agent: CompiledStateGraph,\n    tool_name: Optional[str] = None,\n    tool_description: Optional[str] = None,\n    pre_input_hooks: Optional[\n        tuple[\n            Callable[[str, ToolRuntime], str],\n            Callable[[str, ToolRuntime], Awaitable[str]],\n        ]\n        | Callable[[str, ToolRuntime], str]\n    ] = None,\n    post_output_hooks: Optional[\n        tuple[\n            Callable[[str, list[AnyMessage], ToolRuntime], Any],\n            Callable[[str, list[AnyMessage], ToolRuntime], Awaitable[Any]],\n        ]\n        | Callable[[str, list[AnyMessage], ToolRuntime], Any]\n    ] = None,\n) -&gt; BaseTool\n</code></pre>"},{"location":"zh/api-reference/agent/#_6","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 agent CompiledStateGraph \u662f - \u667a\u80fd\u4f53 tool_name Optional[str] \u5426 None \u5de5\u5177\u540d\u79f0 tool_description Optional[str] \u5426 None \u5de5\u5177\u63cf\u8ff0 pre_input_hooks Optional[tuple[Callable[[str, ToolRuntime], str], Callable[[str, ToolRuntime], Awaitable[str]]] | Callable[[str, ToolRuntime], str]] \u5426 None Agent \u8f93\u5165\u9884\u5904\u7406\u51fd\u6570 post_output_hooks Optional[tuple[Callable[[str, list[AnyMessage], ToolRuntime], Any], Callable[[str, list[AnyMessage], ToolRuntime], Awaitable[Any]]] | Callable[[str, list[AnyMessage], ToolRuntime], Any]] \u5426 None Agent \u8f93\u51fa\u540e\u5904\u7406\u51fd\u6570"},{"location":"zh/api-reference/agent/#_7","title":"\u793a\u4f8b","text":"<pre><code>tool = wrap_agent_as_tool(agent)\n</code></pre>"},{"location":"zh/api-reference/agent/#wrap_all_agents_as_tool","title":"wrap_all_agents_as_tool","text":"<p>\u5c06\u6240\u6709\u667a\u80fd\u4f53\u5305\u88c5\u4e3a\u5355\u4e2a\u5de5\u5177\u3002</p>"},{"location":"zh/api-reference/agent/#_8","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def wrap_all_agents_as_tool(\n    agents: list[CompiledStateGraph],\n    tool_name: Optional[str] = None,\n    tool_description: Optional[str] = None,\n    pre_input_hooks: Optional[\n        tuple[\n            Callable[[str, ToolRuntime], str],\n            Callable[[str, ToolRuntime], Awaitable[str]],\n        ]\n        | Callable[[str, ToolRuntime], str]\n    ] = None,\n    post_output_hooks: Optional[\n        tuple[\n            Callable[[str, list[AnyMessage], ToolRuntime], Any],\n            Callable[[str, list[AnyMessage], ToolRuntime], Awaitable[Any]],\n        ]\n        | Callable[[str, list[AnyMessage], ToolRuntime], Any]\n    ] = None,\n) -&gt; BaseTool:\n</code></pre>"},{"location":"zh/api-reference/agent/#_9","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 agents list[CompiledStateGraph] \u662f - \u667a\u80fd\u4f53\u5217\u8868(\u81f3\u5c11\u5305\u542b2\u4e2a\uff0c\u4e14\u6bcf\u4e2a\u667a\u80fd\u4f53\u5fc5\u987b\u6709\u552f\u4e00\u7684\u540d\u79f0) tool_name Optional[str] \u5426 None \u5de5\u5177\u540d\u79f0 tool_description Optional[str] \u5426 None \u5de5\u5177\u63cf\u8ff0 pre_input_hooks Optional[tuple[Callable[[str, ToolRuntime], str], Callable[[str, ToolRuntime], Awaitable[str]]] | Callable[[str, ToolRuntime], str]] \u5426 None Agent \u8f93\u5165\u9884\u5904\u7406\u51fd\u6570 post_output_hooks Optional[tuple[Callable[[str, list[AnyMessage], ToolRuntime], Any], Callable[[str, list[AnyMessage], ToolRuntime], Awaitable[Any]]] | Callable[[str, list[AnyMessage], ToolRuntime], Any]] \u5426 None Agent \u8f93\u51fa\u540e\u5904\u7406\u51fd\u6570"},{"location":"zh/api-reference/agent/#_10","title":"\u793a\u4f8b","text":"<pre><code>tool = wrap_all_agents_as_tool([time_agent, weather_agent])\n</code></pre>"},{"location":"zh/api-reference/agent/#summarizationmiddleware","title":"SummarizationMiddleware","text":"<p>\u7528\u4e8e\u667a\u80fd\u4f53\u4e0a\u4e0b\u6587\u6458\u8981\u7684\u4e2d\u95f4\u4ef6\u3002</p>"},{"location":"zh/api-reference/agent/#_11","title":"\u7c7b\u5b9a\u4e49","text":"<pre><code>class SummarizationMiddleware(_SummarizationMiddleware):\n    def __init__(\n        self,\n        model: str,\n        *,\n        trigger: ContextSize | list[ContextSize] | None = None,\n        keep: ContextSize = (\"messages\", _DEFAULT_MESSAGES_TO_KEEP),\n        token_counter: TokenCounter = count_tokens_approximately,\n        summary_prompt: str = DEFAULT_SUMMARY_PROMPT,\n        trim_tokens_to_summarize: int | None = _DEFAULT_TRIM_TOKEN_LIMIT,\n        **deprecated_kwargs: Any,\n    ) -&gt; None\n</code></pre>"},{"location":"zh/api-reference/agent/#_12","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 model str \u662f - \u53ef\u7531 <code>load_chat_model</code> \u52a0\u8f7d\u7684\u6a21\u578b\u6807\u8bc6\u7b26\u5b57\u7b26\u4e32\u3002\u53ef\u6307\u5b9a\u4e3a \"provider:model-name\" \u683c\u5f0f trigger ContextSize | list[ContextSize] | None \u5426 None \u89e6\u53d1\u6458\u8981\u7684\u4e0a\u4e0b\u6587\u5927\u5c0f keep ContextSize \u5426 (\"messages\", _DEFAULT_MESSAGES_TO_KEEP) \u4fdd\u7559\u7684\u4e0a\u4e0b\u6587\u5927\u5c0f token_counter TokenCounter \u5426 count_tokens_approximately token \u8ba1\u6570\u5668 summary_prompt str \u5426 DEFAULT_SUMMARY_PROMPT \u6458\u8981\u63d0\u793a\u8bcd trim_tokens_to_summarize int | None \u5426 _DEFAULT_TRIM_TOKEN_LIMIT \u6458\u8981\u524d\u8981\u622a\u53d6\u7684 token \u6570"},{"location":"zh/api-reference/agent/#_13","title":"\u793a\u4f8b","text":"<pre><code>summarization_middleware = SummarizationMiddleware(model=\"vllm:qwen3-4b\")\n</code></pre>"},{"location":"zh/api-reference/agent/#llmtoolselectormiddleware","title":"LLMToolSelectorMiddleware","text":"<p>\u7528\u4e8e\u667a\u80fd\u4f53\u5de5\u5177\u9009\u62e9\u7684\u4e2d\u95f4\u4ef6\u3002</p>"},{"location":"zh/api-reference/agent/#_14","title":"\u7c7b\u5b9a\u4e49","text":"<pre><code>class LLMToolSelectorMiddleware(_LLMToolSelectorMiddleware):\n    def __init__(\n        self,\n        *,\n        model: str,\n        system_prompt: Optional[str] = None,\n        max_tools: Optional[int] = None,\n        always_include: Optional[list[str]] = None,\n    ) -&gt; None\n</code></pre>"},{"location":"zh/api-reference/agent/#_15","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 model str \u662f - \u53ef\u7531 <code>load_chat_model</code> \u52a0\u8f7d\u7684\u6a21\u578b\u6807\u8bc6\u7b26\u5b57\u7b26\u4e32\u3002\u53ef\u6307\u5b9a\u4e3a \"provider:model-name\" \u683c\u5f0f system_prompt Optional[str] \u5426 None \u7cfb\u7edf\u63d0\u793a\u8bcd max_tools Optional[int] \u5426 None \u6700\u5927\u5de5\u5177\u6570 always_include Optional[list[str]] \u5426 None \u603b\u662f\u5305\u542b\u7684\u5de5\u5177"},{"location":"zh/api-reference/agent/#_16","title":"\u793a\u4f8b","text":"<pre><code>llm_tool_selector_middleware = LLMToolSelectorMiddleware(model=\"vllm:qwen3-4b\")\n</code></pre>"},{"location":"zh/api-reference/agent/#planmiddleware","title":"PlanMiddleware","text":"<p>\u7528\u4e8e\u667a\u80fd\u4f53\u8ba1\u5212\u7ba1\u7406\u7684\u4e2d\u95f4\u4ef6\u3002</p>"},{"location":"zh/api-reference/agent/#_17","title":"\u7c7b\u5b9a\u4e49","text":"<pre><code>class PlanMiddleware(AgentMiddleware):\n    state_schema = PlanState\n    def __init__(\n        self,\n        *,\n        system_prompt: Optional[str] = None,\n        custom_plan_tool_descriptions: Optional[PlanToolDescription] = None,\n        use_read_plan_tool: bool = True,\n    ) -&gt; None:\n</code></pre>"},{"location":"zh/api-reference/agent/#_18","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 system_prompt Optional[str] \u5426 None \u7cfb\u7edf\u63d0\u793a\u8bcd custom_plan_tool_descriptions Optional[PlanToolDescription] \u5426 None \u81ea\u5b9a\u4e49\u8ba1\u5212\u5de5\u5177\u7684\u63cf\u8ff0 use_read_plan_tool bool \u5426 True \u662f\u5426\u4f7f\u7528\u8bfb\u8ba1\u5212\u5de5\u5177"},{"location":"zh/api-reference/agent/#_19","title":"\u793a\u4f8b","text":"<pre><code>plan_middleware = PlanMiddleware()\n</code></pre>"},{"location":"zh/api-reference/agent/#modelfallbackmiddleware","title":"ModelFallbackMiddleware","text":"<p>\u7528\u4e8e\u667a\u80fd\u4f53\u6a21\u578b\u56de\u9000\u7684\u4e2d\u95f4\u4ef6\u3002</p>"},{"location":"zh/api-reference/agent/#_20","title":"\u7c7b\u5b9a\u4e49","text":"<pre><code>class ModelFallbackMiddleware(_ModelFallbackMiddleware):\n    def __init__(\n        self,\n        first_model: str,\n        *additional_models: str,\n    ) -&gt; None\n</code></pre>"},{"location":"zh/api-reference/agent/#_21","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 first_model str \u662f - \u53ef\u7531 <code>load_chat_model</code> \u52a0\u8f7d\u7684\u6a21\u578b\u6807\u8bc6\u7b26\u5b57\u7b26\u4e32\u3002\u53ef\u6307\u5b9a\u4e3a \"provider:model-name\" \u683c\u5f0f additional_models str \u5426 - \u5907\u7528\u6a21\u578b\u5217\u8868"},{"location":"zh/api-reference/agent/#_22","title":"\u793a\u4f8b","text":"<pre><code>model_fallback_middleware = ModelFallbackMiddleware(\n    \"vllm:qwen3-4b\",\n    \"vllm:qwen3-8b\"\n)\n</code></pre>"},{"location":"zh/api-reference/agent/#llmtoolemulator","title":"LLMToolEmulator","text":"<p>\u7528\u4e8e\u4f7f\u7528\u5927\u6a21\u578b\u6765\u6a21\u62df\u5de5\u5177\u8c03\u7528\u7684\u4e2d\u95f4\u4ef6\u3002</p>"},{"location":"zh/api-reference/agent/#_23","title":"\u7c7b\u5b9a\u4e49","text":"<pre><code>class LLMToolEmulator(_LLMToolEmulator):\n    def __init__(\n        self,\n        *,\n        model: str,\n        tools: list[str | BaseTool] | None = None,\n    ) -&gt; None\n</code></pre>"},{"location":"zh/api-reference/agent/#_24","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 model str \u662f - \u53ef\u7531 <code>load_chat_model</code> \u52a0\u8f7d\u7684\u6a21\u578b\u6807\u8bc6\u7b26\u5b57\u7b26\u4e32\u3002\u53ef\u6307\u5b9a\u4e3a \"provider:model-name\" \u683c\u5f0f tools list[str | BaseTool] | None \u5426 None \u5de5\u5177\u5217\u8868"},{"location":"zh/api-reference/agent/#_25","title":"\u793a\u4f8b","text":"<pre><code>llm_tool_emulator = LLMToolEmulator(model=\"vllm:qwen3-4b\", tools=[get_current_time])\n</code></pre>"},{"location":"zh/api-reference/agent/#modelroutermiddleware","title":"ModelRouterMiddleware","text":"<p>\u7528\u4e8e\u6839\u636e\u8f93\u5165\u5185\u5bb9\u52a8\u6001\u8def\u7531\u5230\u5408\u9002\u6a21\u578b\u7684\u4e2d\u95f4\u4ef6\u3002</p>"},{"location":"zh/api-reference/agent/#_26","title":"\u7c7b\u5b9a\u4e49","text":"<pre><code>class ModelRouterMiddleware(AgentMiddleware):\n    state_schema = ModelRouterState\n    def __init__(\n        self,\n        router_model: str | BaseChatModel,\n        model_list: list[ModelDict],\n        router_prompt: Optional[str] = None,\n    ) -&gt; None\n</code></pre>"},{"location":"zh/api-reference/agent/#_27","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 router_model str | BaseChatModel \u662f - \u7528\u4e8e\u8def\u7531\u7684\u6a21\u578b\uff0c\u63a5\u6536\u5b57\u7b26\u4e32\u7c7b\u578b\uff08\u4f7f\u7528<code>load_chat_model</code>\u52a0\u8f7d\uff09\u6216\u8005\u76f4\u63a5\u4f20\u5165 ChatModel model_list list[ModelDict] \u662f - \u6a21\u578b\u5217\u8868\uff0c\u6bcf\u4e2a\u6a21\u578b\u9700\u8981\u5305\u542b <code>model_name</code> \u548c <code>model_description</code> \u4e24\u4e2a\u952e\uff0c\u540c\u65f6\u4e5f\u53ef\u4ee5\u9009\u62e9\u6027\u5730\u5305\u542b <code>tools</code>\u3001<code>model_kwargs</code>\u3001<code>model_instance</code>\u3001<code>model_system_prompt</code> \u8fd9\u56db\u4e2a\u952e router_prompt Optional[str] \u5426 None \u8def\u7531\u6a21\u578b\u7684\u63d0\u793a\u8bcd\uff0c\u5982\u679c\u4e3a None \u5219\u4f7f\u7528\u9ed8\u8ba4\u7684\u63d0\u793a\u8bcd"},{"location":"zh/api-reference/agent/#_28","title":"\u793a\u4f8b","text":"<pre><code>model_router_middleware = ModelRouterMiddleware(\n    router_model=\"vllm:qwen3-4b\",\n    model_list=[\n        {\n            \"model_name\": \"vllm:qwen3-4b\",\n            \"model_description\": \"\u9002\u5408\u666e\u901a\u4efb\u52a1\uff0c\u5982\u5bf9\u8bdd\u3001\u6587\u672c\u751f\u6210\u7b49\"\n        },\n        {\n            \"model_name\": \"vllm:qwen3-8b\",\n            \"model_description\": \"\u9002\u5408\u590d\u6742\u4efb\u52a1\uff0c\u5982\u4ee3\u7801\u751f\u6210\u3001\u6570\u636e\u5206\u6790\u7b49\",\n        },\n    ]\n)\n</code></pre>"},{"location":"zh/api-reference/agent/#handoffagentmiddleware","title":"HandoffAgentMiddleware","text":"<p>\u7528\u4e8e\u5b9e\u73b0\u591a\u667a\u80fd\u4f53\u5207\u6362\uff08handoffs\uff09\u7684\u4e2d\u95f4\u4ef6\u3002</p>"},{"location":"zh/api-reference/agent/#_29","title":"\u7c7b\u5b9a\u4e49","text":"<pre><code>class HandoffAgentMiddleware(AgentMiddleware):\n    state_schema = MultiAgentState\n    def __init__(\n        self,\n        agents_config: dict[str, AgentConfig],\n        custom_handoffs_tool_descriptions: Optional[dict[str, str]] = None,\n    ) -&gt; None:\n</code></pre>"},{"location":"zh/api-reference/agent/#_30","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 agents_config dict[str, AgentConfig] \u662f - \u667a\u80fd\u4f53\u914d\u7f6e\u5b57\u5178\uff0c\u952e\u4e3a\u667a\u80fd\u4f53\u540d\u79f0\uff0c\u503c\u4e3a\u667a\u80fd\u4f53\u914d\u7f6e custom_handoffs_tool_descriptions Optional[dict[str, str]] \u5426 None \u81ea\u5b9a\u4e49\u4ea4\u63a5\u5230\u5176\u5b83\u667a\u80fd\u4f53\u7684\u5de5\u5177\u63cf\u8ff0"},{"location":"zh/api-reference/agent/#_31","title":"\u793a\u4f8b","text":"<pre><code>handoffs_agent_middleware = HandoffsAgentMiddleware({\n    \"time_agent\":{\n        \"model\":\"vllm:qwen3-4b\",\n        \"prompt\":\"\u4f60\u662f\u4e00\u4e2a\u65f6\u95f4\u667a\u80fd\u4f53\uff0c\u8d1f\u8d23\u56de\u7b54\u65f6\u95f4\u76f8\u5173\u7684\u95ee\u9898\u3002\",\n        \"tools\":[get_current_time, transfer_to_default_agent],\n        \"handoffs\":[\"default_agent\"]\n    },\n    \"default_agent\":{\n        \"model\":\"vllm:qwen3-8b\",\n        \"prompt\":\"\u4f60\u662f\u4e00\u4e2a\u590d\u6742\u4efb\u52a1\u667a\u80fd\u4f53\uff0c\u8d1f\u8d23\u56de\u7b54\u590d\u6742\u4efb\u52a1\u76f8\u5173\u7684\u95ee\u9898\u3002\",\n        \"default\":True,\n        \"handoffs\":[\"time_agent\"]\n    }\n})\n</code></pre>"},{"location":"zh/api-reference/agent/#toolcallrepairmiddleware","title":"ToolCallRepairMiddleware","text":"<p>\u7528\u4e8e\u4fee\u590d\u65e0\u6548\u5de5\u5177\u8c03\u7528\u7684\u4e2d\u95f4\u4ef6\u3002</p>"},{"location":"zh/api-reference/agent/#_32","title":"\u7c7b\u5b9a\u4e49","text":"<pre><code>class ToolCallRepairMiddleware(AgentMiddleware):\n</code></pre>"},{"location":"zh/api-reference/agent/#_33","title":"\u793a\u4f8b","text":"<pre><code>tool_call_repair_middleware = ToolCallRepairMiddleware()\n</code></pre>"},{"location":"zh/api-reference/agent/#format_prompt","title":"format_prompt","text":"<p>\u7528\u4e8e\u683c\u5f0f\u5316\u63d0\u793a\u8bcd\u7684\u4e2d\u95f4\u4ef6\u3002</p>"},{"location":"zh/api-reference/agent/#_34","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>@dynamic_prompt\ndef format_prompt(request: ModelRequest) -&gt; str\n</code></pre>"},{"location":"zh/api-reference/agent/#planstate","title":"PlanState","text":"<p>\u7528\u4e8e Plan \u7684\u72b6\u6001 Schema\u3002</p>"},{"location":"zh/api-reference/agent/#_35","title":"\u7c7b\u5b9a\u4e49","text":"<pre><code>class Plan(TypedDict):\n    content: str\n    status: Literal[\"pending\", \"in_progress\", \"done\"]\n\n\nclass PlanState(AgentState):\n    plan: NotRequired[list[Plan]]\n</code></pre>"},{"location":"zh/api-reference/agent/#_36","title":"\u5c5e\u6027","text":"\u5c5e\u6027 \u7c7b\u578b \u63cf\u8ff0 plan NotRequired[list[Plan]] \u8ba1\u5212\u5217\u8868 plan.content str \u8ba1\u5212\u5185\u5bb9 plan.status Literal[\"pending\", \"in_progress\", \"done\"] \u8ba1\u5212\u72b6\u6001\uff0c\u53d6\u503c\u4e3a<code>pending</code>\u3001<code>in_progress</code>\u3001<code>done</code>"},{"location":"zh/api-reference/agent/#modeldict","title":"ModelDict","text":"<p>\u6a21\u578b\u5217\u8868\u7684\u7c7b\u578b\u3002</p>"},{"location":"zh/api-reference/agent/#_37","title":"\u7c7b\u5b9a\u4e49","text":"<pre><code>class ModelDict(TypedDict):\n    model_name: str\n    model_description: str\n    tools: NotRequired[list[BaseTool | dict[str, Any]]]\n    model_kwargs: NotRequired[dict[str, Any]]\n    model_instance: NotRequired[BaseChatModel]\n    model_system_prompt: NotRequired[str]\n</code></pre>"},{"location":"zh/api-reference/agent/#_38","title":"\u5c5e\u6027","text":"\u5c5e\u6027 \u7c7b\u578b \u5fc5\u586b \u63cf\u8ff0 model_name str \u662f \u6a21\u578b\u540d\u79f0 model_description str \u662f \u6a21\u578b\u63cf\u8ff0 tools NotRequired[list[BaseTool | dict[str, Any]]] \u5426 \u6a21\u578b\u53ef\u7528\u7684\u5de5\u5177 model_kwargs NotRequired[dict[str, Any]] \u5426 \u4f20\u9012\u7ed9\u6a21\u578b\u7684\u989d\u5916\u53c2\u6570 model_instance NotRequired[BaseChatModel] \u5426 \u6a21\u578b\u5b9e\u4f8b model_system_prompt NotRequired[str] \u5426 \u6a21\u578b\u7684\u7cfb\u7edf\u63d0\u793a\u8bcd"},{"location":"zh/api-reference/agent/#selectmodel","title":"SelectModel","text":"<p>\u7528\u4e8e\u9009\u62e9\u6a21\u578b\u7684\u5de5\u5177\u7c7b\u3002</p>"},{"location":"zh/api-reference/agent/#_39","title":"\u7c7b\u5b9a\u4e49","text":"<pre><code>class SelectModel(BaseModel):\n    \"\"\"Tool for model selection - Must call this tool to return the finally selected model\"\"\"\n\n    model_name: str = Field(\n        ...,\n        description=\"Selected model name (must be the full model name, for example, openai:gpt-4o)\",\n    )\n</code></pre>"},{"location":"zh/api-reference/agent/#_40","title":"\u5c5e\u6027","text":"\u5c5e\u6027 \u7c7b\u578b \u5fc5\u586b \u63cf\u8ff0 model_name str \u662f \u9009\u62e9\u7684\u6a21\u578b\u540d\u79f0\uff08\u5fc5\u987b\u662f\u5b8c\u6574\u7684\u6a21\u578b\u540d\u79f0\uff0c\u4f8b\u5982\uff0copenai:gpt-4o\uff09"},{"location":"zh/api-reference/agent/#multiagentstate","title":"MultiAgentState","text":"<p>\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u5207\u6362\u7684\u72b6\u6001 Schema\u3002</p>"},{"location":"zh/api-reference/agent/#_41","title":"\u7c7b\u5b9a\u4e49","text":"<pre><code>class MultiAgentState(AgentState):\n    active_agent: NotRequired[str]\n</code></pre>"},{"location":"zh/api-reference/agent/#_42","title":"\u5c5e\u6027","text":"\u5c5e\u6027 \u7c7b\u578b \u63cf\u8ff0 active_agent NotRequired[str] \u5f53\u524d\u6fc0\u6d3b\u7684\u667a\u80fd\u4f53\u540d\u79f0"},{"location":"zh/api-reference/agent/#agentconfig","title":"AgentConfig","text":"<p>\u667a\u80fd\u4f53\u914d\u7f6e\u7684\u7c7b\u578b\u3002</p>"},{"location":"zh/api-reference/agent/#_43","title":"\u7c7b\u5b9a\u4e49","text":"<pre><code>class AgentConfig(TypedDict):\n    model: NotRequired[str | BaseChatModel]\n    prompt: str | SystemMessage\n    tools: NotRequired[list[BaseTool | dict[str, Any]]]\n    default: NotRequired[bool]\n    handoffs: list[str] | Literal[\"all\"]\n</code></pre>"},{"location":"zh/api-reference/agent/#_44","title":"\u5c5e\u6027","text":"\u5c5e\u6027 \u7c7b\u578b \u5fc5\u586b \u63cf\u8ff0 model NotRequired[str | BaseChatModel] \u5426 \u6a21\u578b\u540d\u79f0\u6216\u6a21\u578b\u5b9e\u4f8b prompt str | SystemMessage \u662f \u667a\u80fd\u4f53\u7684\u63d0\u793a\u8bcd tools list[BaseTool | dict[str, Any]] \u662f \u667a\u80fd\u4f53\u53ef\u7528\u7684\u5de5\u5177 default NotRequired[bool] \u5426 \u662f\u5426\u4e3a\u9ed8\u8ba4\u667a\u80fd\u4f53 handoffs list[str] | Literal[\"all\"] \u662f \u53ef\u4ee5\u4ea4\u63a5\u5230\u7684\u667a\u80fd\u4f53\u540d\u79f0\u5217\u8868\uff0c\u6216\"all\"\u8868\u793a\u6240\u6709\u667a\u80fd\u4f53"},{"location":"zh/api-reference/chat_model/","title":"ChatModel \u6a21\u5757 API \u53c2\u8003\u6587\u6863","text":""},{"location":"zh/api-reference/chat_model/#register_model_provider","title":"register_model_provider","text":"<p>\u6ce8\u518c\u804a\u5929\u6a21\u578b\u7684\u63d0\u4f9b\u8005\u3002</p>"},{"location":"zh/api-reference/chat_model/#_1","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def register_model_provider(\n    provider_name: str,\n    chat_model: ChatModelType,\n    base_url: Optional[str] = None,\n    model_profiles: Optional[dict[str, dict[str, Any]]] = None,\n    compatibility_options: Optional[CompatibilityOptions] = None,\n) -&gt; None:\n</code></pre>"},{"location":"zh/api-reference/chat_model/#_2","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 provider_name str \u662f - \u81ea\u5b9a\u4e49\u63d0\u4f9b\u5546\u540d\u79f0 chat_model ChatModelType \u662f - ChatModel \u7c7b\u6216\u652f\u6301\u7684\u63d0\u4f9b\u8005\u5b57\u7b26\u4e32\u7c7b\u578b base_url Optional[str] \u5426 None \u63d0\u4f9b\u5546\u7684 BaseURL model_profiles Optional[dict[str, dict[str, Any]]] \u5426 None \u63d0\u4f9b\u5546\u6240\u652f\u6301\u7684\u6a21\u578b\u7684profile\uff0c\u683c\u5f0f\u4e3a <code>{model_name: model_profile}</code> compatibility_options Optional[CompatibilityOptions] \u5426 None \u517c\u5bb9\u6027\u9009\u9879"},{"location":"zh/api-reference/chat_model/#_3","title":"\u793a\u4f8b","text":"<pre><code>register_model_provider(\"fakechat\",FakeChatModel)\nregister_model_provider(\"vllm\", \"openai-compatible\", base_url=\"http://localhost:8000/v1\")\n</code></pre>"},{"location":"zh/api-reference/chat_model/#batch_register_model_provider","title":"batch_register_model_provider","text":"<p>\u6279\u91cf\u6ce8\u518c\u6a21\u578b\u63d0\u4f9b\u8005\u3002</p>"},{"location":"zh/api-reference/chat_model/#_4","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def batch_register_model_provider(\n    providers: list[ChatModelProvider],\n) -&gt; None:\n</code></pre>"},{"location":"zh/api-reference/chat_model/#_5","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 providers list[ChatModelProvider] \u662f - \u63d0\u4f9b\u8005\u914d\u7f6e\u5217\u8868"},{"location":"zh/api-reference/chat_model/#_6","title":"\u793a\u4f8b","text":"<pre><code>batch_register_model_provider([\n    {\"provider_name\": \"fakechat\", \"chat_model\": FakeChatModel},\n    {\"provider_name\": \"vllm\", \"chat_model\": \"openai-compatible\", \"base_url\": \"http://localhost:8000/v1\"},\n])\n</code></pre>"},{"location":"zh/api-reference/chat_model/#load_chat_model","title":"load_chat_model","text":"<p>\u4ece\u5df2\u6ce8\u518c\u7684\u63d0\u4f9b\u8005\u52a0\u8f7d\u804a\u5929\u6a21\u578b\u3002</p>"},{"location":"zh/api-reference/chat_model/#_7","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def load_chat_model(\n    model: str,\n    *,\n    model_provider: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; BaseChatModel:\n</code></pre>"},{"location":"zh/api-reference/chat_model/#_8","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 model str \u662f - \u6a21\u578b\u540d\u79f0\uff0c\u683c\u5f0f\u4e3a <code>model_name</code> \u6216 <code>provider_name:model_name</code> model_provider Optional[str] \u5426 None \u6a21\u578b\u63d0\u4f9b\u8005\u540d\u79f0 **kwargs Any \u5426 - \u989d\u5916\u7684\u6a21\u578b\u53c2\u6570"},{"location":"zh/api-reference/chat_model/#_9","title":"\u793a\u4f8b","text":"<pre><code>model = load_chat_model(\"vllm:qwen3-4b\")\n</code></pre>"},{"location":"zh/api-reference/chat_model/#create_openai_compatible_model","title":"create_openai_compatible_model","text":"<p>\u521b\u5efa\u4e00\u4e2a OpenAI \u517c\u5bb9\u7684\u804a\u5929\u6a21\u578b\u7c7b\u3002</p>"},{"location":"zh/api-reference/chat_model/#_10","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def create_openai_compatible_model(\n    model_provider: str,\n    base_url: Optional[str] = None,\n    compatibility_options: Optional[CompatibilityOptions] = None,\n    model_profiles: Optional[dict[str, dict[str, Any]]] = None,\n    chat_model_cls_name: Optional[str] = None,\n) -&gt; type[BaseChatModel]:\n</code></pre>"},{"location":"zh/api-reference/chat_model/#_11","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 model_provider str \u662f - \u6a21\u578b\u63d0\u4f9b\u8005\u540d\u79f0 base_url Optional[str] \u5426 None \u6a21\u578b\u63d0\u4f9b\u8005\u7684 BaseURL compatibility_options Optional[CompatibilityOptions] \u5426 None \u517c\u5bb9\u6027\u9009\u9879 model_profiles Optional[dict[str, dict[str, Any]]] \u5426 None \u6a21\u578b\u63d0\u4f9b\u8005\u6240\u652f\u6301\u7684\u6a21\u578b\u7684profile\uff0c\u683c\u5f0f\u4e3a <code>{model_name: model_profile}</code> chat_model_cls_name Optional[str] \u5426 None \u81ea\u5b9a\u4e49\u7684\u804a\u5929\u6a21\u578b\u7c7b\u540d"},{"location":"zh/api-reference/chat_model/#_12","title":"\u8fd4\u56de\u503c","text":"\u7c7b\u578b \u63cf\u8ff0 type[BaseChatModel] \u52a8\u6001\u521b\u5efa\u7684 OpenAI \u517c\u5bb9\u804a\u5929\u6a21\u578b\u7c7b"},{"location":"zh/api-reference/chat_model/#_13","title":"\u793a\u4f8b","text":"<pre><code>ChatVLLM = create_openai_compatible_model(\n    model_provider=\"vllm\",\n    base_url=\"http://localhost:8000/v1\",\n    chat_model_cls_name=\"ChatVLLM\",\n)\n</code></pre>"},{"location":"zh/api-reference/chat_model/#chatmodeltype","title":"ChatModelType","text":"<p>\u6ce8\u518c\u6a21\u578b\u63d0\u4f9b\u5546\u65f6<code>chat_model</code>\u53c2\u6570\u652f\u6301\u7684\u7c7b\u578b\u3002</p>"},{"location":"zh/api-reference/chat_model/#_14","title":"\u7c7b\u578b\u5b9a\u4e49","text":"<pre><code>ChatModelType = Union[type[BaseChatModel], Literal[\"openai-compatible\"]]\n</code></pre>"},{"location":"zh/api-reference/chat_model/#toolchoicetype","title":"ToolChoiceType","text":"<p><code>tool_choice</code>\u53c2\u6570\u652f\u6301\u7684\u7c7b\u578b\u3002</p>"},{"location":"zh/api-reference/chat_model/#_15","title":"\u7c7b\u578b\u5b9a\u4e49","text":"<pre><code>ToolChoiceType = list[Literal[\"auto\", \"none\", \"required\", \"specific\"]]\n</code></pre>"},{"location":"zh/api-reference/chat_model/#responseformattype","title":"ResponseFormatType","text":"<p><code>response_format</code>\u652f\u6301\u7684\u7c7b\u578b\u3002</p>"},{"location":"zh/api-reference/chat_model/#_16","title":"\u7c7b\u578b\u5b9a\u4e49","text":"<pre><code>ResponseFormatType = list[Literal[\"json_schema\", \"json_mode\"]]\n</code></pre>"},{"location":"zh/api-reference/chat_model/#reasoningkeeppolicy","title":"ReasoningKeepPolicy","text":"<p>messages\u5217\u8868\u4e2dreasoning_content\u5b57\u6bb5\u7684\u4fdd\u7559\u7b56\u7565\u3002</p>"},{"location":"zh/api-reference/chat_model/#_17","title":"\u7c7b\u578b\u5b9a\u4e49","text":"<pre><code>ReasoningKeepPolicy = Literal[\"never\", \"current\", \"all\"]\n</code></pre>"},{"location":"zh/api-reference/chat_model/#compatibilityoptions","title":"CompatibilityOptions","text":"<p>\u6a21\u578b\u63d0\u4f9b\u5546\u7684\u517c\u5bb9\u6027\u9009\u9879\u3002</p>"},{"location":"zh/api-reference/chat_model/#_18","title":"\u7c7b\u5b9a\u4e49","text":"<pre><code>class CompatibilityOptions(TypedDict):\n    supported_tool_choice: NotRequired[ToolChoiceType]\n    supported_response_format: NotRequired[ResponseFormatType]\n    reasoning_keep_policy: NotRequired[ReasoningKeepPolicy]\n    include_usage: NotRequired[bool]\n</code></pre>"},{"location":"zh/api-reference/chat_model/#_19","title":"\u5b57\u6bb5\u8bf4\u660e","text":"\u5b57\u6bb5 \u7c7b\u578b \u5fc5\u586b \u63cf\u8ff0 supported_tool_choice NotRequired[ToolChoiceType] \u5426 \u652f\u6301\u7684 <code>tool_choice</code> \u7b56\u7565\u5217\u8868 supported_response_format NotRequired[ResponseFormatType] \u5426 \u652f\u6301\u7684 <code>response_format</code> \u65b9\u6cd5\u5217\u8868 reasoning_keep_policy NotRequired[ReasoningKeepPolicy] \u5426 \u4f20\u7ed9\u6a21\u578b\u7684\u5386\u53f2\u6d88\u606f\uff08messages\uff09\u4e2d <code>reasoning_content</code> \u5b57\u6bb5\u7684\u4fdd\u7559\u7b56\u7565\u3002\u53ef\u9009\u503c\u6709<code>never</code>\u3001<code>current</code>\u3001<code>all</code> include_usage NotRequired[bool] \u5426 \u662f\u5426\u5728\u6700\u540e\u4e00\u6761\u6d41\u5f0f\u8fd4\u56de\u7ed3\u679c\u4e2d\u5305\u542b <code>usage</code> \u4fe1\u606f"},{"location":"zh/api-reference/chat_model/#chatmodelprovider","title":"ChatModelProvider","text":"<p>\u804a\u5929\u6a21\u578b\u63d0\u4f9b\u8005\u914d\u7f6e\u7c7b\u578b\u3002</p>"},{"location":"zh/api-reference/chat_model/#_20","title":"\u7c7b\u5b9a\u4e49","text":"<pre><code>class ChatModelProvider(TypedDict):\n    provider_name: str\n    chat_model: ChatModelType\n    base_url: NotRequired[str]\n    model_profiles: NotRequired[dict[str, dict[str, Any]]]\n    compatibility_options: NotRequired[CompatibilityOptions]\n</code></pre>"},{"location":"zh/api-reference/chat_model/#_21","title":"\u5b57\u6bb5\u8bf4\u660e","text":"\u5b57\u6bb5 \u7c7b\u578b \u5fc5\u586b \u63cf\u8ff0 provider_name str \u662f \u63d0\u4f9b\u8005\u540d\u79f0 chat_model ChatModelType \u662f \u652f\u6301\u4f20\u5165\u5bf9\u8bdd\u6a21\u578b\u7c7b\u6216\u5b57\u7b26\u4e32\uff08\u76ee\u524d\u53ea\u652f\u6301<code>openai-compatible</code>\uff09 base_url NotRequired[str] \u5426 \u57fa\u7840 URL model_profiles NotRequired[dict[str, dict[str, Any]]] \u5426 \u63d0\u4f9b\u5546\u6240\u652f\u6301\u7684\u6a21\u578b\u7684profile\uff0c\u683c\u5f0f\u4e3a <code>{model_name: model_profile}</code> compatibility_options NotRequired[CompatibilityOptions] \u5426 \u6a21\u578b\u63d0\u4f9b\u5546\u517c\u5bb9\u6027\u9009\u9879"},{"location":"zh/api-reference/embeddings/","title":"Embeddings \u6a21\u5757 API \u53c2\u8003\u6587\u6863","text":""},{"location":"zh/api-reference/embeddings/#register_embeddings_provider","title":"register_embeddings_provider","text":"<p>\u6ce8\u518c\u5d4c\u5165\u6a21\u578b\u7684\u63d0\u4f9b\u8005\u3002</p>"},{"location":"zh/api-reference/embeddings/#_1","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def register_embeddings_provider(\n    provider_name: str,\n    embeddings_model: EmbeddingsType,\n    base_url: Optional[str] = None,\n) -&gt; None:\n</code></pre>"},{"location":"zh/api-reference/embeddings/#_2","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 provider_name str \u662f - \u81ea\u5b9a\u4e49\u63d0\u4f9b\u8005\u540d\u79f0 embeddings_model EmbeddingsType \u662f - \u5d4c\u5165\u6a21\u578b\u7c7b\u6216\u652f\u6301\u7684\u63d0\u4f9b\u8005\u5b57\u7b26\u4e32\u7c7b\u578b base_url Optional[str] \u5426 None \u63d0\u4f9b\u8005\u7684 BaseURL"},{"location":"zh/api-reference/embeddings/#_3","title":"\u793a\u4f8b","text":"<pre><code>register_embeddings_provider(\"fakeembeddings\", FakeEmbeddings)\nregister_embeddings_provider(\"vllm\", \"openai-compatible\", base_url=\"http://localhost:8000/v1\")\n</code></pre>"},{"location":"zh/api-reference/embeddings/#batch_register_embeddings_provider","title":"batch_register_embeddings_provider","text":"<p>\u6279\u91cf\u6ce8\u518c\u5d4c\u5165\u6a21\u578b\u63d0\u4f9b\u8005\u3002</p>"},{"location":"zh/api-reference/embeddings/#_4","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def batch_register_embeddings_provider(\n    providers: list[EmbeddingProvider]\n) -&gt; None:\n</code></pre>"},{"location":"zh/api-reference/embeddings/#_5","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 providers list[EmbeddingProvider] \u662f - \u63d0\u4f9b\u8005\u914d\u7f6e\u5217\u8868"},{"location":"zh/api-reference/embeddings/#_6","title":"\u793a\u4f8b","text":"<pre><code>batch_register_embeddings_provider([\n    {\"provider_name\": \"fakeembeddings\", \"embeddings_model\": FakeEmbeddings},\n    {\"provider_name\": \"vllm\", \"embeddings_model\": \"openai-compatible\", \"base_url\": \"http://localhost:8000/v1\"},\n])\n</code></pre>"},{"location":"zh/api-reference/embeddings/#load_embeddings","title":"load_embeddings","text":"<p>\u4ece\u5df2\u6ce8\u518c\u7684\u63d0\u4f9b\u8005\u52a0\u8f7d\u5d4c\u5165\u6a21\u578b\u3002</p>"},{"location":"zh/api-reference/embeddings/#_7","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def load_embeddings(\n    model: str,\n    *,\n    provider: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; Embeddings:\n</code></pre>"},{"location":"zh/api-reference/embeddings/#_8","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 model str \u662f - \u6a21\u578b\u540d\u79f0\uff0c\u683c\u5f0f\u4e3a <code>model_name</code> \u6216 <code>provider_name:model_name</code> provider Optional[str] \u5426 None \u6a21\u578b\u63d0\u4f9b\u8005\u540d\u79f0 **kwargs Any \u5426 - \u989d\u5916\u7684\u6a21\u578b\u53c2\u6570"},{"location":"zh/api-reference/embeddings/#_9","title":"\u793a\u4f8b","text":"<pre><code>embeddings = load_embeddings(\"vllm:qwen3-embedding-4b\")\n</code></pre>"},{"location":"zh/api-reference/embeddings/#create_openai_compatible_embedding","title":"create_openai_compatible_embedding","text":"<p>\u521b\u5efa\u4e00\u4e2a OpenAI \u517c\u5bb9\u7684\u5d4c\u5165\u6a21\u578b\u7c7b\u3002</p>"},{"location":"zh/api-reference/embeddings/#_10","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def create_openai_compatible_embedding(\n    embedding_provider: str,\n    base_url: Optional[str] = None,\n    embedding_model_cls_name: Optional[str] = None,\n) -&gt; type[Embeddings]:\n</code></pre>"},{"location":"zh/api-reference/embeddings/#_11","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 embedding_provider str \u662f - \u5d4c\u5165\u6a21\u578b\u63d0\u4f9b\u8005\u540d\u79f0 base_url Optional[str] \u5426 None \u6a21\u578b\u63d0\u4f9b\u8005\u7684 BaseURL embedding_model_cls_name Optional[str] \u5426 None \u81ea\u5b9a\u4e49\u7684\u5d4c\u5165\u6a21\u578b\u7c7b\u540d"},{"location":"zh/api-reference/embeddings/#_12","title":"\u8fd4\u56de\u503c","text":"\u7c7b\u578b \u63cf\u8ff0 type[Embeddings] \u52a8\u6001\u521b\u5efa\u7684 OpenAI \u517c\u5bb9\u5d4c\u5165\u6a21\u578b\u7c7b"},{"location":"zh/api-reference/embeddings/#_13","title":"\u793a\u4f8b","text":""},{"location":"zh/api-reference/embeddings/#embeddingstype","title":"EmbeddingsType","text":"<p>\u6ce8\u518c\u5d4c\u5165\u63d0\u4f9b\u5546\u65f6<code>embeddings_model</code>\u53c2\u6570\u652f\u6301\u7684\u7c7b\u578b\u3002</p>"},{"location":"zh/api-reference/embeddings/#_14","title":"\u7c7b\u578b\u5b9a\u4e49","text":"<pre><code>EmbeddingsType = Union[type[Embeddings], Literal[\"openai-compatible\"]]\n</code></pre>"},{"location":"zh/api-reference/embeddings/#embeddingprovider","title":"EmbeddingProvider","text":"<p>\u5d4c\u5165\u6a21\u578b\u63d0\u4f9b\u8005\u914d\u7f6e\u7c7b\u578b\u3002</p>"},{"location":"zh/api-reference/embeddings/#_15","title":"\u7c7b\u5b9a\u4e49","text":"<pre><code>class EmbeddingProvider(TypedDict):\n    provider_name: str\n    embeddings_model: EmbeddingsType\n    base_url: NotRequired[str]\n</code></pre>"},{"location":"zh/api-reference/embeddings/#_16","title":"\u5b57\u6bb5\u8bf4\u660e","text":"\u5b57\u6bb5 \u7c7b\u578b \u5fc5\u586b \u63cf\u8ff0 provider_name str \u662f \u63d0\u4f9b\u8005\u540d\u79f0 embeddings_model EmbeddingsType \u662f \u5d4c\u5165\u6a21\u578b\u7c7b\u6216\u5b57\u7b26\u4e32 base_url NotRequired[str] \u5426 \u57fa\u7840 URL"},{"location":"zh/api-reference/message_convert/","title":"Message Convert \u6a21\u5757 API \u53c2\u8003\u6587\u6863","text":""},{"location":"zh/api-reference/message_convert/#convert_reasoning_content_for_ai_message","title":"convert_reasoning_content_for_ai_message","text":"<p>\u5c06\u601d\u7ef4\u94fe\u5408\u5e76\u5230\u6700\u7ec8\u56de\u590d\u4e2d\u3002</p>"},{"location":"zh/api-reference/message_convert/#_1","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def convert_reasoning_content_for_ai_message(\n    model_response: AIMessage,\n    think_tag: Tuple[str, str] = (\"&lt;think&gt;\", \"&lt;/think&gt;\"),\n) -&gt; AIMessage\n</code></pre>"},{"location":"zh/api-reference/message_convert/#_2","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 model_response AIMessage \u662f - \u5305\u542b\u63a8\u7406\u5185\u5bb9\u7684 AI \u6d88\u606f think_tag Tuple[str, str] \u5426 <code>(\"&lt;think&gt;\",\"&lt;/think&gt;\")</code> \u63a8\u7406\u5185\u5bb9\u7684\u5f00\u59cb\u548c\u7ed3\u675f\u6807\u7b7e"},{"location":"zh/api-reference/message_convert/#_3","title":"\u793a\u4f8b","text":"<pre><code>response = convert_reasoning_content_for_ai_message(\n    response, think_tag=(\"&lt;start&gt;\", \"&lt;end&gt;\")\n)\n</code></pre>"},{"location":"zh/api-reference/message_convert/#convert_reasoning_content_for_chunk_iterator","title":"convert_reasoning_content_for_chunk_iterator","text":"<p>\u4e3a\u6d41\u5f0f\u6d88\u606f\u5757\u5408\u5e76\u63a8\u7406\u5185\u5bb9\u3002</p>"},{"location":"zh/api-reference/message_convert/#_4","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def convert_reasoning_content_for_chunk_iterator(\n    model_response: Iterator[AIMessageChunk | AIMessage],\n    think_tag: Tuple[str, str] = (\"&lt;think&gt;\", \"&lt;/think&gt;\"),\n) -&gt; Iterator[AIMessageChunk | AIMessage]\n</code></pre>"},{"location":"zh/api-reference/message_convert/#_5","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 model_response Iterator[AIMessageChunk | AIMessage] \u662f - \u6d88\u606f\u5757\u7684\u8fed\u4ee3\u5668 think_tag Tuple[str, str] \u5426 <code>(\"&lt;think&gt;\",\"&lt;/think&gt;\")</code> \u63a8\u7406\u5185\u5bb9\u7684\u5f00\u59cb\u548c\u7ed3\u675f\u6807\u7b7e"},{"location":"zh/api-reference/message_convert/#_6","title":"\u793a\u4f8b","text":"<pre><code>for chunk in convert_reasoning_content_for_chunk_iterator(\n    model.stream(\"Hello\"), think_tag=(\"&lt;think&gt;\", \"&lt;/think&gt;\")\n):\n    print(chunk.content, end=\"\", flush=True)\n</code></pre>"},{"location":"zh/api-reference/message_convert/#aconvert_reasoning_content_for_chunk_iterator","title":"aconvert_reasoning_content_for_chunk_iterator","text":"<p><code>convert_reasoning_content_for_chunk_iterator</code> \u7684\u5f02\u6b65\u7248\u672c\u3002</p>"},{"location":"zh/api-reference/message_convert/#_7","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>async def aconvert_reasoning_content_for_chunk_iterator(\n    model_response: AsyncIterator[AIMessageChunk | AIMessage],\n    think_tag: Tuple[str, str] = (\"&lt;think&gt;\", \"&lt;/think&gt;\"),\n) -&gt; AsyncIterator[AIMessageChunk | AIMessage]\n</code></pre>"},{"location":"zh/api-reference/message_convert/#_8","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 model_response AsyncIterator[AIMessageChunk | AIMessage] \u662f - \u6d88\u606f\u5757\u7684\u5f02\u6b65\u8fed\u4ee3\u5668 think_tag Tuple[str, str] \u5426 <code>(\"&lt;think&gt;\",\"&lt;/think&gt;\")</code> \u63a8\u7406\u5185\u5bb9\u7684\u5f00\u59cb\u548c\u7ed3\u675f\u6807\u7b7e"},{"location":"zh/api-reference/message_convert/#_9","title":"\u793a\u4f8b","text":"<pre><code>async for chunk in aconvert_reasoning_content_for_chunk_iterator(\n    model.astream(\"Hello\"), think_tag=(\"&lt;think&gt;\", \"&lt;/think&gt;\")\n):\n    print(chunk.content, end=\"\", flush=True)\n</code></pre>"},{"location":"zh/api-reference/message_convert/#merge_ai_message_chunk","title":"merge_ai_message_chunk","text":"<p>\u5c06\u6d41\u5f0f\u8f93\u51fa\u7684 chunks \u5408\u5e76\u4e3a\u4e00\u4e2a AIMessage\u3002</p>"},{"location":"zh/api-reference/message_convert/#_10","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def merge_ai_message_chunk(\n    chunks: Sequence[AIMessageChunk]\n) -&gt; AIMessage\n</code></pre>"},{"location":"zh/api-reference/message_convert/#_11","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 chunks Sequence[AIMessageChunk] \u662f - \u5f85\u5408\u5e76\u7684\u6d88\u606f\u5757\u5217\u8868"},{"location":"zh/api-reference/message_convert/#_12","title":"\u793a\u4f8b","text":"<pre><code>chunks = list(model.stream(\"Hello\"))\nmerged = merge_ai_message_chunk(chunks)\n</code></pre>"},{"location":"zh/api-reference/message_convert/#format_sequence","title":"format_sequence","text":"<p>\u5c06 BaseMessage\u3001Document \u6216\u5b57\u7b26\u4e32\u5217\u8868\u683c\u5f0f\u5316\u4e3a\u5355\u4e2a\u5b57\u7b26\u4e32\u3002</p>"},{"location":"zh/api-reference/message_convert/#_13","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def format_sequence(\n    inputs: List[Union[BaseMessage, Document, str]],\n    separator: str = \"-\",\n    with_num: bool = False\n) -&gt; str\n</code></pre>"},{"location":"zh/api-reference/message_convert/#_14","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 inputs List[Union[BaseMessage, Document, str]] \u662f - \u5f85\u683c\u5f0f\u5316\u7684\u9879\u76ee\u5217\u8868 separator str \u5426 \"-\" \u5206\u9694\u7b26\u5b57\u7b26\u4e32 with_num bool \u5426 False \u662f\u5426\u6dfb\u52a0\u6570\u5b57\u524d\u7f00"},{"location":"zh/api-reference/message_convert/#_15","title":"\u793a\u4f8b","text":"<pre><code>formatted = format_sequence(messages, separator=\"\\n\", with_num=True)\n</code></pre>"},{"location":"zh/api-reference/pipeline/","title":"Pipeline \u6a21\u5757 API \u53c2\u8003\u6587\u6863","text":""},{"location":"zh/api-reference/pipeline/#create_sequential_pipeline","title":"create_sequential_pipeline","text":"<p>\u5c06\u591a\u4e2a\u72b6\u6001\u76f8\u540c\u7684\u5b50\u56fe\u4ee5\u4e32\u884c\u65b9\u5f0f\u7ec4\u5408\u3002</p>"},{"location":"zh/api-reference/pipeline/#_1","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def create_sequential_pipeline(\n    sub_graphs: list[SubGraph],\n    state_schema: type[StateT],\n    graph_name: Optional[str] = None,\n    context_schema: type[ContextT] | None = None,\n    input_schema: type[InputT] | None = None,\n    output_schema: type[OutputT] | None = None,\n    checkpointer: Checkpointer | None = None,\n    store: BaseStore | None = None,\n    cache: BaseCache | None = None,\n) -&gt; CompiledStateGraph[StateT, ContextT, InputT, OutputT]:\n</code></pre>"},{"location":"zh/api-reference/pipeline/#_2","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 sub_graphs list[SubGraph] \u662f - \u8981\u7ec4\u5408\u7684\u72b6\u6001\u56fe\u5217\u8868 state_schema type[StateT] \u662f - \u6700\u7ec8\u751f\u6210\u56fe\u7684 State Schema graph_name Optional[str] \u5426 None \u6700\u7ec8\u751f\u6210\u56fe\u7684\u540d\u79f0 context_schema type[ContextT] | None \u5426 None \u6700\u7ec8\u751f\u6210\u56fe\u7684 Context Schema input_schema type[InputT] | None \u5426 None \u6700\u7ec8\u751f\u6210\u56fe\u7684\u8f93\u5165 Schema output_schema type[OutputT] | None \u5426 None \u6700\u7ec8\u751f\u6210\u56fe\u7684\u8f93\u51fa Schema checkpointer Checkpointer | None \u5426 None \u6700\u7ec8\u751f\u6210\u56fe\u7684 Checkpointer store BaseStore | None \u5426 None \u6700\u7ec8\u751f\u6210\u56fe\u7684 Store cache BaseCache | None \u5426 None \u6700\u7ec8\u751f\u6210\u56fe\u7684 Cache"},{"location":"zh/api-reference/pipeline/#_3","title":"\u793a\u4f8b","text":"<pre><code>create_sequential_pipeline(\n    sub_graphs=[graph1, graph2],\n    state_schema=State,\n    graph_name=\"sequential_pipeline\",\n    context_schema=Context,\n    input_schema=Input,\n    output_schema=Output,\n)\n</code></pre>"},{"location":"zh/api-reference/pipeline/#create_parallel_pipeline","title":"create_parallel_pipeline","text":"<p>\u5c06\u591a\u4e2a\u72b6\u6001\u76f8\u540c\u7684\u5b50\u56fe\u4ee5\u5e76\u884c\u65b9\u5f0f\u7ec4\u5408\u3002</p>"},{"location":"zh/api-reference/pipeline/#_4","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def create_parallel_pipeline(\n    sub_graphs: list[SubGraph],\n    state_schema: type[StateT],\n    graph_name: Optional[str] = None,\n    branches_fn: Optional[\n        Union[\n            Callable[..., list[Send]],\n            Callable[..., Awaitable[list[Send]]],\n        ]\n    ] = None,\n    context_schema: type[ContextT] | None = None,\n    input_schema: type[InputT] | None = None,\n    output_schema: type[OutputT] | None = None,\n    checkpointer: Checkpointer | None = None,\n    store: BaseStore | None = None,\n    cache: BaseCache | None = None,\n) -&gt; CompiledStateGraph[StateT, ContextT, InputT, OutputT]:\n</code></pre>"},{"location":"zh/api-reference/pipeline/#_5","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 sub_graphs list[SubGraph] \u662f - \u8981\u7ec4\u5408\u7684\u72b6\u6001\u56fe\u5217\u8868 state_schema type[StateT] \u662f - \u6700\u7ec8\u751f\u6210\u56fe\u7684 State Schema graph_name Optional[str] \u5426 None \u6700\u7ec8\u751f\u6210\u56fe\u7684\u540d\u79f0 branches_fn Optional[Union[Callable[..., list[Send]], Callable[..., Awaitable[list[Send]]]]] \u5426 None \u5e76\u884c\u5206\u652f\u51fd\u6570\uff0c\u8fd4\u56de Send \u5217\u8868\u63a7\u5236\u5e76\u884c\u6267\u884c context_schema type[ContextT] | None \u5426 None \u6700\u7ec8\u751f\u6210\u56fe\u7684 Context Schema input_schema type[InputT] | None \u5426 None \u6700\u7ec8\u751f\u6210\u56fe\u7684\u8f93\u5165 Schema output_schema type[OutputT] | None \u5426 None \u6700\u7ec8\u751f\u6210\u56fe\u7684\u8f93\u51fa Schema checkpointer Checkpointer | None \u5426 None \u6700\u7ec8\u751f\u6210\u56fe\u7684 Checkpointer store BaseStore | None \u5426 None \u6700\u7ec8\u751f\u6210\u56fe\u7684 Store cache BaseCache | None \u5426 None \u6700\u7ec8\u751f\u6210\u56fe\u7684 Cache"},{"location":"zh/api-reference/pipeline/#_6","title":"\u793a\u4f8b","text":"<pre><code>create_parallel_pipeline(\n    sub_graphs=[graph1, graph2],\n    state_schema=State,\n    graph_name=\"parallel_pipeline\",\n    branches_fn=lambda state: [Send(\"graph1\", state), Send(\"graph2\", state)],\n    context_schema=Context,\n    input_schema=Input,\n    output_schema=Output,\n)\n</code></pre>"},{"location":"zh/api-reference/tool_calling/","title":"Tool Calling \u6a21\u5757 API \u53c2\u8003\u6587\u6863","text":""},{"location":"zh/api-reference/tool_calling/#has_tool_calling","title":"has_tool_calling","text":"<p>\u68c0\u67e5\u6d88\u606f\u662f\u5426\u5305\u542b\u5de5\u5177\u8c03\u7528\u3002</p>"},{"location":"zh/api-reference/tool_calling/#_1","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def has_tool_calling(\n    message: AIMessage\n) -&gt; bool\n</code></pre>"},{"location":"zh/api-reference/tool_calling/#_2","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 message AIMessage \u662f - \u5f85\u68c0\u67e5\u7684\u6d88\u606f"},{"location":"zh/api-reference/tool_calling/#_3","title":"\u793a\u4f8b","text":"<pre><code>if has_tool_calling(response):\n    # \u5904\u7406\u5de5\u5177\u8c03\u7528\n    pass\n</code></pre>"},{"location":"zh/api-reference/tool_calling/#parse_tool_calling","title":"parse_tool_calling","text":"<p>\u4ece\u6d88\u606f\u4e2d\u89e3\u6790\u5de5\u5177\u8c03\u7528\u53c2\u6570\u3002</p>"},{"location":"zh/api-reference/tool_calling/#_4","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def parse_tool_calling(\n    message: AIMessage, first_tool_call_only: bool = False\n) -&gt; Union[tuple[str, dict], list[tuple[str, dict]]]\n</code></pre>"},{"location":"zh/api-reference/tool_calling/#_5","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 message AIMessage \u662f - \u5f85\u89e3\u6790\u7684\u6d88\u606f first_tool_call_only bool \u5426 False \u662f\u5426\u4ec5\u8fd4\u56de\u7b2c\u4e00\u4e2a\u5de5\u5177\u8c03\u7528"},{"location":"zh/api-reference/tool_calling/#_6","title":"\u793a\u4f8b","text":"<pre><code># \u83b7\u53d6\u6240\u6709\u5de5\u5177\u8c03\u7528\ntool_calls = parse_tool_calling(response)\n\n# \u4ec5\u83b7\u53d6\u7b2c\u4e00\u4e2a\u5de5\u5177\u8c03\u7528\nname, args = parse_tool_calling(response, first_tool_call_only=True)\n</code></pre>"},{"location":"zh/api-reference/tool_calling/#human_in_the_loop","title":"human_in_the_loop","text":"<p>\u4e3a\u540c\u6b65\u5de5\u5177\u51fd\u6570\u6dfb\u52a0\"\u4eba\u5728\u56de\u8def\"\u4eba\u5de5\u5ba1\u6838\u80fd\u529b\u7684\u88c5\u9970\u5668\u3002</p>"},{"location":"zh/api-reference/tool_calling/#_7","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def human_in_the_loop(\n    func: Optional[Callable] = None,\n    *,\n    handler: Optional[HumanInterruptHandler] = None\n) -&gt; Union[Callable[[Callable], BaseTool], BaseTool]\n</code></pre>"},{"location":"zh/api-reference/tool_calling/#_8","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 func Optional[Callable] \u5426 None \u5f85\u88c5\u9970\u7684\u540c\u6b65\u51fd\u6570\uff08\u88c5\u9970\u5668\u8bed\u6cd5\u7cd6\uff09 handler Optional[HumanInterruptHandler] \u5426 None \u81ea\u5b9a\u4e49\u4e2d\u65ad\u5904\u7406\u51fd\u6570"},{"location":"zh/api-reference/tool_calling/#_9","title":"\u793a\u4f8b","text":"<pre><code>@human_in_the_loop\ndef get_current_time():\n    \"\"\"\u83b7\u53d6\u5f53\u524d\u65f6\u95f4\"\"\"\n    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n</code></pre>"},{"location":"zh/api-reference/tool_calling/#human_in_the_loop_async","title":"human_in_the_loop_async","text":"<p>\u4e3a\u5f02\u6b65\u5de5\u5177\u51fd\u6570\u6dfb\u52a0\"\u4eba\u5728\u56de\u8def\"\u4eba\u5de5\u5ba1\u6838\u80fd\u529b\u7684\u88c5\u9970\u5668\u3002</p>"},{"location":"zh/api-reference/tool_calling/#_10","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def human_in_the_loop_async(\n    func: Optional[Callable] = None,\n    *,\n    handler: Optional[HumanInterruptHandler] = None\n) -&gt; Union[Callable[[Callable], BaseTool], BaseTool]\n</code></pre>"},{"location":"zh/api-reference/tool_calling/#_11","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 func Optional[Callable] \u5426 None \u5f85\u88c5\u9970\u7684\u5f02\u6b65\u51fd\u6570\uff08\u88c5\u9970\u5668\u8bed\u6cd5\u7cd6\uff09 handler Optional[HumanInterruptHandler] \u5426 None \u81ea\u5b9a\u4e49\u4e2d\u65ad\u5904\u7406\u51fd\u6570"},{"location":"zh/api-reference/tool_calling/#_12","title":"\u793a\u4f8b","text":"<pre><code>@human_in_the_loop_async\nasync def get_current_time():\n    \"\"\"\u83b7\u53d6\u5f53\u524d\u65f6\u95f4\"\"\"\n    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n</code></pre>"},{"location":"zh/api-reference/tool_calling/#interruptparams","title":"InterruptParams","text":"<p>\u4f20\u9012\u7ed9\u4e2d\u65ad\u5904\u7406\u51fd\u6570\u7684\u53c2\u6570\u7c7b\u578b\u3002</p>"},{"location":"zh/api-reference/tool_calling/#_13","title":"\u7c7b\u5b9a\u4e49","text":"<pre><code>class InterruptParams(TypedDict):\n    tool_call_name: str\n    tool_call_args: Dict[str, Any]\n    tool: BaseTool\n</code></pre>"},{"location":"zh/api-reference/tool_calling/#_14","title":"\u5b57\u6bb5\u8bf4\u660e","text":"\u5b57\u6bb5 \u7c7b\u578b \u5fc5\u586b \u63cf\u8ff0 tool_call_name str \u662f \u5de5\u5177\u8c03\u7528\u540d\u79f0 tool_call_args Dict[str, Any] \u662f \u5de5\u5177\u8c03\u7528\u53c2\u6570 tool BaseTool \u662f \u5de5\u5177\u5b9e\u4f8b"},{"location":"zh/api-reference/tool_calling/#humaninterrupthandler","title":"HumanInterruptHandler","text":"<p>\u4e2d\u65ad\u5904\u7406\u5668\u51fd\u6570\u7684\u7c7b\u578b\u522b\u540d\u3002</p>"},{"location":"zh/api-reference/tool_calling/#_15","title":"\u7c7b\u578b\u5b9a\u4e49","text":"<pre><code>HumanInterruptHandler = Callable[[InterruptParams], Any]\n</code></pre>"},{"location":"zh/getting-started-guide/chat/","title":"\u5bf9\u8bdd\u6a21\u578b\u7ba1\u7406","text":""},{"location":"zh/getting-started-guide/chat/#_2","title":"\u6982\u8ff0","text":"<p>LangChain \u7684 <code>init_chat_model</code> \u51fd\u6570\u4ec5\u652f\u6301\u6709\u9650\u7684\u6a21\u578b\u63d0\u4f9b\u5546\u3002\u672c\u5e93\u63d0\u4f9b\u66f4\u7075\u6d3b\u7684\u5bf9\u8bdd\u6a21\u578b\u7ba1\u7406\u65b9\u6848\uff0c\u652f\u6301\u81ea\u5b9a\u4e49\u6a21\u578b\u63d0\u4f9b\u5546\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u63a5\u5165\u672a\u5185\u7f6e\u652f\u6301\u7684\u6a21\u578b\u670d\u52a1\uff08\u5982 vLLM\u7b49\uff09\u7684\u573a\u666f\u3002</p>"},{"location":"zh/getting-started-guide/chat/#_3","title":"\u6ce8\u518c\u6a21\u578b\u63d0\u4f9b\u5546","text":"<p>\u6ce8\u518c\u5bf9\u8bdd\u6a21\u578b\u63d0\u4f9b\u5546\u9700\u8c03\u7528 <code>register_model_provider</code>\u3002\u5bf9\u4e8e\u4e0d\u540c\u7684\u60c5\u51b5\uff0c\u6ce8\u518c\u6b65\u9aa4\u7565\u6709\u4e0d\u540c\u3002</p>"},{"location":"zh/getting-started-guide/chat/#langchain","title":"\u5df2\u6709 LangChain \u5bf9\u8bdd\u6a21\u578b\u7c7b","text":"<p>\u82e5\u6a21\u578b\u63d0\u4f9b\u5546\u5df2\u6709\u73b0\u6210\u4e14\u5408\u9002\u7684 LangChain \u96c6\u6210\uff08\u8be6\u89c1\u5bf9\u8bdd\u6a21\u578b\u7c7b\u96c6\u6210\uff09\uff0c\u8bf7\u5c06\u76f8\u5e94\u7684\u96c6\u6210\u5bf9\u8bdd\u6a21\u578b\u7c7b\u4f5c\u4e3a chat_model \u53c2\u6570\u4f20\u5165\u3002</p>"},{"location":"zh/getting-started-guide/chat/#_4","title":"\u53c2\u6570\u8bf4\u660e","text":"\u53c2\u6570 \u8bf4\u660e <code>provider_name</code> \u6a21\u578b\u63d0\u4f9b\u5546\u540d\u79f0\uff0c\u7528\u4e8e\u540e\u7eed\u5728 <code>load_chat_model</code> \u4e2d\u5f15\u7528\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u662f <code>chat_model</code> LangChain \u5bf9\u8bdd\u6a21\u578b\u7c7b\u3002\u7c7b\u578b: <code>type[BaseChatModel]</code>\u5fc5\u586b: \u662f <code>base_url</code> API \u57fa\u7840\u5730\u5740\uff0c\u901a\u5e38\u65e0\u9700\u624b\u52a8\u8bbe\u7f6e\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u5426 <code>model_profiles</code> \u6a21\u578b\u914d\u7f6e\u4fe1\u606f\u5b57\u5178\u3002\u7c7b\u578b: <code>dict</code>\u5fc5\u586b: \u5426"},{"location":"zh/getting-started-guide/chat/#_5","title":"\u4ee3\u7801\u793a\u4f8b","text":"<pre><code>from langchain_core.language_models.fake_chat_models import FakeChatModel\nfrom langchain_dev_utils.chat_models import register_model_provider\n\nregister_model_provider(\n    provider_name=\"fake_provider\",\n    chat_model=FakeChatModel,\n)\n</code></pre>"},{"location":"zh/getting-started-guide/chat/#_6","title":"\u4f7f\u7528\u8bf4\u660e","text":"<ul> <li><code>FakeChatModel</code> \u4ec5\u7528\u4e8e\u6d4b\u8bd5\u3002\u5b9e\u9645\u4f7f\u7528\u4e2d\u5fc5\u987b\u4f20\u5165\u5177\u5907\u771f\u5b9e\u529f\u80fd\u7684 <code>ChatModel</code> \u7c7b\u3002</li> <li><code>provider_name</code> \u4ee3\u8868\u6a21\u578b\u63d0\u4f9b\u5546\u7684\u540d\u79f0\uff0c\u7528\u4e8e\u540e\u7eed\u5728 <code>load_chat_model</code> \u4e2d\u5f15\u7528\u3002\u540d\u79f0\u53ef\u81ea\u5b9a\u4e49\uff0c\u4f46\u4e0d\u8981\u5305\u542b <code>:</code>\u3001<code>-</code> \u7b49\u7279\u6b8a\u5b57\u7b26\u3002</li> </ul>"},{"location":"zh/getting-started-guide/chat/#_7","title":"\u53ef\u9009\u53c2\u6570\u8bf4\u660e","text":"<p>base_url</p> <p>\u6b64\u53c2\u6570\u901a\u5e38\u65e0\u9700\u8bbe\u7f6e\uff08\u56e0\u4e3a\u6a21\u578b\u7c7b\u5185\u90e8\u4e00\u822c\u5df2\u5b9a\u4e49\u9ed8\u8ba4\u7684 API \u5730\u5740\uff09\uff0c\u4ec5\u5f53\u9700\u8981\u8986\u76d6\u6a21\u578b\u7c7b\u9ed8\u8ba4\u5730\u5740\u65f6\u624d\u4f20\u5165 <code>base_url</code>\uff0c\u4e14\u4ec5\u5bf9\u5b57\u6bb5\u540d\u4e3a <code>api_base</code> \u6216 <code>base_url</code>\uff08\u542b\u522b\u540d\uff09\u7684\u5c5e\u6027\u751f\u6548\u3002</p> <p>model_profiles</p> <p>\u5982\u679c\u4f60\u7684 LangChain \u96c6\u6210\u5bf9\u8bdd\u6a21\u578b\u7c7b\u5df2\u5168\u9762\u652f\u6301 <code>profile</code> \u53c2\u6570\uff08\u5373\u53ef\u4ee5\u901a\u8fc7 <code>model.profile</code> \u76f4\u63a5\u8bbf\u95ee\u6a21\u578b\u7684\u76f8\u5173\u5c5e\u6027\uff0c\u4f8b\u5982 <code>max_input_tokens</code>\u3001<code>tool_calling</code> \u7b49\uff09\uff0c\u5219\u65e0\u9700\u989d\u5916\u8bbe\u7f6e <code>model_profiles</code>\u3002</p> <p>\u5982\u679c\u901a\u8fc7 <code>model.profile</code> \u8bbf\u95ee\u65f6\u8fd4\u56de\u7684\u662f\u4e00\u4e2a\u7a7a\u5b57\u5178 <code>{}</code>\uff0c\u8bf4\u660e\u8be5 LangChain \u5bf9\u8bdd\u6a21\u578b\u7c7b\u53ef\u80fd\u6682\u65f6\u672a\u652f\u6301 <code>profile</code> \u53c2\u6570\uff0c\u6b64\u65f6\u53ef\u4ee5\u624b\u52a8\u63d0\u4f9b <code>model_profiles</code>\u3002</p> <p><code>model_profiles</code> \u662f\u4e00\u4e2a\u5b57\u5178\uff0c\u5176\u6bcf\u4e00\u4e2a\u952e\u4e3a\u6a21\u578b\u540d\u79f0\uff0c\u503c\u4e3a\u5bf9\u5e94\u6a21\u578b\u7684 profile \u914d\u7f6e:</p> <pre><code>{\n    \"model_name_1\": {\n        \"max_input_tokens\": 100_000,\n        \"tool_calling\": True,\n        \"structured_output\": True,\n        # ... \u5176\u4ed6\u53ef\u9009\u5b57\u6bb5\n    },\n    \"model_name_2\": {\n        \"max_input_tokens\": 32768,\n        \"image_inputs\": True,\n        \"tool_calling\": False,\n        # ... \u5176\u4ed6\u53ef\u9009\u5b57\u6bb5\n    },\n    # \u53ef\u4ee5\u6709\u4efb\u610f\u591a\u4e2a\u6a21\u578b\u914d\u7f6e\n}\n</code></pre> <p>\u63d0\u793a</p> <p>\u63a8\u8350\u4f7f\u7528 <code>langchain-model-profiles</code> \u5e93\u6765\u83b7\u53d6\u4f60\u6240\u7528\u6a21\u578b\u63d0\u4f9b\u5546\u7684 profiles\u3002</p>"},{"location":"zh/getting-started-guide/chat/#langchain-openai-api","title":"\u672a\u6709 LangChain \u5bf9\u8bdd\u6a21\u578b\u7c7b\uff0c\u4f46\u6a21\u578b\u63d0\u4f9b\u5546\u652f\u6301 OpenAI \u517c\u5bb9 API","text":"<p>\u8fd9\u79cd\u60c5\u51b5\u4e0b\u7684\u53c2\u6570\u8bf4\u660e\u5982\u4e0b\uff1a</p>"},{"location":"zh/getting-started-guide/chat/#_8","title":"\u53c2\u6570\u8bf4\u660e","text":"\u53c2\u6570 \u8bf4\u660e <code>provider_name</code> \u6a21\u578b\u63d0\u4f9b\u5546\u540d\u79f0\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u662f <code>chat_model</code> \u56fa\u5b9a\u53d6\u503c <code>\"openai-compatible\"</code>\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u662f <code>base_url</code> API \u57fa\u7840\u5730\u5740\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u5426 <code>model_profiles</code> \u6a21\u578b\u914d\u7f6e\u4fe1\u606f\u5b57\u5178\u3002\u7c7b\u578b: <code>dict</code>\u5fc5\u586b: \u5426 <code>compatibility_options</code> \u517c\u5bb9\u6027\u9009\u9879\u914d\u7f6e\u3002\u7c7b\u578b: <code>dict</code>\u5fc5\u586b: \u5426"},{"location":"zh/getting-started-guide/chat/#_9","title":"\u4ee3\u7801\u793a\u4f8b","text":"<p>\u65b9\u5f0f\u4e00\uff1a\u663e\u5f0f\u4f20\u53c2</p> <pre><code>register_model_provider(\n    provider_name=\"vllm\",\n    chat_model=\"openai-compatible\",\n    base_url=\"http://localhost:8000/v1\"\n)\n</code></pre> <p>\u65b9\u5f0f\u4e8c\uff1a\u901a\u8fc7\u73af\u5883\u53d8\u91cf\uff08\u63a8\u8350\u7528\u4e8e\u914d\u7f6e\u7ba1\u7406\uff09</p> <pre><code>export VLLM_API_BASE=http://localhost:8000/v1\n</code></pre> <pre><code>register_model_provider(\n    provider_name=\"vllm\",\n    chat_model=\"openai-compatible\"\n    # \u81ea\u52a8\u8bfb\u53d6 VLLM_API_BASE\n)\n</code></pre> <p>\u6ce8\u610f\uff1a\u5173\u4e8e\u8fd9\u90e8\u5206\u66f4\u591a\u7684\u7ec6\u8282\uff0c\u8bf7\u53c2\u8003OpenAI \u517c\u5bb9 API \u96c6\u6210\u3002</p>"},{"location":"zh/getting-started-guide/chat/#_10","title":"\u6279\u91cf\u6ce8\u518c","text":"<p>\u82e5\u9700\u6ce8\u518c\u591a\u4e2a\u63d0\u4f9b\u5546\uff0c\u53ef\u4f7f\u7528 <code>batch_register_model_provider</code> \u907f\u514d\u91cd\u590d\u8c03\u7528\u3002</p>"},{"location":"zh/getting-started-guide/chat/#_11","title":"\u53c2\u6570\u8bf4\u660e","text":"\u53c2\u6570 \u8bf4\u660e <code>providers</code> \u63d0\u4f9b\u5546\u914d\u7f6e\u5217\u8868\uff0c\u6bcf\u4e2a\u5b57\u5178\u5305\u542b\u6ce8\u518c\u53c2\u6570\u3002\u7c7b\u578b: <code>list[dict]</code>\u5fc5\u586b: \u662f"},{"location":"zh/getting-started-guide/chat/#_12","title":"\u4ee3\u7801\u793a\u4f8b","text":"<pre><code>from langchain_dev_utils.chat_models import batch_register_model_provider\nfrom langchain_core.language_models.fake_chat_models import FakeChatModel\n\nbatch_register_model_provider(\n    providers=[\n        {\n            \"provider_name\": \"fake_provider\",\n            \"chat_model\": FakeChatModel,\n        },\n        {\n            \"provider_name\": \"vllm\",\n            \"chat_model\": \"openai-compatible\",\n            \"base_url\": \"http://localhost:8000/v1\",\n        },\n    ]\n)\n</code></pre> <p>\u6ce8\u610f</p> <p>\u4e24\u4e2a\u6ce8\u518c\u51fd\u6570\u5747\u57fa\u4e8e\u5168\u5c40\u5b57\u5178\u5b9e\u73b0\u3002\u4e3a\u907f\u514d\u591a\u7ebf\u7a0b\u95ee\u9898\uff0c\u5fc5\u987b\u5728\u5e94\u7528\u542f\u52a8\u9636\u6bb5\u5b8c\u6210\u6240\u6709\u6ce8\u518c\uff0c\u7981\u6b62\u8fd0\u884c\u65f6\u52a8\u6001\u6ce8\u518c\u3002  </p> <p>\u6b64\u5916\uff0c\u6ce8\u518c\u65f6\u82e5\u5c06 <code>chat_model</code> \u8bbe\u4e3a <code>openai-compatible</code>\uff0c\u5185\u90e8\u4f1a\u901a\u8fc7 <code>pydantic.create_model</code> \u52a8\u6001\u521b\u5efa\u65b0\u7684\u6a21\u578b\u7c7b\uff08\u4ee5 <code>BaseChatOpenAICompatible</code> \u4e3a\u57fa\u7c7b\uff0c\u751f\u6210\u5bf9\u5e94\u7684\u5bf9\u8bdd\u6a21\u578b\u96c6\u6210\u7c7b\uff09\uff0c\u6b64\u8fc7\u7a0b\u6d89\u53ca Python \u5143\u7c7b\u64cd\u4f5c\u548c pydantic \u9a8c\u8bc1\u903b\u8f91\u521d\u59cb\u5316\uff0c\u5b58\u5728\u4e00\u5b9a\u6027\u80fd\u5f00\u9500\uff0c\u56e0\u6b64\u8bf7\u907f\u514d\u5728\u8fd0\u884c\u671f\u9891\u7e41\u6ce8\u518c\u3002</p>"},{"location":"zh/getting-started-guide/chat/#_13","title":"\u52a0\u8f7d\u5bf9\u8bdd\u6a21\u578b","text":"<p>\u4f7f\u7528 <code>load_chat_model</code> \u51fd\u6570\u52a0\u8f7d\u5bf9\u8bdd\u6a21\u578b\uff08\u521d\u59cb\u5316\u5bf9\u8bdd\u6a21\u578b\u5b9e\u4f8b\uff09\u3002</p>"},{"location":"zh/getting-started-guide/chat/#_14","title":"\u53c2\u6570\u8bf4\u660e","text":"\u53c2\u6570 \u8bf4\u660e <code>model</code> \u6a21\u578b\u540d\u79f0\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u662f <code>model_provider</code> \u6a21\u578b\u63d0\u4f9b\u5546\u540d\u79f0\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u5426 <p>\u9664\u6b64\u4e4b\u5916\uff0c\u8fd8\u53ef\u4ee5\u4f20\u5165\u4efb\u610f\u6570\u91cf\u7684\u5173\u952e\u5b57\u53c2\u6570\uff0c\u7528\u4e8e\u4f20\u9012\u5bf9\u8bdd\u6a21\u578b\u7c7b\u7684\u989d\u5916\u53c2\u6570\u3002</p>"},{"location":"zh/getting-started-guide/chat/#_15","title":"\u53c2\u6570\u89c4\u5219","text":"<ul> <li>\u82e5\u672a\u4f20 <code>model_provider</code>\uff0c\u5219 <code>model</code> \u5fc5\u987b\u4e3a <code>provider_name:model_name</code> \u683c\u5f0f\uff1b</li> <li>\u82e5\u4f20 <code>model_provider</code>\uff0c\u5219 <code>model</code> \u5fc5\u987b\u4ec5\u4e3a <code>model_name</code>\u3002</li> </ul>"},{"location":"zh/getting-started-guide/chat/#_16","title":"\u4ee3\u7801\u793a\u4f8b","text":"<pre><code># \u65b9\u5f0f\u4e00\uff1amodel \u5305\u542b provider \u4fe1\u606f\nmodel = load_chat_model(\"vllm:qwen3-4b\")\n\n# \u65b9\u5f0f\u4e8c\uff1a\u5355\u72ec\u6307\u5b9a provider\nmodel = load_chat_model(\"qwen3-4b\", model_provider=\"vllm\")\n</code></pre>"},{"location":"zh/getting-started-guide/chat/#_17","title":"\u6a21\u578b\u65b9\u6cd5\u548c\u53c2\u6570","text":"<p>\u5bf9\u4e8e\u652f\u6301\u7684\u6a21\u578b\u65b9\u6cd5\u548c\u53c2\u6570\uff0c\u9700\u8981\u53c2\u8003\u5bf9\u5e94\u7684\u5bf9\u8bdd\u6a21\u578b\u7c7b\u7684\u4f7f\u7528\u8bf4\u660e\u3002\u5982\u679c\u91c7\u7528\u7684\u662f\u7b2c\u4e8c\u79cd\u60c5\u51b5\uff0c\u5219\u652f\u6301\u6240\u6709\u7684<code>BaseChatOpenAI</code> \u7c7b\u7684\u65b9\u6cd5\u548c\u53c2\u6570\u3002</p>"},{"location":"zh/getting-started-guide/chat/#_18","title":"\u517c\u5bb9\u5b98\u65b9\u63d0\u4f9b\u5546","text":"<p>\u5bf9\u4e8e LangChain \u5b98\u65b9\u5df2\u652f\u6301\u7684\u63d0\u4f9b\u5546\uff08\u5982 <code>openai</code>\uff09\uff0c\u53ef\u76f4\u63a5\u4f7f\u7528 <code>load_chat_model</code> \u65e0\u9700\u6ce8\u518c\uff1a</p> <pre><code>model = load_chat_model(\"openai:gpt-4o-mini\")\n# \u6216\nmodel = load_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n</code></pre> <p>\u6700\u4f73\u5b9e\u8df5</p> <p>\u5bf9\u4e8e\u672c\u6a21\u5757\u7684\u4f7f\u7528\uff0c\u53ef\u4ee5\u6839\u636e\u4e0b\u9762\u4e09\u79cd\u60c5\u51b5\u8fdb\u884c\u9009\u62e9\uff1a</p> <ol> <li> <p>\u82e5\u63a5\u5165\u7684\u6240\u6709\u6a21\u578b\u63d0\u4f9b\u5546\u5747\u88ab\u5b98\u65b9 <code>init_chat_model</code> \u652f\u6301\uff0c\u8bf7\u76f4\u63a5\u4f7f\u7528\u5b98\u65b9\u51fd\u6570\uff0c\u4ee5\u83b7\u5f97\u6700\u4f73\u517c\u5bb9\u6027\u548c\u7a33\u5b9a\u6027\u3002</p> </li> <li> <p>\u82e5\u63a5\u5165\u7684\u90e8\u5206\u6a21\u578b\u63d0\u4f9b\u5546\u4e3a\u975e\u5b98\u65b9\u652f\u6301\uff0c\u53ef\u4f7f\u7528\u672c\u6a21\u5757\u7684\u529f\u80fd\uff0c\u5148\u5229\u7528<code>register_model_provider</code>\u6ce8\u518c\u6a21\u578b\u63d0\u4f9b\u5546\uff0c\u7136\u540e\u4f7f\u7528<code>load_chat_model</code>\u52a0\u8f7d\u6a21\u578b\u3002</p> </li> <li> <p>\u82e5\u63a5\u5165\u7684\u6a21\u578b\u63d0\u4f9b\u5546\u6682\u65e0\u9002\u5408\u7684\u96c6\u6210\uff0c\u4f46\u63d0\u4f9b\u5546\u63d0\u4f9b\u4e86 OpenAI \u517c\u5bb9\u7684 API\uff08\u5982 vLLM\uff09\uff0c\u5219\u63a8\u8350\u4f7f\u7528\u672c\u6a21\u5757\u7684\u529f\u80fd\uff0c\u5148\u5229\u7528<code>register_model_provider</code>\u6ce8\u518c\u6a21\u578b\u63d0\u4f9b\u5546\uff08chat_model\u4f20\u5165<code>openai-compatible</code>\uff09\uff0c\u7136\u540e\u4f7f\u7528<code>load_chat_model</code>\u52a0\u8f7d\u6a21\u578b\u3002</p> </li> </ol>"},{"location":"zh/getting-started-guide/embedding/","title":"\u5d4c\u5165\u6a21\u578b\u7ba1\u7406","text":""},{"location":"zh/getting-started-guide/embedding/#_2","title":"\u6982\u8ff0","text":"<p>LangChain \u7684 <code>init_embeddings</code> \u51fd\u6570\u4ec5\u652f\u6301\u6709\u9650\u7684\u5d4c\u5165\u6a21\u578b\u63d0\u4f9b\u5546\u3002\u672c\u5e93\u63d0\u4f9b\u66f4\u7075\u6d3b\u7684\u5d4c\u5165\u6a21\u578b\u7ba1\u7406\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u63a5\u5165\u672a\u5185\u7f6e\u652f\u6301\u7684\u5d4c\u5165\u670d\u52a1\uff08\u5982 vLLM \u7b49\uff09\u7684\u573a\u666f\u3002</p>"},{"location":"zh/getting-started-guide/embedding/#_3","title":"\u6ce8\u518c\u5d4c\u5165\u6a21\u578b\u63d0\u4f9b\u5546","text":"<p>\u6ce8\u518c\u5d4c\u5165\u6a21\u578b\u63d0\u4f9b\u5546\u9700\u8c03\u7528 <code>register_embeddings_provider</code>\u3002\u6839\u636e <code>embeddings_model</code> \u7c7b\u578b\u4e0d\u540c\uff0c\u6ce8\u518c\u65b9\u5f0f\u7565\u6709\u5dee\u5f02\u3002</p>"},{"location":"zh/getting-started-guide/embedding/#langchain","title":"\u5df2\u6709 LangChain \u5d4c\u5165\u6a21\u578b\u7c7b","text":"<p>\u82e5\u5d4c\u5165\u6a21\u578b\u63d0\u4f9b\u5546\u5df2\u6709\u73b0\u6210\u4e14\u5408\u9002\u7684 LangChain \u96c6\u6210\uff08\u8be6\u89c1 \u5d4c\u5165\u6a21\u578b\u96c6\u6210\u5217\u8868\uff09\uff0c\u8bf7\u5c06\u76f8\u5e94\u7684\u5d4c\u5165\u6a21\u578b\u7c7b\u76f4\u63a5\u4f20\u5165 <code>embeddings_model</code> \u53c2\u6570\u3002</p>"},{"location":"zh/getting-started-guide/embedding/#_4","title":"\u53c2\u6570\u8bf4\u660e","text":"\u53c2\u6570 \u8bf4\u660e <code>provider_name</code> \u6a21\u578b\u63d0\u4f9b\u5546\u540d\u79f0\uff0c\u7528\u4e8e\u540e\u7eed\u5728 <code>load_embeddings</code> \u4e2d\u5f15\u7528\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u662f <code>embeddings_model</code> LangChain \u5d4c\u5165\u6a21\u578b\u7c7b\u3002\u7c7b\u578b: <code>type[Embeddings]</code>\u5fc5\u586b: \u662f <code>base_url</code> API \u57fa\u7840\u5730\u5740\uff0c\u901a\u5e38\u65e0\u9700\u624b\u52a8\u8bbe\u7f6e\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u5426"},{"location":"zh/getting-started-guide/embedding/#_5","title":"\u4ee3\u7801\u793a\u4f8b","text":"<pre><code>from langchain_core.embeddings.fake import FakeEmbeddings\nfrom langchain_dev_utils.embeddings import register_embeddings_provider\n\nregister_embeddings_provider(\n    provider_name=\"fake_provider\",\n    embeddings_model=FakeEmbeddings,\n)\n</code></pre>"},{"location":"zh/getting-started-guide/embedding/#_6","title":"\u4f7f\u7528\u8bf4\u660e","text":"<ul> <li><code>FakeEmbeddings</code> \u4ec5\u7528\u4e8e\u6d4b\u8bd5\u3002\u5b9e\u9645\u4f7f\u7528\u4e2d\u5fc5\u987b\u4f20\u5165\u5177\u5907\u771f\u5b9e\u529f\u80fd\u7684 <code>Embeddings</code> \u7c7b\u3002</li> <li><code>provider_name</code> \u4ee3\u8868\u6a21\u578b\u63d0\u4f9b\u5546\u7684\u540d\u79f0\uff0c\u7528\u4e8e\u540e\u7eed\u5728 <code>load_embeddings</code> \u4e2d\u5f15\u7528\u3002\u540d\u79f0\u53ef\u81ea\u5b9a\u4e49\uff0c\u4f46\u4e0d\u8981\u5305\u542b <code>:</code>\u3001<code>-</code> \u7b49\u7279\u6b8a\u5b57\u7b26\u3002</li> <li><code>base_url</code> \u53c2\u6570\u901a\u5e38\u65e0\u9700\u624b\u52a8\u8bbe\u7f6e\uff08\u56e0\u4e3a\u5d4c\u5165\u6a21\u578b\u7c7b\u5185\u90e8\u5df2\u5b9a\u4e49 API \u5730\u5740\uff09\u3002\u4ec5\u5f53\u9700\u8981\u8986\u76d6\u9ed8\u8ba4\u5730\u5740\u65f6\uff0c\u624d\u663e\u5f0f\u4f20\u5165 <code>base_url</code>\uff1b\u8986\u76d6\u8303\u56f4\u4ec5\u9650\u6a21\u578b\u7c7b\u4e2d\u5b57\u6bb5\u540d\u4e3a <code>api_base</code> \u6216 <code>base_url</code>\uff08\u542b\u522b\u540d\uff09\u7684\u5c5e\u6027\u3002</li> </ul>"},{"location":"zh/getting-started-guide/embedding/#langchain-openai-api","title":"\u672a\u6709 LangChain \u5d4c\u5165\u6a21\u578b\u7c7b\uff0c\u4f46\u63d0\u4f9b\u5546\u652f\u6301 OpenAI \u517c\u5bb9 API","text":"<p>\u8fd9\u79cd\u60c5\u51b5\u4e0b\u7684\u53c2\u6570\u8bf4\u660e\u5982\u4e0b\uff1a</p>"},{"location":"zh/getting-started-guide/embedding/#_7","title":"\u53c2\u6570\u8bf4\u660e","text":"\u53c2\u6570 \u8bf4\u660e <code>provider_name</code> \u6a21\u578b\u63d0\u4f9b\u5546\u540d\u79f0\uff0c\u7528\u4e8e\u540e\u7eed\u5728 <code>load_embeddings</code> \u4e2d\u5f15\u7528\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u662f <code>embeddings_model</code> \u56fa\u5b9a\u53d6\u503c <code>\"openai-compatible\"</code>\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u662f <code>base_url</code> API \u57fa\u7840\u5730\u5740\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u5426"},{"location":"zh/getting-started-guide/embedding/#_8","title":"\u4ee3\u7801\u793a\u4f8b","text":"<p>\u65b9\u5f0f\u4e00\uff1a\u663e\u5f0f\u4f20\u53c2</p> <pre><code>register_embeddings_provider(\n    provider_name=\"vllm\",\n    embeddings_model=\"openai-compatible\",\n    base_url=\"http://localhost:8000/v1\"\n)\n</code></pre> <p>\u65b9\u5f0f\u4e8c\uff1a\u73af\u5883\u53d8\u91cf\uff08\u63a8\u8350\uff09</p> <pre><code>export VLLM_API_BASE=http://localhost:8000/v1\n</code></pre> <pre><code>register_embeddings_provider(\n    provider_name=\"vllm\",\n    embeddings_model=\"openai-compatible\"\n    # \u81ea\u52a8\u8bfb\u53d6 VLLM_API_BASE\n)\n</code></pre> <p>\u6ce8\u610f\uff1a\u5173\u4e8e\u8fd9\u90e8\u5206\u66f4\u591a\u7684\u7ec6\u8282\uff0c\u8bf7\u53c2\u8003OpenAI \u517c\u5bb9 API \u96c6\u6210\u3002</p>"},{"location":"zh/getting-started-guide/embedding/#_9","title":"\u6279\u91cf\u6ce8\u518c","text":"<p>\u82e5\u9700\u6ce8\u518c\u591a\u4e2a\u63d0\u4f9b\u5546\uff0c\u53ef\u4f7f\u7528 <code>batch_register_embeddings_provider</code>\u3002</p>"},{"location":"zh/getting-started-guide/embedding/#_10","title":"\u53c2\u6570\u8bf4\u660e","text":"\u53c2\u6570 \u8bf4\u660e <code>providers</code> \u63d0\u4f9b\u5546\u914d\u7f6e\u5217\u8868\uff0c\u6bcf\u4e2a\u5b57\u5178\u5305\u542b\u6ce8\u518c\u53c2\u6570\u3002\u7c7b\u578b: <code>list[dict]</code>\u5fc5\u586b: \u662f"},{"location":"zh/getting-started-guide/embedding/#_11","title":"\u4ee3\u7801\u793a\u4f8b","text":"<pre><code>from langchain_dev_utils.embeddings import batch_register_embeddings_provider\nfrom langchain_core.embeddings.fake import FakeEmbeddings\n\nbatch_register_embeddings_provider(\n    providers=[\n        {\n            \"provider_name\": \"fake_provider\",\n            \"embeddings_model\": FakeEmbeddings,\n        },\n        {\n            \"provider_name\": \"vllm\",\n            \"embeddings_model\": \"openai-compatible\",\n            \"base_url\": \"http://localhost:8000/v1\",\n        },\n    ]\n)\n</code></pre> <p>\u6ce8\u610f</p> <p>\u4e24\u4e2a\u6ce8\u518c\u51fd\u6570\u5747\u57fa\u4e8e\u5168\u5c40\u5b57\u5178\u5b9e\u73b0\u3002\u5fc5\u987b\u5728\u5e94\u7528\u542f\u52a8\u9636\u6bb5\u5b8c\u6210\u6240\u6709\u6ce8\u518c\uff0c\u7981\u6b62\u8fd0\u884c\u65f6\u52a8\u6001\u6ce8\u518c\uff0c\u4ee5\u907f\u514d\u591a\u7ebf\u7a0b\u95ee\u9898\u3002 </p> <p>\u6b64\u5916\uff0c\u6ce8\u518c\u65f6\u82e5\u5c06 <code>embeddings_model</code> \u8bbe\u4e3a <code>openai-compatible</code>\uff0c\u5185\u90e8\u4f1a\u901a\u8fc7 <code>pydantic.create_model</code> \u52a8\u6001\u521b\u5efa\u65b0\u7684\u6a21\u578b\u7c7b\uff08\u4ee5 <code>BaseEmbeddingOpenAICompatible</code> \u4e3a\u57fa\u7c7b\uff0c\u751f\u6210\u5bf9\u5e94\u7684\u5d4c\u5165\u6a21\u578b\u96c6\u6210\u7c7b\uff09\uff0c\u6b64\u8fc7\u7a0b\u6d89\u53ca Python \u5143\u7c7b\u64cd\u4f5c\u548c pydantic \u9a8c\u8bc1\u903b\u8f91\u521d\u59cb\u5316\uff0c\u5b58\u5728\u4e00\u5b9a\u6027\u80fd\u5f00\u9500\uff0c\u56e0\u6b64\u8bf7\u907f\u514d\u5728\u8fd0\u884c\u671f\u9891\u7e41\u6ce8\u518c\u3002</p>"},{"location":"zh/getting-started-guide/embedding/#_12","title":"\u52a0\u8f7d\u5d4c\u5165\u6a21\u578b","text":"<p>\u4f7f\u7528 <code>load_embeddings</code> \u521d\u59cb\u5316\u5d4c\u5165\u6a21\u578b\u5b9e\u4f8b\u3002</p>"},{"location":"zh/getting-started-guide/embedding/#_13","title":"\u53c2\u6570\u8bf4\u660e","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u8bf4\u660e <code>model</code> <code>str</code> \u662f - \u6a21\u578b\u540d\u79f0 <code>provider</code> <code>str</code> \u5426 <code>None</code> \u6a21\u578b\u63d0\u4f9b\u5546\u540d\u79f0 <p>\u9664\u6b64\u4e4b\u5916\uff0c\u8fd8\u53ef\u4ee5\u4f20\u5165\u4efb\u610f\u6570\u91cf\u7684\u5173\u952e\u5b57\u53c2\u6570\uff0c\u7528\u4e8e\u4f20\u9012\u5d4c\u5165\u6a21\u578b\u7c7b\u7684\u989d\u5916\u53c2\u6570\u3002</p>"},{"location":"zh/getting-started-guide/embedding/#_14","title":"\u53c2\u6570\u89c4\u5219","text":"<ul> <li>\u82e5\u672a\u4f20 <code>provider</code>\uff0c\u5219 <code>model</code> \u5fc5\u987b\u4e3a <code>provider_name:embeddings_name</code> \u683c\u5f0f\uff1b</li> <li>\u82e5\u4f20 <code>provider</code>\uff0c\u5219 <code>model</code> \u4ec5\u4e3a <code>embeddings_name</code>\u3002</li> </ul>"},{"location":"zh/getting-started-guide/embedding/#_15","title":"\u4ee3\u7801\u793a\u4f8b","text":"<pre><code># \u65b9\u5f0f\u4e00\uff1amodel \u5305\u542b provider \u4fe1\u606f\nembedding = load_embeddings(\"vllm:qwen3-embedding-4b\")\n\n# \u65b9\u5f0f\u4e8c\uff1a\u5355\u72ec\u6307\u5b9a provider\nembedding = load_embeddings(\"qwen3-embedding-4b\", provider=\"vllm\")\n</code></pre>"},{"location":"zh/getting-started-guide/embedding/#_16","title":"\u6a21\u578b\u65b9\u6cd5\u548c\u53c2\u6570","text":"<p>\u5bf9\u4e8e\u652f\u6301\u7684\u6a21\u578b\u65b9\u6cd5\u548c\u53c2\u6570\uff0c\u9700\u8981\u53c2\u8003\u5bf9\u5e94\u7684\u5d4c\u5165\u6a21\u578b\u7c7b\u7684\u4f7f\u7528\u8bf4\u660e\u3002\u5982\u679c\u91c7\u7528\u7684\u662f\u7b2c\u4e8c\u79cd\u60c5\u51b5\uff0c\u5219\u652f\u6301\u6240\u6709\u7684<code>OpenAIEmbeddings</code> \u7c7b\u7684\u65b9\u6cd5\u548c\u53c2\u6570\u3002</p>"},{"location":"zh/getting-started-guide/embedding/#_17","title":"\u517c\u5bb9\u5b98\u65b9\u63d0\u4f9b\u5546","text":"<p>\u5bf9\u4e8e LangChain \u5b98\u65b9\u5df2\u652f\u6301\u7684\u63d0\u4f9b\u5546\uff08\u5982 <code>openai</code>\uff09\uff0c\u53ef\u76f4\u63a5\u4f7f\u7528 <code>load_embeddings</code> \u65e0\u9700\u6ce8\u518c\uff1a</p> <pre><code>model = load_embeddings(\"openai:text-embedding-3-large\")\n# \u6216\nmodel = load_embeddings(\"text-embedding-3-large\", provider=\"openai\")\n</code></pre> <p>\u6700\u4f73\u5b9e\u8df5</p> <p>\u5bf9\u4e8e\u672c\u6a21\u5757\u7684\u4f7f\u7528\uff0c\u53ef\u4ee5\u6839\u636e\u4e0b\u9762\u4e09\u79cd\u60c5\u51b5\u8fdb\u884c\u9009\u62e9\uff1a</p> <ol> <li> <p>\u82e5\u63a5\u5165\u7684\u6240\u6709\u5d4c\u5165\u6a21\u578b\u63d0\u4f9b\u5546\u5747\u88ab\u5b98\u65b9 <code>init_embeddings</code> \u652f\u6301\uff0c\u8bf7\u76f4\u63a5\u4f7f\u7528\u5b98\u65b9\u51fd\u6570\uff0c\u4ee5\u83b7\u5f97\u6700\u4f73\u517c\u5bb9\u6027\u3002</p> </li> <li> <p>\u82e5\u63a5\u5165\u7684\u90e8\u5206\u5d4c\u5165\u6a21\u578b\u63d0\u4f9b\u5546\u4e3a\u975e\u5b98\u65b9\u652f\u6301\uff0c\u53ef\u5229\u7528\u672c\u6a21\u5757\u7684\u6ce8\u518c\u4e0e\u52a0\u8f7d\u673a\u5236\uff0c\u5148\u5229\u7528<code>register_embeddings_provider</code>\u6ce8\u518c\u6a21\u578b\u63d0\u4f9b\u5546\uff0c\u7136\u540e\u4f7f\u7528<code>load_embeddings</code>\u52a0\u8f7d\u6a21\u578b\u3002</p> </li> <li> <p>\u82e5\u63a5\u5165\u7684\u5d4c\u5165\u6a21\u578b\u63d0\u4f9b\u5546\u6682\u65e0\u9002\u5408\u7684\u96c6\u6210\uff0c\u4f46\u63d0\u4f9b\u5546\u63d0\u4f9b\u4e86 OpenAI \u517c\u5bb9\u7684 API\uff08\u5982 vLLM\uff09\uff0c\u5219\u63a8\u8350\u5229\u7528\u672c\u6a21\u5757\u7684\u529f\u80fd\uff0c\u5148\u5229\u7528<code>register_embeddings_provider</code>\u6ce8\u518c\u6a21\u578b\u63d0\u4f9b\u5546\uff08embeddings_model\u4f20\u5165<code>openai-compatible</code>\uff09\uff0c\u7136\u540e\u4f7f\u7528<code>load_embeddings</code>\u52a0\u8f7d\u6a21\u578b\u3002</p> </li> </ol>"},{"location":"zh/getting-started-guide/format/","title":"\u683c\u5f0f\u5316\u5e8f\u5217","text":""},{"location":"zh/getting-started-guide/format/#_2","title":"\u6982\u8ff0","text":"<p>\u7528\u4e8e\u5c06\u7531 Document\u3001Message \u6216\u5b57\u7b26\u4e32\u7ec4\u6210\u7684\u5217\u8868\u683c\u5f0f\u5316\u4e3a\u5355\u4e2a\u6587\u672c\u5b57\u7b26\u4e32\u3002\u5177\u4f53\u51fd\u6570\u4e3a <code>format_sequence</code>\u3002</p>"},{"location":"zh/getting-started-guide/format/#_3","title":"\u4f7f\u7528\u793a\u4f8b","text":""},{"location":"zh/getting-started-guide/format/#_4","title":"\u4ee3\u7801\u793a\u4f8b","text":"<pre><code>from langchain_core.documents import Document\nfrom langchain_core.messages import AIMessage\nfrom langchain_dev_utils.message_convert import format_sequence\n\nformated1 = format_sequence(\n    [\n        AIMessage(content=\"Hello1\"),\n        AIMessage(content=\"Hello2\"),\n        AIMessage(content=\"Hello3\"),\n    ]\n)\nprint(formated1)\n</code></pre>"},{"location":"zh/getting-started-guide/format/#_5","title":"\u8f93\u51fa\u7ed3\u679c","text":"<pre><code>-Hello1\n-Hello2\n-Hello3\n</code></pre>"},{"location":"zh/getting-started-guide/format/#_6","title":"\u4ee3\u7801\u793a\u4f8b","text":"<pre><code>format2 = format_sequence(\n    [\n        Document(page_content=\"content1\"),\n        Document(page_content=\"content2\"),\n        Document(page_content=\"content3\"),\n    ],\n    separator=\"&gt;\",\n)\nprint(format2)\n</code></pre>"},{"location":"zh/getting-started-guide/format/#_7","title":"\u8f93\u51fa\u7ed3\u679c","text":"<pre><code>&gt;content1\n&gt;content2\n&gt;content3\n</code></pre>"},{"location":"zh/getting-started-guide/format/#_8","title":"\u4ee3\u7801\u793a\u4f8b","text":"<pre><code>format3 = format_sequence(\n    [\n        \"str1\",\n        \"str2\",\n        \"str3\",\n    ],\n    separator=\"&gt;\",\n    with_num=True,\n)\nprint(format3)\n</code></pre>"},{"location":"zh/getting-started-guide/format/#_9","title":"\u8f93\u51fa\u7ed3\u679c","text":"<pre><code>&gt;1. str1\n&gt;2. str2\n&gt;3. str3\n</code></pre>"},{"location":"zh/getting-started-guide/installation/","title":"\u5b89\u88c5","text":"<p><code>langchain-dev-utils</code>\u652f\u6301\u4f7f\u7528<code>pip</code>\u3001<code>poetry</code>\u3001<code>uv</code>\u7b49\u591a\u79cd\u5305\u7ba1\u7406\u5668\u8fdb\u884c\u5b89\u88c5\u3002</p> <p>\u5b89\u88c5\u57fa\u7840\u7248\u672c\u7684<code>langchain-dev-utils</code>\uff1a</p> pippoetryuv <pre><code>pip install -U langchain-dev-utils\n</code></pre> <pre><code>poetry add langchain-dev-utils\n</code></pre> <pre><code>uv add langchain-dev-utils\n</code></pre> <p>\u5b89\u88c5\u5b8c\u6574\u529f\u80fd\u7248\u672c\u7684<code>langchain-dev-utils</code>\uff1a</p> pippoetryuv <pre><code>pip install -U \"langchain-dev-utils[standard]\"\n</code></pre> <pre><code>poetry add \"langchain-dev-utils[standard]\"\n</code></pre> <pre><code>uv add langchain-dev-utils[standard]\n</code></pre>"},{"location":"zh/getting-started-guide/installation/#_2","title":"\u9a8c\u8bc1\u5b89\u88c5","text":"<p>\u5b89\u88c5\u540e\uff0c\u9a8c\u8bc1\u5305\u662f\u5426\u6b63\u786e\u5b89\u88c5\uff1a</p> <pre><code>import langchain_dev_utils\nprint(langchain_dev_utils.__version__)\n</code></pre>"},{"location":"zh/getting-started-guide/installation/#_3","title":"\u4f9d\u8d56\u9879","text":"<p>\u8be5\u5305\u4f1a\u81ea\u52a8\u5b89\u88c5\u4ee5\u4e0b\u4f9d\u8d56\u9879\uff1a</p> <ul> <li><code>langchain</code></li> <li><code>langgraph</code> (\u5b89\u88c5<code>langchain</code>\u65f6\u4f1a\u540c\u65f6\u4e5f\u4f1a\u5b89\u88c5)</li> </ul> <p>\u5982\u679c\u662f standard \u7248\u672c\uff0c\u8fd8\u4f1a\u5b89\u88c5\u4ee5\u4e0b\u4f9d\u8d56\u9879\uff1a</p> <ul> <li><code>langchain-openai</code>\uff08\u7528\u4e8e\u6a21\u578b\u7ba1\u7406\uff09</li> <li><code>json-repair</code>(\u7528\u4e8e\u4e2d\u95f4\u4ef6\u7684\u5de5\u5177\u8c03\u7528\u9519\u8bef\u4fee\u590d)</li> </ul>"},{"location":"zh/getting-started-guide/message/","title":"\u6d88\u606f\u5904\u7406","text":""},{"location":"zh/getting-started-guide/message/#_2","title":"\u6982\u8ff0","text":"<p>\u4e3b\u8981\u529f\u80fd\u5305\u62ec\uff1a</p> <ul> <li>\u5408\u5e76\u63a8\u7406\u5185\u5bb9\u81f3\u6700\u7ec8\u56de\u590d</li> <li>\u5408\u5e76\u6d41\u5f0f\u8f93\u51fa\u7684 Chunks</li> </ul>"},{"location":"zh/getting-started-guide/message/#_3","title":"\u5408\u5e76\u63a8\u7406\u5185\u5bb9\u81f3\u6700\u7ec8\u56de\u590d","text":"<p>\u7528\u4e8e\u5c06\u63a8\u7406\u5185\u5bb9\uff08<code>reasoning_content</code>\uff09\u5408\u5e76\u81f3\u6700\u7ec8\u56de\u590d\uff08<code>content</code>\uff09\u3002</p>"},{"location":"zh/getting-started-guide/message/#_4","title":"\u529f\u80fd\u8bf4\u660e","text":"\u51fd\u6570 \u8bf4\u660e <code>convert_reasoning_content_for_ai_message</code> \u5c06 AIMessage \u4e2d\u7684\u63a8\u7406\u5185\u5bb9\u5408\u5e76\u5230\u5185\u5bb9\u5b57\u6bb5\uff08\u7528\u4e8e\u6a21\u578b\u7684 invoke \u548c ainvoke\uff09 <code>convert_reasoning_content_for_chunk_iterator</code> \u5c06\u6d41\u5f0f\u54cd\u5e94\u4e2d\u7684\u63a8\u7406\u5185\u5bb9\u5408\u5e76\u5230\u5185\u5bb9\u5b57\u6bb5\uff08\u7528\u4e8e\u6a21\u578b\u7684 stream\uff09 <code>aconvert_reasoning_content_for_chunk_iterator</code> <code>convert_reasoning_content_for_chunk_iterator</code> \u7684\u5f02\u6b65\u7248\u672c\uff0c\u7528\u4e8e\u5f02\u6b65\u6d41\u5f0f\u5904\u7406\uff08\u7528\u4e8e\u6a21\u578b\u7684 astream\uff09"},{"location":"zh/getting-started-guide/message/#_5","title":"\u4ee3\u7801\u793a\u4f8b","text":"<pre><code>from langchain_dev_utils.message_convert import (\n    convert_reasoning_content_for_ai_message,\n    convert_reasoning_content_for_chunk_iterator,\n)\n\nresponse = model.invoke(\"\u4f60\u597d\")\nconverted_response = convert_reasoning_content_for_ai_message(\n    response, think_tag=(\"&lt;start&gt;\", \"&lt;end&gt;\")\n)\nprint(converted_response.content)\n\nfor chunk in convert_reasoning_content_for_chunk_iterator(\n    model.stream(\"\u4f60\u597d\"), think_tag=(\"&lt;start&gt;\", \"&lt;end&gt;\")\n):\n    print(chunk.content, end=\"\", flush=True)\n</code></pre>"},{"location":"zh/getting-started-guide/message/#chunks","title":"\u5408\u5e76\u6d41\u5f0f\u8f93\u51fa\u7684 Chunks","text":"<p>\u63d0\u4f9b\u5c06\u591a\u4e2a\u56e0\u4e3a\u6d41\u5f0f\u8f93\u51fa\u800c\u4ea7\u751f\u7684 AIMessageChunk \u5408\u5e76\u4e3a\u5355\u4e2a AIMessage \u7684\u5de5\u5177\u51fd\u6570\u3002</p>"},{"location":"zh/getting-started-guide/message/#_6","title":"\u6838\u5fc3\u51fd\u6570","text":"\u51fd\u6570 \u8bf4\u660e <code>merge_ai_message_chunk</code> \u5408\u5e76 AI \u6d88\u606f\u5757"},{"location":"zh/getting-started-guide/message/#_7","title":"\u4ee3\u7801\u793a\u4f8b","text":"<pre><code>from langchain_dev_utils.message_convert import merge_ai_message_chunk\n\nchunks = []\nfor chunk in model.stream(\"\u4f60\u597d\"):\n    chunks.append(chunk)\n\nmerged_message = merge_ai_message_chunk(chunks)\nprint(merged_message)\n</code></pre>"},{"location":"zh/getting-started-guide/tool/","title":"\u5de5\u5177\u8c03\u7528\u5904\u7406","text":""},{"location":"zh/getting-started-guide/tool/#_2","title":"\u6982\u8ff0","text":"<p>\u63d0\u4f9b\u68c0\u6d4b\u4ee5\u53ca\u89e3\u6790\u5de5\u5177\u8c03\u7528\u53c2\u6570\u7684\u5b9e\u7528\u5de5\u5177\u3002</p>"},{"location":"zh/getting-started-guide/tool/#_3","title":"\u68c0\u6d4b\u5de5\u5177\u8c03\u7528","text":"<p>\u68c0\u6d4b\u6d88\u606f\u662f\u5426\u5305\u542b\u5de5\u5177\u8c03\u7528\uff0c\u6838\u5fc3\u51fd\u6570\u662f <code>has_tool_calling</code>\u3002</p>"},{"location":"zh/getting-started-guide/tool/#_4","title":"\u4ee3\u7801\u793a\u4f8b","text":"<pre><code>import datetime\nfrom langchain_core.tools import tool\nfrom langchain_dev_utils.tool_calling import has_tool_calling\n\n@tool\ndef get_current_time() -&gt; str:\n    \"\"\"\u83b7\u53d6\u5f53\u524d\u65f6\u95f4\u6233\"\"\"\n    return str(datetime.datetime.now().timestamp())\n\nresponse = model.bind_tools([get_current_time]).invoke(\"\u73b0\u5728\u51e0\u70b9\u4e86\uff1f\")\nprint(has_tool_calling(response))\n</code></pre>"},{"location":"zh/getting-started-guide/tool/#_5","title":"\u89e3\u6790\u5de5\u5177\u8c03\u7528\u53c2\u6570","text":"<p>\u63d0\u4f9b\u4e00\u4e2a\u5b9e\u7528\u51fd\u6570\u6765\u89e3\u6790\u5de5\u5177\u8c03\u7528\u53c2\u6570\uff0c\u4ece\u6d88\u606f\u4e2d\u63d0\u53d6\u53c2\u6570\u4fe1\u606f\uff0c\u6838\u5fc3\u51fd\u6570\u662f <code>parse_tool_calling</code>\u3002</p>"},{"location":"zh/getting-started-guide/tool/#_6","title":"\u4ee3\u7801\u793a\u4f8b","text":"<pre><code>import datetime\nfrom langchain_core.tools import tool\nfrom langchain_dev_utils.tool_calling import has_tool_calling, parse_tool_calling\n\n@tool\ndef get_current_time() -&gt; str:\n    \"\"\"\u83b7\u53d6\u5f53\u524d\u65f6\u95f4\u6233\"\"\"\n    return str(datetime.datetime.now().timestamp())\n\nresponse = model.bind_tools([get_current_time]).invoke(\"\u73b0\u5728\u51e0\u70b9\u4e86\uff1f\")\n\nif has_tool_calling(response):\n    name, args = parse_tool_calling(\n        response, first_tool_call_only=True\n    )\n    print(name, args)\n</code></pre>"}]}