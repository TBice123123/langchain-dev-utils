{"config":{"lang":["en","zh"],"separator":"[\\s\\u200b\\u3000\\-\u3001\u3002\uff0c\uff0e\uff1f\uff01\uff1b]+","pipeline":["stemmer"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"\ud83e\udd9c\ufe0f\ud83e\uddf0 langchain-dev-utils","text":"<p> \ud83d\ude80 An efficient toolkit designed specifically for LangChain and LangGraph developers </p> <p> </p>"},{"location":"#why-choose-langchain-dev-utils","title":"Why choose langchain-dev-utils?","text":"<p>Tired of writing repetitive code in LangChain development? <code>langchain-dev-utils</code> is exactly the solution you need! This lightweight yet powerful toolkit is designed to enhance the development experience of LangChain and LangGraph, helping you:</p> <ul> <li>Boost development efficiency - Reduce boilerplate code, allowing you to focus on core functionality</li> <li>Simplify complex workflows - Easily manage multi-model, multi-tool, and multi-agent applications</li> <li>Enhance code quality - Improve consistency and readability, reduce maintenance costs</li> <li>Accelerate prototype development - Quickly implement ideas, iterate and validate faster</li> </ul>"},{"location":"#core-features","title":"Core Features","text":"<ul> <li> <p> Unified Model Management</p> <p>Specify model providers through strings, easily switch and combine different models.</p> </li> <li> <p> Built-in OpenAI-Compatible Integration Class</p> <p>Built-in OpenAI-Compatible API integration class, improving model compatibility through explicit configuration.</p> </li> <li> <p> Flexible Message Processing</p> <p>Supports chain-of-thought concatenation, streaming processing, and message formatting</p> </li> <li> <p> Powerful Tool Calling</p> <p>Built-in tool calling detection, parameter parsing, and human review functions</p> </li> <li> <p> Efficient Agent Development</p> <p>Simplifies the agent creation process and expands more common middleware</p> </li> <li> <p> Flexible State Graph Composition</p> <p>Supports combining multiple StateGraphs in serial and parallel ways</p> </li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>1. Install <code>langchain-dev-utils</code></p> <pre><code>pip install -U \"langchain-dev-utils[standard]\"\n</code></pre> <p>2. Get Started</p> <pre><code>from langchain.tools import tool\nfrom langchain_core.messages import HumanMessage\nfrom langchain_dev_utils.chat_models import register_model_provider, load_chat_model\nfrom langchain_dev_utils.agents import create_agent\n\n# Register model provider\nregister_model_provider(\"vllm\", \"openai-compatible\", base_url=\"http://localhost:8000/v1\")\n\n@tool\ndef get_current_weather(location: str) -&gt; str:\n    \"\"\"Get the current weather for the specified location\"\"\"\n    return f\"25 degrees, {location}\"\n\n# Dynamically load model using string\nmodel = load_chat_model(\"vllm:qwen3-4b\")\nresponse = model.invoke(\"Hello\")\nprint(response)\n\n# Create agent\nagent = create_agent(\"vllm:qwen3-4b\", tools=[get_current_weather])\nresponse = agent.invoke({\"messages\": [HumanMessage(content=\"What's the weather like in New York today?\")]})\nprint(response)\n</code></pre>"},{"location":"#github-repository","title":"GitHub Repository","text":"<p>Visit the GitHub Repository to view source code and issues.</p>"},{"location":"example-project/","title":"Langchain-dev-utils Example Project","text":"<p>This repository provides an example project <code>langchain-dev-utils-example</code> designed to help developers quickly understand how to use the utility functions provided by <code>langchain-dev-utils</code> to efficiently build two typical agent systems:</p> <ul> <li>Single Agent: Suitable for executing simple tasks and tasks related to long-term memory storage.</li> <li>Supervisor-Multi-Agent Architecture: Coordinates multiple specialized agents through a central supervisor, suitable for complex scenarios requiring task decomposition, planning, and iterative optimization.</li> </ul> <p> </p>"},{"location":"example-project/#quick-start","title":"Quick Start","text":"<ol> <li>Clone this repository: <pre><code>git clone https://github.com/TBice123123/langchain-dev-utils-example.git  \ncd langchain-dev-utils-example\n</code></pre></li> <li>Install dependencies using uv: <pre><code>uv sync\n</code></pre></li> <li>Create a .env file <pre><code>cp .env.example .env\n</code></pre></li> <li> <p>Edit the <code>.env</code> file and fill in your API keys (requires <code>ZhipuAI</code> and <code>Tavily</code> API keys).</p> </li> <li> <p>Start the project <pre><code>langgraph dev\n</code></pre></p> </li> </ol>"},{"location":"example-project/#features-used","title":"Features Used","text":"<p>Single Agent:</p> <p>Features from this library used:</p> <ul> <li>Chat model management (including OpenAI compatible API integration): <code>register_model_provider</code>, <code>load_chat_model</code></li> <li>Embedding model management: <code>register_embeddings_provider</code>, <code>load_embeddings</code></li> <li>Format sequence: <code>format_sequence</code></li> <li>Middleware: <code>format_prompt</code></li> </ul> <p>Supervisor-Multi-Agent Architecture:</p> <p>Features from this library used:</p> <ul> <li>Chat model management (including OpenAI compatible API integration): <code>register_model_provider</code>, <code>load_chat_model</code></li> <li>Multi-agent construction: <code>wrap_agent_as_tool</code></li> </ul>"},{"location":"example-project/#how-to-customize","title":"How to Customize","text":"<p>You can customize this project according to your actual needs.</p>"},{"location":"example-project/#1-replace-chat-model-provider","title":"1. Replace Chat Model Provider","text":"<p>This project uses ZhipuAI's GLM series as the core model by default, as follows:</p> <ul> <li><code>GLM-4.7</code>: Used for <code>simple-agent</code></li> <li><code>GLM-4.6</code>: Used for the <code>supervisor</code> in <code>supervisor-agent</code></li> <li><code>GLM-4.5</code>: Used for <code>subagent</code> in <code>supervisor-agent</code></li> <li><code>GLM-4.6V</code>: Used for <code>vision subagent</code> in <code>supervisor-agent</code></li> </ul> <p>To customize the model provider, please modify <code>src/utils/providers/chat_models/register.py</code> and register your model provider using the <code>register_model_provider</code> function in the <code>register_all_model_providers</code> function.</p> <p>It is also recommended to modify <code>src/utils/providers/chat_models/load.py</code> and add corresponding loading logic in the <code>load_chat_model</code> function.</p> <p>Chat Model Management Best Practice</p> <p>The <code>load_chat_model</code> function uses keyword arguments to receive additional parameters for different chat model classes (LangChain official functions also use this approach). This approach improves universality but weakens IDE type hints and increases the risk of parameter misuse. Therefore, if a specific provider is already determined, you can extend the parameter signature for its integrated chat model class (or embedding model class) to restore type hints. Refer to <code>src/utils/providers/chat_models/load.py</code> for targeted modifications.</p>"},{"location":"example-project/#2-register-embedding-model-provider","title":"2. Register Embedding Model Provider","text":"<p>The registration method for embedding model providers is similar to chat models. Please modify <code>src/utils/providers/embeddings/register.py</code> and register your embedding model provider using the <code>register_embeddings_provider</code> function in the <code>register_all_embeddings_providers</code> function.</p> <p>To customize the loading logic, you can modify <code>src/utils/providers/embeddings/load.py</code> and add corresponding loading logic in the <code>load_embeddings</code> function.</p>"},{"location":"example-project/#3-customize-tools","title":"3. Customize Tools","text":"<p>Single Agent (simple-agent) Tool implementations are located in <code>src/agents/simple_agent/tools.py</code>, with built-in: - <code>save_user_memory</code> \u2014 Persist user memory - <code>get_user_memory</code> \u2014 Read user memory  </p> <p>To extend, simply add new tool implementations directly in this file.</p> <p>Supervisor-Multi-Agent (supervisor-agent) Tool implementations are located in <code>src/agents/supervisor/subagent/tools.py</code>. These are tool implementations for sub-agents. To add custom tools for sub-agents, simply add new tool implementations directly in this file.</p> <p>Note: The <code>supervisor</code> only has two tools for \"calling sub-agents\" by default. If you need to add custom tools for the <code>supervisor</code>, it is recommended to create a new <code>tools.py</code> under <code>src/agents/supervisor/</code>, write the implementations, and then import and pass them to the <code>create_agent</code> function in <code>src/agents/supervisor/agent.py</code>.</p>"},{"location":"adavance-guide/human-in-the-loop/","title":"Adding Human Review to Tool Calls","text":""},{"location":"adavance-guide/human-in-the-loop/#overview","title":"Overview","text":"<p>This library provides decorator functions to add \"human-in-the-loop\" review support for tool calls, enabling human review during tool execution.</p> Decorator Applicable Scenario <code>human_in_the_loop</code> For synchronous tool functions <code>human_in_the_loop_async</code> For asynchronous tool functions"},{"location":"adavance-guide/human-in-the-loop/#parameter-description","title":"Parameter Description","text":"Parameter Description <code>handler</code> Custom handler function. If <code>None</code>, the default handler is used.Type: <code>Callable</code>Required: No"},{"location":"adavance-guide/human-in-the-loop/#usage-examples","title":"Usage Examples","text":""},{"location":"adavance-guide/human-in-the-loop/#using-the-default-handler","title":"Using the Default Handler","text":"<pre><code>from langchain_dev_utils.tool_calling import human_in_the_loop\nimport datetime\n\n\n@human_in_the_loop\ndef get_current_time() -&gt; str:\n    \"\"\"Gets the current timestamp\"\"\"\n    return str(datetime.datetime.now().timestamp())\n</code></pre>"},{"location":"adavance-guide/human-in-the-loop/#async-tool-example","title":"Async Tool Example","text":"<pre><code>from langchain_dev_utils.tool_calling import human_in_the_loop_async\nimport asyncio\nimport datetime\n\n\n@human_in_the_loop_async\nasync def async_get_current_time() -&gt; str:\n    \"\"\"Asynchronously gets the current timestamp\"\"\"\n    await asyncio.sleep(1)\n    return str(datetime.datetime.now().timestamp())\n</code></pre>"},{"location":"adavance-guide/human-in-the-loop/#default-handler-implementation","title":"Default Handler Implementation","text":"<p>The implementation of the default handler is as follows:</p> <pre><code>def _get_human_in_the_loop_request(params: InterruptParams) -&gt; dict[str, Any]:\n    return {\n        \"action_request\": {\n            \"action\": params[\"tool_call_name\"],\n            \"args\": params[\"tool_call_args\"],\n        },\n        \"config\": {\n            \"allow_accept\": True,\n            \"allow_edit\": True,\n            \"allow_respond\": True,\n        },\n        \"description\": f\"Please review tool call: {params['tool_call_name']}\",\n    }\n\n\ndef default_handler(params: InterruptParams) -&gt; Any:\n    request = _get_human_in_the_loop_request(params)\n    response = interrupt(request)\n\n    if response[\"type\"] == \"accept\":\n        return params[\"tool\"].invoke(params[\"tool_call_args\"])\n    elif response[\"type\"] == \"edit\":\n        updated_args = response[\"args\"]\n        return params[\"tool\"].invoke(updated_args)\n    elif response[\"type\"] == \"response\":\n        return response[\"args\"]\n    else:\n        raise ValueError(f\"Unsupported interrupt response type: {response['type']}\")\n</code></pre>"},{"location":"adavance-guide/human-in-the-loop/#interrupt-request-format","title":"Interrupt Request Format","text":"<p>An interrupt sends a request in the following JSON Schema format:</p> Field Description <code>action_request.action</code> The name of the tool call.Type: <code>str</code> <code>action_request.args</code> The arguments for the tool call.Type: <code>dict</code> <code>config.allow_accept</code> Whether to allow accepting the action.Type: <code>bool</code> <code>config.allow_edit</code> Whether to allow editing the arguments.Type: <code>bool</code> <code>config.allow_respond</code> Whether to allow responding directly.Type: <code>bool</code> <code>description</code> A description of the action.Type: <code>str</code>"},{"location":"adavance-guide/human-in-the-loop/#interrupt-response-format","title":"Interrupt Response Format","text":"<p>The response must be returned in the following JSON Schema format:</p> Field Description <code>type</code> Response type, with optional values <code>accept</code>, <code>edit</code>, <code>response</code>.Type: <code>str</code>Required: Yes <code>args</code> When <code>type</code> is <code>edit</code> or <code>response</code>, contains the updated arguments or response content.Type: <code>dict</code>Required: No"},{"location":"adavance-guide/human-in-the-loop/#custom-handler-example","title":"Custom Handler Example","text":"<p>You can have full control over the interrupt behavior, for example, by only allowing \"accept/reject\" or customizing the prompt:</p> <pre><code>from typing import Any\nfrom langchain_dev_utils.tool_calling import human_in_the_loop_async, InterruptParams\nfrom langgraph.types import interrupt\n\n\nasync def custom_handler(params: InterruptParams) -&gt; Any:\n    response = interrupt(\n        f\"I am about to call the tool {params['tool_call_name']} with arguments {params['tool_call_args']}. Please confirm if I should proceed.\"\n    )\n    if response[\"type\"] == \"accept\":\n        return await params[\"tool\"].ainvoke(params[\"tool_call_args\"])\n    elif response[\"type\"] == \"reject\":\n        return \"The user rejected calling this tool.\"\n    else:\n        raise ValueError(f\"Unsupported response type: {response['type']}\")\n\n\n@human_in_the_loop_async(handler=custom_handler)\nasync def get_weather(city: str) -&gt; str:\n    \"\"\"Gets weather information\"\"\"\n    return f\"The weather in {city} is sunny.\"\n</code></pre> <p>Best Practice</p> <p>When implementing custom human-in-the-loop logic with this decorator, you need to pass the <code>handler</code> parameter. This <code>handler</code> parameter is a function that must internally use LangGraph's <code>interrupt</code> function to perform the interrupt operation. Therefore, if you are only adding custom human-in-the-loop logic for a single tool, it is recommended to use LangGraph's <code>interrupt</code> function directly. When multiple tools require the same custom human-in-the-loop logic, using this decorator can effectively avoid code duplication.</p>"},{"location":"adavance-guide/middleware/","title":"Middleware","text":""},{"location":"adavance-guide/middleware/#overview","title":"Overview","text":"<p>Middleware is a component specifically built for LangChain's pre-built Agents. The official library provides some built-in middleware, and this library offers more practical middleware based on actual usage scenarios.</p> <p>The middleware provided by this library includes:</p> <ul> <li><code>PlanMiddleware</code>: Task planning that breaks down complex tasks into ordered subtasks</li> <li><code>ModelRouterMiddleware</code>: Dynamically routes to the most suitable model based on input content</li> <li><code>HandoffAgentMiddleware</code>: Flexibly switches between multiple sub-agents</li> <li><code>ToolCallRepairMiddleware</code>: Automatically fixes invalid tool calls from large models</li> <li><code>format_prompt</code>: Dynamically formats placeholders in system prompts</li> </ul> <p>Additionally, this library extends the functionality of official middleware, supporting model specification through string parameters:</p> <ul> <li>SummarizationMiddleware</li> <li>LLMToolSelectorMiddleware</li> <li>ModelFallbackMiddleware</li> <li>LLMToolEmulator</li> </ul>"},{"location":"adavance-guide/middleware/#task-planning","title":"Task Planning","text":"<p><code>PlanMiddleware</code> is a middleware for structured decomposition and process management before executing complex tasks.</p> <p>Additional Information</p> <p>Task planning is an efficient context engineering management strategy. Before executing a task, the large model first breaks down the overall task into multiple ordered subtasks, forming a task planning list (called a \"plan\" in this library). It then executes each subtask in sequence and dynamically updates the task status after completing each step until all subtasks are finished.</p>"},{"location":"adavance-guide/middleware/#parameter-description","title":"Parameter Description","text":"Parameter Description <code>system_prompt</code> System prompt. If <code>None</code>, the default prompt is used.Type: <code>str</code>Required: No <code>custom_plan_tool_descriptions</code> Custom descriptions for plan-related tools.Type: <code>dict</code>Required: No <code>use_read_plan_tool</code> Whether to enable the read plan tool.Type: <code>bool</code>Required: NoDefault: <code>True</code> <p>The keys in the <code>custom_plan_tool_descriptions</code> dictionary can take the following three values:</p> Key Description <code>write_plan</code> Description of the write plan tool <code>finish_sub_plan</code> Description of the finish subplan tool <code>read_plan</code> Description of the read plan tool"},{"location":"adavance-guide/middleware/#usage-example","title":"Usage Example","text":"<pre><code>from langchain_dev_utils.agents.middleware import PlanMiddleware\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    middleware=[\n        PlanMiddleware(\n            custom_plan_tool_descriptions={\n                \"write_plan\": \"Used for writing plans, breaking tasks into multiple ordered subtasks.\",\n                \"finish_sub_plan\": \"Used for completing subtasks, updating subtask status to completed.\",\n                \"read_plan\": \"Used for querying the current task planning list.\"\n            },\n            use_read_plan_tool=True,  # If not using the read plan tool, set this parameter to False\n        )\n    ],\n)\n\nresponse = agent.invoke(\n    {\"messages\": [HumanMessage(content=\"I want to visit New York for a few days, help me plan my itinerary\")]}\n)\nprint(response)\n</code></pre>"},{"location":"adavance-guide/middleware/#tool-description","title":"Tool Description","text":"<p><code>PlanMiddleware</code> requires the use of two tools: <code>write_plan</code> and <code>finish_sub_plan</code>, while the <code>read_plan</code> tool is enabled by default; if not needed, the <code>use_read_plan_tool</code> parameter can be set to <code>False</code>.</p>"},{"location":"adavance-guide/middleware/#comparison-with-official-to-do-list-middleware","title":"Comparison with Official To-do List Middleware","text":"<p>This middleware has a similar functional positioning to LangChain's official To-do list middleware, but there are differences in tool design:</p> Feature Official To-do List Middleware This Library's PlanMiddleware Number of Tools 1 (<code>write_todo</code>) 3 (<code>write_plan</code>, <code>finish_sub_plan</code>, <code>read_plan</code>) Functional Positioning Focused on to-do lists Specifically for planning lists Operation Method Adding and modifying done through one tool Writing, modifying, and querying done through different tools <p>Whether it's <code>todo</code> or <code>plan</code>, they are essentially the same concept. The key difference between this middleware and the official one is that it provides three specialized tools:</p> <ul> <li><code>write_plan</code>: Used for writing or updating plan content</li> <li><code>finish_sub_plan</code>: Used to update the status after completing a subtask</li> <li><code>read_plan</code>: Used for querying plan content</li> </ul>"},{"location":"adavance-guide/middleware/#model-routing","title":"Model Routing","text":"<p><code>ModelRouterMiddleware</code> is a middleware for dynamically routing to the most suitable model based on input content. It analyzes user requests through a \"routing model\" and selects the most appropriate model from a predefined list to handle the current task.</p>"},{"location":"adavance-guide/middleware/#parameter-description_1","title":"Parameter Description","text":"Parameter Description <code>router_model</code> Model used for routing decisions.Type: <code>str</code> | <code>BaseChatModel</code>Required: Yes <code>model_list</code> List of model configurations.Type: <code>list[ModelDict]</code>Required: Yes <code>router_prompt</code> Custom prompt for the routing model.Type: <code>str</code>Required: No"},{"location":"adavance-guide/middleware/#model_list-configuration-description","title":"<code>model_list</code> Configuration Description","text":"<p>Each model configuration is a dictionary containing the following fields:</p> Field Description <code>model_name</code> Unique identifier for the model, using <code>provider:model-name</code> format.Type: <code>str</code>Required: Yes <code>model_description</code> Brief description of the model's capabilities or applicable scenarios.Type: <code>str</code>Required: Yes <code>tools</code> Whitelist of tools that this model can call.Type: <code>list[BaseTool]</code>Required: No <code>model_kwargs</code> Additional parameters when loading the model.Type: <code>dict</code>Required: No <code>model_system_prompt</code> System-level prompt for the model.Type: <code>str</code>Required: No <code>model_instance</code> Instantiated model object.Type: <code>BaseChatModel</code>Required: No <p>model_instance Field Description</p> <ul> <li>If provided: Directly uses this instance, <code>model_name</code> is only for identification, <code>model_kwargs</code> is ignored; suitable for cases not using this library's conversation model management functionality.</li> <li>If not provided: Loads the model using <code>load_chat_model</code> based on <code>model_name</code> and <code>model_kwargs</code>.</li> <li>Naming format: In either case, it's recommended to use the <code>provider:model-name</code> format for <code>model_name</code>.</li> </ul>"},{"location":"adavance-guide/middleware/#usage-example_1","title":"Usage Example","text":""},{"location":"adavance-guide/middleware/#step-1-define-the-model-list","title":"Step 1: Define the Model List","text":"<pre><code>from langchain_dev_utils.agents.middleware.model_router import ModelDict\n\nmodel_list: list[ModelDict] = [\n    {\n        \"model_name\": \"vllm:qwen3-8b\",\n        \"model_description\": \"Suitable for ordinary tasks, such as dialogue, text generation, etc.\",\n        \"model_kwargs\": {\n            \"temperature\": 0.7,\n            \"extra_body\": {\"chat_template_kwargs\": {\"enable_thinking\": False}},\n        },\n        \"model_system_prompt\": \"You are an assistant, good at handling ordinary tasks, such as dialogue, text generation, etc.\",\n    },\n    {\n        \"model_name\": \"vllm:qwen3-vl-2b\",\n        \"model_description\": \"Suitable for visual tasks\",\n        \"tools\": [],  # If this model doesn't need any tools, set this field to an empty list []\n    },\n    {\n        \"model_name\": \"vllm:qwen3-coder-flash\",\n        \"model_description\": \"Suitable for code generation tasks\",\n        \"tools\": [run_python_code],  # Only allow the use of run_python_code tool\n    },\n    {\n        \"model_name\": \"openai:gpt-4o\",\n        \"model_description\": \"Suitable for comprehensive high-difficulty tasks\",\n        \"model_system_prompt\": \"You are an assistant, good at handling comprehensive high-difficulty tasks\",\n        \"model_instance\": ChatOpenAI(\n            model_name=\"gpt-4o\"\n        ),  # Directly pass the instance, where model_name is only for identification, model_kwargs is ignored\n    },\n]\n</code></pre>"},{"location":"adavance-guide/middleware/#step-2-create-agent-and-enable-middleware","title":"Step 2: Create Agent and Enable Middleware","text":"<pre><code>from langchain_dev_utils.agents.middleware import ModelRouterMiddleware\nfrom langchain_core.messages import HumanMessage\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",  # This model is only a placeholder, actually dynamically replaced by middleware\n    tools=[run_python_code, get_current_time],\n    middleware=[\n        ModelRouterMiddleware(\n            router_model=\"vllm:qwen3-4b\",\n            model_list=model_list,\n        )\n    ],\n)\n\n# The routing middleware will automatically select the most suitable model based on input content\nresponse = agent.invoke({\"messages\": [HumanMessage(content=\"Help me write a bubble sort code\")]})\nprint(response)\n</code></pre> <p>Through <code>ModelRouterMiddleware</code>, you can easily build a multi-model, multi-capability Agent that automatically selects the optimal model based on task type, improving response quality and efficiency.</p> <p>Parallel Execution</p> <p>Using middleware to implement model routing, only one task is assigned for execution at a time. If you want to decompose a task into multiple subtasks and execute them in parallel with multiple models, please refer to State Graph Orchestration.</p>"},{"location":"adavance-guide/middleware/#agent-handoffs","title":"Agent Handoffs","text":"<p><code>HandoffAgentMiddleware</code> is a middleware used for flexibly switching between multiple sub-agents, fully implementing LangChain's official <code>handoffs</code> multi-agent collaboration scheme.</p>"},{"location":"adavance-guide/middleware/#parameter-description_2","title":"Parameter Description","text":"Parameter Description <code>agents_config</code> A dictionary of agent configurations, where the key is the agent name and the value is the agent configuration dictionary.Type: <code>dict[str, AgentConfig]</code>Required: Yes <code>custom_handoffs_tool_descriptions</code> Custom descriptions for handoff tools, where the key is the agent name and the value is the corresponding handoff tool description.Type: <code>dict[str, str]</code>Required: No <code>handoffs_tool_overrides</code> Custom implementations of handoff tools, where the key is the agent name and the value is the corresponding handoff tool implementation.Type: <code>dict[str, BaseTool]</code>Required: No"},{"location":"adavance-guide/middleware/#agents_config-configuration-description","title":"<code>agents_config</code> Configuration Description","text":"<p>Each agent is configured as a dictionary containing the following fields:</p> Field Description <code>model</code> Specifies the model used by this agent; if not passed, it inherits the model corresponding to the <code>model</code> parameter of <code>create_agent</code>. Supports strings (must be in <code>provider:model-name</code> format, e.g., <code>vllm:qwen3-4b</code>) or a <code>BaseChatModel</code> instance.Type: <code>str</code> | <code>BaseChatModel</code>Required: No <code>prompt</code> The system prompt for the agent.Type: <code>str</code> | <code>SystemMessage</code>Required: Yes <code>tools</code> A list of tools that the agent can call.Type: <code>list[BaseTool]</code>Required: No <code>default</code> Whether to set as the default agent; defaults to <code>False</code>. There must be exactly one agent set to <code>True</code> in the entire configuration.Type: <code>bool</code>Required: No <code>handoffs</code> A list of other agent names that this agent can hand off to. If set to <code>\"all\"</code>, it means this agent can hand off to all other agents.Type: <code>list[str]</code> | <code>str</code>Required: Yes <p>For this paradigm of multi-agent implementation, a tool used for handoffs is often required. This middleware utilizes the <code>handoffs</code> configuration of each agent to automatically create the corresponding handoff tool for each agent. If you wish to customize the description of the handoff tool, you can achieve this via the <code>custom_handoffs_tool_descriptions</code> parameter.</p> <p>Usage Example</p> <p>In this example, we will use four agents: <code>time_agent</code>, <code>weather_agent</code>, <code>code_agent</code>, and <code>default_agent</code>.</p> <p>Next, we need to create the corresponding agent configuration dictionary <code>agent_config</code>.</p> <pre><code>from langchain_dev_utils.agents.middleware.handoffs import AgentConfig\n\nagent_config: dict[str, AgentConfig] = {\n    \"time_agent\": {\n        \"model\": \"vllm:qwen3-8b\",\n        \"prompt\": \"You are a time assistant\",\n        \"tools\": [get_current_time],\n        \"handoffs\": [\"default_agent\"],  # This agent can only hand off to default_agent\n    },\n    \"weather_agent\": {\n        \"prompt\": \"You are a weather assistant\",\n        \"tools\": [get_current_weather, get_current_city],\n        \"handoffs\": [\"default_agent\"],\n    },\n    \"code_agent\": {\n        \"model\": load_chat_model(\"vllm:qwen3-coder-flash\"),\n        \"prompt\": \"You are a coding assistant\",\n        \"tools\": [\n            run_code,\n        ],\n        \"handoffs\": [\"default_agent\"],\n    },\n    \"default_agent\": {\n        \"prompt\": \"You are an assistant\",\n        \"default\": True, # Set as the default agent\n        \"handoffs\": \"all\",  # This agent can hand off to all other agents\n    },\n}\n</code></pre> <p>Finally, simply pass this configuration to <code>HandoffAgentMiddleware</code>.</p> <pre><code>from langchain_dev_utils.agents.middleware import HandoffAgentMiddleware\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    tools=[\n        get_current_time,\n        get_current_weather,\n        get_current_city,\n        run_code,\n    ],\n    middleware=[HandoffAgentMiddleware(agents_config=agent_config)],\n)\n\nresponse = agent.invoke({\"messages\": [HumanMessage(content=\"What is the current time?\")]})\nprint(response)\n</code></pre> <p>If you want to customize the description of the handoff tools, you can pass the second parameter <code>custom_handoffs_tool_descriptions</code>.</p> <pre><code>from langchain_dev_utils.agents.middleware import HandoffAgentMiddleware\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    tools=[\n        get_current_time,\n        get_current_weather,\n        get_current_city,\n        run_code,\n    ],\n    middleware=[\n        HandoffAgentMiddleware(\n            agents_config=agent_config,\n            custom_handoffs_tool_descriptions={\n                \"time_agent\": \"Use this tool to hand off to the time assistant to resolve time query issues\",\n                \"weather_agent\": \"Use this tool to hand off to the weather assistant to resolve weather query issues\",\n                \"code_agent\": \"Use this tool to hand off to the code assistant to resolve coding issues\",\n                \"default_agent\": \"Use this tool to hand off to the default assistant\",\n            },\n        )\n    ],\n)\n</code></pre> <p>If you want to completely customize the logic of the handoff tool implementation, you can pass the third parameter <code>handoffs_tool_overrides</code>. Similar to the second parameter, it is also a dictionary where the key is the agent name and the value is the corresponding handoff tool implementation.</p> <p>Custom handoff tools must return a <code>Command</code> object whose <code>update</code> attribute must include the <code>messages</code> key (for returning tool responses) and the <code>active_agent</code> key (whose value is the name of the agent to hand off to, used to switch the current agent).</p> <p>For example:</p> <pre><code>@tool\ndef transfer_to_code_agent(runtime: ToolRuntime) -&gt; Command:\n    \"\"\"This tool help you transfer to the code agent.\"\"\"\n    #You can add custom logic here\n    return Command(\n        update={\n            \"messages\": [\n                ToolMessage(\n                    content=\"transfer to code agent\",\n                    tool_call_id=runtime.tool_call_id,\n                )\n            ],\n            \"active_agent\": \"code_agent\",\n            #You can add other keys to update here\n        }\n    )\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    tools=[\n        get_current_time,\n        get_current_weather,\n        get_current_city,\n        run_code,\n    ],\n    middleware=[\n        HandoffAgentMiddleware(\n            agents_config=agent_config,\n            handoffs_tool_overrides={\n                \"code_agent\": transfer_to_code_agent,\n            },\n        )\n    ],\n)\n</code></pre> <p><code>handoffs_tool_overrides</code> is used for highly customized handoff tool implementations. If you only want to customize the description of the handoff tool, you should use <code>custom_handoffs_tool_descriptions</code>.</p>"},{"location":"adavance-guide/middleware/#tool-call-repair","title":"Tool Call Repair","text":"<p><code>ToolCallRepairMiddleware</code> is a middleware for automatically fixing invalid tool calls (<code>invalid_tool_calls</code>) from large models.</p> <p>When large models output the JSON Schema for tool calls, they may generate JSON format errors due to the model's own reasons (errors are common in the <code>arguments</code> field), leading to JSON parsing failures. Such calls are stored in the <code>invalid_tool_calls</code> field. <code>ToolCallRepairMiddleware</code> will automatically detect <code>invalid_tool_calls</code> after the model returns results and attempt to fix them by calling <code>json-repair</code>, allowing the tool calls to execute normally.</p> <p>Please ensure you have installed <code>langchain-dev-utils[standard]</code>, see the Installation Guide for details.</p>"},{"location":"adavance-guide/middleware/#parameter-description_3","title":"Parameter Description","text":"<p>This middleware is zero-configuration and ready to use out of the box, no additional parameters required.</p>"},{"location":"adavance-guide/middleware/#usage-example_2","title":"Usage Example","text":"<pre><code>from langchain_dev_utils.agents.middleware import ToolCallRepairMiddleware\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    tools=[run_python_code, get_current_time],\n    middleware=[\n        ToolCallRepairMiddleware()\n    ],\n)\n</code></pre> <p>Note</p> <p>This middleware cannot guarantee 100% fixing of all invalid tool calls; the actual effect depends on the repair capability of <code>json-repair</code>. Additionally, it only acts on invalid tool call content in the <code>invalid_tool_calls</code> field.</p>"},{"location":"adavance-guide/middleware/#formatting-system-prompts","title":"Formatting System Prompts","text":"<p><code>format_prompt</code> is a middleware instance that allows you to use <code>f-string</code> style placeholders (like <code>{name}</code>) in <code>system_prompt</code> and dynamically replace them with actual values at runtime.</p>"},{"location":"adavance-guide/middleware/#parameter-description_4","title":"Parameter Description","text":"<p>The values of variables in placeholders follow a clear resolution order:</p> <ol> <li>First look up in <code>state</code>: It first looks for fields with the same name as the placeholder in the <code>state</code> dictionary</li> <li>Then look up in <code>context</code>: If the field is not found in <code>state</code>, it continues to look in the <code>context</code> object</li> </ol> <p>This order means that values in <code>state</code> have higher priority and can override values with the same name in <code>context</code>.</p>"},{"location":"adavance-guide/middleware/#usage-example_3","title":"Usage Example","text":""},{"location":"adavance-guide/middleware/#getting-variables-only-from-state","title":"Getting Variables Only from <code>state</code>","text":"<p>This is the most basic usage, where all placeholder variables are provided by <code>state</code>.</p> <pre><code>from langchain_dev_utils.agents.middleware import format_prompt\nfrom langchain.agents import AgentState\n\nclass AssistantState(AgentState):\n    name: str\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    system_prompt=\"You are an intelligent assistant, your name is {name}.\",\n    middleware=[format_prompt],\n    state_schema=AssistantState,\n)\n\n# When calling, you must provide a value for 'name' in state\nresponse = agent.invoke(\n    {\"messages\": [HumanMessage(content=\"Hello\")], \"name\": \"assistant\"}\n)\nprint(response)\n</code></pre>"},{"location":"adavance-guide/middleware/#getting-variables-from-both-state-and-context","title":"Getting Variables from Both <code>state</code> and <code>context</code>","text":"<p>Using both <code>state</code> and <code>context</code> simultaneously:</p> <pre><code>from dataclasses import dataclass\n\n@dataclass\nclass Context:\n    user: str\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    # {name} will be obtained from state, {user} will be obtained from context\n    system_prompt=\"You are an intelligent assistant, your name is {name}. Your user is named {user}.\",\n    middleware=[format_prompt],\n    state_schema=AssistantState,\n    context_schema=Context,\n)\n\n# When calling, provide 'name' for state and 'user' for context\nresponse = agent.invoke(\n    {\n        \"messages\": [HumanMessage(content=\"I want to visit New York for a few days, help me plan my itinerary\")],\n        \"name\": \"assistant\",\n    },\n    context=Context(user=\"Zhang San\"),\n)\nprint(response)\n</code></pre>"},{"location":"adavance-guide/middleware/#variable-override-example","title":"Variable Override Example","text":"<p>This example shows that when <code>state</code> and <code>context</code> have variables with the same name, the value in <code>state</code> takes precedence.</p> <pre><code>from dataclasses import dataclass\n\n@dataclass\nclass Context:\n    # 'name' is defined in context\n    name: str\n    user: str\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    system_prompt=\"You are an intelligent assistant, your name is {name}. Your user is named {user}.\",\n    middleware=[format_prompt],\n    state_schema=AssistantState, # 'name' is also defined in state\n    context_schema=Context,\n)\n\n# When calling, both state and context provide a value for 'name'\nresponse = agent.invoke(\n    {\n        \"messages\": [HumanMessage(content=\"What is your name?\")],\n        \"name\": \"assistant-1\",\n    },\n    context=Context(name=\"assistant-2\", user=\"Zhang San\"),\n)\n\n# The final system prompt will be \"You are an intelligent assistant, your name is assistant-1. Your user is named Zhang San.\"\n# Because state has higher priority\nprint(response)\n</code></pre> <p>Note</p> <p>There are two ways to implement custom middleware: decorators or class inheritance. - Class inheritance implementation: <code>PlanMiddleware</code>, <code>ModelMiddleware</code>, <code>HandoffAgentMiddleware</code>, <code>ToolCallRepairMiddleware</code> - Decorator implementation: <code>format_prompt</code> (the decorator directly turns the function into a middleware instance, so no manual instantiation is needed to use it)</p> <p>Official Middleware Extensions</p> <p>This library extends the following official middleware, supporting model specification through string parameters for models registered with <code>register_model_provider</code>:</p> <p>You only need to import these middleware from this library to use string parameters for models registered with <code>register_model_provider</code>. The usage of the middleware remains consistent with the official middleware, for example: <pre><code>from langchain_core.messages import AIMessage\nfrom langchain_dev_utils.agents.middleware import SummarizationMiddleware\nfrom langchain_dev_utils.chat_models import register_model_provider\n\nregister_model_provider(\n    provider_name=\"vllm\",\n    chat_model=\"openai-compatible\",\n    base_url=\"http://localhost:8000/v1\",\n)\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    middleware=[\n        SummarizationMiddleware(\n            model=\"vllm:qwen3-4b\",\n            trigger=(\"tokens\", 50),\n            keep=(\"messages\", 1),\n        )\n    ],\n    system_prompt=\"You are an intelligent AI assistant that can solve user problems\",\n)\nresponse = agent.invoke({\"messages\": messages})\nprint(response)\n</code></pre></p>"},{"location":"adavance-guide/multi-agent/","title":"Subagent Tools (Agent as Tool)","text":""},{"location":"adavance-guide/multi-agent/#overview","title":"Overview","text":"<p>When building complex AI applications, multi-agent collaboration is a powerful architectural pattern. By assigning different responsibilities to specialized agents, you can achieve specialized division of labor and efficient collaboration.</p> <p>There are various ways to implement multi-agent collaboration, among which Tool Calling is a common and flexible approach. By encapsulating subagents as tools, a master agent can dynamically delegate tasks to specialized subagents based on requirements.</p> <p>This library provides two pre-built functions to simplify this implementation:</p> Function Name Description <code>wrap_agent_as_tool</code> Encapsulates a single agent instance as an independent tool <code>wrap_all_agents_as_tool</code> Encapsulates multiple agent instances into a unified tool, specifying which subagent to call via parameters"},{"location":"adavance-guide/multi-agent/#wrapping-a-single-agent-as-a-tool","title":"Wrapping a Single Agent as a Tool","text":"<p>Wrapping a single agent involves just three steps:</p> <ol> <li>Import <code>wrap_agent_as_tool</code></li> <li>Pass the agent instance as a parameter</li> <li>Obtain a tool object that can be directly invoked by other agents</li> </ol>"},{"location":"adavance-guide/multi-agent/#function-parameter-description","title":"Function Parameter Description","text":"Parameter Description <code>agent</code> Agent instance, must have a defined <code>name</code> attribute.Type: <code>CompiledStateGraph</code>Required: Yes <code>tool_name</code> Tool name, defaults to <code>transfer_to_{agent_name}</code>.Type: <code>str</code>Required: No <code>tool_description</code> Tool description, defaults to <code>This tool transforms input to {agent_name}</code>.Type: <code>str</code>Required: No <code>pre_input_hooks</code> Hook functions before the agent runs.Type: <code>tuple[Callable, Callable] | Callable</code>Required: No <code>post_output_hooks</code> Hook functions after the agent runs.Type: <code>tuple[Callable, Callable] | Callable</code>Required: No"},{"location":"adavance-guide/multi-agent/#usage-example","title":"Usage Example","text":"<p>Below, we use the <code>supervisor</code> agent as an example to demonstrate how to wrap a subagent as a tool using <code>wrap_agent_as_tool</code>.</p> <p>First, implement two subagents: one for sending emails and one for calendar queries and scheduling.</p>"},{"location":"adavance-guide/multi-agent/#email-agent","title":"Email Agent","text":"<pre><code>from langchain_core.tools import tool\nfrom langchain_dev_utils.chat_models import register_model_provider\nfrom langchain_dev_utils.agents import create_agent, wrap_agent_as_tool \n\nregister_model_provider(\n    \"vllm\",\n    \"openai-compatible\",\n    base_url=\"http://localhost:8000/v1\",\n)\n\n\n@tool\ndef send_email(\n    to: list[str],  # Email addresses\n    subject: str,\n    body: str,\n    cc: list[str] = [],\n) -&gt; str:\n    \"\"\"Sends an email via the email API. Requires correctly formatted addresses.\"\"\"\n    # Stub: In a real application, you would call SendGrid, Gmail API, etc. here\n    return f\"Email sent to {', '.join(to)} - Subject: {subject}\"\n\n\nEMAIL_AGENT_PROMPT = (\n    \"You are an email assistant. \"\n    \"Draft professional emails based on natural language requests. \"\n    \"Extract recipient information and create appropriate subject lines and body content. \"\n    \"Use send_email to send emails. \"\n    \"Always confirm what was sent in your final response.\"\n)\n\nemail_agent = create_agent(\n    \"vllm:qwen3-4b\",\n    tools=[send_email],\n    system_prompt=EMAIL_AGENT_PROMPT,\n    name=\"email_agent\",\n)\n</code></pre>"},{"location":"adavance-guide/multi-agent/#calendar-agent","title":"Calendar Agent","text":"<pre><code>@tool\ndef create_calendar_event(\n    title: str,\n    start_time: str,  # ISO format: \"2024-01-15T14:00:00\"\n    end_time: str,  # ISO format: \"2024-01-15T15:00:00\"\n    attendees: list[str],  # Email addresses\n    location: str = \"\",\n) -&gt; str:\n    \"\"\"Creates a calendar event. Requires precise ISO date-time format.\"\"\"\n    # Stub: In a real application, you would call Google Calendar API, Outlook API, etc. here\n    return f\"Event created: {title} from {start_time} to {end_time} with {len(attendees)} participants\"\n\n\n@tool\ndef get_available_time_slots(\n    attendees: list[str],\n    date: str,  # ISO format: \"2024-01-15\"\n    duration_minutes: int,\n) -&gt; list[str]:\n    \"\"\"Queries calendar availability for attendees on a specific date.\"\"\"\n    # Stub: In a real application, you would query the calendar API here\n    return [\"09:00\", \"14:00\", \"16:00\"]\n\n\nCALENDAR_AGENT_PROMPT = (\n    \"You are a calendar scheduling assistant. \"\n    \"Parse natural language scheduling requests (e.g., 'next Tuesday at 2 PM') into the correct ISO date-time format. \"\n    \"Use get_available_time_slots to check availability when needed. \"\n    \"Use create_calendar_event to schedule events. \"\n    \"Always confirm what was scheduled in your final response.\"\n)\n\ncalendar_agent = create_agent(\n    \"vllm:qwen3-4b\",\n    tools=[create_calendar_event, get_available_time_slots],\n    system_prompt=CALENDAR_AGENT_PROMPT,\n    name=\"calendar_agent\",\n)\n</code></pre> <p>Next, use <code>wrap_agent_as_tool</code> to wrap these two subagents as tools.</p> <pre><code>schedule_event = wrap_agent_as_tool(\n    calendar_agent,\n    tool_name=\"schedule_event\",\n    tool_description=(\n        \"Schedule calendar events using natural language. \"\n        \"Use this when the user wants to create, modify, or check calendar appointments. \"\n        \"Capable of handling date/time parsing, querying available times, and creating events. \"\n        \"Input: Natural language scheduling request (e.g., 'meeting with the design team next Tuesday at 2 PM')\"\n    ),\n)\nmanage_email = wrap_agent_as_tool(\n    email_agent,\n    tool_name=\"manage_email\",\n    tool_description=(\n        \"Send emails using natural language. \"\n        \"Use this when the user wants to send notifications, reminders, or any email communication. \"\n        \"Capable of extracting recipient information, subject generation, and email drafting. \"\n        \"Input: Natural language email request (e.g., 'send them a meeting reminder')\"\n    ),\n)\n</code></pre> <p>Finally, create a <code>supervisor_agent</code> that can invoke these two tools.</p> <pre><code>SUPERVISOR_PROMPT = (\n    \"You are a helpful personal assistant. \"\n    \"You can schedule calendar events and send emails. \"\n    \"Break down user requests into appropriate tool calls and coordinate results. \"\n    \"When a request involves multiple operations, please use multiple tools in sequence.\"\n)\n\n\nsupervisor_agent = create_agent(\n    \"vllm:qwen3-4b\",\n    tools=[schedule_event, manage_email],\n    system_prompt=SUPERVISOR_PROMPT,\n)\n\nprint(\n    supervisor_agent.invoke({\"messages\": [HumanMessage(content=\"Check available time for tomorrow\")]})\n)\nprint(\n    supervisor_agent.invoke(\n        {\"messages\": [HumanMessage(content=\"Send a meeting reminder to test@123.com\")]}\n    )\n)\n</code></pre> <p>Hint</p> <p>In the example above, we imported <code>create_agent</code> from <code>langchain_dev_utils.agents</code> instead of <code>langchain.agents</code>. This is because this library also provides a function identical in functionality to the official <code>create_agent</code>, but with the added capability to specify models via strings. This allows you to directly use models registered via <code>register_model_provider</code> without needing to initialize model instances first.</p>"},{"location":"adavance-guide/multi-agent/#wrapping-multiple-agents-as-a-single-tool","title":"Wrapping Multiple Agents as a Single Tool","text":"<p>Wrapping multiple agents as a single tool involves just three steps:</p> <ol> <li>Import <code>wrap_all_agents_as_tool</code></li> <li>Pass multiple agent instances as a list at once</li> <li>Obtain a unified tool object that can be directly invoked by other agents</li> </ol>"},{"location":"adavance-guide/multi-agent/#function-parameter-description_1","title":"Function Parameter Description","text":"Parameter Description <code>agents</code> List of agent instances.Type: <code>list[CompiledStateGraph]</code>Required: Yes <code>tool_name</code> Tool name, defaults to <code>task</code>.Type: <code>str</code>Required: No <code>tool_description</code> Tool description, defaults to including all available agent information.Type: <code>str</code>Required: No <code>pre_input_hooks</code> Hook functions before the agent runs.Type: <code>tuple[Callable, Callable] | Callable</code>Required: No <code>post_output_hooks</code> Hook functions after the agent runs.Type: <code>tuple[Callable, Callable] | Callable</code>Required: No"},{"location":"adavance-guide/multi-agent/#usage-example_1","title":"Usage Example","text":"<p>For the <code>calendar_agent</code> and <code>email_agent</code> from the previous example, we can wrap them into a single tool <code>call_subagent</code>:</p> <pre><code>call_subagent_tool = wrap_all_agents_as_tool(\n    [calendar_agent, email_agent],\n    tool_name=\"call_subagent\",\n    tool_description=(\n        \"Call subagents to execute tasks. \"\n        \"Available agents include: \"\n        \"- calendar_agent: for scheduling calendar events \"\n        \"- email_agent: for sending emails\"\n    ),\n)\n\nMAIN_AGENT_PROMPT = (\n    \"You are a helpful personal assistant. \"\n    \"You can use the **call_subagent** tool to call subagents to execute tasks. \"\n    \"Break down user requests into appropriate tool calls and coordinate results. \"\n    \"When a request involves multiple operations, please use multiple tools in sequence.\"\n)\n\nmain_agent = create_agent(\n    \"vllm:qwen3-4b\",\n    tools=[call_subagent_tool],\n    system_prompt=MAIN_AGENT_PROMPT,\n)\n</code></pre> <p>Hint</p> <p>In addition to using <code>wrap_all_agents_as_tool</code> provided by this library to wrap multiple agents into a single tool, you can also achieve similar effects using the <code>SubAgentMiddleware</code> middleware provided by the <code>deepagents</code> library.</p>"},{"location":"adavance-guide/multi-agent/#hook-functions","title":"Hook Functions","text":"<p>This library includes a flexible hook mechanism that allows you to insert custom logic before and after a subagent runs. This mechanism applies to both <code>wrap_agent_as_tool</code> and <code>wrap_all_agents_as_tool</code>. The following description uses <code>wrap_agent_as_tool</code> as an example.</p>"},{"location":"adavance-guide/multi-agent/#1-pre_input_hooks","title":"1. pre_input_hooks","text":"<p>Preprocesses the input before the agent runs. Useful for input enhancement, context injection, format validation, permission checks, etc.</p>"},{"location":"adavance-guide/multi-agent/#supported-input-types","title":"Supported Input Types","text":"Type Description Single sync function Used for both sync (<code>invoke</code>) and async (<code>ainvoke</code>) call paths (will not be awaited in the async path, called directly) Tuple <code>(sync_func, async_func)</code> The first function is used for the sync call path; the second function (must be <code>async def</code>) is used for the async call path and will be <code>awaited</code>"},{"location":"adavance-guide/multi-agent/#function-signature","title":"Function Signature","text":"<pre><code>def pre_input_hook(request: str, runtime: ToolRuntime) -&gt; str | dict[str, Any]:\n    \"\"\"\n    Args:\n        request: The original tool call input\n        runtime: langchain's ToolRuntime\n\n    Returns:\n        The processed input, which serves as the actual input for the agent (must be str or dict)\n    \"\"\"\n</code></pre> <p>Note:</p> <ul> <li> <p>The return value of the hook function must be a <code>str</code> or <code>dict</code>; otherwise, a <code>ValueError</code> will be raised.</p> </li> <li> <p>If a <code>dict</code> is returned, it will be used directly as the agent's actual input.</p> </li> <li> <p>If a <code>str</code> is returned, it will be wrapped as <code>HumanMessage(content=...)</code>, ultimately serving as the agent's actual input in the format <code>{\"messages\": [HumanMessage(content=...)]}</code>.</p> </li> <li> <p>If <code>pre_input_hooks</code> is not provided, the original input is used directly as the agent's actual input in the format <code>{\"messages\": [HumanMessage(content=request)]}</code>.</p> </li> </ul>"},{"location":"adavance-guide/multi-agent/#usage-example_2","title":"Usage Example","text":"<p>For instance, before calling a subagent, you can use a model to summarize the master agent's conversation history, providing more precise task context for the subagent.</p> <pre><code>from langchain.tools import ToolRuntime\nfrom langchain_core.messages import SystemMessage\nfrom langchain_dev_utils.agents import wrap_agent_as_tool\n\n\ndef process_input(request: str, runtime: ToolRuntime) -&gt; str:\n    messages = runtime.state.get(\"messages\", [])\n\n    new_messages = [\n        SystemMessage(\n            content=\"\"\"Please generate a concise and accurate summary based on the following conversation history.\n            The summary should include:\n            1) Topic of the conversation;\n            2) Key content;\n            3) Current status or progress.\n            Keep the summary length under 200 characters.\"\"\"\n        ),\n        *messages,\n    ]\n\n    if messages:\n        summary = model.invoke(new_messages)\n\n        return (\n            \"&lt;history_summary&gt;\\n\"\n            + summary.content\n            + \"\\n&lt;/history_summary&gt;\\n\"\n            + \"&lt;task_description&gt;\\n\"\n            + request\n            + \"\\n&lt;/task_description&gt;\"\n        )\n    return \"&lt;task_description&gt;\\n\" + request + \"\\n&lt;/task_description&gt;\"\n\n\nasync def process_input_async(request: str, runtime: ToolRuntime) -&gt; str:\n    messages = runtime.state.get(\"messages\", [])\n\n    new_messages = [\n        SystemMessage(\n            content=\"\"\"Please generate a concise and accurate summary based on the following conversation history.\n            The summary should include:\n            1) Topic of the conversation;\n            2) Key information points;\n            3) Current status or progress.\n            Keep the summary length under 200 characters.\"\"\"\n        ),\n        *messages,\n    ]\n\n    if messages:\n        summary = await model.ainvoke(new_messages)\n\n        return (\n            \"&lt;history_summary&gt;\\n\"\n            + summary.content\n            + \"\\n&lt;/history_summary&gt;\\n\"\n            + \"&lt;task_description&gt;\\n\"\n            + request\n            + \"\\n&lt;/task_description&gt;\"\n        )\n    return \"&lt;task_description&gt;\\n\" + request + \"\\n&lt;/task_description&gt;\"\n\n\n# Usage\ncall_agent_tool = wrap_agent_as_tool(\n    agent, pre_input_hooks=(process_input, process_input_async)\n)\n</code></pre>"},{"location":"adavance-guide/multi-agent/#2-post_output_hooks","title":"2. post_output_hooks","text":"<p>Performs post-processing on the complete list of messages returned by the agent after it has finished running, to generate the final return value of the tool. Useful for result extraction, structured transformation, etc.</p>"},{"location":"adavance-guide/multi-agent/#supported-input-types_1","title":"Supported Input Types","text":"Type Description Single function Used for both sync and async paths (will not be awaited in async path) Tuple <code>(sync_func, async_func)</code> The first is used for the sync path; the second (<code>async def</code>) is used for the async path and will be <code>awaited</code>"},{"location":"adavance-guide/multi-agent/#function-signature_1","title":"Function Signature","text":"<pre><code>def post_output_hook(request: str, response: dict[str, Any], runtime: ToolRuntime) -&gt; Union[str, Command]:\n    \"\"\"\n    Args:\n        request: The unprocessed original input\n        response: The complete response returned by the agent\n        runtime: langchain's ToolRuntime\n\n    Returns:\n        A value that can be serialized to a string, or a Command object\n    \"\"\"\n</code></pre> <p>Note:</p> <ul> <li> <p>The return value of the hook function must be a value that can be serialized to a string or a <code>Command</code> object.</p> </li> <li> <p>The two input arguments of the hook function are: <code>request</code>, which is the unprocessed original input, and <code>response</code>, which is the complete response returned by the agent (i.e., the return value of <code>agent.invoke(input)</code>).</p> </li> <li> <p>If <code>post_output_hooks</code> is not provided, the agent's final response is used directly as the tool's return value (i.e., <code>response[\"messages\"][-1].content</code>).</p> </li> </ul>"},{"location":"adavance-guide/multi-agent/#usage-example_3","title":"Usage Example","text":"<p>For example, after the subagent finishes execution, besides updating the <code>messages</code> key, it might update other state keys. If you need to save these additional state keys to the master agent's state, you can use a <code>Command</code> object for the return.</p> <pre><code>from typing import Any\nfrom langchain.tools import ToolRuntime\nfrom langchain_core.messages import ToolMessage\nfrom langgraph.types import Command\n\n\ndef process_output_sync(\n    request: str, response: dict[str, Any], runtime: ToolRuntime\n) -&gt; Command:\n    return Command(\n        update={\n            \"messages\": [\n                ToolMessage(\n                    content=response[\"messages\"][-1].content,\n                    tool_call_id=runtime.tool_call_id,\n                )\n            ],\n            \"example_state_key\": response[\"example_state_key\"],\n        }\n    )\n\n\nasync def process_output_async(\n    request: str, response: dict[str, Any], runtime: ToolRuntime\n) -&gt; Command:\n    return Command(\n        update={\n            \"messages\": [\n                ToolMessage(\n                    content=response[\"messages\"][-1].content,\n                    tool_call_id=runtime.tool_call_id,\n                )\n            ],\n            \"example_state_key\": response[\"example_state_key\"],\n        }\n    )\n\n\n# Usage\ncall_agent_tool = wrap_agent_as_tool(\n    agent, post_output_hooks=(process_output_sync, process_output_async)\n)\n</code></pre>"},{"location":"adavance-guide/openai-compatible/","title":"OpenAI Compatible API Model Provider Integration","text":"<p>Prerequisites</p> <p>When using this feature, the standard version of the <code>langchain-dev-utils</code> library must be installed. Please refer to the installation section for details.</p>"},{"location":"adavance-guide/openai-compatible/#overview","title":"Overview","text":"<p>Many model providers offer OpenAI Compatible API services, such as vLLM, OpenRouter, and Together AI. This library provides an OpenAI compatible API integration solution covering both chat models and embedding models. It is particularly suitable for scenarios where \"the provider offers an OpenAI compatible API but there is no corresponding LangChain integration yet.\"</p> <p>This library provides two utility functions for creating chat model integration classes and embedding model integration classes:</p> Function Name Description <code>create_openai_compatible_model</code> Creates a chat model integration class <code>create_openai_compatible_embedding</code> Creates an embedding model integration class <p>Note</p> <p>The initial inspiration for the two utility functions provided by this library comes from the JavaScript ecosystem's @ai-sdk/openai-compatible.</p> <p>The following example demonstrates how to use this feature by integrating with vLLM.</p> vLLM Introduction <p>vLLM is a popular LLM inference framework for high-performance local or self-hosted serving. It can expose chat and embedding models through an OpenAI-compatible API, enabling reuse of existing SDKs and calling patterns, and it supports multi-model serving, tool calling, and reasoning outputs for chat, tool use, and multimodal scenarios.</p> <p>The following deployment commands are for the models used later in this guide:</p> <p>Qwen3-4B:</p> <pre><code>vllm serve Qwen/Qwen3-4B \\\n--reasoning-parser qwen3 \\\n--enable-auto-tool-choice --tool-call-parser hermes \\\n--host 0.0.0.0 --port 8000 \\\n--served-model-name qwen3-4b\n</code></pre> <p>GLM-4.7-Flash:</p> <pre><code>vllm serve zai-org/GLM-4.7-Flash \\\n --tensor-parallel-size 4 \\\n --speculative-config.method mtp \\\n --speculative-config.num_speculative_tokens 1 \\\n --tool-call-parser glm47 \\\n --reasoning-parser glm45 \\\n --enable-auto-tool-choice \\\n --served-model-name glm-4.7-flash\n</code></pre> <p>Qwen3-VL-2B-Instruct:</p> <pre><code>vllm serve Qwen/Qwen3-VL-2B-Instruct \\\n--trust-remote-code \\\n--host 0.0.0.0 --port 8000 \\\n--served-model-name qwen3-vl-2b\n</code></pre> <p>Qwen3-Embedding-4B:</p> <p><pre><code>vllm serve Qwen/Qwen3-Embedding-4B \\\n--task embed \\\n--served-model-name qwen3-embedding-4b \\\n--host 0.0.0.0 --port 8000\n</code></pre> The service address is <code>http://localhost:8000/v1</code>.</p>"},{"location":"adavance-guide/openai-compatible/#creation-and-usage-of-chat-models","title":"Creation and Usage of Chat Models","text":""},{"location":"adavance-guide/openai-compatible/#creating-a-chat-model-class","title":"Creating a Chat Model Class","text":"<p>You can use the <code>create_openai_compatible_model</code> function to create a chat model integration class. This function accepts the following parameters:</p> Parameter Description <code>model_provider</code> Model provider name, e.g., <code>vllm</code>. Must start with a letter or number, contain only letters, numbers, and underscores, and be no longer than 20 characters.Type: <code>str</code>Required: Yes <code>base_url</code> Default API address for the model provider.Type: <code>str</code>Required: No <code>compatibility_options</code> Compatibility option configuration.Type: <code>dict</code>Required: No <code>model_profiles</code> Dictionary of profile configurations for the provider's models.Type: <code>dict</code>Required: No <code>chat_model_cls_name</code> Chat model class name (must conform to Python class naming conventions). Default is <code>Chat{model_provider}</code> (with <code>{model_provider}</code> capitalized).Type: <code>str</code>Required: No <p>Among these, <code>compatibility_options</code> is a dictionary used to declare the provider's support for certain OpenAI API features to improve compatibility and stability.</p> <p>Currently, the following configuration items are supported:</p> Configuration Item Description <code>supported_tool_choice</code> List of supported <code>tool_choice</code> strategies.Type: <code>list[str]</code>Default: <code>[\"auto\"]</code> <code>supported_response_format</code> List of supported <code>response_format</code> formats (<code>json_schema</code>, <code>json_object</code>).Type: <code>list[str]</code>Default: <code>[]</code> <code>reasoning_keep_policy</code> Retention policy for the <code>reasoning_content</code> field in historical messages.Type: <code>str</code>Default: <code>\"never\"</code> <code>include_usage</code> Whether to include <code>usage</code> information in streaming results.Type: <code>bool</code>Default: <code>True</code> <p>Supplement</p> <p>Since different models from the same provider may have varying support for parameters like <code>tool_choice</code> and <code>response_format</code>, these four compatibility options are instance attributes of the class. Therefore, when creating the chat model class, you can pass values as global defaults (representing the configuration supported by most models of that provider). If you need to fine-tune for a specific model later, you can override the parameters with the same name during instantiation.</p> <p>Hint</p> <p>Based on the parameters provided by the user, this library uses the built-in <code>BaseChatOpenAICompatible</code> to construct a provider-specific chat model class. This class inherits from <code>BaseChatOpenAI</code> in <code>langchain-openai</code> and includes the following enhancements:</p> <ul> <li>Support for more reasoning content formats: In addition to the official OpenAI format, it also supports the reasoning content format returned via the <code>reasoning_content</code> parameter.</li> <li>Support for <code>video</code> type content_block: Fills the capability gap of <code>ChatOpenAI</code> regarding video type <code>content_block</code>.</li> <li>Automatic selection of optimal structured output method: Automatically selects the better solution between <code>function_calling</code> and <code>json_schema</code> based on actual provider support.</li> <li>Fine adaptation of differences via <code>compatibility_options</code>: Configure support differences for parameters like <code>tool_choice</code> and <code>response_format</code> as needed.</li> </ul> <p>Use the following code to create a chat model class:</p> <pre><code>from langchain_dev_utils.chat_models.adapters import create_openai_compatible_model\n\nChatVLLM = create_openai_compatible_model(\n    model_provider=\"vllm\",\n    base_url=\"http://localhost:8000/v1\",\n    chat_model_cls_name=\"ChatVLLM\",\n)\n\nmodel = ChatVLLM(model=\"qwen3-4b\")\nprint(model.invoke(\"Hello\"))\n</code></pre> <p>When creating the chat model class, the <code>base_url</code> parameter can be omitted. If not passed, the library will default to reading the corresponding environment variable, for example:</p> <pre><code>export VLLM_API_BASE=http://localhost:8000/v1\n</code></pre> <p>In this case, the code can omit <code>base_url</code>:</p> <pre><code>from langchain_dev_utils.chat_models.adapters import create_openai_compatible_model\n\nChatVLLM = create_openai_compatible_model(\n    model_provider=\"vllm\",\n    chat_model_cls_name=\"ChatVLLM\",\n)\n\nmodel = ChatVLLM(model=\"qwen3-4b\")\nprint(model.invoke(\"Hello\"))\n</code></pre> <p>Note: The prerequisite for the above code to run successfully is that the environment variable <code>VLLM_API_KEY</code> has been configured. Although vLLM itself does not require an API Key, the chat model class requires one during initialization. Therefore, please set this variable first, for example:</p> <pre><code>export VLLM_API_KEY=vllm_api_key\n</code></pre> <p>Hint</p> <p>The naming rules for environment variables of the created chat model class (embedding model classes follow this rule as well):</p> <ul> <li> <p>API Address: <code>${PROVIDER_NAME}_API_BASE</code> (uppercase, separated by underscores).</p> </li> <li> <p>API Key: <code>${PROVIDER_NAME}_API_KEY</code> (uppercase, separated by underscores).</p> </li> </ul>"},{"location":"adavance-guide/openai-compatible/#using-the-chat-model-class","title":"Using the Chat Model Class","text":""},{"location":"adavance-guide/openai-compatible/#basic-invocation","title":"Basic Invocation","text":"<p>You can perform basic invocation via the <code>invoke</code> method, which returns the model response.</p> <pre><code>from langchain_core.messages import HumanMessage\n\nmodel = ChatVLLM(model=\"qwen3-4b\")\nresponse = model.invoke([HumanMessage(\"Hello\")])\nprint(response)\n</code></pre> <p>It also supports <code>ainvoke</code> for asynchronous invocation:</p> <pre><code>from langchain_core.messages import HumanMessage\n\nmodel = ChatVLLM(model=\"qwen3-4b\")\nresponse = await model.ainvoke([HumanMessage(\"Hello\")])\nprint(response)\n</code></pre>"},{"location":"adavance-guide/openai-compatible/#streaming-invocation","title":"Streaming Invocation","text":"<p>You can perform streaming invocation via the <code>stream</code> method, used to stream the model response back.</p> <pre><code>from langchain_core.messages import HumanMessage\n\nmodel = ChatVLLM(model=\"qwen3-4b\")\nfor chunk in model.stream([HumanMessage(\"Hello\")]):\n    print(chunk)\n</code></pre> <p>And asynchronous streaming via <code>astream</code>:</p> <pre><code>from langchain_core.messages import HumanMessage\n\nmodel = ChatVLLM(model=\"qwen3-4b\")\nasync for chunk in model.astream([HumanMessage(\"Hello\")]):\n    print(chunk)\n</code></pre> Streaming Output Options <p>You can append token usage (<code>prompt_tokens</code> and <code>completion_tokens</code>) at the end of the streaming response via <code>stream_options={\"include_usage\": True}</code>. This library enables this option by default; if you need to disable it, you can pass the compatibility option <code>include_usage=False</code> when creating the model class or instantiating it.</p>"},{"location":"adavance-guide/openai-compatible/#tool-calling","title":"Tool Calling","text":"<p>If the model itself supports tool calling, you can use <code>bind_tools</code> directly for tool calling:</p> <pre><code>from langchain_core.messages import HumanMessage\nfrom langchain_core.tools import tool\nimport datetime\n\n@tool\ndef get_current_time() -&gt; str:\n    \"\"\"Get the current timestamp\"\"\"\n    return str(datetime.datetime.now().timestamp())\n\nmodel = ChatVLLM(model=\"qwen3-4b\").bind_tools([get_current_time])\nresponse = model.invoke([HumanMessage(\"Get the current timestamp\")])\nprint(response)\n</code></pre> Parallel Tool Calling <p>If the model supports parallel tool calling, you can pass <code>parallel_tool_calls=True</code> in <code>bind_tools</code> to enable parallel tool calling (some model providers enable it by default, so explicit passing is not required).</p> <p>For example:</p> <pre><code>from langchain_core.messages import HumanMessage\nfrom langchain_core.tools import tool\n\n\n@tool\ndef get_current_weather(location: str) -&gt; str:\n    \"\"\"Get the current weather\"\"\"\n    return f\"Current weather in {location} is sunny\"\n\nmodel = ChatVLLM(model=\"qwen3-4b\").bind_tools(\n    [get_current_weather], parallel_tool_calls=True\n)\nresponse = model.invoke([HumanMessage(\"Get the current weather in Los Angeles and London\")])\nprint(response)\n</code></pre> Forcing Tool Calling <p>Through the <code>tool_choice</code> parameter, you can control whether the model calls tools and which tool it calls during response, to improve accuracy, reliability, and controllability. Common values include:</p> <ul> <li><code>\"auto\"</code>: The model decides autonomously whether to call tools (default behavior);</li> <li><code>\"none\"</code>: Prohibit calling tools;</li> <li><code>\"required\"</code>: Force calling at least one tool;</li> <li>Specify a specific tool (specifically <code>{\"type\": \"function\", \"function\": {\"name\": \"xxx\"}}</code> in OpenAI compatible APIs).</li> </ul> <p>Different providers have different ranges of support for <code>tool_choice</code>. To resolve differences, this library introduces the compatibility configuration item <code>supported_tool_choice</code>, with a default value of <code>[\"auto\"]</code>. At this point, <code>tool_choice</code> passed in <code>bind_tools</code> can only be <code>auto</code>, and other values will be filtered out.</p> <p>To support passing other <code>tool_choice</code> values, the supported items must be configured. The configuration value is a list of strings, with optional values for each string:</p> <ul> <li><code>\"auto\"</code>, <code>\"none\"</code>, <code>\"required\"</code>: Corresponding standard strategies;</li> <li><code>\"specific\"</code>: Unique identifier for this library, indicating support for specifying a specific tool.</li> </ul> <p>For example, vLLM supports all strategies:</p> <pre><code>from langchain_dev_utils.chat_models.adapters import create_openai_compatible_model\n\nChatVLLM = create_openai_compatible_model(\n    model_provider=\"vllm\",\n    chat_model_cls_name=\"ChatVLLM\",\n    compatibility_options={\n        \"supported_tool_choice\": [\"auto\", \"required\", \"none\", \"specific\"]\n    },\n)\n\nmodel = ChatVLLM(model=\"qwen3-4b\").bind_tools(\n    [get_current_weather], tool_choice=\"required\"\n)\n</code></pre>"},{"location":"adavance-guide/openai-compatible/#structured-output","title":"Structured Output","text":"<pre><code>from langchain_core.messages import HumanMessage\nfrom pydantic import BaseModel\n\nclass User(BaseModel):\n    name: str\n    age: int\n\nmodel = ChatVLLM(model=\"qwen3-4b\").with_structured_output(User)\nresponse = model.invoke([HumanMessage(\"Hello, I am Tom, and I am 25 years old\")])\nprint(response)\n</code></pre> Default Structured Output Method <p>Currently, there are three common structured output methods: <code>json_schema</code>, <code>function_calling</code>, and <code>json_mode</code>. Among them, <code>json_schema</code> has the best effect, so this library's <code>with_structured_output</code> prioritizes <code>json_schema</code> as the structured output method; it only automatically downgrades to <code>function_calling</code> when the provider does not support it. Different model providers have varying degrees of support for structured output. This library uses the compatibility configuration item <code>supported_response_format</code> to declare the structured output methods supported by the provider. The default value is <code>[]</code>, indicating that neither <code>json_schema</code> nor <code>json_mode</code> is supported. At this point, <code>with_structured_output(method=...)</code> will fix the use of <code>function_calling</code>; even if <code>json_schema</code> / <code>json_mode</code> is passed in, it will automatically be converted to <code>function_calling</code>. If you want to use the corresponding structured output method, you need to explicitly pass the corresponding parameters (especially <code>json_schema</code>).</p> <p>For example, models deployed by vLLM support the <code>json_schema</code> structured output method, which can be declared during registration:</p> <pre><code>from langchain_dev_utils.chat_models.adapters import create_openai_compatible_model\n\nChatVLLM = create_openai_compatible_model(\n    model_provider=\"vllm\",\n    chat_model_cls_name=\"ChatVLLM\",\n    compatibility_options={\"supported_response_format\": [\"json_schema\"]},\n)\n\nmodel = ChatVLLM(model=\"qwen3-4b\")\n</code></pre> <p>Attention</p> <p><code>supported_response_format</code> currently only affects the <code>model.with_structured_output</code> method. For structured output in <code>create_agent</code>, if you need to use the <code>json_schema</code> implementation, you need to ensure that the corresponding model's <code>profile</code> contains the <code>structured_output</code> field with a value of <code>True</code>.</p>"},{"location":"adavance-guide/openai-compatible/#passing-extra-parameters","title":"Passing Extra Parameters","text":"<p>Since this class inherits from <code>BaseChatOpenAI</code>, it supports passing model parameters of <code>BaseChatOpenAI</code>, such as <code>temperature</code>, <code>extra_body</code>, etc.</p> <p>For example, use <code>extra_body</code> to pass extra parameters (here to disable thinking mode):</p> <pre><code>from langchain_core.messages import HumanMessage\n\nmodel = ChatVLLM(\n    model=\"qwen3-4b\",\n    extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},\n)\nresponse = model.invoke([HumanMessage(\"Hello\")])\nprint(response)\n</code></pre>"},{"location":"adavance-guide/openai-compatible/#passing-multimodal-data","title":"Passing Multimodal Data","text":"<p>Passing multimodal data is supported. You can use the OpenAI compatible multimodal data format or directly use <code>content_block</code> in LangChain.</p> <p>Passing image type data:</p> <p><pre><code>from langchain_core.messages import HumanMessage\nmessages = [\n    HumanMessage(\n        content_blocks=[\n            {\n                \"type\": \"image\",\n                \"url\": \"https://example.com/image.png\",\n            },\n            {\"type\": \"text\", \"text\": \"Describe this image\"},\n        ]\n    )\n]\n\nmodel = ChatVLLM(model=\"qwen3-vl-2b\")\nresponse = model.invoke(messages)\nprint(response)\n</code></pre> Passing video type data:</p> <pre><code>from langchain_core.messages import HumanMessage\n\nmessages = [\n    HumanMessage(\n        content_blocks=[\n            {\n                \"type\": \"video\",\n                \"url\": \"https://example.com/video.mp4\",\n            },\n            {\"type\": \"text\", \"text\": \"Describe this video\"},\n        ]\n    )\n]\n\nmodel = ChatVLLM(model=\"qwen3-vl-2b\")\nresponse = model.invoke(messages)\nprint(response)\n</code></pre>"},{"location":"adavance-guide/openai-compatible/#using-reasoning-models","title":"Using Reasoning Models","text":"<p>A key feature of the model classes created by this library is their enhanced compatibility with a wider range of reasoning models.</p> <p>For example:</p> <pre><code>from langchain_core.messages import HumanMessage\n\nmodel = ChatVLLM(model=\"qwen3-4b\")\nresponse = model.invoke(\"Why is the feathers of a parrot so vibrant?\")\nreasoning_steps = [b for b in response.content_blocks if b[\"type\"] == \"reasoning\"]\nprint(\" \".join(step[\"reasoning\"] for step in reasoning_steps))\n</code></pre> Support for Different Reasoning Modes <p>Reasoning modes vary across models, which is especially important in Agent development: some models require explicitly passing the <code>reasoning_content</code> field in the current call, while others do not. This library provides the <code>reasoning_keep_policy</code> compatibility configuration to adapt to these differences.</p> <p>This configuration item supports the following values:</p> <ul> <li> <p><code>never</code>: Do not retain any reasoning content in historical messages (default);</p> </li> <li> <p><code>current</code>: Retain only the current conversation's <code>reasoning_content</code> field;</p> </li> <li> <p><code>all</code>: Retain all conversations' <code>reasoning_content</code> field.</p> </li> </ul> <pre><code>graph LR\n    A[reasoning_content Retention Policy] --&gt; B{Value?};\n    B --&gt;|never| C[Do not include any&lt;br&gt;reasoning_content];\n    B --&gt;|current| D[Include only current conversation's&lt;br&gt;reasoning_content&lt;br&gt;Adapt to interleaved thinking mode];\n    B --&gt;|all| E[Include all conversations'&lt;br&gt;reasoning_content];\n    C --&gt; F[Send to model];\n    D --&gt; F;\n    E --&gt; F;</code></pre> <p>For example, a user first asks \"What's the weather in New York?\" and then follows up with \"What's the weather in London?\" The second round of dialogue is about to begin, and the final model call is imminent.</p> <ul> <li>When the value is <code>never</code></li> </ul> <p>The messages passed to the model will not include any <code>reasoning_content</code> field. The messages received by the model are:</p> <pre><code>messages = [\n    {\"content\": \"What's the weather in New York?\", \"role\": \"user\"},\n    {\"content\": \"\", \"role\": \"assistant\", \"tool_calls\": [...]},\n    {\"content\": \"Partly cloudy, 7\u201313\u00b0C\", \"role\": \"tool\", \"tool_call_id\": \"...\"},\n    {\"content\": \"It's partly cloudy in New York today, 7\u201313\u00b0C.\", \"role\": \"assistant\"},\n    {\"content\": \"What's the weather in London?\", \"role\": \"user\"},\n    {\"content\": \"\", \"role\": \"assistant\", \"tool_calls\": [...]},\n    {\"content\": \"Rainy, 14\u201320\u00b0C\", \"role\": \"tool\", \"tool_call_id\": \"...\"},\n]\n</code></pre> <ul> <li>When the value is <code>current</code></li> </ul> <p>Only retain the <code>reasoning_content</code> field of the current conversation. This policy suits Interleaved Thinking scenarios, where the model alternates between explicit reasoning and tool calls, so the reasoning content of the current round needs to be retained. The messages received by the model are: <pre><code>messages = [\n    {\"content\": \"What's the weather in New York?\", \"role\": \"user\"},\n    {\"content\": \"\", \"role\": \"assistant\", \"tool_calls\": [...]},\n    {\"content\": \"Partly cloudy, 7\u201313\u00b0C\", \"role\": \"tool\", \"tool_call_id\": \"...\"},\n    {\"content\": \"It's partly cloudy in New York today, 7\u201313\u00b0C.\", \"role\": \"assistant\"},\n    {\"content\": \"What's the weather in London?\", \"role\": \"user\"},\n    {\n        \"content\": \"\",\n        \"reasoning_content\": \"To check London's weather, I need to call the weather tool directly.\",  # Only retain current round's reasoning_content\n        \"role\": \"assistant\",\n        \"tool_calls\": [...],\n    },\n    {\"content\": \"Rainy, 14\u201320\u00b0C\", \"role\": \"tool\", \"tool_call_id\": \"...\"},\n]\n</code></pre></p> <ul> <li>When the value is <code>all</code></li> </ul> <p>Retain the <code>reasoning_content</code> field of all conversations. The messages received by the model are: <pre><code>messages = [\n    {\"content\": \"What's the weather in New York?\", \"role\": \"user\"},\n    {\n        \"content\": \"\",\n        \"reasoning_content\": \"To check New York's weather, I need to call the weather tool directly.\",  # Retain reasoning_content\n        \"role\": \"assistant\",\n        \"tool_calls\": [...],\n    },\n    {\"content\": \"Partly cloudy, 7\u201313\u00b0C\", \"role\": \"tool\", \"tool_call_id\": \"...\"},\n    {\n        \"content\": \"It's partly cloudy in New York today, 7\u201313\u00b0C.\",\n        \"reasoning_content\": \"Return the weather result for New York directly.\",  # Retain reasoning_content\n        \"role\": \"assistant\",\n    },\n    {\"content\": \"What's the weather in London?\", \"role\": \"user\"},\n    {\n        \"content\": \"\",\n        \"reasoning_content\": \"To check London's weather, I need to call the weather tool directly.\",  # Retain reasoning_content\n        \"role\": \"assistant\",\n        \"tool_calls\": [...],\n    },\n    {\"content\": \"Rainy, 14\u201320\u00b0C\", \"role\": \"tool\", \"tool_call_id\": \"...\"},\n]\n</code></pre></p> <p>Note: If the current round of conversation does not involve tool calls, <code>current</code> and <code>never</code> have the same effect.</p> <p>It is worth noting that although this parameter is part of the compatibility configuration, different models from the same provider\u2014and even the same model in different scenarios\u2014may require different <code>reasoning_content</code> retention policies. Therefore, it is recommended to specify it explicitly during instantiation, and there is no need to assign a value when creating the class.</p> <p>For example, with the GLM-4.7-Flash model: since it supports Interleaved Thinking, you typically need to set <code>reasoning_keep_policy</code> to <code>current</code> during instantiation so that only the current turn's <code>reasoning_content</code> is retained. For example:</p> <p><pre><code>from langchain_core.messages import HumanMessage\n\nmodel = ChatVLLM(model=\"glm-4.7-flash\", reasoning_keep_policy=\"current\")\nagent = create_agent(\n    model=model,\n    tools=[get_current_weather],\n)\nresponse = agent.invoke({\"messages\": [HumanMessage(content=\"What's the weather in New York?\")]})\nprint(response)\n</code></pre> GLM-4.7-Flash also supports another reasoning mode called Preserved Thinking. In this case, you need to retain all <code>reasoning_content</code> fields in historical messages, so you can set <code>reasoning_keep_policy</code> to <code>all</code>. For example:</p> <pre><code>from langchain_core.messages import HumanMessage\n\nmodel = ChatVLLM(\n    model=\"glm-4.7-flash\",\n    reasoning_keep_policy=\"all\",\n    extra_body={\"chat_template_kwargs\": {\"clear_thinking\": False}},\n)\n\nagent = create_agent(\n    model=model,\n    tools=[get_current_weather],\n)\nresponse = agent.invoke({\"messages\": [HumanMessage(content=\"What's the weather in New York?\")]})\nprint(response)\n</code></pre>"},{"location":"adavance-guide/openai-compatible/#model-profiles","title":"Model Profiles","text":"<p>You can get the model's profile via <code>model.profile</code>. By default, it returns an empty dictionary.</p> <p>You can also explicitly pass the <code>profile</code> parameter during instantiation to specify the model profile.</p> <p>For example: <pre><code>from langchain_core.messages import HumanMessage\n\ncustom_profile = {\n    \"max_input_tokens\": 100_000,\n    \"tool_calling\": True,\n    \"structured_output\": True,\n    # ...\n}\nmodel = ChatVLLM(model=\"qwen3-4b\", profile=custom_profile)\nprint(model.profile)\n</code></pre> Or directly pass the <code>profile</code> parameter for all models of the model provider when creating.</p> <p>For example: <pre><code>from langchain_dev_utils.chat_models.adapters import create_openai_compatible_model\n\nmodel_profiles = {\n    \"qwen3-4b\": {\n        \"max_input_tokens\": 131072,\n        \"max_output_tokens\": 8192,\n        \"image_inputs\": False,\n        \"audio_inputs\": False,\n        \"video_inputs\": False,\n        \"image_outputs\": False,\n        \"audio_outputs\": False,\n        \"video_outputs\": False,\n        \"reasoning_output\": True,\n        \"tool_calling\": True,\n    }\n    # More model profiles can be written here\n}\n\nChatVLLM = create_openai_compatible_model(\n    model_provider=\"vllm\",\n    base_url=\"http://localhost:8000/v1\",\n    model_profiles=model_profiles,\n)\n\nmodel = ChatVLLM(\n    model=\"qwen3-4b\",\n)\nprint(model.profile)\n</code></pre></p>"},{"location":"adavance-guide/openai-compatible/#support-for-openais-latest-responses-api","title":"Support for OpenAI's Latest Responses API","text":"<p>This model class also supports OpenAI's latest <code>responses</code> API (parameter name <code>use_responses_api</code>). Currently, only a few providers support this style of interface; if your provider supports it, you can enable it via <code>use_responses_api=True</code>.</p> <p>For example, if vLLM supports the <code>responses</code> API, you can use it like this:</p> <pre><code>from langchain_core.messages import HumanMessage\n\nmodel = ChatVLLM(model=\"qwen3-4b\", use_responses_api=True)\nresponse = model.invoke([HumanMessage(content=\"Hello\")])\nprint(response)\n</code></pre> <p>Attention</p> <p>This feature is not yet guaranteed to be fully supported. It can be used for simple testing but do not use it in a production environment.</p> <p>Attention</p> <p>This library cannot currently guarantee 100% compatibility with all OpenAI compatible interfaces (although compatibility configurations can be used to improve compatibility). If the model provider already has an official or community integration class, please prioritize that integration class. If you encounter any compatibility issues, feel free to submit an issue in this library's GitHub repository.</p>"},{"location":"adavance-guide/openai-compatible/#creation-and-usage-of-embedding-models","title":"Creation and Usage of Embedding Models","text":""},{"location":"adavance-guide/openai-compatible/#creating-an-embedding-model-class","title":"Creating an Embedding Model Class","text":"<p>Similar to the chat model class, you can use <code>create_openai_compatible_embedding</code> to create an embedding model integration class. This function accepts the following parameters:</p> Parameter Description <code>embedding_provider</code> Embedding model provider name, e.g., <code>vllm</code>. Must start with a letter or number, can only contain letters, numbers, and underscores, and must be no more than 20 characters long.Type: <code>str</code>Required: Yes <code>base_url</code> Default API address for the model provider.Type: <code>str</code>Required: No <code>embedding_model_cls_name</code> Embedding model class name (must conform to Python class naming conventions). Default is <code>{Provider}Embeddings</code> (where <code>{Provider}</code> is the provider name capitalized).Type: <code>str</code>Required: No <p>Similarly, we use <code>create_openai_compatible_embedding</code> to integrate vLLM's embedding model.</p> <pre><code>from langchain_dev_utils.embeddings.adapters import create_openai_compatible_embedding\n\nVLLMEmbeddings = create_openai_compatible_embedding(\n    embedding_provider=\"vllm\",\n    base_url=\"http://localhost:8000/v1\",\n    embedding_model_cls_name=\"VLLMEmbeddings\",\n)\n\nembedding = VLLMEmbeddings(model=\"qwen3-embedding-4b\")\nprint(embedding.embed_query(\"Hello\"))\n</code></pre> <p><code>base_url</code> can also be omitted. If not passed, the library will default to reading the environment variable <code>VLLM_API_BASE</code>:</p> <pre><code>export VLLM_API_BASE=\"http://localhost:8000/v1\"\n</code></pre> <p>In this case, the code can omit <code>base_url</code>:</p> <pre><code>from langchain_dev_utils.embeddings.adapters import create_openai_compatible_embedding\n\nVLLMEmbeddings = create_openai_compatible_embedding(\n    embedding_provider=\"vllm\",\n    embedding_model_cls_name=\"VLLMEmbeddings\",\n)\n\nembedding = VLLMEmbeddings(model=\"qwen3-embedding-4b\")\nprint(embedding.embed_query(\"Hello\"))\n</code></pre> <p>Note: The prerequisite for the above code to run successfully is that the environment variable <code>VLLM_API_KEY</code> has been configured. Although vLLM itself does not require an API Key, the embedding model class requires one during initialization. Therefore, please set this variable first, for example:</p> <pre><code>export VLLM_API_KEY=vllm_api_key\n</code></pre>"},{"location":"adavance-guide/openai-compatible/#using-the-embedding-model-class","title":"Using the Embedding Model Class","text":"<p>Here, use the previously created <code>VLLMEmbeddings</code> class to initialize an embedding model instance.</p>"},{"location":"adavance-guide/openai-compatible/#vectorizing-query","title":"Vectorizing Query","text":"<pre><code>embedding = VLLMEmbeddings(model=\"qwen3-embedding-4b\")\nprint(embedding.embed_query(\"Hello\"))\n</code></pre> <p>Similarly, it supports asynchronous calls:</p> <pre><code>embedding = VLLMEmbeddings(model=\"qwen3-embedding-4b\")\nres = await embedding.aembed_query(\"Hello\")\nprint(res)\n</code></pre>"},{"location":"adavance-guide/openai-compatible/#vectorizing-string-list","title":"Vectorizing String List","text":"<p><pre><code>documents = [\"Hello\", \"Hello, I am Tom\"]\nembedding = VLLMEmbeddings(model=\"qwen3-embedding-4b\")\nprint(embedding.embed_documents(documents))\n</code></pre> Similarly, it supports asynchronous calls:</p> <pre><code>documents = [\"Hello\", \"Hello, I am Tom\"]\nembedding = VLLMEmbeddings(model=\"qwen3-embedding-4b\")\nres = await embedding.aembed_documents(documents)\nprint(res)\n</code></pre> <p>Embedding Model Compatibility Note</p> <p>Embedding APIs compatible with OpenAI generally exhibit good compatibility, but you should still pay attention to the following differences:</p> <ol> <li> <p><code>check_embedding_ctx_length</code>: Set to <code>True</code> only when using the official OpenAI embedding service; for all other embedding models, set to <code>False</code>.</p> </li> <li> <p><code>dimensions</code>: If the model supports custom dimensions (e.g., 1024, 4096), you can pass this parameter directly.</p> </li> <li> <p><code>chunk_size</code>: The maximum number of texts that can be processed in a single API call. For example, if <code>chunk_size</code> is 10, you can send up to 10 texts in one request for embedding.</p> </li> <li> <p>Single text token limit: Cannot be controlled via parameters; you need to ensure it yourself during the pre-processing chunking stage.</p> </li> </ol> <p>Note: The chat model class and embedding model class created using this feature both support passing parameters from <code>BaseChatOpenAI</code> and <code>OpenAIEmbeddings</code>, such as <code>temperature</code>, <code>extra_body</code>, <code>dimensions</code>, etc.</p> <p>Attention</p> <p>Similar to model management, the two functions mentioned above use <code>pydantic.create_model</code> underneath to create model classes, which incurs some performance overhead. Additionally, <code>create_openai_compatible_model</code> uses a global dictionary to record the <code>profiles</code> of each model provider. To avoid multi-threading concurrency issues, it is recommended to create the integration classes during the project startup phase and avoid dynamic creation later.</p>"},{"location":"adavance-guide/openai-compatible/#integration-with-model-management-feature","title":"Integration with Model Management Feature","text":"<p>This library has seamlessly integrated this feature into the model management functionality. When registering a chat model, just set <code>chat_model</code> to <code>\"openai-compatible\"</code>; when registering an embedding model, set <code>embeddings_model</code> to <code>\"openai-compatible\"</code>.</p>"},{"location":"adavance-guide/openai-compatible/#chat-model-class-registration","title":"Chat Model Class Registration","text":"<p>Specific code is as follows:</p> <p>Method 1: Explicit Parameters</p> <pre><code>from langchain_dev_utils.chat_models import register_model_provider\n\nregister_model_provider(\n    provider_name=\"vllm\",\n    chat_model=\"openai-compatible\",\n    base_url=\"http://localhost:8000/v1\"\n)\n</code></pre> <p>Method 2: Via Environment Variables (Recommended for Configuration Management)</p> <pre><code>from langchain_dev_utils.chat_models import register_model_provider\n\nregister_model_provider(\n    provider_name=\"vllm\",\n    chat_model=\"openai-compatible\"\n    # Automatically reads VLLM_API_BASE\n)\n</code></pre> <p>At the same time, the parameters <code>base_url</code>, <code>compatibility_options</code>, and <code>model_profiles</code> in the <code>create_openai_compatible_model</code> function also support being passed in. You just need to pass the corresponding parameters in the <code>register_model_provider</code> function.</p> <p>For example:</p> <pre><code>from langchain_dev_utils.chat_models import register_model_provider\n\nregister_model_provider(\n    provider_name=\"vllm\",\n    chat_model=\"openai-compatible\",\n    base_url=\"http://localhost:8000/v1\",\n    compatibility_options={\n        \"supported_tool_choice\": [\"auto\", \"none\", \"required\", \"specific\"],\n        \"supported_response_format\": [\"json_schema\"]\n    },\n    model_profiles=model_profiles,\n)\n</code></pre>"},{"location":"adavance-guide/openai-compatible/#embedding-model-class-registration","title":"Embedding Model Class Registration","text":"<p>Similar to chat model class registration:</p> <p>Method 1: Explicit Parameters</p> <pre><code>from langchain_dev_utils.embeddings import register_embeddings_provider\n\nregister_embeddings_provider(\n    provider_name=\"vllm\",\n    embeddings_model=\"openai-compatible\",\n    base_url=\"http://localhost:8000/v1\",\n)\n</code></pre> <p>Method 2: Environment Variables (Recommended)</p> <pre><code>export VLLM_API_BASE=http://localhost:8000/v1\n</code></pre> <pre><code>from langchain_dev_utils.embeddings import register_embeddings_provider\n\nregister_embeddings_provider(\n    provider_name=\"vllm\",\n    embeddings_model=\"openai-compatible\"\n)\n</code></pre> <p>Best Practice</p> <p>When integrating an OpenAI compatible API, you can directly use <code>ChatOpenAI</code> or <code>OpenAIEmbeddings</code> from <code>langchain-openai</code> and point the <code>base_url</code> and <code>api_key</code> to your provider's service. This method is simple enough and suitable for relatively simple scenarios (especially when using ordinary chat models rather than reasoning models).</p> <p>However, the following issues exist:</p> <ol> <li> <p>Cannot display the chain of thought of non-OpenAI official reasoning models (i.e., content returned by <code>reasoning_content</code>).</p> </li> <li> <p>Does not support <code>video</code> type content_block.</p> </li> <li> <p>The default strategy coverage for structured output is relatively low.</p> </li> </ol> <p>When you encounter the above differences, you can use the OpenAI compatible integration class provided by this library for adaptation. For embedding models, compatibility is generally better: in most cases, using <code>OpenAIEmbeddings</code> directly and setting <code>check_embedding_ctx_length=False</code> is sufficient.</p>"},{"location":"adavance-guide/pipeline/","title":"State Graph Orchestration","text":""},{"location":"adavance-guide/pipeline/#overview","title":"Overview","text":"<p>In LangGraph, state graph orchestration is a key capability for building complex AI applications. By combining multiple state graphs in specific patterns, you can form powerful and logically clear workflows.</p> <p>This library provides two of the most common orchestration methods:</p> Orchestration Method Functional Description Applicable Scenarios Sequential Orchestration Combines multiple state graphs in sequence to form a sequential workflow Tasks need to be executed step-by-step and depend on the output of the previous step Parallel Orchestration Combines multiple state graphs in parallel to form a parallel workflow Multiple tasks are independent of each other and can be executed simultaneously to improve efficiency"},{"location":"adavance-guide/pipeline/#sequential-orchestration","title":"Sequential Orchestration","text":"<p>Sequential Orchestration (Sequential Pipeline) breaks down complex tasks into continuous, ordered sub-tasks and assigns them to different specialized agents to process in turn.</p> <p>Use <code>create_sequential_pipeline</code> to combine multiple state graphs in a sequential manner.</p>"},{"location":"adavance-guide/pipeline/#typical-application-scenarios","title":"Typical Application Scenarios","text":"<p>Taking a user purchasing a product as an example, the typical process is as follows:</p> <pre><code>graph LR\n    Start([User Order Request])\n    Inv[Inventory Confirmation]\n    Ord[Create Order]\n    Pay[Complete Payment]\n    Del[Confirm Shipment]\n    End([Order Complete])\n\n    Start --&gt; Inv --&gt; Ord --&gt; Pay --&gt; Del --&gt; End</code></pre> <p>This process is interlinked and the order cannot be reversed.</p> <p>The four steps\u2014Inventory Confirmation, Create Order, Complete Payment, and Confirm Shipment\u2014are handled by specialized agents respectively. By orchestrating these four state graphs sequentially using <code>create_sequential_pipeline</code>, you can form a highly automated and clearly defined product purchasing workflow.</p>"},{"location":"adavance-guide/pipeline/#basic-example","title":"Basic Example","text":"<p>The following example shows how to build a product purchasing sequential workflow using <code>create_sequential_pipeline</code>:</p> <p>First, create the chat model object. Here, we use <code>qwen3-4b</code> deployed locally via vllm as an example. Since its interface is compatible with OpenAI, we can directly use <code>create_openai_compatible_model</code> to construct the model class.</p> <p><pre><code>from langchain_dev_utils.chat_models.adapters import create_openai_compatible_model\n\nChatVLLM = create_openai_compatible_model(\n    model_provider=\"vllm\",\n    base_url=\"http://localhost:8000/v1\",\n    chat_model_cls_name=\"ChatVLLM\",\n)\n</code></pre> Then instantiate a <code>ChatVLLM</code> object for subsequent agents to call.</p> <p><pre><code>model = ChatVLLM(model=\"qwen3-4b\")\n</code></pre> Next, create the relevant tools, such as checking inventory, creating orders, making payments, etc.</p> Reference for tool implementation <pre><code>from langchain_core.tools import tool\n\n@tool\ndef check_inventory(product_name: str) -&gt; dict:\n    \"\"\"Check inventory\"\"\"\n    return {\"product_name\": product_name, \"in_stock\": True, \"available\": 42}\n\n@tool\ndef create_order(product_name: str, quantity: int) -&gt; str:\n    \"\"\"Create an order\"\"\"\n    return f\"Order ORD-10001 created, product: {product_name}, quantity: {quantity}.\"\n\n@tool\ndef pay_order(order_id: str) -&gt; str:\n    \"\"\"Pay for the order\"\"\"\n    return f\"Order {order_id} paid successfully.\"\n\n@tool\ndef confirm_delivery(order_id: str, address: str) -&gt; str:\n    \"\"\"Confirm shipment\"\"\"\n    return f\"Order {order_id} arranged for shipment, shipping address: {address}.\"\n</code></pre> <p>Then create the corresponding four sub-agents.</p> <pre><code>from langchain.agents import create_agent\n\ninventory_agent = create_agent(\n    model=model,\n    tools=[check_inventory],\n    system_prompt=\"You are an inventory assistant responsible for confirming if the product is in stock.\",\n    name=\"inventory_agent\",\n)\n\norder_agent = create_agent(\n    model=model,\n    tools=[create_order],\n    system_prompt=\"You are an order assistant responsible for creating orders.\",\n    name=\"order_agent\",\n)\n\npayment_agent = create_agent(\n    model=model,\n    tools=[pay_order],\n    system_prompt=\"You are a payment assistant responsible for completing payments.\",\n    name=\"payment_agent\",\n)\n\ndelivery_agent = create_agent(\n    model=model,\n    tools=[confirm_delivery],\n    system_prompt=\"You are a delivery assistant responsible for confirming shipment information.\",\n    name=\"delivery_agent\",\n)\n</code></pre> <p>Finally, use <code>create_sequential_pipeline</code> to orchestrate these four agents in sequence.</p> <p><pre><code>from langchain_dev_utils.pipeline import create_sequential_pipeline\nfrom langchain.agents import AgentState\n\ngraph = create_sequential_pipeline(\n    sub_graphs=[\n        inventory_agent,\n        order_agent,\n        payment_agent,\n        delivery_agent,\n    ],\n    state_schema=AgentState,\n)\n</code></pre> Run the test:</p> <pre><code>response = graph.invoke(\n    {\n        \"messages\": [\n            HumanMessage(\"I want to buy a pair of wireless headphones, quantity 2, please place the order, shipping address: X District, X Road, X City\")\n        ]\n    }\n)\nprint(response)\n</code></pre>"},{"location":"adavance-guide/pipeline/#context-engineering-optimization","title":"Context Engineering Optimization","text":"<p>This example passes the complete context of all previous agents to the current agent in sequence, which may cause context bloat and affect performance and effectiveness.</p> <p>The following solutions can be used to streamline the context:</p> Solution Description Pros Use Middleware Use <code>create_agent</code> with middleware to extract and pass only necessary information Simple to implement, minimal code changes Custom State Graph Build a custom state graph based on <code>LangGraph</code>, explicitly controlling state fields and message flow High flexibility, precise control Click to view reference code for solving via middleware <pre><code>from typing import Any\n\nfrom langchain.agents.middleware import AgentMiddleware\nfrom langchain_core.messages import RemoveMessage\nfrom langgraph.runtime import Runtime\n\nfrom langchain_dev_utils.agents.middleware import format_prompt\n\n\nclass PurchaseState(AgentState, total=False):\n    stock: str\n    order: str\n    payment: str\n    delivery: str\n\n\nclass ClearAgentContextMiddleware(AgentMiddleware):\n    state_schema = PurchaseState\n\n    def __init__(self, result_save_key: str) -&gt; None:\n        super().__init__()\n        self.result_save_key = result_save_key\n\n    def after_agent(\n        self, state: PurchaseState, runtime: Runtime\n    ) -&gt; dict[str, Any] | None:\n        final_message = state[\"messages\"][-1]\n        update_key = self.result_save_key\n        return {\n            \"messages\": [\n                RemoveMessage(id=msg.id or \"\") for msg in state[\"messages\"][1:]\n            ],\n            update_key: final_message.content,\n        }\n\n\ninventory_agent = create_agent(\n    model=model,\n    tools=[check_inventory],\n    system_prompt=\"You are an inventory assistant responsible for confirming if the product is in stock. Please output the inventory check result.\",\n    name=\"inventory_agent\",\n    state_schema=PurchaseState,\n    middleware=[format_prompt, ClearAgentContextMiddleware(\"stock\")],\n)\n\norder_agent = create_agent(\n    model=model,\n    tools=[create_order],\n    system_prompt=(\n        \"You are an order assistant responsible for creating orders.\\n\"\n        \"Inventory result: {stock}\\n\"\n        \"Please create an order based on the inventory result and output the order number.\"\n    ),\n    name=\"order_agent\",\n    state_schema=PurchaseState,\n    middleware=[format_prompt, ClearAgentContextMiddleware(\"order\")],\n)\n\npayment_agent = create_agent(\n    model=model,\n    tools=[pay_order],\n    system_prompt=(\n        \"You are a payment assistant responsible for completing payments.\\n\"\n        \"Order result: {order}\\n\"\n        \"Please retrieve the order number from the order result and complete the payment.\"\n    ),\n    name=\"payment_agent\",\n    state_schema=PurchaseState,\n    middleware=[format_prompt, ClearAgentContextMiddleware(\"payment\")],\n)\n\ndelivery_agent = create_agent(\n    model=model,\n    tools=[confirm_delivery],\n    system_prompt=(\n        \"You are a delivery assistant responsible for confirming shipment information.\\n\"\n        \"Order result: {order}\\n\"\n        \"Payment result: {payment}\\n\"\n        \"Please confirm shipment and repeat the shipping address.\"\n    ),\n    name=\"delivery_agent\",\n    state_schema=PurchaseState,\n    middleware=[format_prompt, ClearAgentContextMiddleware(\"delivery\")],\n)\n\ngraph = create_sequential_pipeline(\n    sub_graphs=[\n        inventory_agent,\n        order_agent,\n        payment_agent,\n        delivery_agent,\n    ],\n    state_schema=PurchaseState,\n)\n\nresponse = graph.invoke(\n    {\n        \"messages\": [\n            HumanMessage(\"I want to buy a pair of wireless headphones, quantity 2, please place the order, shipping address: X District, X Road, X City\")\n        ]\n    }\n)\nprint(response)\n</code></pre> <p>Implementation Notes:</p> <ol> <li> <p>Extend State Schema: Add four fields\u2014<code>stock</code>, <code>order</code>, <code>payment</code>, <code>delivery</code>\u2014to the agent's State Schema to store the final output of each agent.</p> </li> <li> <p>Custom Middleware: Create the <code>ClearAgentContextMiddleware</code> middleware. After each agent finishes, it uses <code>RemoveMessage</code> to clear the context and then writes the final result (<code>final_message.content</code>) to the corresponding field.</p> </li> <li> <p>Dynamic Prompt Formatting: Use the <code>format_prompt</code> middleware to stitch previous outputs into the <code>system_prompt</code> at runtime as needed.</p> </li> </ol> <p>Hint</p> <p>For sequentially combined graphs, LangGraph's <code>StateGraph</code> provides <code>add_sequence</code> as a shorthand, which is more suitable for scenarios where nodes are functions (rather than sub-graphs).</p> <pre><code>graph = StateGraph(AgentState)\ngraph.add_sequence([(\"graph1\", graph1), (\"graph2\", graph2), (\"graph3\", graph3)])\ngraph.add_edge(\"__start__\", \"graph1\")\ngraph = graph.compile()\n</code></pre> <p>However, the above method is still somewhat cumbersome. It is recommended to use <code>create_sequential_pipeline</code>, which allows you to quickly build a sequentially executing graph with a single line of code.</p>"},{"location":"adavance-guide/pipeline/#parallel-orchestration","title":"Parallel Orchestration","text":"<p>Parallel Orchestration (Parallel Pipeline) combines multiple state graphs in parallel, executing tasks concurrently to improve efficiency.</p> <p>Use <code>create_parallel_pipeline</code> to combine multiple state graphs in a parallel manner to achieve parallel execution.</p>"},{"location":"adavance-guide/pipeline/#typical-application-scenarios_1","title":"Typical Application Scenarios","text":"<p>In a product purchasing scenario, a user might need multiple queries simultaneously, such as product details, inventory, promotions, and shipping cost estimation, which can be executed in parallel.</p> <p>The process is as follows:</p> <pre><code>graph LR\n    Start([User Request])\n\n    subgraph Parallel [Parallel Execution]\n        direction TB\n        Prod[Product Detail Query]\n        Inv[Inventory Query]\n        Prom[Promotion Calculation]\n        Ship[Shipping Estimation]\n    end\n\n    End([Aggregate Results])\n\n    Start --&gt; Prod\n    Start --&gt; Inv\n    Start --&gt; Prom\n    Start --&gt; Ship\n\n    Prod --&gt; End\n    Inv --&gt; End\n    Prom --&gt; End\n    Ship --&gt; End</code></pre>"},{"location":"adavance-guide/pipeline/#basic-example_1","title":"Basic Example","text":"<p>First, create a few tools.</p> Reference for tool implementation <pre><code>@tool\ndef get_product_detail(product_name: str) -&gt; dict:\n    \"\"\"Query product details\"\"\"\n    return {\n        \"product_name\": product_name,\n        \"sku\": \"SKU-10001\",\n        \"price\": 299,\n        \"highlights\": [\"Active Noise Cancellation\", \"Bluetooth 5.3\", \"30-hour Battery\"],\n    }\n\n@tool\ndef check_inventory(product_name: str) -&gt; dict:\n    \"\"\"Query inventory\"\"\"\n    return {\"product_name\": product_name, \"in_stock\": True, \"available\": 42}\n\n@tool\ndef calculate_promotions(product_name: str, quantity: int) -&gt; dict:\n    \"\"\"Calculate promotions\"\"\"\n    return {\n        \"product_name\": product_name,\n        \"quantity\": quantity,\n        \"discounts\": [\"30 off 300\", \"Member 5% off\"],\n        \"estimated_discount\": 45,\n    }\n\n@tool\ndef estimate_shipping(address: str) -&gt; dict:\n    \"\"\"Estimate shipping cost and time\"\"\"\n    return {\n        \"address\": address,\n        \"fee\": 12,\n        \"eta_days\": 2,\n    }\n</code></pre> <p>And the corresponding sub-agents:</p> <pre><code>product_agent = create_agent(\n    model,\n    tools=[get_product_detail],\n    system_prompt=\"You are a product assistant responsible for parsing user needs and querying product details.\",\n    name=\"product_agent\",\n    state_schema=AgentState,\n)\n\ninventory_agent = create_agent(\n    model,\n    tools=[check_inventory],\n    system_prompt=\"You are an inventory assistant responsible for checking inventory based on SKU.\",\n    name=\"inventory_agent\",\n    state_schema=AgentState,\n)\n\npromotion_agent = create_agent(\n    model,\n    tools=[calculate_promotions],\n    system_prompt=\"You are a promotion assistant responsible for calculating current available promotions and estimated discounts.\",\n    name=\"promotion_agent\",\n    state_schema=AgentState,\n)\n\nshipping_agent = create_agent(\n    model,\n    tools=[estimate_shipping],\n    system_prompt=\"You are a delivery assistant responsible for estimating shipping costs and time.\",\n    name=\"shipping_agent\",\n    state_schema=AgentState,\n)\n</code></pre> <p>Combine the sub-agents using <code>create_parallel_pipeline</code>.</p> <p><pre><code>from langchain_dev_utils.pipeline import create_parallel_pipeline\n\ngraph = create_parallel_pipeline(\n    sub_graphs=[\n        product_agent,\n        inventory_agent,\n        promotion_agent,\n        shipping_agent,\n    ],\n    state_schema=AgentState,\n)\n</code></pre> Run the test:</p> <pre><code>response = graph.invoke(\n    {\"messages\": [HumanMessage(\"I want to buy a pair of wireless headphones, quantity 2, shipping address: X District, X Road, X City\")]}\n)\nprint(response)\n</code></pre>"},{"location":"adavance-guide/pipeline/#using-branch-functions-to-specify-sub-graphs-for-parallel-execution","title":"Using Branch Functions to Specify Sub-graphs for Parallel Execution","text":"<p>In most scenarios, you may not want all sub-graphs to execute in parallel, but rather execute a subset conditionally. In this case, you need to use <code>branches_fn</code> to specify a branch function. The branch function must return a list of <code>Send</code>, where each <code>Send</code> contains the sub-graph name and the input.</p>"},{"location":"adavance-guide/pipeline/#application-scenarios","title":"Application Scenarios","text":"<p><code>Router</code> is a typical architecture for multi-agent systems: a routing model analyzes user requests and decomposes tasks, then distributes them to several business agents for execution. In an order query scenario, users might care about order status, product information, or refunds simultaneously. The routing model can assign requests to order, product, or refund agents accordingly.</p> <p>First, write the tools.</p> Reference for tool implementation <pre><code>@tool\ndef list_orders() -&gt; dict:\n    \"\"\"Query user order list\"\"\"\n    return {\n        \"orders\": [\n            {\n                \"order_id\": \"ORD-20250101-0001\",\n                \"status\": \"Shipped\",\n                \"items\": [{\"product_name\": \"Wireless Headphones\", \"qty\": 1}],\n                \"created_at\": \"2025-01-01 10:02:11\",\n            },\n            {\n                \"order_id\": \"ORD-20241215-0234\",\n                \"status\": \"Completed\",\n                \"items\": [{\"product_name\": \"Mechanical Keyboard\", \"qty\": 1}],\n                \"created_at\": \"2024-12-15 21:18:03\",\n            },\n        ],\n    }\n\n@tool\ndef get_order_detail(order_id: str) -&gt; dict:\n    \"\"\"Query order details\"\"\"\n    return {\n        \"status\": \"Shipped\",\n        \"receiver\": {\"name\": \"Zhang San\", \"phone\": \"138****0000\"},\n        \"items\": [\n            {\n                \"product_id\": \"P-10001\",\n                \"product_name\": \"Wireless Headphones\",\n                \"qty\": 1,\n                \"price\": 299,\n            }\n        ],\n    }\n\n@tool\ndef get_shipping_trace(tracking_no: str) -&gt; dict:\n    \"\"\"Query shipping trajectory\"\"\"\n    return {\n        \"events\": [\n            {\"time\": \"2025-01-02 09:10\", \"status\": \"Package Picked Up\"},\n            {\"time\": \"2025-01-02 18:45\", \"status\": \"In Transit\"},\n            {\"time\": \"2025-01-03 11:20\", \"status\": \"Arrived at Delivery Station\"},\n        ],\n    }\n\n@tool\ndef search_products(query: str) -&gt; dict:\n    \"\"\"Search products\"\"\"\n    return {\n        \"results\": [\n            {\n                \"product_id\": \"P-10001\",\n                \"name\": \"Wireless Headphones Pro\",\n                \"price\": 299,\n                \"highlights\": [\"ANC\", \"Bluetooth 5.3\", \"30h Battery\"],\n            },\n            {\n                \"product_id\": \"P-10002\",\n                \"name\": \"Wireless Headphones Lite\",\n                \"price\": 199,\n                \"highlights\": [\"Lightweight\", \"Low Latency\", \"24h Battery\"],\n            },\n        ],\n    }\n\n@tool\ndef get_product_detail(product_id: str) -&gt; dict:\n    \"\"\"Query product details\"\"\"\n    return {\n        \"product_id\": product_id,\n        \"name\": \"Wireless Headphones Pro\",\n        \"price\": 299,\n        \"specs\": {\"color\": [\"Black\", \"White\"], \"warranty_months\": 12},\n        \"description\": \"True wireless headphones featuring noise cancellation and long battery life.\",\n    }\n\n\n@tool\ndef check_inventory(product_name: str) -&gt; dict:\n    \"\"\"Query inventory\"\"\"\n    return {\"product_name\": product_name, \"in_stock\": True, \"available\": 42}\n\n@tool\ndef create_refund(order_id: str, reason: str) -&gt; dict:\n    \"\"\"Initiate refund\"\"\"\n    return {\n        \"refund_id\": \"RFD-20250103-0009\",\n        \"status\": \"Submitted\",\n        \"reason\": reason,\n        \"estimated_days\": 3,\n    }\n\n@tool\ndef get_refund_status(refund_id: str) -&gt; dict:\n    \"\"\"Query refund status\"\"\"\n    return {\n        \"refund_id\": refund_id,\n        \"status\": \"Processing\",\n        \"progress\": [\n            {\"time\": \"2025-01-03 12:05\", \"status\": \"Submitted\"},\n            {\"time\": \"2025-01-03 12:20\", \"status\": \"CS Reviewing\"},\n        ],\n        \"estimated_days\": 2,\n    }\n\n@tool\ndef refund_policy() -&gt; dict:\n    \"\"\"View refund policy\"\"\"\n    return {\n        \"window_days\": 7,\n        \"requirements\": [\"Product in good condition\", \"Accessories complete\", \"Provide order number\"],\n        \"notes\": [\"Some promotional items do not support no-reason refunds\", \"Refund time depends on payment channel\"],\n    }\n</code></pre> <p>Then create the corresponding sub-agents.</p> <pre><code>ORDER_AGENT_PROMPT = (\n    \"You are an order management assistant.\\n\"\n    \"You can use tools to query order lists, order details, and shipping trajectories.\\n\"\n    \"Prioritize using tools to get information, then draw conclusions based on tool results.\\n\"\n    \"Output requirements: Answer in Chinese, clear structure, list order information in bullet points if necessary.\\n\"\n)\n\norder_agent = create_agent(\n    model,\n    system_prompt=ORDER_AGENT_PROMPT,\n    tools=[list_orders, get_order_detail, get_shipping_trace],\n    name=\"order_agent\",\n)\n\n\nPRODUCT_AGENT_PROMPT = (\n    \"You are a product management assistant.\\n\"\n    \"You can use tools to search products, view product details, and check inventory.\\n\"\n    \"Prioritize using tools to get information, then provide suggestions based on tool results.\\n\"\n    \"When user needs are unclear, ask a clarifying question first (e.g., category/budget/usage).\\n\"\n    \"Output requirements: Answer in Chinese, give actionable next steps.\\n\"\n)\n\n\nproduct_agent = create_agent(\n    model,\n    system_prompt=PRODUCT_AGENT_PROMPT,\n    tools=[search_products, get_product_detail, check_inventory],\n    name=\"product_agent\",\n)\n\n\nREFUND_AGENT_PROMPT = (\n    \"You are a refund management assistant.\\n\"\n    \"You can use tools to initiate refunds, query refund status, and view refund policies.\\n\"\n    \"Prioritize using tools to get information; if the user is missing key fields (e.g., order number), ask for it first.\\n\"\n    \"Output requirements: Answer in Chinese, clearly state refund progress/required materials/estimated time.\\n\"\n)\n\n\nrefund_agent = create_agent(\n    model,\n    system_prompt=REFUND_AGENT_PROMPT,\n    tools=[create_refund, get_refund_status, refund_policy],\n    name=\"refund_agent\",\n)\n</code></pre> <p>Next, write the branch function: The routing model returns the name of the agent to execute and the corresponding task description based on the request.</p> <pre><code>from typing import Literal, cast\nfrom typing_extensions import TypedDict\n\n\nfrom langchain_core.messages import SystemMessage\nfrom langgraph.types import Send\nfrom pydantic import BaseModel, Field\n\n\nclass RouterInput(TypedDict):\n    query: str\n\n\nclass RouterState(AgentState):\n    query: str\n\n\nROUTER_SYSTEM_PROMPT = (\n    \"You are a Router model, only responsible for splitting user questions and distributing them to appropriate business sub-agents.\\n\"\n    \"Available business domains are only: order (orders), product (products), refund (refunds).\\n\"\n    \"You must output a classifications list (used to call multiple sub-agents in parallel).\\n\"\n    \"Rules:\\n\"\n    \"1) source must be one of the three above;\\n\"\n    \"2) query must be a task description for that sub-agent that can be directly executed;\\n\"\n    \"3) If one sentence from the user involves multiple business domains (e.g., 'check order' + 'see product' + 'ask refund'), it must be split into multiple classifications for parallel execution;\\n\"\n    \"4) If unable to judge, prioritize product and pass the question to it as is.\\n\"\n    \"Example A: User: 'Check logistics for ORD-1 and see if these headphones are in stock' -&gt; Return 2: order(check logistics)+product(check inventory).\\n\"\n    \"Example B: User: 'I want to return ORD-1, how long for refund' -&gt; Return 1: refund(initiate/query refund).\\n\"\n    \"Example C: User: 'I want to know the specs of this headphone' -&gt; Return 1: product(query details).\\n\"\n)\n\n\nclass Classification(TypedDict):\n    \"\"\"A routing decision: which agent to call and with what query.\"\"\"\n\n    source: Literal[\"order\", \"refund\", \"product\"]\n    query: str\n\n\nclass ClassificationResult(BaseModel):\n    \"\"\"Result of classifying a user query into agent-oriented sub-questions.\"\"\"\n\n    classifications: list[Classification] = Field(\n        description=\"List of agents to call and their corresponding sub-questions\"\n    )\n\n\ndef branch_fn(state: RouterState) -&gt; list[Send]:\n    structured_llm = model.with_structured_output(ClassificationResult)\n\n    query = state.get(\"query\")\n    classification_result = cast(\n        ClassificationResult,\n        structured_llm.invoke(\n            [\n                SystemMessage(ROUTER_SYSTEM_PROMPT),\n                HumanMessage(query),\n            ]\n        ),\n    )\n\n    classifications = classification_result.classifications or []\n    if not classifications:\n        classifications = [{\"source\": \"product\", \"query\": query}]\n\n    sends: list[Send] = []\n    for res in classifications:\n        source = res.get(\"source\")\n        if source not in {\"order\", \"refund\", \"product\"}:\n            source = \"product\"\n        sub_query = (res.get(\"query\") or query).strip() or query\n        sends.append(Send(f\"{source}_agent\", {\"messages\": [HumanMessage(sub_query)]}))\n    return sends\n</code></pre> <p>Finally, use <code>create_parallel_agent</code> to create the parallel agent and pass in the branch function.</p> <p>Run the test:</p> <pre><code>response_single = graph.invoke(\n    {\n        \"query\": \"Hello, I want to check the products I purchased before\",\n    }\n)\nprint(response_single)\n\nresponse_parallel = graph.invoke(\n    {\n        \"query\": \"Recommend a wireless headset suitable for commuting and check stock; also, tell me your product refund policy?\",\n    }\n)\nprint(response_parallel)\n</code></pre> <p>Tip</p> <ul> <li>When <code>branches_fn</code> is NOT passed: All sub-graphs will be executed in parallel</li> <li>When <code>branches_fn</code> IS passed: Which sub-graphs are executed is determined by the return value of that function</li> </ul>"},{"location":"api-reference/agent/","title":"Agent Module API Reference","text":""},{"location":"api-reference/agent/#create_agent","title":"create_agent","text":"<p>Creates an agent with the same functionality as the official <code>langchain</code> <code>create_agent</code>, but extends the model specification to a string.</p>"},{"location":"api-reference/agent/#function-signature","title":"Function Signature","text":"<pre><code>def create_agent(  # noqa: PLR0915\n    model: str,\n    tools: Sequence[BaseTool | Callable | dict[str, Any]] | None = None,\n    *,\n    system_prompt: str | SystemMessage | None = None,\n    response_format: ResponseFormat[ResponseT] | type[ResponseT] | None = None,\n    middleware: Sequence[AgentMiddleware[StateT_co, ContextT]] = (),\n    state_schema: type[AgentState[ResponseT]] | None = None,\n    context_schema: type[ContextT] | None = None,\n    checkpointer: Checkpointer | None = None,\n    store: BaseStore | None = None,\n    interrupt_before: list[str] | None = None,\n    interrupt_after: list[str] | None = None,\n    debug: bool = False,\n    name: str | None = None,\n    cache: BaseCache | None = None,\n) -&gt; CompiledStateGraph[\n    AgentState[ResponseT], ContextT, _InputAgentState, _OutputAgentState[ResponseT]\n]:\n</code></pre>"},{"location":"api-reference/agent/#parameters","title":"Parameters","text":"Parameter Type Required Default Description model str Yes - Model identifier string that can be loaded by <code>load_chat_model</code>. Can be specified in \"provider:model-name\" format tools Sequence[BaseTool | Callable | dict[str, Any]] | None No None List of tools available to the agent system_prompt str | SystemMessage | None No None Custom system prompt for the agent middleware Sequence[AgentMiddleware[AgentState[ResponseT], ContextT]] No () Agent middleware response_format ResponseFormat[ResponseT] | type[ResponseT] | None No None Response format for the agent state_schema type[AgentState[ResponseT]] | None No None State schema for the agent context_schema type[ContextT] | None No None Context schema for the agent checkpointer Checkpointer | None No None Checkpointer for state persistence store BaseStore | None No None Storage for data persistence interrupt_before list[str] | None No None Nodes to interrupt before execution interrupt_after list[str] | None No None Nodes to interrupt after execution debug bool No False Enable debug mode name str | None No None Agent name cache BaseCache | None No None Cache"},{"location":"api-reference/agent/#notes","title":"Notes","text":"<p>This function provides the same functionality as the official <code>langchain</code> <code>create_agent</code>, but extends the model selection. The main difference is that the <code>model</code> parameter must be a string that can be loaded by the <code>load_chat_model</code> function, allowing more flexible model selection using registered model providers.</p>"},{"location":"api-reference/agent/#example","title":"Example","text":"<pre><code>agent = create_agent(model=\"vllm:qwen3-4b\", tools=[get_current_time])\n</code></pre>"},{"location":"api-reference/agent/#wrap_agent_as_tool","title":"wrap_agent_as_tool","text":"<p>Wraps an agent as a tool.</p>"},{"location":"api-reference/agent/#function-signature_1","title":"Function Signature","text":"<pre><code>def wrap_agent_as_tool(\n    agent: CompiledStateGraph,\n    tool_name: Optional[str] = None,\n    tool_description: Optional[str] = None,\n    pre_input_hooks: Optional[\n        tuple[\n            Callable[[str, ToolRuntime], str | dict[str, Any]],\n            Callable[[str, ToolRuntime], Awaitable[str | dict[str, Any]]],\n        ]\n        | Callable[[str, ToolRuntime], str | dict[str, Any]]\n    ] = None,\n    post_output_hooks: Optional[\n        tuple[\n            Callable[[str, dict[str, Any], ToolRuntime], Any],\n            Callable[[str, dict[str, Any], ToolRuntime], Awaitable[Any]],\n        ]\n        | Callable[[str, dict[str, Any], ToolRuntime], Any]\n    ] = None,\n) -&gt; BaseTool:\n</code></pre>"},{"location":"api-reference/agent/#parameters_1","title":"Parameters","text":"Parameter Type Required Default Description agent CompiledStateGraph Yes - The agent graph to wrap tool_name Optional[str] No None Name of the resulting tool tool_description Optional[str] No None Description of the resulting tool pre_input_hooks - No None Hooks for processing input before passing to the agent post_output_hooks - No None Hooks for processing output after receiving from the agent"},{"location":"api-reference/agent/#example_1","title":"Example","text":"<pre><code>tool = wrap_agent_as_tool(agent)\n</code></pre>"},{"location":"api-reference/agent/#wrap_all_agents_as_tool","title":"wrap_all_agents_as_tool","text":"<p>Wraps all agents as a single tool.</p>"},{"location":"api-reference/agent/#function-signature_2","title":"Function Signature","text":"<pre><code>def wrap_all_agents_as_tool(\n    agents: list[CompiledStateGraph],\n    tool_name: Optional[str] = None,\n    tool_description: Optional[str] = None,\n    pre_input_hooks: Optional[\n        tuple[\n            Callable[[str, ToolRuntime], str | dict[str, Any]],\n            Callable[[str, ToolRuntime], Awaitable[str | dict[str, Any]]],\n        ]\n        | Callable[[str, ToolRuntime], str | dict[str, Any]]\n    ] = None,\n    post_output_hooks: Optional[\n        tuple[\n            Callable[[str, dict[str, Any], ToolRuntime], Any],\n            Callable[[str, dict[str, Any], ToolRuntime], Awaitable[Any]],\n        ]\n        | Callable[[str, dict[str, Any], ToolRuntime], Any]\n    ] = None,\n) -&gt; BaseTool:\n</code></pre>"},{"location":"api-reference/agent/#parameters_2","title":"Parameters","text":"Parameter Type Required Default Description agents list[CompiledStateGraph] Yes - List of agents (must contain at least 2 agents, and each agent must have a unique name) tool_name Optional[str] No None Name of the resulting tool tool_description Optional[str] No None Description of the resulting tool pre_input_hooks - No None Hooks for processing input before passing to the agents post_output_hooks - No None Hooks for processing output after receiving from the agents"},{"location":"api-reference/agent/#example_2","title":"Example","text":"<pre><code>tool = wrap_all_agents_as_tool([time_agent, weather_agent])\n</code></pre>"},{"location":"api-reference/agent/#summarizationmiddleware","title":"SummarizationMiddleware","text":"<p>Middleware for agent context summarization.</p>"},{"location":"api-reference/agent/#class-definition","title":"Class Definition","text":"<pre><code>class SummarizationMiddleware(_SummarizationMiddleware):\n    def __init__(\n        self,\n        model: str,\n        *,\n        trigger: ContextSize | list[ContextSize] | None = None,\n        keep: ContextSize = (\"messages\", _DEFAULT_MESSAGES_TO_KEEP),\n        token_counter: TokenCounter = count_tokens_approximately,\n        summary_prompt: str = DEFAULT_SUMMARY_PROMPT,\n        trim_tokens_to_summarize: int | None = _DEFAULT_TRIM_TOKEN_LIMIT,\n        **deprecated_kwargs: Any,\n    ) -&gt; None\n</code></pre>"},{"location":"api-reference/agent/#parameters_3","title":"Parameters","text":"Parameter Type Required Default Description model str Yes - Model identifier string that can be loaded by <code>load_chat_model</code>. Can be specified in \"provider:model-name\" format trigger ContextSize | list[ContextSize] | None No None Context size threshold that triggers summarization keep ContextSize No (\"messages\", _DEFAULT_MESSAGES_TO_KEEP) Context size to preserve after summarization token_counter TokenCounter No count_tokens_approximately Token counting function to use summary_prompt str No DEFAULT_SUMMARY_PROMPT System prompt used for summarization trim_tokens_to_summarize int | None No _DEFAULT_TRIM_TOKEN_LIMIT Number of tokens to trim from the context before summarizing"},{"location":"api-reference/agent/#example_3","title":"Example","text":"<pre><code>summarization_middleware = SummarizationMiddleware(model=\"vllm:qwen3-4b\")\n</code></pre>"},{"location":"api-reference/agent/#llmtoolselectormiddleware","title":"LLMToolSelectorMiddleware","text":"<p>Middleware for agent tool selection.</p>"},{"location":"api-reference/agent/#class-definition_1","title":"Class Definition","text":"<pre><code>class LLMToolSelectorMiddleware(_LLMToolSelectorMiddleware):\n    def __init__(\n        self,\n        *,\n        model: str,\n        system_prompt: Optional[str] = None,\n        max_tools: Optional[int] = None,\n        always_include: Optional[list[str]] = None,\n    ) -&gt; None\n</code></pre>"},{"location":"api-reference/agent/#parameters_4","title":"Parameters","text":"Parameter Type Required Default Description model str Yes - Model identifier string that can be loaded by <code>load_chat_model</code>. Can be specified in \"provider:model-name\" format system_prompt Optional[str] No None System prompt for the selection model max_tools Optional[int] No None Maximum number of tools to select always_include Optional[list[str]] No None List of tool names to always include in the selection"},{"location":"api-reference/agent/#example_4","title":"Example","text":"<pre><code>llm_tool_selector_middleware = LLMToolSelectorMiddleware(model=\"vllm:qwen3-4b\")\n</code></pre>"},{"location":"api-reference/agent/#planmiddleware","title":"PlanMiddleware","text":"<p>Middleware for agent plan management.</p>"},{"location":"api-reference/agent/#class-definition_2","title":"Class Definition","text":"<pre><code>class PlanMiddleware(AgentMiddleware):\n    state_schema = PlanState\n    def __init__(\n        self,\n        *,\n        system_prompt: Optional[str] = None,\n        custom_plan_tool_descriptions: Optional[PlanToolDescription] = None,\n        use_read_plan_tool: bool = True,\n    ) -&gt; None\n</code></pre>"},{"location":"api-reference/agent/#parameters_5","title":"Parameters","text":"Parameter Type Required Default Description system_prompt Optional[str] No None System prompt for the planning agent custom_plan_tool_descriptions Optional[PlanToolDescription] No None Custom descriptions for plan-related tools use_read_plan_tool bool No True Whether to enable the tool that allows the model to read the current plan"},{"location":"api-reference/agent/#example_5","title":"Example","text":"<pre><code>plan_middleware = PlanMiddleware()\n</code></pre>"},{"location":"api-reference/agent/#modelfallbackmiddleware","title":"ModelFallbackMiddleware","text":"<p>Middleware for agent model fallback.</p>"},{"location":"api-reference/agent/#class-definition_3","title":"Class Definition","text":"<pre><code>class ModelFallbackMiddleware(_ModelFallbackMiddleware):\n    def __init__(\n        self,\n        first_model: str,\n        *additional_models: str,\n    ) -&gt; None\n</code></pre>"},{"location":"api-reference/agent/#parameters_6","title":"Parameters","text":"Parameter Type Required Default Description first_model str Yes - Primary model identifier string (can be loaded by <code>load_chat_model</code>) additional_models str No - Fallback model identifier strings to use if the primary model fails"},{"location":"api-reference/agent/#example_6","title":"Example","text":"<pre><code>model_fallback_middleware = ModelFallbackMiddleware(\n    \"vllm:qwen3-4b\",\n    \"vllm:qwen3-8b\"\n)\n</code></pre>"},{"location":"api-reference/agent/#llmtoolemulator","title":"LLMToolEmulator","text":"<p>Middleware for simulating tool calls using large language models.</p>"},{"location":"api-reference/agent/#class-definition_4","title":"Class Definition","text":"<pre><code>class LLMToolEmulator(_LLMToolEmulator):\n    def __init__(\n        self,\n        *,\n        model: str,\n        tools: list[str | BaseTool] | None = None,\n    ) -&gt; None\n</code></pre>"},{"location":"api-reference/agent/#parameters_7","title":"Parameters","text":"Parameter Type Required Default Description model str Yes - Model identifier string that can be loaded by <code>load_chat_model</code>. Can be specified in \"provider:model-name\" format tools list[str | BaseTool] | None No None List of tools to be emulated by the LLM"},{"location":"api-reference/agent/#example_7","title":"Example","text":"<pre><code>llm_tool_emulator = LLMToolEmulator(model=\"vllm:qwen3-4b\", tools=[get_current_time])\n</code></pre>"},{"location":"api-reference/agent/#modelroutermiddleware","title":"ModelRouterMiddleware","text":"<p>Middleware for dynamically routing to appropriate models based on input content.</p>"},{"location":"api-reference/agent/#class-definition_5","title":"Class Definition","text":"<pre><code>class ModelRouterMiddleware(AgentMiddleware):\n    state_schema = ModelRouterState\n    def __init__(\n        self,\n        router_model: str | BaseChatModel,\n        model_list: list[ModelDict],\n        router_prompt: Optional[str] = None,\n    ) -&gt; None\n</code></pre>"},{"location":"api-reference/agent/#parameters_8","title":"Parameters","text":"Parameter Type Required Default Description router_model str | BaseChatModel Yes - Model used for routing. Accepts a string (loaded via <code>load_chat_model</code>) or a ChatModel instance model_list list[ModelDict] Yes - List of available models. Each entry must contain <code>model_name</code> and <code>model_description</code>, and optionally <code>tools</code>, <code>model_kwargs</code>, <code>model_instance</code>, and <code>model_system_prompt</code> router_prompt Optional[str] No None Custom prompt for the router model. Uses default prompt if not provided"},{"location":"api-reference/agent/#example_8","title":"Example","text":"<pre><code>model_router_middleware = ModelRouterMiddleware(\n    router_model=\"vllm:qwen3-4b\",\n    model_list=[\n        {\n            \"model_name\": \"vllm:qwen3-4b\",\n            \"model_description\": \"Suitable for general tasks such as conversation, text generation, etc.\"\n        },\n        {\n            \"model_name\": \"vllm:qwen3-8b\",\n            \"model_description\": \"Suitable for complex tasks such as code generation, data analysis, etc.\",\n        },\n    ]\n)\n</code></pre>"},{"location":"api-reference/agent/#handoffagentmiddleware","title":"HandoffAgentMiddleware","text":"<p>Middleware for implementing multi-agent handoffs.</p>"},{"location":"api-reference/agent/#class-definition_6","title":"Class Definition","text":"<pre><code>class HandoffAgentMiddleware(AgentMiddleware):\n    state_schema = MultiAgentState\n    def __init__(\n        self,\n        agents_config: dict[str, AgentConfig],\n        custom_handoffs_tool_descriptions: Optional[dict[str, str]] = None,\n        handoffs_tool_overrides: Optional[dict[str, BaseTool]] = None,\n    ) -&gt; None\n</code></pre>"},{"location":"api-reference/agent/#parameters_9","title":"Parameters","text":"Parameter Type Required Default Description agents_config dict[str, AgentConfig] Yes - Configuration dictionary for agents. Keys are agent names, values are agent configurations custom_handoffs_tool_descriptions Optional[dict[str, str]] No None Custom descriptions for handoff tools targeting other agents handoffs_tool_overrides Optional[dict[str, BaseTool]] No None Custom handoff tools targeting other agents"},{"location":"api-reference/agent/#example_9","title":"Example","text":"<pre><code>handoffs_agent_middleware = HandoffsAgentMiddleware({\n    \"time_agent\":{\n        \"model\":\"vllm:qwen3-4b\",\n        \"prompt\":\"You are a time agent responsible for answering time-related questions.\",\n        \"tools\":[get_current_time, transfer_to_default_agent],\n        \"handoffs\":[\"default_agent\"]\n    },\n    \"default_agent\":{\n        \"model\":\"vllm:qwen3-8b\",\n        \"prompt\":\"You are a complex task agent responsible for answering complex task-related questions.\",\n        \"default\":True,\n        \"handoffs\":[\"time_agent\"]\n    }\n})\n</code></pre>"},{"location":"api-reference/agent/#toolcallrepairmiddleware","title":"ToolCallRepairMiddleware","text":"<p>Middleware for repairing invalid tool calls.</p>"},{"location":"api-reference/agent/#class-definition_7","title":"Class Definition","text":"<pre><code>class ToolCallRepairMiddleware(AgentMiddleware):\n</code></pre>"},{"location":"api-reference/agent/#example_10","title":"Example","text":"<pre><code>tool_call_repair_middleware = ToolCallRepairMiddleware()\n</code></pre>"},{"location":"api-reference/agent/#format_prompt","title":"format_prompt","text":"<p>Helper for formatting prompts.</p>"},{"location":"api-reference/agent/#function-signature_3","title":"Function Signature","text":"<pre><code>@dynamic_prompt\ndef format_prompt(request: ModelRequest) -&gt; str\n</code></pre>"},{"location":"api-reference/agent/#planstate","title":"PlanState","text":"<p>State Schema for Plan.</p>"},{"location":"api-reference/agent/#class-definition_8","title":"Class Definition","text":"<pre><code>class Plan(TypedDict):\n    content: str\n    status: Literal[\"pending\", \"in_progress\", \"done\"]\n\n\nclass PlanState(AgentState):\n    plan: NotRequired[list[Plan]]\n</code></pre>"},{"location":"api-reference/agent/#attributes","title":"Attributes","text":"Attribute Type Description plan NotRequired[list[Plan]] List of plan steps plan.content str Content of the plan step plan.status Literal[\"pending\", \"in_progress\", \"done\"] Status of the plan step. Valid values are <code>pending</code>, <code>in_progress</code>, <code>done</code>"},{"location":"api-reference/agent/#modeldict","title":"ModelDict","text":"<p>Type definition for the model list.</p>"},{"location":"api-reference/agent/#class-definition_9","title":"Class Definition","text":"<pre><code>class ModelDict(TypedDict):\n    model_name: str\n    model_description: str\n    tools: NotRequired[list[BaseTool | dict[str, Any]]]\n    model_kwargs: NotRequired[dict[str, Any]]\n    model_instance: NotRequired[BaseChatModel]\n    model_system_prompt: NotRequired[str]\n</code></pre>"},{"location":"api-reference/agent/#attributes_1","title":"Attributes","text":"Attribute Type Required Description model_name str Yes Name of the model model_description str Yes Description of the model's capabilities tools NotRequired[list[BaseTool | dict[str, Any]]] No Tools available to this model model_kwargs NotRequired[dict[str, Any]] No Additional keyword arguments to pass to the model model_instance NotRequired[BaseChatModel] No A specific model instance to use model_system_prompt NotRequired[str] No System prompt specific to this model"},{"location":"api-reference/agent/#selectmodel","title":"SelectModel","text":"<p>Tool class for selecting a model.</p>"},{"location":"api-reference/agent/#class-definition_10","title":"Class Definition","text":"<pre><code>class SelectModel(BaseModel):\n    \"\"\"Tool for model selection - Must call this tool to return the finally selected model\"\"\"\n\n    model_name: str = Field(\n        ...,\n        description=\"Selected model name (must be the full model name, for example, openai:gpt-4o)\",\n    )\n</code></pre>"},{"location":"api-reference/agent/#attributes_2","title":"Attributes","text":"Attribute Type Required Description model_name str Yes The name of the selected model (must be the full model name, e.g., openai:gpt-4o)"},{"location":"api-reference/agent/#multiagentstate","title":"MultiAgentState","text":"<p>State Schema for multi-agent handoffs.</p>"},{"location":"api-reference/agent/#class-definition_11","title":"Class Definition","text":"<pre><code>class MultiAgentState(AgentState):\n    active_agent: NotRequired[str]\n</code></pre>"},{"location":"api-reference/agent/#attributes_3","title":"Attributes","text":"Attribute Type Description active_agent NotRequired[str] The name of the currently active agent"},{"location":"api-reference/agent/#agentconfig","title":"AgentConfig","text":"<p>Type definition for agent configuration.</p>"},{"location":"api-reference/agent/#class-definition_12","title":"Class Definition","text":"<pre><code>class AgentConfig(TypedDict):\n    model: NotRequired[str | BaseChatModel]\n    prompt: str | SystemMessage\n    tools: NotRequired[list[BaseTool | dict[str, Any]]]\n    default: NotRequired[bool]\n    handoffs: list[str] | Literal[\"all\"]\n</code></pre>"},{"location":"api-reference/agent/#attributes_4","title":"Attributes","text":"Attribute Type Required Description model NotRequired[str | BaseChatModel] No Model name or model instance prompt str | SystemMessage Yes System prompt for the agent tools list[BaseTool | dict[str, Any]] Yes Tools available to the agent default NotRequired[bool] No Whether this agent is the default fallback handoffs list[str] | Literal[\"all\"] Yes List of agent names to hand off to, or \"all\" to allow handoffs to any agent"},{"location":"api-reference/chat_model/","title":"ChatModel Module API Reference Documentation","text":""},{"location":"api-reference/chat_model/#register_model_provider","title":"register_model_provider","text":"<p>Register a provider for chat models.</p>"},{"location":"api-reference/chat_model/#function-signature","title":"Function Signature","text":"<pre><code>def register_model_provider(\n    provider_name: str,\n    chat_model: ChatModelType,\n    base_url: Optional[str] = None,\n    model_profiles: Optional[dict[str, dict[str, Any]]] = None,\n    compatibility_options: Optional[CompatibilityOptions] = None,\n) -&gt; None:\n</code></pre>"},{"location":"api-reference/chat_model/#parameters","title":"Parameters","text":"Parameter Type Required Default Description provider_name str Yes - Custom provider name chat_model ChatModelType Yes - ChatModel class or supported provider string type base_url Optional[str] No None BaseURL of the provider model_profiles Optional[dict[str, dict[str, Any]]] No None Profiles of models supported by the provider, format: <code>{model_name: model_profile}</code> compatibility_options Optional[CompatibilityOptions] No None Compatibility options"},{"location":"api-reference/chat_model/#example","title":"Example","text":"<pre><code>register_model_provider(\"fakechat\", FakeChatModel)\nregister_model_provider(\"vllm\", \"openai-compatible\", base_url=\"http://localhost:8000/v1\")\n</code></pre>"},{"location":"api-reference/chat_model/#batch_register_model_provider","title":"batch_register_model_provider","text":"<p>Batch register model providers.</p>"},{"location":"api-reference/chat_model/#function-signature_1","title":"Function Signature","text":"<pre><code>def batch_register_model_provider(\n    providers: list[ChatModelProvider],\n) -&gt; None:\n</code></pre>"},{"location":"api-reference/chat_model/#parameters_1","title":"Parameters","text":"Parameter Type Required Default Description providers list[ChatModelProvider] Yes - List of provider configurations"},{"location":"api-reference/chat_model/#example_1","title":"Example","text":"<pre><code>batch_register_model_provider([\n    {\"provider_name\": \"fakechat\", \"chat_model\": FakeChatModel},\n    {\"provider_name\": \"vllm\", \"chat_model\": \"openai-compatible\", \"base_url\": \"http://localhost:8000/v1\"},\n])\n</code></pre>"},{"location":"api-reference/chat_model/#load_chat_model","title":"load_chat_model","text":"<p>Load a chat model from registered providers.</p>"},{"location":"api-reference/chat_model/#function-signature_2","title":"Function Signature","text":"<pre><code>def load_chat_model(\n    model: str,\n    *,\n    model_provider: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; BaseChatModel:\n</code></pre>"},{"location":"api-reference/chat_model/#parameters_2","title":"Parameters","text":"Parameter Type Required Default Description model str Yes - Model name, format: <code>model_name</code> or <code>provider_name:model_name</code> model_provider Optional[str] No None Model provider name **kwargs Any No - Additional model parameters"},{"location":"api-reference/chat_model/#example_2","title":"Example","text":"<pre><code>model = load_chat_model(\"vllm:qwen3-4b\")\n</code></pre>"},{"location":"api-reference/chat_model/#create_openai_compatible_model","title":"create_openai_compatible_model","text":"<p>Create an OpenAI compatible chat model class.</p>"},{"location":"api-reference/chat_model/#function-signature_3","title":"Function Signature","text":"<pre><code>def create_openai_compatible_model(\n    model_provider: str,\n    base_url: Optional[str] = None,\n    compatibility_options: Optional[CompatibilityOptions] = None,\n    model_profiles: Optional[dict[str, dict[str, Any]]] = None,\n    chat_model_cls_name: Optional[str] = None,\n) -&gt; type[BaseChatModel]:\n</code></pre>"},{"location":"api-reference/chat_model/#parameters_3","title":"Parameters","text":"Parameter Type Required Default Description model_provider str Yes - Model provider name base_url Optional[str] No None BaseURL of the model provider compatibility_options Optional[CompatibilityOptions] No None Compatibility options model_profiles Optional[dict[str, dict[str, Any]]] No None Profiles of models supported by the provider, format: <code>{model_name: model_profile}</code> chat_model_cls_name Optional[str] No None Custom chat model class name"},{"location":"api-reference/chat_model/#return-value","title":"Return Value","text":"Type Description type[BaseChatModel] Dynamically created OpenAI compatible chat model class"},{"location":"api-reference/chat_model/#example_3","title":"Example","text":"<pre><code>ChatVLLM = create_openai_compatible_model(\n    model_provider=\"vllm\",\n    base_url=\"http://localhost:8000/v1\",\n    chat_model_cls_name=\"ChatVLLM\",\n)\n</code></pre>"},{"location":"api-reference/chat_model/#chatmodeltype","title":"ChatModelType","text":"<p>Types supported by the <code>chat_model</code> parameter when registering model providers.</p>"},{"location":"api-reference/chat_model/#type-definition","title":"Type Definition","text":"<pre><code>ChatModelType = Union[type[BaseChatModel], Literal[\"openai-compatible\"]]\n</code></pre>"},{"location":"api-reference/chat_model/#toolchoicetype","title":"ToolChoiceType","text":"<p>Types supported by the <code>tool_choice</code> parameter.</p>"},{"location":"api-reference/chat_model/#type-definition_1","title":"Type Definition","text":"<pre><code>ToolChoiceType = list[Literal[\"auto\", \"none\", \"required\", \"specific\"]]\n</code></pre>"},{"location":"api-reference/chat_model/#responseformattype","title":"ResponseFormatType","text":"<p>Types supported by <code>response_format</code>.</p>"},{"location":"api-reference/chat_model/#type-definition_2","title":"Type Definition","text":"<pre><code>ResponseFormatType = list[Literal[\"json_schema\", \"json_mode\"]]\n</code></pre>"},{"location":"api-reference/chat_model/#reasoningkeeppolicy","title":"ReasoningKeepPolicy","text":"<p>Retention policy for the reasoning_content field in the messages list.</p>"},{"location":"api-reference/chat_model/#type-definition_3","title":"Type Definition","text":"<pre><code>ReasoningKeepPolicy = Literal[\"never\", \"current\", \"all\"]\n</code></pre>"},{"location":"api-reference/chat_model/#compatibilityoptions","title":"CompatibilityOptions","text":"<p>Compatibility options for model providers.</p>"},{"location":"api-reference/chat_model/#class-definition","title":"Class Definition","text":"<pre><code>class CompatibilityOptions(TypedDict):\n    supported_tool_choice: NotRequired[ToolChoiceType]\n    supported_response_format: NotRequired[ResponseFormatType]\n    reasoning_keep_policy: NotRequired[ReasoningKeepPolicy]\n    include_usage: NotRequired[bool]\n</code></pre>"},{"location":"api-reference/chat_model/#field-description","title":"Field Description","text":"Field Type Required Description supported_tool_choice NotRequired[ToolChoiceType] No List of supported <code>tool_choice</code> strategies supported_response_format NotRequired[ResponseFormatType] No List of supported <code>response_format</code> methods reasoning_keep_policy NotRequired[ReasoningKeepPolicy] No Retention policy for the <code>reasoning_content</code> field in historical messages (messages) passed to the model. Optional values are <code>never</code>, <code>current</code>, <code>all</code> include_usage NotRequired[bool] No Whether to include <code>usage</code> information in the last streaming response result"},{"location":"api-reference/chat_model/#chatmodelprovider","title":"ChatModelProvider","text":"<p>Chat model provider configuration type.</p>"},{"location":"api-reference/chat_model/#class-definition_1","title":"Class Definition","text":"<pre><code>class ChatModelProvider(TypedDict):\n    provider_name: str\n    chat_model: ChatModelType\n    base_url: NotRequired[str]\n    model_profiles: NotRequired[dict[str, dict[str, Any]]]\n    compatibility_options: NotRequired[CompatibilityOptions]\n</code></pre>"},{"location":"api-reference/chat_model/#field-description_1","title":"Field Description","text":"Field Type Required Description provider_name str Yes Provider name chat_model ChatModelType Yes Support passing chat model class or string (currently only supports <code>openai-compatible</code>) base_url NotRequired[str] No Base URL model_profiles NotRequired[dict[str, dict[str, Any]]] No Profiles of models supported by the provider, format: <code>{model_name: model_profile}</code> compatibility_options NotRequired[CompatibilityOptions] No Model provider compatibility options"},{"location":"api-reference/embeddings/","title":"Embeddings Module API Reference Documentation","text":""},{"location":"api-reference/embeddings/#register_embeddings_provider","title":"register_embeddings_provider","text":"<p>Register a provider for embedding models.</p>"},{"location":"api-reference/embeddings/#function-signature","title":"Function Signature","text":"<pre><code>def register_embeddings_provider(\n    provider_name: str,\n    embeddings_model: EmbeddingsType,\n    base_url: Optional[str] = None,\n) -&gt; None:\n</code></pre>"},{"location":"api-reference/embeddings/#parameters","title":"Parameters","text":"Parameter Type Required Default Description provider_name str Yes - Custom provider name embeddings_model EmbeddingsType Yes - Embedding model class or supported provider string type base_url Optional[str] No None BaseURL of the provider"},{"location":"api-reference/embeddings/#example","title":"Example","text":"<pre><code>register_embeddings_provider(\"fakeembeddings\", FakeEmbeddings)\nregister_embeddings_provider(\"vllm\", \"openai-compatible\", base_url=\"http://localhost:8000/v1\")\n</code></pre>"},{"location":"api-reference/embeddings/#batch_register_embeddings_provider","title":"batch_register_embeddings_provider","text":"<p>Batch register embedding model providers.</p>"},{"location":"api-reference/embeddings/#function-signature_1","title":"Function Signature","text":"<pre><code>def batch_register_embeddings_provider(\n    providers: list[EmbeddingProvider]\n) -&gt; None:\n</code></pre>"},{"location":"api-reference/embeddings/#parameters_1","title":"Parameters","text":"Parameter Type Required Default Description providers list[EmbeddingProvider] Yes - List of provider configurations"},{"location":"api-reference/embeddings/#example_1","title":"Example","text":"<pre><code>batch_register_embeddings_provider([\n    {\"provider_name\": \"fakeembeddings\", \"embeddings_model\": FakeEmbeddings},\n    {\"provider_name\": \"vllm\", \"embeddings_model\": \"openai-compatible\", \"base_url\": \"http://localhost:8000/v1\"},\n])\n</code></pre>"},{"location":"api-reference/embeddings/#load_embeddings","title":"load_embeddings","text":"<p>Load an embedding model from registered providers.</p>"},{"location":"api-reference/embeddings/#function-signature_2","title":"Function Signature","text":"<pre><code>def load_embeddings(\n    model: str,\n    *,\n    provider: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; Embeddings:\n</code></pre>"},{"location":"api-reference/embeddings/#parameters_2","title":"Parameters","text":"Parameter Type Required Default Description model str Yes - Model name, format: <code>model_name</code> or <code>provider_name:model_name</code> provider Optional[str] No None Model provider name **kwargs Any No - Additional model parameters"},{"location":"api-reference/embeddings/#example_2","title":"Example","text":"<pre><code>embeddings = load_embeddings(\"vllm:qwen3-embedding-4b\")\n</code></pre>"},{"location":"api-reference/embeddings/#create_openai_compatible_embedding","title":"create_openai_compatible_embedding","text":"<p>Create an OpenAI compatible embedding model class.</p>"},{"location":"api-reference/embeddings/#function-signature_3","title":"Function Signature","text":"<pre><code>def create_openai_compatible_embedding(\n    embedding_provider: str,\n    base_url: Optional[str] = None,\n    embedding_model_cls_name: Optional[str] = None,\n) -&gt; type[Embeddings]:\n</code></pre>"},{"location":"api-reference/embeddings/#parameters_3","title":"Parameters","text":"Parameter Type Required Default Description embedding_provider str Yes - Embedding model provider name base_url Optional[str] No None BaseURL of the model provider embedding_model_cls_name Optional[str] No None Custom embedding model class name"},{"location":"api-reference/embeddings/#return-value","title":"Return Value","text":"Type Description type[Embeddings] Dynamically created OpenAI compatible embedding model class"},{"location":"api-reference/embeddings/#example_3","title":"Example","text":""},{"location":"api-reference/embeddings/#vllmembeddings-create_openai_compatible_embedding-embedding_providervllm-base_urlhttplocalhost8000v1-embedding_model_cls_namevllmembeddings","title":"<pre><code>VLLMEmbeddings = create_openai_compatible_embedding(\n    embedding_provider=\"vllm\",\n    base_url=\"http://localhost:8000/v1\",\n    embedding_model_cls_name=\"VLLMEmbeddings\",\n)\n</code></pre>","text":""},{"location":"api-reference/embeddings/#embeddingstype","title":"EmbeddingsType","text":"<p>Types supported by the <code>embeddings_model</code> parameter when registering embedding providers.</p>"},{"location":"api-reference/embeddings/#type-definition","title":"Type Definition","text":"<pre><code>EmbeddingsType = Union[type[Embeddings], Literal[\"openai-compatible\"]]\n</code></pre>"},{"location":"api-reference/embeddings/#embeddingprovider","title":"EmbeddingProvider","text":"<p>Embedding model provider configuration type.</p>"},{"location":"api-reference/embeddings/#class-definition","title":"Class Definition","text":"<pre><code>class EmbeddingProvider(TypedDict):\n    provider_name: str\n    embeddings_model: EmbeddingsType\n    base_url: NotRequired[str]\n</code></pre>"},{"location":"api-reference/embeddings/#field-description","title":"Field Description","text":"Field Type Required Description provider_name str Yes Provider name embeddings_model EmbeddingsType Yes Embedding model class or string base_url NotRequired[str] No Base URL"},{"location":"api-reference/message_convert/","title":"Message Convert Module API Reference Documentation","text":""},{"location":"api-reference/message_convert/#convert_reasoning_content_for_ai_message","title":"convert_reasoning_content_for_ai_message","text":"<p>Merges the chain of thought into the final response.</p>"},{"location":"api-reference/message_convert/#function-signature","title":"Function Signature","text":"<pre><code>def convert_reasoning_content_for_ai_message(\n    model_response: AIMessage,\n    think_tag: Tuple[str, str] = (\"&lt;think&gt;\", \"&lt;/think&gt;\"),\n) -&gt; AIMessage\n</code></pre>"},{"location":"api-reference/message_convert/#parameters","title":"Parameters","text":"Parameter Type Required Default Description model_response AIMessage Yes - AI message containing reasoning content think_tag Tuple[str, str] No <code>(\"&lt;think&gt;\",\"&lt;/think&gt;\")</code> Start and end tags for reasoning content"},{"location":"api-reference/message_convert/#example","title":"Example","text":"<pre><code>response = convert_reasoning_content_for_ai_message(\n    response, think_tag=(\"&lt;start&gt;\", \"&lt;end&gt;\")\n)\n</code></pre>"},{"location":"api-reference/message_convert/#convert_reasoning_content_for_chunk_iterator","title":"convert_reasoning_content_for_chunk_iterator","text":"<p>Merges reasoning content for streaming message chunks.</p>"},{"location":"api-reference/message_convert/#function-signature_1","title":"Function Signature","text":"<pre><code>def convert_reasoning_content_for_chunk_iterator(\n    model_response: Iterator[AIMessageChunk | AIMessage],\n    think_tag: Tuple[str, str] = (\"think\", \"think\"),\n) -&gt; Iterator[AIMessageChunk | AIMessage]\n</code></pre>"},{"location":"api-reference/message_convert/#parameters_1","title":"Parameters","text":"Parameter Type Required Default Description model_response Iterator[AIMessageChunk | AIMessage] Yes - Iterator of message chunks think_tag Tuple[str, str] No <code>(\"&lt;think&gt;\",\"&lt;/think&gt;\")</code> Start and end tags for reasoning content"},{"location":"api-reference/message_convert/#example_1","title":"Example","text":"<pre><code>for chunk in convert_reasoning_content_for_chunk_iterator(\n    model.stream(\"Hello\"), think_tag=(\"&lt;think&gt;\", \"&lt;/think&gt;\")\n):\n    print(chunk.content, end=\"\", flush=True)\n</code></pre>"},{"location":"api-reference/message_convert/#aconvert_reasoning_content_for_chunk_iterator","title":"aconvert_reasoning_content_for_chunk_iterator","text":"<p>Asynchronous version of <code>convert_reasoning_content_for_chunk_iterator</code>.</p>"},{"location":"api-reference/message_convert/#function-signature_2","title":"Function Signature","text":"<pre><code>async def aconvert_reasoning_content_for_chunk_iterator(\n    model_response: AsyncIterator[AIMessageChunk | AIMessage],\n    think_tag: Tuple[str, str] = (\"think\", \"think\"),\n) -&gt; AsyncIterator[AIMessageChunk | AIMessage]\n</code></pre>"},{"location":"api-reference/message_convert/#parameters_2","title":"Parameters","text":"Parameter Type Required Default Description model_response AsyncIterator[AIMessageChunk | AIMessage] Yes - Async iterator of message chunks think_tag Tuple[str, str] No <code>(\"&lt;think&gt;\",\"&lt;/think&gt;\")</code> Start and end tags for reasoning content"},{"location":"api-reference/message_convert/#example_2","title":"Example","text":"<pre><code>async for chunk in aconvert_reasoning_content_for_chunk_iterator(\n    model.astream(\"Hello\"), think_tag=(\"&lt;think&gt;\", \"&lt;/think&gt;\")\n):\n    print(chunk.content, end=\"\", flush=True)\n</code></pre>"},{"location":"api-reference/message_convert/#merge_ai_message_chunk","title":"merge_ai_message_chunk","text":"<p>Merges streaming output chunks into a single AIMessage.</p>"},{"location":"api-reference/message_convert/#function-signature_3","title":"Function Signature","text":"<pre><code>def merge_ai_message_chunk(\n    chunks: Sequence[AIMessageChunk]\n) -&gt; AIMessage\n</code></pre>"},{"location":"api-reference/message_convert/#parameters_3","title":"Parameters","text":"Parameter Type Required Default Description chunks Sequence[AIMessageChunk] Yes - List of message chunks to merge"},{"location":"api-reference/message_convert/#example_3","title":"Example","text":"<pre><code>chunks = list(model.stream(\"Hello\"))\nmerged = merge_ai_message_chunk(chunks)\n</code></pre>"},{"location":"api-reference/message_convert/#format_sequence","title":"format_sequence","text":"<p>Formats a list of BaseMessage, Document, or strings into a single string.</p>"},{"location":"api-reference/message_convert/#function-signature_4","title":"Function Signature","text":"<pre><code>def format_sequence(\n    inputs: List[Union[BaseMessage, Document, str]],\n    separator: str = \"-\",\n    with_num: bool = False\n) -&gt; str\n</code></pre>"},{"location":"api-reference/message_convert/#parameters_4","title":"Parameters","text":"Parameter Type Required Default Description inputs List[Union[BaseMessage, Document, str]] Yes - List of items to format separator str No \"-\" Separator string with_num bool No False Whether to add number prefix"},{"location":"api-reference/message_convert/#example_4","title":"Example","text":"<pre><code>formatted = format_sequence(messages, separator=\"\\n\", with_num=True)\n</code></pre>"},{"location":"api-reference/pipeline/","title":"Pipeline Module API Reference Documentation","text":""},{"location":"api-reference/pipeline/#create_sequential_pipeline","title":"create_sequential_pipeline","text":"<p>Combines multiple subgraphs with the same state in a sequential manner.</p>"},{"location":"api-reference/pipeline/#function-signature","title":"Function Signature","text":"<pre><code>def create_sequential_pipeline(\n    sub_graphs: list[SubGraph],\n    state_schema: type[StateT],\n    graph_name: Optional[str] = None,\n    context_schema: type[ContextT] | None = None,\n    input_schema: type[InputT] | None = None,\n    output_schema: type[OutputT] | None = None,\n    checkpointer: Checkpointer | None = None,\n    store: BaseStore | None = None,\n    cache: BaseCache | None = None,\n) -&gt; CompiledStateGraph[StateT, ContextT, InputT, OutputT]:\n</code></pre>"},{"location":"api-reference/pipeline/#parameters","title":"Parameters","text":"Parameter Type Required Default Description sub_graphs list[SubGraph] Yes - List of state graphs to combine state_schema type[StateT] Yes - State Schema of the final generated graph graph_name Optional[str] No None Name of the final generated graph context_schema type[ContextT] | None No None Context Schema of the final generated graph input_schema type[InputT] | None No None Input Schema of the final generated graph output_schema type[OutputT] | None No None Output Schema of the final generated graph checkpointer Checkpointer | None No None Checkpointer of the final generated graph store BaseStore | None No None Store of the final generated graph cache BaseCache | None No None Cache of the final generated graph"},{"location":"api-reference/pipeline/#example","title":"Example","text":"<pre><code>create_sequential_pipeline(\n    sub_graphs=[graph1, graph2],\n    state_schema=State,\n    graph_name=\"sequential_pipeline\",\n    context_schema=Context,\n    input_schema=Input,\n    output_schema=Output,\n)\n</code></pre>"},{"location":"api-reference/pipeline/#create_parallel_pipeline","title":"create_parallel_pipeline","text":"<p>Combines multiple subgraphs with the same state in a parallel manner.</p>"},{"location":"api-reference/pipeline/#function-signature_1","title":"Function Signature","text":"<pre><code>def create_parallel_pipeline(\n    sub_graphs: list[SubGraph],\n    state_schema: type[StateT],\n    graph_name: Optional[str] = None,\n    branches_fn: Optional[\n        Union[\n            Callable[..., list[Send]],\n            Callable[..., Awaitable[list[Send]]],\n        ]\n    ] = None,\n    context_schema: type[ContextT] | None = None,\n    input_schema: type[InputT] | None = None,\n    output_schema: type[OutputT] | None = None,\n    checkpointer: Checkpointer | None = None,\n    store: BaseStore | None = None,\n    cache: BaseCache | None = None,\n) -&gt; CompiledStateGraph[StateT, ContextT, InputT, OutputT]:\n</code></pre>"},{"location":"api-reference/pipeline/#parameters_1","title":"Parameters","text":"Parameter Type Required Default Description sub_graphs list[SubGraph] Yes - List of state graphs to combine state_schema type[StateT] Yes - State Schema of the final generated graph graph_name Optional[str] No None Name of the final generated graph branches_fn Optional[Union[Callable[..., list[Send]], Callable[..., Awaitable[list[Send]]]]] No None Parallel branch function, returns a list of <code>Send</code> objects to control parallel execution context_schema type[ContextT] | None No None Context Schema of the final generated graph input_schema type[InputT] | None No None Input Schema of the final generated graph output_schema type[OutputT] | None No None Output Schema of the final generated graph checkpointer Checkpointer | None No None Checkpointer of the final generated graph store BaseStore | None No None Store of the final generated graph cache BaseCache | None No None Cache of the final generated graph"},{"location":"api-reference/pipeline/#example_1","title":"Example","text":"<pre><code>create_parallel_pipeline(\n    sub_graphs=[graph1, graph2],\n    state_schema=State,\n    graph_name=\"parallel_pipeline\",\n    branches_fn=lambda state: [Send(\"graph1\", state), Send(\"graph2\", state)],\n    context_schema=Context,\n    input_schema=Input,\n    output_schema=Output,\n)\n</code></pre>"},{"location":"api-reference/tool_calling/","title":"Tool Calling Module API Reference Documentation","text":""},{"location":"api-reference/tool_calling/#has_tool_calling","title":"has_tool_calling","text":"<p>Checks if a message contains a tool call.</p>"},{"location":"api-reference/tool_calling/#function-signature","title":"Function Signature","text":"<pre><code>def has_tool_calling(\n    message: AIMessage\n) -&gt; bool\n</code></pre>"},{"location":"api-reference/tool_calling/#parameters","title":"Parameters","text":"Parameter Type Required Default Description message AIMessage Yes - The message to check"},{"location":"api-reference/tool_calling/#example","title":"Example","text":"<pre><code>if has_tool_calling(response):\n    # Handle tool call\n    pass\n</code></pre>"},{"location":"api-reference/tool_calling/#parse_tool_calling","title":"parse_tool_calling","text":"<p>Parses tool call arguments from a message.</p>"},{"location":"api-reference/tool_calling/#function-signature_1","title":"Function Signature","text":"<pre><code>def parse_tool_calling(\n    message: AIMessage, first_tool_call_only: bool = False\n) -&gt; Union[tuple[str, dict], list[tuple[str, dict]]]\n</code></pre>"},{"location":"api-reference/tool_calling/#parameters_1","title":"Parameters","text":"Parameter Type Required Default Description message AIMessage Yes - The message to parse first_tool_call_only bool No False Whether to return only the first tool call"},{"location":"api-reference/tool_calling/#example_1","title":"Example","text":"<pre><code># Get all tool calls\ntool_calls = parse_tool_calling(response)\n\n# Get only the first tool call\nname, args = parse_tool_calling(response, first_tool_call_only=True)\n</code></pre>"},{"location":"api-reference/tool_calling/#human_in_the_loop","title":"human_in_the_loop","text":"<p>A decorator to add \"human-in-the-loop\" manual review capability to synchronous tool functions.</p>"},{"location":"api-reference/tool_calling/#function-signature_2","title":"Function Signature","text":"<pre><code>def human_in_the_loop(\n    func: Optional[Callable] = None,\n    *,\n    handler: Optional[HumanInterruptHandler] = None\n) -&gt; Union[Callable[[Callable], BaseTool], BaseTool]\n</code></pre>"},{"location":"api-reference/tool_calling/#parameters_2","title":"Parameters","text":"Parameter Type Required Default Description func Optional[Callable] No None The synchronous function to be decorated (decorator syntactic sugar) handler Optional[HumanInterruptHandler] No None Custom interrupt handler function"},{"location":"api-reference/tool_calling/#example_2","title":"Example","text":"<pre><code>@human_in_the_loop\ndef get_current_time():\n    \"\"\"Get the current time\"\"\"\n    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n</code></pre>"},{"location":"api-reference/tool_calling/#human_in_the_loop_async","title":"human_in_the_loop_async","text":"<p>A decorator to add \"human-in-the-loop\" manual review capability to asynchronous tool functions.</p>"},{"location":"api-reference/tool_calling/#function-signature_3","title":"Function Signature","text":"<pre><code>def human_in_the_loop_async(\n    func: Optional[Callable] = None,\n    *,\n    handler: Optional[HumanInterruptHandler] = None\n) -&gt; Union[Callable[[Callable], BaseTool], BaseTool]\n</code></pre>"},{"location":"api-reference/tool_calling/#parameters_3","title":"Parameters","text":"Parameter Type Required Default Description func Optional[Callable] No None The asynchronous function to be decorated (decorator syntactic sugar) handler Optional[HumanInterruptHandler] No None Custom interrupt handler function"},{"location":"api-reference/tool_calling/#example_3","title":"Example","text":"<pre><code>@human_in_the_loop_async\nasync def get_current_time():\n    \"\"\"Get the current time\"\"\"\n    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n</code></pre>"},{"location":"api-reference/tool_calling/#interruptparams","title":"InterruptParams","text":"<p>The type of parameters passed to the interrupt handler function.</p>"},{"location":"api-reference/tool_calling/#class-definition","title":"Class Definition","text":"<pre><code>class InterruptParams(TypedDict):\n    tool_call_name: str\n    tool_call_args: Dict[str, Any]\n    tool: BaseTool\n</code></pre>"},{"location":"api-reference/tool_calling/#field-description","title":"Field Description","text":"Field Type Required Description tool_call_name str Yes The name of the tool call tool_call_args Dict[str, Any] Yes The arguments of the tool call tool BaseTool Yes The tool instance"},{"location":"api-reference/tool_calling/#humaninterrupthandler","title":"HumanInterruptHandler","text":"<p>Type alias for the interrupt handler function.</p>"},{"location":"api-reference/tool_calling/#type-definition","title":"Type Definition","text":"<pre><code>HumanInterruptHandler = Callable[[InterruptParams], Any]\n</code></pre>"},{"location":"getting-started-guide/chat/","title":"Chat Model Management","text":""},{"location":"getting-started-guide/chat/#overview","title":"Overview","text":"<p>LangChain's <code>init_chat_model</code> function only supports a limited number of model providers. This library offers a more flexible chat model management solution that supports custom model providers, particularly suitable for scenarios where you need to integrate model services not natively supported (such as vLLM).</p>"},{"location":"getting-started-guide/chat/#registering-model-providers","title":"Registering Model Providers","text":"<p>To register a chat model provider, call <code>register_model_provider</code>. The registration steps vary slightly for different situations.</p>"},{"location":"getting-started-guide/chat/#existing-langchain-chat-model-class","title":"Existing LangChain Chat Model Class","text":"<p>If the model provider already has a suitable LangChain integration (see Chat Model Class Integration), pass the corresponding integrated chat model class as the chat_model parameter.</p>"},{"location":"getting-started-guide/chat/#parameter-description","title":"Parameter Description","text":"Parameter Description <code>provider_name</code> Model provider name, used for subsequent reference in <code>load_chat_model</code>.Type: <code>str</code>Required: Yes <code>chat_model</code> LangChain chat model class.Type: <code>type[BaseChatModel]</code>Required: Yes <code>base_url</code> API base URL, usually no need to set manually.Type: <code>str</code>Required: No <code>model_profiles</code> Dictionary of model configuration information.Type: <code>dict</code>Required: No"},{"location":"getting-started-guide/chat/#code-example","title":"Code Example","text":"<pre><code>from langchain_core.language_models.fake_chat_models import FakeChatModel\nfrom langchain_dev_utils.chat_models import register_model_provider\n\nregister_model_provider(\n    provider_name=\"fake_provider\",\n    chat_model=FakeChatModel,\n)\n</code></pre>"},{"location":"getting-started-guide/chat/#usage-notes","title":"Usage Notes","text":"<ul> <li><code>FakeChatModel</code> is for testing only. In real usage, you must pass a <code>ChatModel</code> class with actual functionality.</li> <li><code>provider_name</code> is the name of the model provider, used later in <code>load_chat_model</code>.</li> </ul> <p>Note</p> <p><code>provider_name</code> must start with a letter or digit, can only contain letters, digits, and underscores, and must not exceed 20 characters in length.</p>"},{"location":"getting-started-guide/chat/#optional-parameter-description","title":"Optional Parameter Description","text":"<p>base_url</p> <p>This parameter usually does not need to be set (because the chat model class typically already defines a default API address). Only pass <code>base_url</code> when you need to override the default address defined by the chat model class, and it only takes effect for fields named <code>api_base</code> or <code>base_url</code> (including aliases).</p> <p>model_profiles</p> <p>If your LangChain integrated chat model class fully supports the <code>profile</code> parameter (i.e., you can directly access model-related properties through <code>model.profile</code>, such as <code>max_input_tokens</code>, <code>tool_calling</code>, etc.), there's no need to set <code>model_profiles</code> additionally.</p> <p>If accessing through <code>model.profile</code> returns an empty dictionary <code>{}</code>, it indicates that the LangChain chat model class may not support the <code>profile</code> parameter yet, in which case you can manually provide <code>model_profiles</code>.</p> <p><code>model_profiles</code> is a dictionary where each key is a model name, and the value is the profile configuration for the corresponding model:</p> <pre><code>{\n    \"model_name_1\": {\n        \"max_input_tokens\": 100_000,\n        \"tool_calling\": True,\n        \"structured_output\": True,\n        # ... other optional fields\n    },\n    \"model_name_2\": {\n        \"max_input_tokens\": 32768,\n        \"image_inputs\": True,\n        \"tool_calling\": False,\n        # ... other optional fields\n    },\n    # you can have any number of model configurations\n}\n</code></pre> <p>Tip</p> <p>It's recommended to use the <code>langchain-model-profiles</code> library to get profiles for your model provider.</p>"},{"location":"getting-started-guide/chat/#no-langchain-chat-model-class-but-provider-supports-openai-compatible-api","title":"No LangChain Chat Model Class, but Provider Supports OpenAI Compatible API","text":"<p>The parameter description for this situation is as follows:</p>"},{"location":"getting-started-guide/chat/#parameter-description_1","title":"Parameter Description","text":"Parameter Description <code>provider_name</code> Model provider name.Type: <code>str</code>Required: Yes <code>chat_model</code> Fixed value <code>\"openai-compatible\"</code>.Type: <code>str</code>Required: Yes <code>base_url</code> API base URL.Type: <code>str</code>Required: No <code>model_profiles</code> Dictionary of model configuration information.Type: <code>dict</code>Required: No <code>compatibility_options</code> Compatibility options configuration.Type: <code>dict</code>Required: No"},{"location":"getting-started-guide/chat/#code-example_1","title":"Code Example","text":"<p>Method 1: Explicit Parameters</p> <pre><code>register_model_provider(\n    provider_name=\"vllm\",\n    chat_model=\"openai-compatible\",\n    base_url=\"http://localhost:8000/v1\"\n)\n</code></pre> <p>Method 2: Through Environment Variables (Recommended for Configuration Management)</p> <pre><code>export VLLM_API_BASE=http://localhost:8000/v1\n</code></pre> <pre><code>register_model_provider(\n    provider_name=\"vllm\",\n    chat_model=\"openai-compatible\"\n    # Automatically reads VLLM_API_BASE\n)\n</code></pre> <p>Note: For more details on this part, please refer to OpenAI Compatible API Integration.</p>"},{"location":"getting-started-guide/chat/#batch-registration","title":"Batch Registration","text":"<p>If you need to register multiple providers, you can use <code>batch_register_model_provider</code> to avoid repeated calls.</p>"},{"location":"getting-started-guide/chat/#parameter-description_2","title":"Parameter Description","text":"Parameter Description <code>providers</code> List of provider configurations, each dictionary contains registration parameters.Type: <code>list[dict]</code>Required: Yes"},{"location":"getting-started-guide/chat/#code-example_2","title":"Code Example","text":"<pre><code>from langchain_dev_utils.chat_models import batch_register_model_provider\nfrom langchain_core.language_models.fake_chat_models import FakeChatModel\n\nbatch_register_model_provider(\n    providers=[\n        {\n            \"provider_name\": \"fake_provider\",\n            \"chat_model\": FakeChatModel,\n        },\n        {\n            \"provider_name\": \"vllm\",\n            \"chat_model\": \"openai-compatible\",\n            \"base_url\": \"http://localhost:8000/v1\",\n        },\n    ]\n)\n</code></pre> <p>Note</p> <p>Both registration functions are implemented based on a global dictionary. To avoid multi-threading issues, all registrations must be completed during the application startup phase, and dynamic registration during runtime is prohibited.</p> <p>Additionally, when registering with <code>chat_model</code> set to <code>openai-compatible</code>, the system internally uses <code>pydantic.create_model</code> to dynamically create new model classes (with <code>BaseChatOpenAICompatible</code> as the base class, generating corresponding chat model integration classes). This process involves Python metaclass operations and pydantic validation logic initialization, which has certain performance overhead, so please avoid frequent registration during runtime.</p>"},{"location":"getting-started-guide/chat/#loading-chat-models","title":"Loading Chat Models","text":"<p>Use the <code>load_chat_model</code> function to load chat models (initialize chat model instances).</p>"},{"location":"getting-started-guide/chat/#parameter-description_3","title":"Parameter Description","text":"Parameter Description <code>model</code> Model name.Type: <code>str</code>Required: Yes <code>model_provider</code> Model provider name.Type: <code>str</code>Required: No <p>In addition, any number of keyword arguments can be passed to provide additional parameters for the chat model class.</p>"},{"location":"getting-started-guide/chat/#parameter-rules","title":"Parameter Rules","text":"<ul> <li>If <code>model_provider</code> is not passed, then <code>model</code> must be in the format <code>provider_name:model_name</code>;</li> <li>If <code>model_provider</code> is passed, then <code>model</code> must only be <code>model_name</code>.</li> </ul>"},{"location":"getting-started-guide/chat/#code-example_3","title":"Code Example","text":"<pre><code># Method 1: model includes provider information\nmodel = load_chat_model(\"vllm:qwen3-4b\")\n\n# Method 2: specify provider separately\nmodel = load_chat_model(\"qwen3-4b\", model_provider=\"vllm\")\n</code></pre>"},{"location":"getting-started-guide/chat/#model-methods-and-parameters","title":"Model Methods and Parameters","text":"<p>For supported model methods and parameters, refer to the usage instructions of the corresponding chat model class. If you're using the second situation, all methods and parameters of the <code>BaseChatOpenAI</code> class are supported.</p>"},{"location":"getting-started-guide/chat/#compatibility-with-official-providers","title":"Compatibility with Official Providers","text":"<p>For providers already supported by LangChain (such as <code>openai</code>), you can directly use <code>load_chat_model</code> without registration:</p> <pre><code>model = load_chat_model(\"openai:gpt-4o-mini\")\n# or\nmodel = load_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n</code></pre> <p>Best Practice</p> <p>For the use of this module, you can choose based on the following three situations:</p> <ol> <li> <p>If all model providers you're integrating are supported by the official <code>init_chat_model</code>, please use the official function directly for the best compatibility and stability.</p> </li> <li> <p>If some model providers you're integrating are not officially supported, you can use the functionality of this module. First, use <code>register_model_provider</code> to register the model provider, then use <code>load_chat_model</code> to load the model.</p> </li> <li> <p>If the model provider you're integrating doesn't have a suitable integration yet, but the provider offers an OpenAI-compatible API (such as vLLM), it's recommended to use the functionality of this module. First, use <code>register_model_provider</code> to register the model provider (passing <code>openai-compatible</code> for chat_model), then use <code>load_chat_model</code> to load the model.</p> </li> </ol>"},{"location":"getting-started-guide/embedding/","title":"Embedding Model Management","text":""},{"location":"getting-started-guide/embedding/#overview","title":"Overview","text":"<p>LangChain's <code>init_embeddings</code> function only supports a limited number of embedding model providers. This library provides a more flexible embedding model management solution, particularly suitable for scenarios where you need to integrate embedding services not natively supported (such as vLLM).</p>"},{"location":"getting-started-guide/embedding/#registering-embedding-model-providers","title":"Registering Embedding Model Providers","text":"<p>To register an embedding model provider, call <code>register_embeddings_provider</code>. The registration method varies slightly depending on the type of <code>embeddings_model</code>.</p>"},{"location":"getting-started-guide/embedding/#existing-langchain-embedding-model-class","title":"Existing LangChain Embedding Model Class","text":"<p>If the embedding model provider already has a ready and suitable LangChain integration (see Embedding Model Integration List), directly pass the corresponding embedding model class to the <code>embeddings_model</code> parameter.</p>"},{"location":"getting-started-guide/embedding/#parameter-description","title":"Parameter Description","text":"Parameter Description <code>provider_name</code> Model provider name, used for subsequent reference in <code>load_embeddings</code>.Type: <code>str</code>Required: Yes <code>embeddings_model</code> LangChain embedding model class.Type: <code>type[Embeddings]</code>Required: Yes <code>base_url</code> API base URL, usually no need to set manually.Type: <code>str</code>Required: No"},{"location":"getting-started-guide/embedding/#code-example","title":"Code Example","text":"<pre><code>from langchain_core.embeddings.fake import FakeEmbeddings\nfrom langchain_dev_utils.embeddings import register_embeddings_provider\n\nregister_embeddings_provider(\n    provider_name=\"fake_provider\",\n    embeddings_model=FakeEmbeddings,\n)\n</code></pre>"},{"location":"getting-started-guide/embedding/#usage-instructions","title":"Usage Instructions","text":"<ul> <li><code>FakeEmbeddings</code> is only for testing. In actual use, you must pass an <code>Embeddings</code> class with real functionality.</li> <li><code>provider_name</code> represents the name of the model provider, used for subsequent reference in <code>load_embeddings</code>. The name can be customized, but should not contain special characters such as <code>:</code>, <code>-</code>, etc.</li> </ul> <p>Warning</p> <p><code>provider_name</code> must start with a letter or number, can only contain letters, numbers, and underscores, and must be 20 characters or fewer.</p>"},{"location":"getting-started-guide/embedding/#optional-parameters","title":"Optional Parameters","text":"<p>base_url</p> <p>This parameter usually does not need to be set (because the embedding model class already defines a default API address internally). Only pass <code>base_url</code> when you need to override the default address of the embedding model class, and it only takes effect for fields named <code>api_base</code> or <code>base_url</code> (including aliases).</p>"},{"location":"getting-started-guide/embedding/#no-langchain-embedding-model-class-but-provider-supports-openai-compatible-api","title":"No LangChain Embedding Model Class, but Provider Supports OpenAI Compatible API","text":"<p>The parameter description for this situation is as follows:</p>"},{"location":"getting-started-guide/embedding/#parameter-description_1","title":"Parameter Description","text":"Parameter Description <code>provider_name</code> Model provider name, used for subsequent reference in <code>load_embeddings</code>.Type: <code>str</code>Required: Yes <code>embeddings_model</code> Fixed value <code>\"openai-compatible\"</code>.Type: <code>str</code>Required: Yes <code>base_url</code> API base URL.Type: <code>str</code>Required: No"},{"location":"getting-started-guide/embedding/#code-example_1","title":"Code Example","text":"<p>Method 1: Explicit Parameters</p> <pre><code>register_embeddings_provider(\n    provider_name=\"vllm\",\n    embeddings_model=\"openai-compatible\",\n    base_url=\"http://localhost:8000/v1\"\n)\n</code></pre> <p>Method 2: Environment Variables (Recommended)</p> <pre><code>export VLLM_API_BASE=http://localhost:8000/v1\n</code></pre> <pre><code>register_embeddings_provider(\n    provider_name=\"vllm\",\n    embeddings_model=\"openai-compatible\"\n    # Automatically reads VLLM_API_BASE\n)\n</code></pre> <p>Note: For more details on this part, please refer to OpenAI Compatible API Integration.</p>"},{"location":"getting-started-guide/embedding/#batch-registration","title":"Batch Registration","text":"<p>If you need to register multiple providers, you can use <code>batch_register_embeddings_provider</code>.</p>"},{"location":"getting-started-guide/embedding/#parameter-description_2","title":"Parameter Description","text":"Parameter Description <code>providers</code> List of provider configurations, each dictionary contains registration parameters.Type: <code>list[dict]</code>Required: Yes"},{"location":"getting-started-guide/embedding/#code-example_2","title":"Code Example","text":"<pre><code>from langchain_dev_utils.embeddings import batch_register_embeddings_provider\nfrom langchain_core.embeddings.fake import FakeEmbeddings\n\nbatch_register_embeddings_provider(\n    providers=[\n        {\n            \"provider_name\": \"fake_provider\",\n            \"embeddings_model\": FakeEmbeddings,\n        },\n        {\n            \"provider_name\": \"vllm\",\n            \"embeddings_model\": \"openai-compatible\",\n            \"base_url\": \"http://localhost:8000/v1\",\n        },\n    ]\n)\n</code></pre> <p>Note</p> <p>Both registration functions are implemented based on a global dictionary. All registrations must be completed during the application startup phase, and dynamic registration during runtime is prohibited to avoid multi-threading issues.</p> <p>Additionally, when registering with <code>embeddings_model</code> set to <code>openai-compatible</code>, the system internally uses <code>pydantic.create_model</code> to dynamically create new model classes (with <code>BaseEmbeddingOpenAICompatible</code> as the base class, generating corresponding embedding model integration classes). This process involves Python metaclass operations and pydantic validation logic initialization, which has certain performance overhead, so please avoid frequent registration during runtime.</p>"},{"location":"getting-started-guide/embedding/#loading-embedding-models","title":"Loading Embedding Models","text":"<p>Use <code>load_embeddings</code> to initialize embedding model instances.</p>"},{"location":"getting-started-guide/embedding/#parameter-description_3","title":"Parameter Description","text":"Parameter Type Required Default Description <code>model</code> <code>str</code> Yes - Model name <code>provider</code> <code>str</code> No <code>None</code> Model provider name <p>In addition, any number of keyword arguments can be passed to provide additional parameters for the embedding model class.</p>"},{"location":"getting-started-guide/embedding/#parameter-rules","title":"Parameter Rules","text":"<ul> <li>If <code>provider</code> is not passed, then <code>model</code> must be in the format <code>provider_name:embeddings_name</code>;</li> <li>If <code>provider</code> is passed, then <code>model</code> is only <code>embeddings_name</code>.</li> </ul>"},{"location":"getting-started-guide/embedding/#code-example_3","title":"Code Example","text":"<pre><code># Method 1: model includes provider information\nembedding = load_embeddings(\"vllm:qwen3-embedding-4b\")\n\n# Method 2: specify provider separately\nembedding = load_embeddings(\"qwen3-embedding-4b\", provider=\"vllm\")\n</code></pre>"},{"location":"getting-started-guide/embedding/#model-methods-and-parameters","title":"Model Methods and Parameters","text":"<p>For supported model methods and parameters, refer to the usage instructions of the corresponding embedding model class. If you're using the second situation, all methods and parameters of the <code>OpenAIEmbeddings</code> class are supported.</p>"},{"location":"getting-started-guide/embedding/#compatibility-with-official-providers","title":"Compatibility with Official Providers","text":"<p>For providers already supported by LangChain (such as <code>openai</code>), you can directly use <code>load_embeddings</code> without registration:</p> <pre><code>model = load_embeddings(\"openai:text-embedding-3-large\")\n# or\nmodel = load_embeddings(\"text-embedding-3-large\", provider=\"openai\")\n</code></pre> <p>Best Practice</p> <p>For the use of this module, you can choose based on the following three situations:</p> <ol> <li> <p>If all embedding model providers you're integrating are supported by the official <code>init_embeddings</code>, please use the official function directly for the best compatibility.</p> </li> <li> <p>If some embedding model providers you're integrating are not officially supported, you can use the registration and loading mechanism of this module. First, use <code>register_embeddings_provider</code> to register the model provider, then use <code>load_embeddings</code> to load the model.</p> </li> <li> <p>If the embedding model provider you're integrating doesn't have a suitable integration yet, but the provider offers an OpenAI-compatible API (such as vLLM), it's recommended to use the functionality of this module. First, use <code>register_embeddings_provider</code> to register the model provider (passing <code>openai-compatible</code> for embeddings_model), then use <code>load_embeddings</code> to load the model.</p> </li> </ol>"},{"location":"getting-started-guide/format/","title":"Formatting Sequence","text":""},{"location":"getting-started-guide/format/#overview","title":"Overview","text":"<p>Used to format a list consisting of Documents, Messages, or strings into a single text string. The specific function is <code>format_sequence</code>.</p>"},{"location":"getting-started-guide/format/#usage-examples","title":"Usage Examples","text":""},{"location":"getting-started-guide/format/#message","title":"Message","text":""},{"location":"getting-started-guide/format/#code-example","title":"Code Example","text":"<pre><code>from langchain_core.documents import Document\nfrom langchain_core.messages import AIMessage\nfrom langchain_dev_utils.message_convert import format_sequence\n\nformated1 = format_sequence(\n    [\n        AIMessage(content=\"Hello1\"),\n        AIMessage(content=\"Hello2\"),\n        AIMessage(content=\"Hello3\"),\n    ]\n)\nprint(formated1)\n</code></pre>"},{"location":"getting-started-guide/format/#output-result","title":"Output Result","text":"<pre><code>-Hello1\n-Hello2\n-Hello3\n</code></pre>"},{"location":"getting-started-guide/format/#document","title":"Document","text":""},{"location":"getting-started-guide/format/#code-example_1","title":"Code Example","text":"<pre><code>format2 = format_sequence(\n    [\n        Document(page_content=\"content1\"),\n        Document(page_content=\"content2\"),\n        Document(page_content=\"content3\"),\n    ],\n    separator=\"&gt;\",\n)\nprint(format2)\n</code></pre>"},{"location":"getting-started-guide/format/#output-result_1","title":"Output Result","text":"<pre><code>&gt;content1\n&gt;content2\n&gt;content3\n</code></pre>"},{"location":"getting-started-guide/format/#string","title":"String","text":""},{"location":"getting-started-guide/format/#code-example_2","title":"Code Example","text":"<pre><code>format3 = format_sequence(\n    [\n        \"str1\",\n        \"str2\",\n        \"str3\",\n    ],\n    separator=\"&gt;\",\n    with_num=True,\n)\nprint(format3)\n</code></pre>"},{"location":"getting-started-guide/format/#output-result_2","title":"Output Result","text":"<pre><code>&gt;1. str1\n&gt;2. str2\n&gt;3. str3\n</code></pre>"},{"location":"getting-started-guide/installation/","title":"Installation","text":"<p><code>langchain-dev-utils</code> supports installation via multiple package managers, including <code>pip</code>, <code>poetry</code>, and <code>uv</code>.</p> <p>To install the base version of <code>langchain-dev-utils</code>:</p> pippoetryuv <pre><code>pip install -U langchain-dev-utils\n</code></pre> <pre><code>poetry add langchain-dev-utils\n</code></pre> <pre><code>uv add langchain-dev-utils\n</code></pre> <p>To install the full-featured version of <code>langchain-dev-utils</code>:</p> pippoetryuv <pre><code>pip install -U \"langchain-dev-utils[standard]\"\n</code></pre> <pre><code>poetry add \"langchain-dev-utils[standard]\"\n</code></pre> <pre><code>uv add langchain-dev-utils[standard]\n</code></pre>"},{"location":"getting-started-guide/installation/#verifying-the-installation","title":"Verifying the Installation","text":"<p>After installation, verify that the package is correctly installed:</p> <pre><code>import langchain_dev_utils\nprint(langchain_dev_utils.__version__)\n</code></pre>"},{"location":"getting-started-guide/installation/#dependencies","title":"Dependencies","text":"<p>The package automatically installs the following dependencies:</p> <ul> <li><code>langchain</code></li> <li><code>langgraph</code> (installed alongside <code>langchain</code>)</li> </ul> <p>If you install the <code>standard</code> version, the following additional dependencies will also be installed:</p> <ul> <li><code>langchain-openai</code> (for model management)</li> <li><code>json-repair</code> (for fixing tool-call errors in middleware)</li> </ul>"},{"location":"getting-started-guide/message/","title":"Message Processing","text":""},{"location":"getting-started-guide/message/#overview","title":"Overview","text":"<p>Main features include:</p> <ul> <li>Merge reasoning content into final responses</li> <li>Merge streaming output Chunks</li> </ul>"},{"location":"getting-started-guide/message/#merge-reasoning-content-into-final-response","title":"Merge Reasoning Content into Final Response","text":"<p>Used to merge reasoning content (<code>reasoning_content</code>) into the final response (<code>content</code>).</p>"},{"location":"getting-started-guide/message/#function-description","title":"Function Description","text":"Function Description <code>convert_reasoning_content_for_ai_message</code> Merge reasoning content in AIMessage into the content field (for model's invoke and ainvoke) <code>convert_reasoning_content_for_chunk_iterator</code> Merge reasoning content in streaming responses into the content field (for model's stream) <code>aconvert_reasoning_content_for_chunk_iterator</code> Async version of <code>convert_reasoning_content_for_chunk_iterator</code>, for async streaming processing (for model's astream)"},{"location":"getting-started-guide/message/#code-example","title":"Code Example","text":"<pre><code>from langchain_dev_utils.message_convert import (\n    convert_reasoning_content_for_ai_message,\n    convert_reasoning_content_for_chunk_iterator,\n)\n\nresponse = model.invoke(\"\u4f60\u597d\")\nconverted_response = convert_reasoning_content_for_ai_message(\n    response, think_tag=(\"&lt;start&gt;\", \"&lt;end&gt;\")\n)\nprint(converted_response.content)\n\nfor chunk in convert_reasoning_content_for_chunk_iterator(\n    model.stream(\"\u4f60\u597d\"), think_tag=(\"&lt;start&gt;\", \"&lt;end&gt;\")\n):\n    print(chunk.content, end=\"\", flush=True)\n</code></pre>"},{"location":"getting-started-guide/message/#merge-streaming-output-chunks","title":"Merge Streaming Output Chunks","text":"<p>Provides utility functions to merge multiple AIMessageChunks generated due to streaming output into a single AIMessage.</p>"},{"location":"getting-started-guide/message/#core-functions","title":"Core Functions","text":"Function Description <code>merge_ai_message_chunk</code> Merge AI message chunks"},{"location":"getting-started-guide/message/#code-example_1","title":"Code Example","text":"<pre><code>from langchain_dev_utils.message_convert import merge_ai_message_chunk\n\nchunks = []\nfor chunk in model.stream(\"\u4f60\u597d\"):\n    chunks.append(chunk)\n\nmerged_message = merge_ai_message_chunk(chunks)\nprint(merged_message)\n</code></pre>"},{"location":"getting-started-guide/tool/","title":"Tool Call Processing","text":""},{"location":"getting-started-guide/tool/#overview","title":"Overview","text":"<p>Provides utilities for detecting and parsing tool call arguments.</p>"},{"location":"getting-started-guide/tool/#detect-tool-calls","title":"Detect Tool Calls","text":"<p>Detects whether a message contains a tool call. The core function is <code>has_tool_calling</code>.</p>"},{"location":"getting-started-guide/tool/#code-example","title":"Code Example","text":"<pre><code>import datetime\nfrom langchain_core.tools import tool\nfrom langchain_dev_utils.tool_calling import has_tool_calling\n\n@tool\ndef get_current_time() -&gt; str:\n    \"\"\"Get the current timestamp\"\"\"\n    return str(datetime.datetime.now().timestamp())\n\nresponse = model.bind_tools([get_current_time]).invoke(\"What time is it now?\")\nprint(has_tool_calling(response))\n</code></pre>"},{"location":"getting-started-guide/tool/#parse-tool-call-arguments","title":"Parse Tool Call Arguments","text":"<p>Provides a utility function to parse tool call arguments, extracting parameter information from a message. The core function is <code>parse_tool_calling</code>.</p>"},{"location":"getting-started-guide/tool/#code-example_1","title":"Code Example","text":"<pre><code>import datetime\nfrom langchain_core.tools import tool\nfrom langchain_dev_utils.tool_calling import has_tool_calling, parse_tool_calling\n\n@tool\ndef get_current_time() -&gt; str:\n    \"\"\"Get the current timestamp\"\"\"\n    return str(datetime.datetime.now().timestamp())\n\nresponse = model.bind_tools([get_current_time]).invoke(\"What time is it now?\")\n\nif has_tool_calling(response):\n    name, args = parse_tool_calling(\n        response, first_tool_call_only=True\n    )\n    print(name, args)\n</code></pre>"},{"location":"zh/","title":"\ud83e\udd9c\ufe0f\ud83e\uddf0 langchain-dev-utils","text":"<p> \ud83d\ude80 \u4e13\u4e3a LangChain \u548c LangGraph \u5f00\u53d1\u8005\u6253\u9020\u7684\u9ad8\u6548\u5de5\u5177\u5e93 </p> <p> </p>"},{"location":"zh/#langchain-dev-utils_1","title":"\u4e3a\u4ec0\u4e48\u9009\u62e9 langchain-dev-utils\uff1f","text":"<p>\u538c\u5026\u4e86\u5728 LangChain \u5f00\u53d1\u4e2d\u7f16\u5199\u91cd\u590d\u4ee3\u7801\uff1f<code>langchain-dev-utils</code> \u6b63\u662f\u60a8\u9700\u8981\u7684\u89e3\u51b3\u65b9\u6848\uff01\u8fd9\u4e2a\u8f7b\u91cf\u4f46\u529f\u80fd\u5f3a\u5927\u7684\u5de5\u5177\u5e93\u4e13\u4e3a\u63d0\u5347 LangChain \u548c LangGraph \u5f00\u53d1\u4f53\u9a8c\u800c\u8bbe\u8ba1\uff0c\u5e2e\u52a9\u60a8\uff1a</p> <ul> <li>\u63d0\u5347\u5f00\u53d1\u6548\u7387 - \u51cf\u5c11\u6837\u677f\u4ee3\u7801\uff0c\u8ba9\u60a8\u4e13\u6ce8\u4e8e\u6838\u5fc3\u529f\u80fd</li> <li>\u7b80\u5316\u590d\u6742\u6d41\u7a0b - \u8f7b\u677e\u7ba1\u7406\u591a\u6a21\u578b\u3001\u591a\u5de5\u5177\u548c\u591a\u667a\u80fd\u4f53\u5e94\u7528</li> <li>\u589e\u5f3a\u4ee3\u7801\u8d28\u91cf - \u63d0\u9ad8\u4e00\u81f4\u6027\u548c\u53ef\u8bfb\u6027\uff0c\u51cf\u5c11\u7ef4\u62a4\u6210\u672c</li> <li>\u52a0\u901f\u539f\u578b\u5f00\u53d1 - \u5feb\u901f\u5b9e\u73b0\u60f3\u6cd5\uff0c\u66f4\u5feb\u8fed\u4ee3\u9a8c\u8bc1</li> </ul>"},{"location":"zh/#_1","title":"\u6838\u5fc3\u529f\u80fd","text":"<ul> <li> <p> \u7edf\u4e00\u7684\u6a21\u578b\u7ba1\u7406</p> <p>\u901a\u8fc7\u5b57\u7b26\u4e32\u6307\u5b9a\u6a21\u578b\u63d0\u4f9b\u5546\uff0c\u8f7b\u677e\u5207\u6362\u548c\u7ec4\u5408\u4e0d\u540c\u6a21\u578b\u3002</p> </li> <li> <p> \u5185\u7f6eOpenAI-Compatible\u96c6\u6210\u7c7b</p> <p>\u5185\u7f6e OpenAI-Compatible API\u96c6\u6210\u7c7b\uff0c\u53ef\u901a\u8fc7\u663e\u5f0f\u914d\u7f6e\uff0c\u63d0\u5347\u6a21\u578b\u517c\u5bb9\u6027\u3002</p> </li> <li> <p> \u7075\u6d3b\u7684\u6d88\u606f\u5904\u7406</p> <p>\u652f\u6301\u601d\u7ef4\u94fe\u62fc\u63a5\u3001\u6d41\u5f0f\u5904\u7406\u548c\u6d88\u606f\u683c\u5f0f\u5316</p> </li> <li> <p> \u5f3a\u5927\u7684\u5de5\u5177\u8c03\u7528</p> <p>\u5185\u7f6e\u5de5\u5177\u8c03\u7528\u68c0\u6d4b\u3001\u53c2\u6570\u89e3\u6790\u548c\u4eba\u5de5\u5ba1\u6838\u529f\u80fd</p> </li> <li> <p> \u9ad8\u6548\u7684 Agent \u5f00\u53d1</p> <p>\u7b80\u5316\u667a\u80fd\u4f53\u521b\u5efa\u6d41\u7a0b\uff0c\u6269\u5145\u66f4\u591a\u7684\u5e38\u7528\u4e2d\u95f4\u4ef6</p> </li> <li> <p> \u7075\u6d3b\u7684\u72b6\u6001\u56fe\u7ec4\u5408</p> <p>\u652f\u6301\u4e32\u884c\u548c\u5e76\u884c\u65b9\u5f0f\u7ec4\u5408\u591a\u4e2a StateGraph</p> </li> </ul>"},{"location":"zh/#_2","title":"\u5feb\u901f\u5f00\u59cb","text":"<p>1. \u5b89\u88c5 <code>langchain-dev-utils</code></p> <pre><code>pip install -U \"langchain-dev-utils[standard]\"\n</code></pre> <p>2. \u5f00\u59cb\u4f7f\u7528</p> <pre><code>from langchain.tools import tool\nfrom langchain_core.messages import HumanMessage\nfrom langchain_dev_utils.chat_models import register_model_provider, load_chat_model\nfrom langchain_dev_utils.agents import create_agent\n\n# \u6ce8\u518c\u6a21\u578b\u63d0\u4f9b\u5546\nregister_model_provider(\"vllm\", \"openai-compatible\", base_url=\"http://localhost:8000/v1\")\n\n@tool\ndef get_current_weather(location: str) -&gt; str:\n    \"\"\"\u83b7\u53d6\u6307\u5b9a\u5730\u70b9\u7684\u5f53\u524d\u5929\u6c14\"\"\"\n    return f\"25\u5ea6\uff0c{location}\"\n\n# \u4f7f\u7528\u5b57\u7b26\u4e32\u52a8\u6001\u52a0\u8f7d\u6a21\u578b\nmodel = load_chat_model(\"vllm:qwen3-4b\")\nresponse = model.invoke(\"\u4f60\u597d\")\nprint(response)\n\n# \u521b\u5efa\u667a\u80fd\u4f53\nagent = create_agent(\"vllm:qwen3-4b\", tools=[get_current_weather])\nresponse = agent.invoke({\"messages\": [HumanMessage(content=\"\u4eca\u5929\u7ebd\u7ea6\u7684\u5929\u6c14\u5982\u4f55\uff1f\")]})\nprint(response)\n</code></pre>"},{"location":"zh/#github","title":"GitHub \u4ed3\u5e93","text":"<p>\u8bbf\u95ee GitHub \u4ed3\u5e93 \u67e5\u770b\u6e90\u4ee3\u7801\u548c\u95ee\u9898\u3002</p>"},{"location":"zh/example-project/","title":"Langchain-dev-utils Example Project","text":"<p>\u8be5\u4ed3\u5e93\u63d0\u4f9b\u4e86\u4e00\u4e2a\u793a\u4f8b\u9879\u76ee<code>langchain-dev-utils-example</code>\uff0c\u76ee\u7684\u662f\u4e3a\u4e86\u5e2e\u52a9\u5f00\u53d1\u8005\u5feb\u901f\u4e86\u89e3\u5982\u4f55\u5229\u7528 <code>langchain-dev-utils</code> \u63d0\u4f9b\u7684\u5de5\u5177\u51fd\u6570\uff0c\u9ad8\u6548\u6784\u5efa\u4e24\u79cd\u5178\u578b\u7684\u667a\u80fd\u4f53\uff08agent\uff09\u7cfb\u7edf\uff1a</p> <ul> <li>\u5355\u667a\u80fd\u4f53\uff08Single Agent\uff09\uff1a\u9002\u7528\u4e8e\u6267\u884c\u7b80\u5355\u4efb\u52a1\u4ee5\u53ca\u957f\u671f\u8bb0\u5fc6\u5b58\u50a8\u76f8\u5173\u7684\u4efb\u52a1\u3002</li> <li>\u76d1\u7763\u8005-\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff08Supervisor-Multi-Agent Architecture\uff09\uff1a\u901a\u8fc7\u4e00\u4e2a\u4e2d\u592e\u76d1\u7763\u8005\u534f\u8c03\u591a\u4e2a\u4e13\u4e1a\u5316\u667a\u80fd\u4f53\uff0c\u9002\u7528\u4e8e\u9700\u8981\u4efb\u52a1\u5206\u89e3\u3001\u89c4\u5212\u548c\u8fed\u4ee3\u4f18\u5316\u7684\u590d\u6742\u573a\u666f\u3002</li> </ul> <p> </p>"},{"location":"zh/example-project/#_1","title":"\u5feb\u901f\u5f00\u59cb","text":"<ol> <li>\u514b\u9686\u672c\u4ed3\u5e93\uff1a <pre><code>git clone https://github.com/TBice123123/langchain-dev-utils-example.git  \ncd langchain-dev-utils-example\n</code></pre></li> <li>\u4f7f\u7528 uv \u5b89\u88c5\u4f9d\u8d56\uff1a <pre><code>uv sync\n</code></pre></li> <li>\u521b\u5efa.env\u6587\u4ef6 <pre><code>cp .env.example .env\n</code></pre></li> <li> <p>\u7f16\u8f91 <code>.env</code> \u6587\u4ef6\uff0c\u586b\u5165\u4f60\u7684 API \u5bc6\u94a5\uff08\u9700\u8981 <code>ZhipuAI</code> \u548c <code>Tavily</code> \u7684 API \u5bc6\u94a5\uff09\u3002</p> </li> <li> <p>\u542f\u52a8\u9879\u76ee <pre><code>langgraph dev\n</code></pre></p> </li> </ol>"},{"location":"zh/example-project/#_2","title":"\u4f7f\u7528\u7684\u529f\u80fd","text":"<p>\u5355\u667a\u80fd\u4f53\uff08Simple Agent\uff09\uff1a</p> <p>\u4f7f\u7528\u7684\u672c\u5e93\u7684\u529f\u80fd\uff1a</p> <ul> <li>\u5bf9\u8bdd\u6a21\u578b\u7ba1\u7406\uff08\u542bOpenAI\u517c\u5bb9API\u96c6\u6210\uff09\uff1a<code>register_model_provider</code>\u3001<code>load_chat_model</code></li> <li>\u5d4c\u5165\u6a21\u578b\u7ba1\u7406\uff1a<code>register_embeddings_provider</code>\u3001<code>load_embeddings</code></li> <li>\u683c\u5f0f\u5316\u5e8f\u5217\uff1a<code>format_sequence</code></li> <li>\u4e2d\u95f4\u4ef6\uff1a<code>format_prompt</code></li> </ul> <p>\u76d1\u7763\u8005-\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff08Supervisor-Multi-Agent Architecture\uff09\uff1a</p> <p>\u4f7f\u7528\u7684\u672c\u5e93\u7684\u529f\u80fd\uff1a</p> <ul> <li>\u5bf9\u8bdd\u6a21\u578b\u7ba1\u7406\uff08\u542bOpenAI\u517c\u5bb9API\u96c6\u6210\uff09\uff1a<code>register_model_provider</code>\u3001<code>load_chat_model</code></li> <li>\u591a\u667a\u80fd\u4f53\u6784\u5efa\uff1a<code>wrap_agent_as_tool</code></li> </ul>"},{"location":"zh/example-project/#_3","title":"\u5982\u4f55\u81ea\u5b9a\u4e49","text":"<p>\u53ef\u4ee5\u6839\u636e\u5b9e\u9645\u7684\u9700\u6c42\uff0c\u5bf9\u672c\u9879\u76ee\u8fdb\u884c\u81ea\u5b9a\u4e49\u4fee\u6539\u3002</p>"},{"location":"zh/example-project/#1","title":"1. \u66ff\u6362\u5bf9\u8bdd\u6a21\u578b\u63d0\u4f9b\u5546","text":"<p>\u672c\u9879\u76ee\u9ed8\u8ba4\u4f7f\u7528\u667a\u8c31AI\u7684GLM\u7cfb\u5217\u4f5c\u4e3a\u6838\u5fc3\u6a21\u578b\uff0c\u5177\u4f53\u5982\u4e0b\uff1a</p> <ul> <li><code>GLM-4.7</code>\uff1a\u7528\u4e8e<code>simple-agent</code></li> <li><code>GLM-4.6</code>\uff1a\u7528\u4e8e<code>supervisor-agent</code>\u7684<code>supervisor</code></li> <li><code>GLM-4.5</code>\uff1a\u7528\u4e8e<code>supervisor-agent</code>\u7684<code>subagent</code></li> <li><code>GLM-4.6V</code>\uff1a\u7528\u4e8e<code>supervisor-agent</code>\u7684<code>vision subagent</code></li> </ul> <p>\u5982\u9700\u81ea\u5b9a\u4e49\u6a21\u578b\u63d0\u4f9b\u5546\uff0c\u8bf7\u4fee\u6539<code>src/utils/providers/chat_models/register.py</code>\uff0c\u5728<code>register_all_model_providers</code>\u51fd\u6570\u4e2d\u4f7f\u7528<code>register_model_provider</code>\u51fd\u6570\u6ce8\u518c\u4f60\u7684\u6a21\u578b\u63d0\u4f9b\u5546\u3002</p> <p>\u540c\u65f6\u5efa\u8bae\u4fee\u6539<code>src/utils/providers/chat_models/load.py</code>\uff0c\u5728<code>load_chat_model</code>\u51fd\u6570\u4e2d\u6dfb\u52a0\u5bf9\u5e94\u7684\u52a0\u8f7d\u903b\u8f91\u3002</p> <p>\u5bf9\u8bdd\u6a21\u578b\u7ba1\u7406\u6700\u4f73\u5b9e\u8df5</p> <p><code>load_chat_model</code>\u51fd\u6570\u91c7\u7528\u5173\u952e\u5b57\u53c2\u6570\u63a5\u6536\u4e0d\u540c\u5bf9\u8bdd\u6a21\u578b\u7c7b\u7684\u989d\u5916\u53c2\u6570\uff08LangChain\u5b98\u65b9\u51fd\u6570\u4e5f\u91c7\u7528\u6b64\u65b9\u5f0f\uff09\u3002\u8fd9\u79cd\u65b9\u5f0f\u63d0\u5347\u4e86\u901a\u7528\u6027\uff0c\u4f46\u4f1a\u524a\u5f31IDE\u7c7b\u578b\u63d0\u793a\uff0c\u589e\u52a0\u53c2\u6570\u8bef\u7528\u98ce\u9669\u3002\u56e0\u6b64\uff0c\u82e5\u5df2\u786e\u5b9a\u5177\u4f53\u63d0\u4f9b\u5546\uff0c\u53ef\u9488\u5bf9\u5176\u96c6\u6210\u5bf9\u8bdd\u6a21\u578b\u7c7b\uff08\u6216\u5d4c\u5165\u6a21\u578b\u7c7b\uff09\u6269\u5c55\u53c2\u6570\u7b7e\u540d\u4ee5\u6062\u590d\u7c7b\u578b\u63d0\u793a\uff0c\u53c2\u8003<code>src/utils/providers/chat_models/load.py</code>\u8fdb\u884c\u9488\u5bf9\u6027\u4fee\u6539\u3002</p>"},{"location":"zh/example-project/#2","title":"2. \u6ce8\u518c\u5d4c\u5165\u6a21\u578b\u63d0\u4f9b\u5546","text":"<p>\u5d4c\u5165\u6a21\u578b\u63d0\u4f9b\u5546\u7684\u6ce8\u518c\u65b9\u5f0f\u4e0e\u5bf9\u8bdd\u6a21\u578b\u7c7b\u4f3c\u3002\u8bf7\u4fee\u6539<code>src/utils/providers/embeddings/register.py</code>\uff0c\u5728<code>register_all_embeddings_providers</code>\u51fd\u6570\u4e2d\u4f7f\u7528<code>register_embeddings_provider</code>\u51fd\u6570\u6ce8\u518c\u4f60\u7684\u5d4c\u5165\u6a21\u578b\u63d0\u4f9b\u5546\u3002</p> <p>\u5982\u9700\u81ea\u5b9a\u4e49\u52a0\u8f7d\u903b\u8f91\uff0c\u53ef\u4fee\u6539<code>src/utils/providers/embeddings/load.py</code>\uff0c\u5728<code>load_embeddings</code>\u51fd\u6570\u4e2d\u6dfb\u52a0\u76f8\u5e94\u7684\u52a0\u8f7d\u903b\u8f91\u3002</p>"},{"location":"zh/example-project/#3","title":"3. \u81ea\u5b9a\u4e49\u5de5\u5177","text":"<p>\u5355\u667a\u80fd\u4f53\uff08simple-agent\uff09 \u5de5\u5177\u5b9e\u73b0\u4f4d\u4e8e <code>src/agents/simple_agent/tools.py</code>\uff0c\u5df2\u5185\u7f6e\uff1a - <code>save_user_memory</code> \u2014\u2014 \u6301\u4e45\u5316\u7528\u6237\u8bb0\u5fc6 - <code>get_user_memory</code> \u2014\u2014 \u8bfb\u53d6\u7528\u6237\u8bb0\u5fc6  </p> <p>\u5982\u9700\u6269\u5c55\uff0c\u76f4\u63a5\u5728\u8be5\u6587\u4ef6\u5185\u65b0\u589e\u5bf9\u5e94\u7684\u5de5\u5177\u5b9e\u73b0\u5373\u53ef\u3002</p> <p>\u76d1\u7763\u8005-\u591a\u667a\u80fd\u4f53\uff08supervisor-agent\uff09 \u5de5\u5177\u5b9e\u73b0\u4f4d\u4e8e <code>src/agents/supervisor/subagent/tools.py</code>\u3002\u662f\u5b50\u667a\u80fd\u4f53\u7684\u5de5\u5177\u5b9e\u73b0\uff0c\u5982\u9700\u4e3a\u5b50\u667a\u80fd\u4f53\u6dfb\u52a0\u81ea\u5b9a\u4e49\u5de5\u5177\uff0c\u76f4\u63a5\u5728\u8be5\u6587\u4ef6\u5185\u65b0\u589e\u5bf9\u5e94\u7684\u5de5\u5177\u5b9e\u73b0\u5373\u53ef\u3002</p> <p>\u6ce8\u610f\uff1a<code>supervisor</code> \u9ed8\u8ba4\u4ec5\u6301\u6709\u201c\u8c03\u7528\u5b50\u667a\u80fd\u4f53\u201d\u7684\u4e24\u4e2a\u5de5\u5177\u3002\u82e5\u9700\u4e3a <code>supervisor</code> \u8ffd\u52a0\u81ea\u5b9a\u4e49\u5de5\u5177\uff0c\u5efa\u8bae\u5728 <code>src/agents/supervisor/</code> \u4e0b\u65b0\u5efa <code>tools.py</code>\uff0c\u7f16\u5199\u5b8c\u6210\u540e\u5728 <code>src/agents/supervisor/agent.py</code> \u4e2d\u5bfc\u5165\u5e76\u4f20\u9012\u7ed9 <code>create_agent</code> \u51fd\u6570\u5373\u53ef\u3002</p>"},{"location":"zh/adavance-guide/human-in-the-loop/","title":"\u4e3a\u5de5\u5177\u8c03\u7528\u6dfb\u52a0\u4eba\u5de5\u5ba1\u6838","text":""},{"location":"zh/adavance-guide/human-in-the-loop/#_2","title":"\u6982\u8ff0","text":"<p>\u672c\u5e93\u63d0\u4f9b\u4e86\u88c5\u9970\u5668\u51fd\u6570\uff0c\u7528\u4e8e\u4e3a\u5de5\u5177\u8c03\u7528\u6dfb\u52a0\"\u4eba\u5728\u56de\u8def\"\u5ba1\u6838\u652f\u6301\uff0c\u5728\u5de5\u5177\u6267\u884c\u671f\u95f4\u542f\u7528\u4eba\u5de5\u5ba1\u6838\u3002</p> \u88c5\u9970\u5668 \u9002\u7528\u573a\u666f <code>human_in_the_loop</code> \u7528\u4e8e\u540c\u6b65\u5de5\u5177\u51fd\u6570 <code>human_in_the_loop_async</code> \u7528\u4e8e\u5f02\u6b65\u5de5\u5177\u51fd\u6570"},{"location":"zh/adavance-guide/human-in-the-loop/#_3","title":"\u53c2\u6570\u8bf4\u660e","text":"\u53c2\u6570 \u8bf4\u660e <code>handler</code> \u81ea\u5b9a\u4e49\u5904\u7406\u51fd\u6570\uff0c\u82e5\u4e3a <code>None</code> \u5219\u4f7f\u7528\u9ed8\u8ba4\u5904\u7406\u51fd\u6570\u3002\u7c7b\u578b: <code>Callable</code>\u5fc5\u586b: \u5426"},{"location":"zh/adavance-guide/human-in-the-loop/#_4","title":"\u4f7f\u7528\u793a\u4f8b","text":""},{"location":"zh/adavance-guide/human-in-the-loop/#handler","title":"\u4f7f\u7528\u9ed8\u8ba4\u7684 handler","text":"<pre><code>from langchain_dev_utils.tool_calling import human_in_the_loop\nimport datetime\n\n\n@human_in_the_loop\ndef get_current_time() -&gt; str:\n    \"\"\"\u83b7\u53d6\u5f53\u524d\u65f6\u95f4\u6233\"\"\"\n    return str(datetime.datetime.now().timestamp())\n</code></pre>"},{"location":"zh/adavance-guide/human-in-the-loop/#_5","title":"\u5f02\u6b65\u5de5\u5177\u793a\u4f8b","text":"<pre><code>from langchain_dev_utils.tool_calling import human_in_the_loop_async\nimport asyncio\nimport datetime\n\n\n@human_in_the_loop_async\nasync def async_get_current_time() -&gt; str:\n    \"\"\"\u5f02\u6b65\u83b7\u53d6\u5f53\u524d\u65f6\u95f4\u6233\"\"\"\n    await asyncio.sleep(1)\n    return str(datetime.datetime.now().timestamp())\n</code></pre>"},{"location":"zh/adavance-guide/human-in-the-loop/#handler_1","title":"\u9ed8\u8ba4 handler \u7684\u5b9e\u73b0","text":"<p>\u9ed8\u8ba4 handler \u7684\u5b9e\u73b0\u5982\u4e0b\uff1a</p> <pre><code>def _get_human_in_the_loop_request(params: InterruptParams) -&gt; dict[str, Any]:\n    return {\n        \"action_request\": {\n            \"action\": params[\"tool_call_name\"],\n            \"args\": params[\"tool_call_args\"],\n        },\n        \"config\": {\n            \"allow_accept\": True,\n            \"allow_edit\": True,\n            \"allow_respond\": True,\n        },\n        \"description\": f\"Please review tool call: {params['tool_call_name']}\",\n    }\n\n\ndef default_handler(params: InterruptParams) -&gt; Any:\n    request = _get_human_in_the_loop_request(params)\n    response = interrupt(request)\n\n    if response[\"type\"] == \"accept\":\n        return params[\"tool\"].invoke(params[\"tool_call_args\"])\n    elif response[\"type\"] == \"edit\":\n        updated_args = response[\"args\"]\n        return params[\"tool\"].invoke(updated_args)\n    elif response[\"type\"] == \"response\":\n        return response[\"args\"]\n    else:\n        raise ValueError(f\"Unsupported interrupt response type: {response['type']}\")\n</code></pre>"},{"location":"zh/adavance-guide/human-in-the-loop/#_6","title":"\u4e2d\u65ad\u8bf7\u6c42\u683c\u5f0f","text":"<p>\u4e2d\u65ad\u65f6\u4f1a\u53d1\u9001\u5982\u4e0b JSON Schema \u683c\u5f0f\u7684\u8bf7\u6c42\uff1a</p> \u5b57\u6bb5 \u8bf4\u660e <code>action_request.action</code> \u5de5\u5177\u8c03\u7528\u540d\u79f0\u3002\u7c7b\u578b: <code>str</code> <code>action_request.args</code> \u5de5\u5177\u8c03\u7528\u53c2\u6570\u3002\u7c7b\u578b: <code>dict</code> <code>config.allow_accept</code> \u662f\u5426\u5141\u8bb8\u63a5\u53d7\u64cd\u4f5c\u3002\u7c7b\u578b: <code>bool</code> <code>config.allow_edit</code> \u662f\u5426\u5141\u8bb8\u7f16\u8f91\u53c2\u6570\u3002\u7c7b\u578b: <code>bool</code> <code>config.allow_respond</code> \u662f\u5426\u5141\u8bb8\u76f4\u63a5\u54cd\u5e94\u3002\u7c7b\u578b: <code>bool</code> <code>description</code> \u64cd\u4f5c\u63cf\u8ff0\u3002\u7c7b\u578b: <code>str</code>"},{"location":"zh/adavance-guide/human-in-the-loop/#_7","title":"\u4e2d\u65ad\u54cd\u5e94\u683c\u5f0f","text":"<p>\u54cd\u5e94\u65f6\u9700\u8981\u8fd4\u56de\u5982\u4e0b JSON Schema \u683c\u5f0f\u7684\u6570\u636e\uff1a</p> \u5b57\u6bb5 \u8bf4\u660e <code>type</code> \u54cd\u5e94\u7c7b\u578b\uff0c\u53ef\u9009\u503c\u4e3a <code>accept</code>\u3001<code>edit</code>\u3001<code>response</code>\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u662f <code>args</code> \u5f53 <code>type</code> \u4e3a <code>edit</code> \u6216 <code>response</code> \u65f6\uff0c\u5305\u542b\u66f4\u65b0\u540e\u7684\u53c2\u6570\u6216\u54cd\u5e94\u5185\u5bb9\u3002\u7c7b\u578b: <code>dict</code>\u5fc5\u586b: \u5426"},{"location":"zh/adavance-guide/human-in-the-loop/#handler_2","title":"\u81ea\u5b9a\u4e49 Handler \u793a\u4f8b","text":"<p>\u4f60\u53ef\u4ee5\u5b8c\u5168\u63a7\u5236\u4e2d\u65ad\u884c\u4e3a\uff0c\u4f8b\u5982\u53ea\u5141\u8bb8\"\u63a5\u53d7/\u62d2\u7edd\"\uff0c\u6216\u81ea\u5b9a\u4e49\u63d0\u793a\u8bed\uff1a</p> <pre><code>from typing import Any\nfrom langchain_dev_utils.tool_calling import human_in_the_loop_async, InterruptParams\nfrom langgraph.types import interrupt\n\n\nasync def custom_handler(params: InterruptParams) -&gt; Any:\n    response = interrupt(\n        f\"\u6211\u8981\u8c03\u7528\u5de5\u5177 {params['tool_call_name']}\uff0c\u53c2\u6570\u4e3a {params['tool_call_args']}\uff0c\u8bf7\u786e\u8ba4\u662f\u5426\u8c03\u7528\"\n    )\n    if response[\"type\"] == \"accept\":\n        return await params[\"tool\"].ainvoke(params[\"tool_call_args\"])\n    elif response[\"type\"] == \"reject\":\n        return \"\u7528\u6237\u62d2\u7edd\u8c03\u7528\u8be5\u5de5\u5177\"\n    else:\n        raise ValueError(f\"\u4e0d\u652f\u6301\u7684\u54cd\u5e94\u7c7b\u578b: {response['type']}\")\n\n\n@human_in_the_loop_async(handler=custom_handler)\nasync def get_weather(city: str) -&gt; str:\n    \"\"\"\u83b7\u53d6\u5929\u6c14\u4fe1\u606f\"\"\"\n    return f\"{city}\u5929\u6c14\u6674\u6717\"\n</code></pre> <p>\u6700\u4f73\u5b9e\u8df5</p> <p>\u8be5\u88c5\u9970\u5668\u5728\u5b9e\u73b0\u81ea\u5b9a\u4e49\u4eba\u5728\u56de\u8def\u903b\u8f91\u65f6\uff0c\u9700\u8981\u4f20\u5165 <code>handler</code> \u53c2\u6570\u3002\u6b64 <code>handler</code> \u53c2\u6570\u662f\u4e00\u4e2a\u51fd\u6570\uff0c\u5185\u90e8\u5fc5\u987b\u4f7f\u7528 LangGraph \u7684 <code>interrupt</code> \u51fd\u6570\u6765\u6267\u884c\u4e2d\u65ad\u64cd\u4f5c\u3002\u56e0\u6b64\uff0c\u5982\u679c\u4ec5\u4e3a\u5355\u4e2a\u5de5\u5177\u6dfb\u52a0\u81ea\u5b9a\u4e49\u7684\u4eba\u5728\u56de\u8def\u903b\u8f91\uff0c\u5efa\u8bae\u76f4\u63a5\u4f7f\u7528 LangGraph \u7684 <code>interrupt</code> \u51fd\u6570\u3002\u5f53\u591a\u4e2a\u5de5\u5177\u9700\u8981\u76f8\u540c\u81ea\u5b9a\u4e49\u4eba\u5728\u56de\u8def\u903b\u8f91\u65f6\uff0c\u4f7f\u7528\u672c\u88c5\u9970\u5668\u53ef\u4ee5\u6709\u6548\u907f\u514d\u4ee3\u7801\u91cd\u590d\u3002</p>"},{"location":"zh/adavance-guide/middleware/","title":"\u4e2d\u95f4\u4ef6","text":""},{"location":"zh/adavance-guide/middleware/#_2","title":"\u6982\u8ff0","text":"<p>\u4e2d\u95f4\u4ef6\u662f\u4e13\u95e8\u9488\u5bf9 LangChain \u9884\u6784\u5efa\u7684 Agent \u800c\u6784\u5efa\u7684\u7ec4\u4ef6\u3002\u5b98\u65b9\u63d0\u4f9b\u4e86\u4e00\u4e9b\u5185\u7f6e\u7684\u4e2d\u95f4\u4ef6\uff0c\u672c\u5e93\u5219\u6839\u636e\u5b9e\u9645\u4f7f\u7528\u573a\u666f\u63d0\u4f9b\u4e86\u66f4\u591a\u5b9e\u7528\u7684\u4e2d\u95f4\u4ef6\u3002</p> <p>\u672c\u5e93\u63d0\u4f9b\u7684\u4e2d\u95f4\u4ef6\u5305\u62ec\uff1a</p> <ul> <li><code>PlanMiddleware</code>\uff1a\u4efb\u52a1\u89c4\u5212\uff0c\u5c06\u590d\u6742\u4efb\u52a1\u62c6\u89e3\u4e3a\u6709\u5e8f\u5b50\u4efb\u52a1</li> <li><code>ModelRouterMiddleware</code>\uff1a\u6839\u636e\u8f93\u5165\u5185\u5bb9\u52a8\u6001\u8def\u7531\u5230\u6700\u9002\u914d\u7684\u6a21\u578b</li> <li><code>HandoffAgentMiddleware</code>\uff1a\u5728\u591a\u4e2a\u5b50 Agent \u4e4b\u95f4\u7075\u6d3b\u4ea4\u63a5\u4efb\u52a1</li> <li><code>ToolCallRepairMiddleware</code>\uff1a\u81ea\u52a8\u4fee\u590d\u5927\u6a21\u578b\u65e0\u6548\u5de5\u5177\u8c03\u7528</li> <li><code>format_prompt</code>\uff1a\u52a8\u6001\u683c\u5f0f\u5316\u7cfb\u7edf\u63d0\u793a\u8bcd\u4e2d\u7684\u5360\u4f4d\u7b26</li> </ul> <p>\u6b64\u5916\uff0c\u672c\u5e93\u8fd8\u6269\u5145\u4e86\u5b98\u65b9\u4e2d\u95f4\u4ef6\u7684\u529f\u80fd\uff0c\u652f\u6301\u901a\u8fc7\u5b57\u7b26\u4e32\u53c2\u6570\u6307\u5b9a\u6a21\u578b\uff1a</p> <ul> <li>SummarizationMiddleware</li> <li>LLMToolSelectorMiddleware</li> <li>ModelFallbackMiddleware</li> <li>LLMToolEmulator</li> </ul>"},{"location":"zh/adavance-guide/middleware/#_3","title":"\u4efb\u52a1\u89c4\u5212","text":"<p><code>PlanMiddleware</code> \u662f\u4e00\u4e2a\u7528\u4e8e\u5728\u6267\u884c\u590d\u6742\u4efb\u52a1\u524d\u8fdb\u884c\u7ed3\u6784\u5316\u5206\u89e3\u4e0e\u8fc7\u7a0b\u7ba1\u7406\u7684\u4e2d\u95f4\u4ef6\u3002</p> <p>\u8865\u5145\u8bf4\u660e</p> <p>\u4efb\u52a1\u89c4\u5212\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u4e0a\u4e0b\u6587\u5de5\u7a0b\u7ba1\u7406\u7b56\u7565\u3002\u5728\u6267\u884c\u4efb\u52a1\u4e4b\u524d\uff0c\u5927\u6a21\u578b\u9996\u5148\u5c06\u6574\u4f53\u4efb\u52a1\u62c6\u89e3\u4e3a\u591a\u4e2a\u6709\u5e8f\u7684\u5b50\u4efb\u52a1\uff0c\u5f62\u6210\u4efb\u52a1\u89c4\u5212\u5217\u8868\uff08\u5728\u672c\u5e93\u4e2d\u79f0\u4e3a plan\uff09\u3002\u968f\u540e\u6309\u987a\u5e8f\u6267\u884c\u5404\u5b50\u4efb\u52a1\uff0c\u5e76\u5728\u6bcf\u5b8c\u6210\u4e00\u4e2a\u6b65\u9aa4\u540e\u52a8\u6001\u66f4\u65b0\u4efb\u52a1\u72b6\u6001\uff0c\u76f4\u81f3\u6240\u6709\u5b50\u4efb\u52a1\u6267\u884c\u5b8c\u6bd5\u3002</p>"},{"location":"zh/adavance-guide/middleware/#_4","title":"\u53c2\u6570\u8bf4\u660e","text":"\u53c2\u6570 \u8bf4\u660e <code>system_prompt</code> \u7cfb\u7edf\u63d0\u793a\u8bcd\uff0c\u82e5\u4e3a <code>None</code> \u5219\u4f7f\u7528\u9ed8\u8ba4\u63d0\u793a\u8bcd\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u5426 <code>custom_plan_tool_descriptions</code> \u81ea\u5b9a\u4e49\u8ba1\u5212\u76f8\u5173\u5de5\u5177\u7684\u63cf\u8ff0\u3002\u7c7b\u578b: <code>dict</code>\u5fc5\u586b: \u5426 <code>use_read_plan_tool</code> \u662f\u5426\u542f\u7528\u8bfb\u8ba1\u5212\u5de5\u5177\u3002\u7c7b\u578b: <code>bool</code>\u5fc5\u586b: \u5426\u9ed8\u8ba4\u503c: <code>True</code> <p><code>custom_plan_tool_descriptions</code> \u5b57\u5178\u7684\u952e\u53ef\u53d6\u4ee5\u4e0b\u4e09\u4e2a\u503c\uff1a</p> \u952e \u8bf4\u660e <code>write_plan</code> \u5199\u8ba1\u5212\u5de5\u5177\u7684\u63cf\u8ff0 <code>finish_sub_plan</code> \u5b8c\u6210\u5b50\u8ba1\u5212\u5de5\u5177\u7684\u63cf\u8ff0 <code>read_plan</code> \u8bfb\u8ba1\u5212\u5de5\u5177\u7684\u63cf\u8ff0"},{"location":"zh/adavance-guide/middleware/#_5","title":"\u4f7f\u7528\u793a\u4f8b","text":"<pre><code>from langchain_dev_utils.agents.middleware import PlanMiddleware\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    middleware=[\n        PlanMiddleware(\n            custom_plan_tool_descriptions={\n                \"write_plan\": \"\u7528\u4e8e\u5199\u8ba1\u5212\uff0c\u5c06\u4efb\u52a1\u62c6\u89e3\u4e3a\u591a\u4e2a\u6709\u5e8f\u7684\u5b50\u4efb\u52a1\u3002\",\n                \"finish_sub_plan\": \"\u7528\u4e8e\u5b8c\u6210\u5b50\u4efb\u52a1\uff0c\u66f4\u65b0\u5b50\u4efb\u52a1\u72b6\u6001\u4e3a\u5df2\u5b8c\u6210\u3002\",\n                \"read_plan\": \"\u7528\u4e8e\u67e5\u8be2\u5f53\u524d\u7684\u4efb\u52a1\u89c4\u5212\u5217\u8868\u3002\"\n            },\n            use_read_plan_tool=True,  # \u5982\u679c\u4e0d\u4f7f\u7528\u8bfb\u8ba1\u5212\u5de5\u5177\uff0c\u53ef\u4ee5\u8bbe\u7f6e\u6b64\u53c2\u6570\u4e3a False\n        )\n    ],\n)\n\nresponse = agent.invoke(\n    {\"messages\": [HumanMessage(content=\"\u6211\u8981\u53bbNew York\u73a9\u51e0\u5929\uff0c\u5e2e\u6211\u89c4\u5212\u884c\u7a0b\")]}\n)\nprint(response)\n</code></pre>"},{"location":"zh/adavance-guide/middleware/#_6","title":"\u5de5\u5177\u8bf4\u660e","text":"<p><code>PlanMiddleware</code> \u8981\u6c42\u5fc5\u987b\u4f7f\u7528 <code>write_plan</code> \u548c <code>finish_sub_plan</code> \u4e24\u4e2a\u5de5\u5177\uff0c\u800c <code>read_plan</code> \u5de5\u5177\u9ed8\u8ba4\u542f\u7528\uff1b\u82e5\u4e0d\u9700\u8981\u4f7f\u7528\uff0c\u53ef\u5c06 <code>use_read_plan_tool</code> \u53c2\u6570\u8bbe\u4e3a <code>False</code>\u3002</p>"},{"location":"zh/adavance-guide/middleware/#to-do-list","title":"\u4e0e\u5b98\u65b9 To-do list \u4e2d\u95f4\u4ef6\u7684\u5bf9\u6bd4","text":"<p>\u672c\u4e2d\u95f4\u4ef6\u4e0e LangChain \u5b98\u65b9\u63d0\u4f9b\u7684 To-do list \u4e2d\u95f4\u4ef6 \u529f\u80fd\u5b9a\u4f4d\u76f8\u4f3c\uff0c\u4f46\u5728\u5de5\u5177\u8bbe\u8ba1\u4e0a\u5b58\u5728\u5dee\u5f02\uff1a</p> \u7279\u6027 \u5b98\u65b9 To-do list \u4e2d\u95f4\u4ef6 \u672c\u5e93 PlanMiddleware \u5de5\u5177\u6570\u91cf 1 \u4e2a\uff08<code>write_todo</code>\uff09 3 \u4e2a\uff08<code>write_plan</code>\u3001<code>finish_sub_plan</code>\u3001<code>read_plan</code>\uff09 \u529f\u80fd\u5b9a\u4f4d \u9762\u5411\u5f85\u529e\u6e05\u5355\uff08todo list\uff09 \u4e13\u95e8\u7528\u4e8e\u89c4\u5212\u5217\u8868\uff08plan list\uff09 \u64cd\u4f5c\u65b9\u5f0f \u6dfb\u52a0\u548c\u4fee\u6539\u901a\u8fc7\u4e00\u4e2a\u5de5\u5177\u5b8c\u6210 \u5199\u5165\u3001\u4fee\u6539\u3001\u67e5\u8be2\u5206\u522b\u7531\u4e0d\u540c\u5de5\u5177\u5b8c\u6210 <p>\u65e0\u8bba\u662f <code>todo</code> \u8fd8\u662f <code>plan</code>\uff0c\u5176\u672c\u8d28\u90fd\u662f\u540c\u4e00\u4e2a\u6982\u5ff5\u3002\u672c\u4e2d\u95f4\u4ef6\u533a\u522b\u4e8e\u5b98\u65b9\u7684\u5173\u952e\u70b9\u5728\u4e8e\u63d0\u4f9b\u4e86\u4e09\u4e2a\u4e13\u7528\u5de5\u5177\uff1a</p> <ul> <li><code>write_plan</code>\uff1a\u7528\u4e8e\u5199\u5165\u8ba1\u5212\u6216\u66f4\u65b0\u8ba1\u5212\u5185\u5bb9</li> <li><code>finish_sub_plan</code>\uff1a\u7528\u4e8e\u5728\u5b8c\u6210\u67d0\u4e2a\u5b50\u4efb\u52a1\u540e\u66f4\u65b0\u5176\u72b6\u6001</li> <li><code>read_plan</code>\uff1a\u7528\u4e8e\u67e5\u8be2\u8ba1\u5212\u5185\u5bb9</li> </ul>"},{"location":"zh/adavance-guide/middleware/#_7","title":"\u6a21\u578b\u8def\u7531","text":"<p><code>ModelRouterMiddleware</code> \u662f\u4e00\u4e2a\u7528\u4e8e\u6839\u636e\u8f93\u5165\u5185\u5bb9\u52a8\u6001\u8def\u7531\u5230\u6700\u9002\u914d\u6a21\u578b\u7684\u4e2d\u95f4\u4ef6\u3002\u5b83\u901a\u8fc7\u4e00\u4e2a\"\u8def\u7531\u6a21\u578b\"\u5206\u6790\u7528\u6237\u8bf7\u6c42\uff0c\u4ece\u9884\u5b9a\u4e49\u7684\u6a21\u578b\u5217\u8868\u4e2d\u9009\u62e9\u6700\u9002\u5408\u5f53\u524d\u4efb\u52a1\u7684\u6a21\u578b\u8fdb\u884c\u5904\u7406\u3002</p>"},{"location":"zh/adavance-guide/middleware/#_8","title":"\u53c2\u6570\u8bf4\u660e","text":"\u53c2\u6570 \u8bf4\u660e <code>router_model</code> \u7528\u4e8e\u6267\u884c\u8def\u7531\u51b3\u7b56\u7684\u6a21\u578b\u3002\u7c7b\u578b: <code>str</code> | <code>BaseChatModel</code>\u5fc5\u586b: \u662f <code>model_list</code> \u6a21\u578b\u914d\u7f6e\u5217\u8868\u3002\u7c7b\u578b: <code>list[ModelDict]</code>\u5fc5\u586b: \u662f <code>router_prompt</code> \u81ea\u5b9a\u4e49\u8def\u7531\u6a21\u578b\u7684\u63d0\u793a\u8bcd\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u5426"},{"location":"zh/adavance-guide/middleware/#model_list","title":"<code>model_list</code> \u914d\u7f6e\u8bf4\u660e","text":"<p>\u6bcf\u4e2a\u6a21\u578b\u914d\u7f6e\u4e3a\u4e00\u4e2a\u5b57\u5178\uff0c\u5305\u542b\u4ee5\u4e0b\u5b57\u6bb5\uff1a</p> \u5b57\u6bb5 \u8bf4\u660e <code>model_name</code> \u6a21\u578b\u7684\u552f\u4e00\u6807\u8bc6\uff0c\u4f7f\u7528 <code>provider:model-name</code> \u683c\u5f0f\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u662f <code>model_description</code> \u6a21\u578b\u80fd\u529b\u6216\u9002\u7528\u573a\u666f\u7684\u7b80\u8981\u63cf\u8ff0\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u662f <code>tools</code> \u8be5\u6a21\u578b\u53ef\u8c03\u7528\u7684\u5de5\u5177\u767d\u540d\u5355\u3002\u7c7b\u578b: <code>list[BaseTool]</code>\u5fc5\u586b: \u5426 <code>model_kwargs</code> \u6a21\u578b\u52a0\u8f7d\u65f6\u7684\u989d\u5916\u53c2\u6570\u3002\u7c7b\u578b: <code>dict</code>\u5fc5\u586b: \u5426 <code>model_system_prompt</code> \u6a21\u578b\u7684\u7cfb\u7edf\u7ea7\u63d0\u793a\u8bcd\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u5426 <code>model_instance</code> \u5df2\u5b9e\u4f8b\u5316\u7684\u6a21\u578b\u5bf9\u8c61\u3002\u7c7b\u578b: <code>BaseChatModel</code>\u5fc5\u586b: \u5426 <p>model_instance \u5b57\u6bb5\u8bf4\u660e</p> <ul> <li>\u82e5\u63d0\u4f9b\uff1a\u76f4\u63a5\u4f7f\u7528\u8be5\u5b9e\u4f8b\uff0c<code>model_name</code> \u4ec5\u4f5c\u6807\u8bc6\uff0c<code>model_kwargs</code> \u88ab\u5ffd\u7565\uff1b\u9002\u7528\u4e8e\u4e0d\u4f7f\u7528\u672c\u5e93\u7684\u5bf9\u8bdd\u6a21\u578b\u7ba1\u7406\u529f\u80fd\u7684\u60c5\u51b5\u3002</li> <li>\u82e5\u672a\u63d0\u4f9b\uff1a\u6839\u636e <code>model_name</code> \u548c <code>model_kwargs</code> \u4f7f\u7528 <code>load_chat_model</code> \u52a0\u8f7d\u6a21\u578b\u3002</li> <li>\u547d\u540d\u683c\u5f0f\uff1a\u65e0\u8bba\u54ea\u79cd\u60c5\u51b5\uff0c<code>model_name</code> \u7684\u547d\u540d\u90fd\u63a8\u8350\u91c7\u7528 <code>provider:model-name</code> \u683c\u5f0f\u3002</li> </ul>"},{"location":"zh/adavance-guide/middleware/#_9","title":"\u4f7f\u7528\u793a\u4f8b","text":""},{"location":"zh/adavance-guide/middleware/#_10","title":"\u6b65\u9aa4\u4e00\uff1a\u5b9a\u4e49\u6a21\u578b\u5217\u8868","text":"<pre><code>from langchain_dev_utils.agents.middleware.model_router import ModelDict\n\nmodel_list: list[ModelDict] = [\n    {\n        \"model_name\": \"vllm:qwen3-8b\",\n        \"model_description\": \"\u9002\u5408\u666e\u901a\u4efb\u52a1\uff0c\u5982\u5bf9\u8bdd\u3001\u6587\u672c\u751f\u6210\u7b49\",\n        \"model_kwargs\": {\n            \"temperature\": 0.7,\n            \"extra_body\": {\"chat_template_kwargs\": {\"enable_thinking\": False}},\n        },\n        \"model_system_prompt\": \"\u4f60\u662f\u4e00\u4e2a\u52a9\u624b\uff0c\u64c5\u957f\u5904\u7406\u666e\u901a\u4efb\u52a1\uff0c\u5982\u5bf9\u8bdd\u3001\u6587\u672c\u751f\u6210\u7b49\u3002\",\n    },\n    {\n        \"model_name\": \"vllm:qwen3-vl-2b\",\n        \"model_description\": \"\u9002\u5408\u89c6\u89c9\u4efb\u52a1\",\n        \"tools\": [],  # \u5982\u679c\u8be5\u6a21\u578b\u4e0d\u9700\u8981\u4efb\u4f55\u5de5\u5177\uff0c\u8bf7\u5c06\u6b64\u5b57\u6bb5\u8bbe\u7f6e\u4e3a\u7a7a\u5217\u8868 []\n    },\n    {\n        \"model_name\": \"vllm:qwen3-coder-flash\",\n        \"model_description\": \"\u9002\u5408\u4ee3\u7801\u751f\u6210\u4efb\u52a1\",\n        \"tools\": [run_python_code],  # \u4ec5\u5141\u8bb8\u4f7f\u7528 run_python_code \u5de5\u5177\n    },\n    {\n        \"model_name\": \"openai:gpt-4o\",\n        \"model_description\": \"\u9002\u5408\u7efc\u5408\u7c7b\u9ad8\u96be\u5ea6\u4efb\u52a1\",\n        \"model_system_prompt\": \"\u4f60\u662f\u4e00\u4e2a\u52a9\u624b\uff0c\u64c5\u957f\u5904\u7406\u7efc\u5408\u7c7b\u7684\u9ad8\u96be\u5ea6\u4efb\u52a1\",\n        \"model_instance\": ChatOpenAI(\n            model_name=\"gpt-4o\"\n        ),  # \u76f4\u63a5\u4f20\u5165\u5b9e\u4f8b\uff0c\u6b64\u65f6 model_name \u4ec5\u4f5c\u6807\u8bc6\uff0cmodel_kwargs \u88ab\u5ffd\u7565\n    },\n]\n</code></pre>"},{"location":"zh/adavance-guide/middleware/#agent","title":"\u6b65\u9aa4\u4e8c\uff1a\u521b\u5efa Agent \u5e76\u542f\u7528\u4e2d\u95f4\u4ef6","text":"<pre><code>from langchain_dev_utils.agents.middleware import ModelRouterMiddleware\nfrom langchain_core.messages import HumanMessage\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",  # \u6b64\u6a21\u578b\u4ec5\u4f5c\u5360\u4f4d\uff0c\u5b9e\u9645\u7531\u4e2d\u95f4\u4ef6\u52a8\u6001\u66ff\u6362\n    tools=[run_python_code, get_current_time],\n    middleware=[\n        ModelRouterMiddleware(\n            router_model=\"vllm:qwen3-4b\",\n            model_list=model_list,\n        )\n    ],\n)\n\n# \u8def\u7531\u4e2d\u95f4\u4ef6\u4f1a\u6839\u636e\u8f93\u5165\u5185\u5bb9\u81ea\u52a8\u9009\u62e9\u6700\u5408\u9002\u7684\u6a21\u578b\nresponse = agent.invoke({\"messages\": [HumanMessage(content=\"\u5e2e\u6211\u5199\u4e00\u4e2a\u5192\u6ce1\u6392\u5e8f\u4ee3\u7801\")]})\nprint(response)\n</code></pre> <p>\u901a\u8fc7 <code>ModelRouterMiddleware</code>\uff0c\u4f60\u53ef\u4ee5\u8f7b\u677e\u6784\u5efa\u4e00\u4e2a\u591a\u6a21\u578b\u3001\u591a\u80fd\u529b\u7684 Agent\uff0c\u6839\u636e\u4efb\u52a1\u7c7b\u578b\u81ea\u52a8\u9009\u62e9\u6700\u4f18\u6a21\u578b\uff0c\u63d0\u5347\u54cd\u5e94\u8d28\u91cf\u4e0e\u6548\u7387\u3002</p> <p>\u5e76\u884c\u6267\u884c</p> <p>\u91c7\u7528\u4e2d\u95f4\u4ef6\u5b9e\u73b0\u6a21\u578b\u8def\u7531\uff0c\u6bcf\u6b21\u4ec5\u4f1a\u5206\u914d\u4e00\u4e2a\u4efb\u52a1\u8fdb\u884c\u6267\u884c\uff0c\u5982\u679c\u4f60\u60f3\u8981\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u591a\u4e2a\u5b50\u4efb\u52a1\u7531\u591a\u4e2a\u6a21\u578b\u8fdb\u884c\u5e76\u884c\u6267\u884c\uff0c\u8bf7\u53c2\u8003\u72b6\u6001\u56fe\u7f16\u6392\u3002</p>"},{"location":"zh/adavance-guide/middleware/#_11","title":"\u667a\u80fd\u4f53\u4ea4\u63a5","text":"<p><code>HandoffAgentMiddleware</code> \u662f\u4e00\u4e2a\u7528\u4e8e\u5728\u591a\u4e2a\u5b50 Agent \u4e4b\u95f4\u7075\u6d3b\u5207\u6362\u7684\u4e2d\u95f4\u4ef6\uff0c\u5b8c\u6574\u5b9e\u73b0\u4e86 LangChain \u5b98\u65b9\u7684 <code>handoffs</code> \u591a\u667a\u80fd\u4f53\u534f\u4f5c\u65b9\u6848\u3002</p>"},{"location":"zh/adavance-guide/middleware/#_12","title":"\u53c2\u6570\u8bf4\u660e","text":"\u53c2\u6570 \u8bf4\u660e <code>agents_config</code> \u667a\u80fd\u4f53\u914d\u7f6e\u5b57\u5178\uff0c\u952e\u4e3a\u667a\u80fd\u4f53\u540d\u79f0\uff0c\u503c\u4e3a\u667a\u80fd\u4f53\u914d\u7f6e\u5b57\u5178\u3002\u7c7b\u578b: <code>dict[str, AgentConfig]</code>\u5fc5\u586b: \u662f <code>custom_handoffs_tool_descriptions</code> \u81ea\u5b9a\u4e49\u4ea4\u63a5\u5de5\u5177\u7684\u63cf\u8ff0\uff0c\u952e\u4e3a\u667a\u80fd\u4f53\u540d\u79f0\uff0c\u503c\u4e3a\u5bf9\u5e94\u7684\u4ea4\u63a5\u5de5\u5177\u63cf\u8ff0\u3002\u7c7b\u578b: <code>dict[str, str]</code>\u5fc5\u586b: \u5426 <code>handoffs_tool_overrides</code> \u81ea\u5b9a\u4e49\u4ea4\u63a5\u5de5\u5177\u7684\u5b9e\u73b0\uff0c\u952e\u4e3a\u667a\u80fd\u4f53\u540d\u79f0\uff0c\u503c\u4e3a\u5bf9\u5e94\u7684\u4ea4\u63a5\u5de5\u5177\u5b9e\u73b0\u3002\u7c7b\u578b: <code>dict[str, BaseTool]</code>\u5fc5\u586b: \u5426"},{"location":"zh/adavance-guide/middleware/#agents_config","title":"<code>agents_config</code> \u914d\u7f6e\u8bf4\u660e","text":"<p>\u6bcf\u4e2a\u667a\u80fd\u4f53\u914d\u7f6e\u4e3a\u4e00\u4e2a\u5b57\u5178\uff0c\u5305\u542b\u4ee5\u4e0b\u5b57\u6bb5\uff1a</p> \u5b57\u6bb5 \u8bf4\u660e <code>model</code> \u6307\u5b9a\u8be5\u667a\u80fd\u4f53\u4f7f\u7528\u7684\u6a21\u578b\uff1b\u82e5\u4e0d\u4f20\uff0c\u5219\u6cbf\u7528 <code>create_agent</code> \u7684 <code>model</code> \u53c2\u6570\u5bf9\u5e94\u7684\u6a21\u578b\u3002\u652f\u6301\u5b57\u7b26\u4e32\uff08\u987b\u4e3a <code>provider:model-name</code> \u683c\u5f0f\uff0c\u5982 <code>vllm:qwen3-4b</code>\uff09\u6216 <code>BaseChatModel</code> \u5b9e\u4f8b\u3002\u7c7b\u578b: <code>str</code> | <code>BaseChatModel</code>\u5fc5\u586b: \u5426 <code>prompt</code> \u667a\u80fd\u4f53\u7684\u7cfb\u7edf\u63d0\u793a\u8bcd\u3002\u7c7b\u578b: <code>str</code> | <code>SystemMessage</code>\u5fc5\u586b: \u662f <code>tools</code> \u667a\u80fd\u4f53\u53ef\u8c03\u7528\u7684\u5de5\u5177\u5217\u8868\u3002\u7c7b\u578b: <code>list[BaseTool]</code>\u5fc5\u586b: \u5426 <code>default</code> \u662f\u5426\u8bbe\u4e3a\u9ed8\u8ba4\u667a\u80fd\u4f53\uff1b\u7f3a\u7701\u4e3a <code>False</code>\u3002\u5168\u90e8\u914d\u7f6e\u4e2d\u5fc5\u987b\u4e14\u53ea\u80fd\u6709\u4e00\u4e2a\u667a\u80fd\u4f53\u8bbe\u4e3a <code>True</code>\u3002\u7c7b\u578b: <code>bool</code>\u5fc5\u586b: \u5426 <code>handoffs</code> \u8be5\u667a\u80fd\u4f53\u53ef\u4ea4\u63a5\u7ed9\u7684\u5176\u5b83\u667a\u80fd\u4f53\u540d\u79f0\u5217\u8868\u3002\u82e5\u8bbe\u4e3a <code>\"all\"</code>\uff0c\u5219\u8868\u793a\u8be5\u667a\u80fd\u4f53\u53ef\u4ea4\u63a5\u7ed9\u6240\u6709\u5176\u5b83\u667a\u80fd\u4f53\u3002\u7c7b\u578b: <code>list[str]</code> | <code>str</code>\u5fc5\u586b: \u662f <p>\u5bf9\u4e8e\u8fd9\u79cd\u8303\u5f0f\u7684\u591a\u667a\u80fd\u4f53\u5b9e\u73b0\uff0c\u5f80\u5f80\u9700\u8981\u4e00\u4e2a\u7528\u4e8e\u4ea4\u63a5\uff08handoffs\uff09\u7684\u5de5\u5177\u3002\u672c\u4e2d\u95f4\u4ef6\u5229\u7528\u6bcf\u4e2a\u667a\u80fd\u4f53\u7684 <code>handoffs</code> \u914d\u7f6e\uff0c\u81ea\u52a8\u4e3a\u6bcf\u4e2a\u667a\u80fd\u4f53\u521b\u5efa\u5bf9\u5e94\u7684\u4ea4\u63a5\u5de5\u5177\u3002\u5982\u679c\u8981\u81ea\u5b9a\u4e49\u4ea4\u63a5\u5de5\u5177\u7684\u63cf\u8ff0\uff0c\u5219\u53ef\u4ee5\u901a\u8fc7 <code>custom_handoffs_tool_descriptions</code> \u53c2\u6570\u5b9e\u73b0\u3002</p> <p>\u4f7f\u7528\u793a\u4f8b</p> <p>\u672c\u793a\u4f8b\u4e2d\uff0c\u5c06\u4f7f\u7528\u56db\u4e2a\u667a\u80fd\u4f53\uff1a<code>time_agent</code>\u3001<code>weather_agent</code>\u3001<code>code_agent</code> \u548c <code>default_agent</code>\u3002</p> <p>\u63a5\u4e0b\u6765\u8981\u521b\u5efa\u5bf9\u5e94\u667a\u80fd\u4f53\u7684\u914d\u7f6e\u5b57\u5178 <code>agent_config</code>\u3002</p> <pre><code>from langchain_dev_utils.agents.middleware.handoffs import AgentConfig\n\nagent_config: dict[str, AgentConfig] = {\n    \"time_agent\": {\n        \"model\": \"vllm:qwen3-8b\",\n        \"prompt\": \"\u4f60\u662f\u4e00\u4e2a\u65f6\u95f4\u52a9\u624b\",\n        \"tools\": [get_current_time],\n        \"handoffs\": [\"default_agent\"],  # \u8be5\u667a\u80fd\u4f53\u53ea\u80fd\u4ea4\u63a5\u5230default_agent\n    },\n    \"weather_agent\": {\n        \"prompt\": \"\u4f60\u662f\u4e00\u4e2a\u5929\u6c14\u52a9\u624b\",\n        \"tools\": [get_current_weather, get_current_city],\n        \"handoffs\": [\"default_agent\"],\n    },\n    \"code_agent\": {\n        \"model\": load_chat_model(\"vllm:qwen3-coder-flash\"),\n        \"prompt\": \"\u4f60\u662f\u4e00\u4e2a\u4ee3\u7801\u52a9\u624b\",\n        \"tools\": [\n            run_code,\n        ],\n        \"handoffs\": [\"default_agent\"],\n    },\n    \"default_agent\": {\n        \"prompt\": \"\u4f60\u662f\u4e00\u4e2a\u52a9\u624b\",\n        \"default\": True, # \u8bbe\u4e3a\u9ed8\u8ba4\u667a\u80fd\u4f53\n        \"handoffs\": \"all\",  # \u8be5\u667a\u80fd\u4f53\u53ef\u4ee5\u4ea4\u63a5\u5230\u6240\u6709\u5176\u5b83\u667a\u80fd\u4f53\n    },\n}\n</code></pre> <p>\u6700\u7ec8\u5c06\u8fd9\u4e2a\u914d\u7f6e\u4f20\u9012\u7ed9 <code>HandoffAgentMiddleware</code> \u5373\u53ef\u3002</p> <pre><code>from langchain_dev_utils.agents.middleware import HandoffAgentMiddleware\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    tools=[\n        get_current_time,\n        get_current_weather,\n        get_current_city,\n        run_code,\n    ],\n    middleware=[HandoffAgentMiddleware(agents_config=agent_config)],\n)\n\nresponse = agent.invoke({\"messages\": [HumanMessage(content=\"\u5f53\u524d\u65f6\u95f4\u662f\u591a\u5c11\uff1f\")]})\nprint(response)\n</code></pre> <p>\u5982\u679c\u60f3\u8981\u81ea\u5b9a\u4e49\u4ea4\u63a5\u5de5\u5177\u7684\u63cf\u8ff0\uff0c\u53ef\u4ee5\u4f20\u9012\u7b2c\u4e8c\u4e2a\u53c2\u6570 <code>custom_handoffs_tool_descriptions</code>\u3002</p> <pre><code>agent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    tools=[\n        get_current_time,\n        get_current_weather,\n        get_current_city,\n        run_code,\n    ],\n    middleware=[\n        HandoffAgentMiddleware(\n            agents_config=agent_config,\n            custom_handoffs_tool_descriptions={\n                \"time_agent\": \"\u6b64\u5de5\u5177\u7528\u4e8e\u4ea4\u63a5\u5230\u65f6\u95f4\u52a9\u624b\u53bb\u89e3\u51b3\u65f6\u95f4\u67e5\u8be2\u95ee\u9898\",\n                \"weather_agent\": \"\u6b64\u5de5\u5177\u7528\u4e8e\u4ea4\u63a5\u5230\u5929\u6c14\u52a9\u624b\u53bb\u89e3\u51b3\u5929\u6c14\u67e5\u8be2\u95ee\u9898\",\n                \"code_agent\": \"\u6b64\u5de5\u5177\u7528\u4e8e\u4ea4\u63a5\u5230\u4ee3\u7801\u52a9\u624b\u53bb\u89e3\u51b3\u4ee3\u7801\u95ee\u9898\",\n                \"default_agent\": \"\u6b64\u5de5\u5177\u7528\u4e8e\u4ea4\u63a5\u5230\u9ed8\u8ba4\u7684\u52a9\u624b\",\n            },\n        )\n    ],\n)\n</code></pre> <p>\u5982\u679c\u4f60\u60f3\u5b8c\u5168\u81ea\u5b9a\u4e49\u5b9e\u73b0\u4ea4\u63a5\u5de5\u5177\u7684\u903b\u8f91\uff0c\u5219\u53ef\u4ee5\u4f20\u9012\u7b2c\u4e09\u4e2a\u53c2\u6570 <code>handoffs_tool_overrides</code>\u3002\u4e0e\u7b2c\u4e8c\u4e2a\u53c2\u6570\u7c7b\u4f3c\uff0c\u5b83\u4e5f\u662f\u4e00\u4e2a\u5b57\u5178\uff0c\u952e\u4e3a\u667a\u80fd\u4f53\u540d\u79f0\uff0c\u503c\u4e3a\u5bf9\u5e94\u7684\u4ea4\u63a5\u5de5\u5177\u5b9e\u73b0\u3002</p> <p>\u81ea\u5b9a\u4e49\u4ea4\u63a5\u5de5\u5177\u5fc5\u987b\u8fd4\u56de\u4e00\u4e2a <code>Command</code> \u5bf9\u8c61\uff0c\u5176 <code>update</code> \u5c5e\u6027\u9700\u5305\u542b <code>messages</code> \u952e\uff08\u8fd4\u56de\u5de5\u5177\u54cd\u5e94\uff09\u548c <code>active_agent</code> \u952e\uff08\u503c\u4e3a\u8981\u4ea4\u63a5\u7684\u667a\u80fd\u4f53\u540d\u79f0\uff0c\u7528\u4e8e\u5207\u6362\u5f53\u524d\u667a\u80fd\u4f53\uff09\u3002</p> <p>\u4f8b\u5982\uff1a</p> <pre><code>@tool\ndef transfer_to_code_agent(runtime: ToolRuntime) -&gt; Command:\n    \"\"\"This tool help you transfer to the code agent.\"\"\"\n    #\u8fd9\u91cc\u4f60\u53ef\u4ee5\u6dfb\u52a0\u81ea\u5b9a\u4e49\u903b\u8f91\n    return Command(\n        update={\n            \"messages\": [\n                ToolMessage(\n                    content=\"transfer to code agent\",\n                    tool_call_id=runtime.tool_call_id,\n                )\n            ],\n            \"active_agent\": \"code_agent\",\n            #\u8fd9\u91cc\u4f60\u53ef\u4ee5\u6dfb\u52a0\u5176\u5b83\u7684\u8981\u66f4\u65b0\u7684\u952e\n        }\n    )\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    tools=[\n        get_current_time,\n        get_current_weather,\n        get_current_city,\n        run_code,\n    ],\n    middleware=[\n        HandoffAgentMiddleware(\n            agents_config=agent_config,\n            handoffs_tool_overrides={\n                \"code_agent\": transfer_to_code_agent,\n            },\n        )\n    ],\n)\n</code></pre> <p><code>handoffs_tool_overrides</code> \u7528\u4e8e\u9ad8\u5ea6\u5b9a\u5236\u5316\u4ea4\u63a5\u5de5\u5177\u7684\u5b9e\u73b0\uff0c\u5982\u679c\u4ec5\u4ec5\u662f\u60f3\u8981\u81ea\u5b9a\u4e49\u4ea4\u63a5\u5de5\u5177\u7684\u63cf\u8ff0\uff0c\u5219\u5e94\u8be5\u4f7f\u7528 <code>custom_handoffs_tool_descriptions</code>\u3002</p>"},{"location":"zh/adavance-guide/middleware/#_13","title":"\u5de5\u5177\u8c03\u7528\u4fee\u590d","text":"<p><code>ToolCallRepairMiddleware</code> \u662f\u4e00\u4e2a\u81ea\u52a8\u4fee\u590d\u5927\u6a21\u578b\u65e0\u6548\u5de5\u5177\u8c03\u7528\uff08<code>invalid_tool_calls</code>\uff09\u7684\u4e2d\u95f4\u4ef6\u3002</p> <p>\u5927\u6a21\u578b\u5728\u8f93\u51fa\u5de5\u5177\u8c03\u7528\u7684 JSON Schema \u65f6\uff0c\u53ef\u80fd\u56e0\u6a21\u578b\u81ea\u8eab\u539f\u56e0\u751f\u6210 JSON \u683c\u5f0f\u9519\u8bef\u7684\u5185\u5bb9\uff08\u9519\u8bef\u7684\u5185\u5bb9\u5e38\u89c1\u4e8e <code>arguments</code> \u5b57\u6bb5\uff09\uff0c\u5bfc\u81f4 JSON \u89e3\u6790\u5931\u8d25\u3002\u8fd9\u7c7b\u8c03\u7528\u4f1a\u88ab\u5b58\u5230 <code>invalid_tool_calls</code> \u5b57\u6bb5\u4e2d\u3002<code>ToolCallRepairMiddleware</code> \u4f1a\u5728\u6a21\u578b\u8fd4\u56de\u7ed3\u679c\u540e\u81ea\u52a8\u68c0\u6d4b <code>invalid_tool_calls</code>\uff0c\u5e76\u5c1d\u8bd5\u8c03\u7528 <code>json-repair</code> \u8fdb\u884c\u4fee\u590d\uff0c\u4f7f\u5de5\u5177\u8c03\u7528\u5f97\u4ee5\u6b63\u5e38\u6267\u884c\u3002</p> <p>\u8bf7\u786e\u4fdd\u5df2\u5b89\u88c5 <code>langchain-dev-utils[standard]</code>\uff0c\u8be6\u89c1\u5b89\u88c5\u6307\u5357\u3002</p>"},{"location":"zh/adavance-guide/middleware/#_14","title":"\u53c2\u6570\u8bf4\u660e","text":"<p>\u8be5\u4e2d\u95f4\u4ef6\u96f6\u914d\u7f6e\u5f00\u7bb1\u5373\u7528\uff0c\u65e0\u9700\u989d\u5916\u53c2\u6570\u3002</p>"},{"location":"zh/adavance-guide/middleware/#_15","title":"\u4f7f\u7528\u793a\u4f8b","text":"<pre><code>from langchain_dev_utils.agents.middleware import ToolCallRepairMiddleware\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    tools=[run_python_code, get_current_time],\n    middleware=[\n        ToolCallRepairMiddleware()\n    ],\n)\n</code></pre> <p>\u6ce8\u610f</p> <p>\u672c\u4e2d\u95f4\u4ef6\u65e0\u6cd5\u4fdd\u8bc1 100% \u4fee\u590d\u6240\u6709\u65e0\u6548\u5de5\u5177\u8c03\u7528\uff0c\u5b9e\u9645\u6548\u679c\u53d6\u51b3\u4e8e <code>json-repair</code> \u7684\u4fee\u590d\u80fd\u529b\uff1b\u6b64\u5916\uff0c\u5b83\u4ec5\u4f5c\u7528\u4e8e <code>invalid_tool_calls</code> \u5b57\u6bb5\u4e2d\u7684\u65e0\u6548\u5de5\u5177\u8c03\u7528\u5185\u5bb9\u3002</p>"},{"location":"zh/adavance-guide/middleware/#_16","title":"\u683c\u5f0f\u5316\u7cfb\u7edf\u63d0\u793a\u8bcd","text":"<p><code>format_prompt</code> \u662f\u4e00\u4e2a\u4e2d\u95f4\u4ef6\u5b9e\u4f8b\uff0c\u5141\u8bb8\u60a8\u5728 <code>system_prompt</code> \u4e2d\u4f7f\u7528 <code>f-string</code> \u98ce\u683c\u7684\u5360\u4f4d\u7b26\uff08\u5982 <code>{name}</code>\uff09\uff0c\u5e76\u5728\u8fd0\u884c\u65f6\u52a8\u6001\u5730\u7528\u5b9e\u9645\u503c\u66ff\u6362\u5b83\u4eec\u3002</p>"},{"location":"zh/adavance-guide/middleware/#_17","title":"\u53c2\u6570\u8bf4\u660e","text":"<p>\u5360\u4f4d\u7b26\u4e2d\u7684\u53d8\u91cf\u503c\u9075\u5faa\u4e00\u4e2a\u660e\u786e\u7684\u89e3\u6790\u987a\u5e8f\uff1a</p> <ol> <li>\u4f18\u5148\u4ece <code>state</code> \u4e2d\u67e5\u627e\uff1a\u4f1a\u5148\u4ece <code>state</code> \u5b57\u5178\u4e2d\u67e5\u627e\u4e0e\u5360\u4f4d\u7b26\u540c\u540d\u7684\u5b57\u6bb5</li> <li>\u5176\u6b21\u4ece <code>context</code> \u4e2d\u67e5\u627e\uff1a\u5982\u679c\u5728 <code>state</code> \u4e2d\u672a\u627e\u5230\u8be5\u5b57\u6bb5\uff0c\u5219\u4f1a\u7ee7\u7eed\u5728 <code>context</code> \u5bf9\u8c61\u4e2d\u67e5\u627e</li> </ol> <p>\u8fd9\u4e2a\u987a\u5e8f\u610f\u5473\u7740 <code>state</code> \u4e2d\u7684\u503c\u62e5\u6709\u66f4\u9ad8\u7684\u4f18\u5148\u7ea7\uff0c\u53ef\u4ee5\u8986\u76d6 <code>context</code> \u4e2d\u540c\u540d\u7684\u503c\u3002</p>"},{"location":"zh/adavance-guide/middleware/#_18","title":"\u4f7f\u7528\u793a\u4f8b","text":""},{"location":"zh/adavance-guide/middleware/#state","title":"\u4ec5\u4ece <code>state</code> \u4e2d\u83b7\u53d6\u53d8\u91cf","text":"<p>\u8fd9\u662f\u6700\u57fa\u7840\u7684\u7528\u6cd5\uff0c\u6240\u6709\u5360\u4f4d\u7b26\u53d8\u91cf\u90fd\u7531 <code>state</code> \u63d0\u4f9b\u3002</p> <pre><code>from langchain_dev_utils.agents.middleware import format_prompt\nfrom langchain.agents import AgentState\n\nclass AssistantState(AgentState):\n    name: str\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    system_prompt=\"\u4f60\u662f\u4e00\u4e2a\u667a\u80fd\u52a9\u624b\uff0c\u4f60\u7684\u540d\u5b57\u53eb\u505a{name}\u3002\",\n    middleware=[format_prompt],\n    state_schema=AssistantState,\n)\n\n# \u5728\u8c03\u7528\u65f6\uff0c\u5fc5\u987b\u4e3a state \u63d0\u4f9b 'name' \u7684\u503c\nresponse = agent.invoke(\n    {\"messages\": [HumanMessage(content=\"\u4f60\u597d\u554a\")], \"name\": \"assistant\"}\n)\nprint(response)\n</code></pre>"},{"location":"zh/adavance-guide/middleware/#state-context","title":"\u540c\u65f6\u4ece <code>state</code> \u548c <code>context</code> \u4e2d\u83b7\u53d6\u53d8\u91cf","text":"<p>\u540c\u65f6\u4f7f\u7528 <code>state</code> \u548c <code>context</code>\uff1a</p> <pre><code>from dataclasses import dataclass\n\n@dataclass\nclass Context:\n    user: str\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    # {name} \u5c06\u4ece state \u83b7\u53d6\uff0c{user} \u5c06\u4ece context \u83b7\u53d6\n    system_prompt=\"\u4f60\u662f\u4e00\u4e2a\u667a\u80fd\u52a9\u624b\uff0c\u4f60\u7684\u540d\u5b57\u53eb\u505a{name}\u3002\u4f60\u7684\u4f7f\u7528\u8005\u53eb\u505a{user}\u3002\",\n    middleware=[format_prompt],\n    state_schema=AssistantState,\n    context_schema=Context,\n)\n\n# \u5728\u8c03\u7528\u65f6\uff0c\u4e3a state \u63d0\u4f9b 'name'\uff0c\u4e3a context \u63d0\u4f9b 'user'\nresponse = agent.invoke(\n    {\n        \"messages\": [HumanMessage(content=\"\u6211\u8981\u53bbNew York\u73a9\u51e0\u5929\uff0c\u5e2e\u6211\u89c4\u5212\u884c\u7a0b\")],\n        \"name\": \"assistant\",\n    },\n    context=Context(user=\"\u5f20\u4e09\"),\n)\nprint(response)\n</code></pre>"},{"location":"zh/adavance-guide/middleware/#_19","title":"\u53d8\u91cf\u8986\u76d6\u793a\u4f8b","text":"<p>\u6b64\u793a\u4f8b\u5c55\u793a\u4e86\u5f53 <code>state</code> \u548c <code>context</code> \u4e2d\u5b58\u5728\u540c\u540d\u53d8\u91cf\u65f6\uff0c<code>state</code> \u7684\u503c\u4f1a\u4f18\u5148\u751f\u6548\u3002</p> <pre><code>from dataclasses import dataclass\n\n@dataclass\nclass Context:\n    # context \u4e2d\u5b9a\u4e49\u4e86 'name'\n    name: str\n    user: str\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    system_prompt=\"\u4f60\u662f\u4e00\u4e2a\u667a\u80fd\u52a9\u624b\uff0c\u4f60\u7684\u540d\u5b57\u53eb\u505a{name}\u3002\u4f60\u7684\u4f7f\u7528\u8005\u53eb\u505a{user}\u3002\",\n    middleware=[format_prompt],\n    state_schema=AssistantState, # state \u4e2d\u4e5f\u5b9a\u4e49\u4e86 'name'\n    context_schema=Context,\n)\n\n# \u5728\u8c03\u7528\u65f6\uff0cstate \u548c context \u90fd\u63d0\u4f9b\u4e86 'name' \u7684\u503c\nresponse = agent.invoke(\n    {\n        \"messages\": [HumanMessage(content=\"\u4f60\u53eb\u4ec0\u4e48\u540d\u5b57\uff1f\")],\n        \"name\": \"assistant-1\",\n    },\n    context=Context(name=\"assistant-2\", user=\"\u5f20\u4e09\"),\n)\n\n# \u6700\u7ec8\u7684\u7cfb\u7edf\u63d0\u793a\u8bcd\u4f1a\u662f \"\u4f60\u662f\u4e00\u4e2a\u667a\u80fd\u52a9\u624b\uff0c\u4f60\u7684\u540d\u5b57\u53eb\u505aassistant-1\u3002\u4f60\u7684\u4f7f\u7528\u8005\u53eb\u505a\u5f20\u4e09\u3002\"\n# \u56e0\u4e3a state \u7684\u4f18\u5148\u7ea7\u66f4\u9ad8\nprint(response)\n</code></pre> <p>\u6ce8\u610f</p> <p>\u81ea\u5b9a\u4e49\u4e2d\u95f4\u4ef6\u6709\u4e24\u79cd\u5b9e\u73b0\u65b9\u5f0f\uff1a\u88c5\u9970\u5668\u6216\u7ee7\u627f\u7c7b\u3002 - \u7ee7\u627f\u7c7b\u5b9e\u73b0\uff1a<code>PlanMiddleware</code>\u3001<code>ModelMiddleware</code>\u3001<code>HandoffAgentMiddleware</code>\u3001<code>ToolCallRepairMiddleware</code> - \u88c5\u9970\u5668\u5b9e\u73b0\uff1a<code>format_prompt</code>\uff08\u88c5\u9970\u5668\u4f1a\u628a\u51fd\u6570\u76f4\u63a5\u53d8\u6210\u4e2d\u95f4\u4ef6\u5b9e\u4f8b\uff0c\u56e0\u6b64\u65e0\u9700\u624b\u52a8\u5b9e\u4f8b\u5316\u5373\u53ef\u4f7f\u7528\uff09</p> <p>\u5b98\u65b9\u4e2d\u95f4\u4ef6\u6269\u5145</p> <p>\u672c\u5e93\u6269\u5145\u4e86\u4ee5\u4e0b\u5b98\u65b9\u4e2d\u95f4\u4ef6\uff0c\u652f\u6301\u901a\u8fc7\u5b57\u7b26\u4e32\u53c2\u6570\u6307\u5b9a\u5df2\u88ab <code>register_model_provider</code> \u6ce8\u518c\u7684\u6a21\u578b\uff1a</p> <p>\u4f60\u53ea\u9700\u8981\u5bfc\u5165\u672c\u5e93\u4e2d\u7684\u8fd9\u4e9b\u4e2d\u95f4\u4ef6\uff0c\u5373\u53ef\u4f7f\u7528\u5b57\u7b26\u4e32\u6307\u5b9a\u5df2\u7ecf\u88ab<code>register_model_provider</code>\u6ce8\u518c\u7684\u6a21\u578b\u3002\u4e2d\u95f4\u4ef6\u4f7f\u7528\u65b9\u6cd5\u548c\u5b98\u65b9\u4e2d\u95f4\u4ef6\u4fdd\u6301\u4e00\u81f4\uff0c\u4f8b\u5982\uff1a <pre><code>from langchain_core.messages import AIMessage\nfrom langchain_dev_utils.agents.middleware import SummarizationMiddleware\nfrom langchain_dev_utils.chat_models import register_model_provider\n\nregister_model_provider(\n    provider_name=\"vllm\",\n    chat_model=\"openai-compatible\",\n    base_url=\"http://localhost:8000/v1\",\n)\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    middleware=[\n        SummarizationMiddleware(\n            model=\"vllm:qwen3-4b\",\n            trigger=(\"tokens\", 50),\n            keep=(\"messages\", 1),\n        )\n    ],\n    system_prompt=\"\u4f60\u662f\u4e00\u4e2a\u667a\u80fd\u7684AI\u52a9\u624b\uff0c\u53ef\u4ee5\u89e3\u51b3\u7528\u6237\u7684\u95ee\u9898\",\n)\nresponse = agent.invoke({\"messages\": messages})\nprint(response)\n</code></pre></p>"},{"location":"zh/adavance-guide/multi-agent/","title":"\u5b50\u667a\u80fd\u4f53\u5de5\u5177\uff08Agent as Tool\uff09","text":""},{"location":"zh/adavance-guide/multi-agent/#_1","title":"\u6982\u8ff0","text":"<p>\u5728\u6784\u5efa\u590d\u6742\u7684 AI \u5e94\u7528\u65f6\uff0c\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u662f\u4e00\u79cd\u5f3a\u5927\u7684\u67b6\u6784\u6a21\u5f0f\u3002\u901a\u8fc7\u5c06\u4e0d\u540c\u804c\u8d23\u5206\u914d\u7ed9\u4e13\u95e8\u7684\u667a\u80fd\u4f53\uff0c\u53ef\u4ee5\u5b9e\u73b0\u4efb\u52a1\u7684\u4e13\u4e1a\u5316\u5206\u5de5\u548c\u9ad8\u6548\u534f\u4f5c\u3002</p> <p>\u5b9e\u73b0\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6709\u591a\u79cd\u65b9\u5f0f\uff0c\u5176\u4e2d\u5de5\u5177\u8c03\u7528\u662f\u4e00\u79cd\u5e38\u7528\u4e14\u7075\u6d3b\u7684\u5b9e\u73b0\u65b9\u5f0f\u3002\u901a\u8fc7\u5c06\u5b50\u667a\u80fd\u4f53\uff08subagents\uff09\u5c01\u88c5\u4e3a\u5de5\u5177\uff0c\u4e3b\u667a\u80fd\u4f53\u53ef\u4ee5\u6839\u636e\u4efb\u52a1\u9700\u6c42\u52a8\u6001\u59d4\u6d3e\u7ed9\u4e13\u95e8\u7684\u5b50\u667a\u80fd\u4f53\u5904\u7406\u3002</p> <p>\u672c\u5e93\u63d0\u4f9b\u4e86\u4e24\u4e2a\u9884\u6784\u5efa\u51fd\u6570\u6765\u7b80\u5316\u8fd9\u79cd\u5b9e\u73b0\u65b9\u5f0f\uff1a</p> \u51fd\u6570\u540d \u529f\u80fd\u63cf\u8ff0 <code>wrap_agent_as_tool</code> \u5c06\u5355\u4e2a\u667a\u80fd\u4f53\u5b9e\u4f8b\u5c01\u88c5\u4e3a\u4e00\u4e2a\u72ec\u7acb\u5de5\u5177 <code>wrap_all_agents_as_tool</code> \u5c06\u591a\u4e2a\u667a\u80fd\u4f53\u5b9e\u4f8b\u5c01\u88c5\u4e3a\u4e00\u4e2a\u7edf\u4e00\u5de5\u5177\uff0c\u901a\u8fc7\u53c2\u6570\u6307\u5b9a\u8c03\u7528\u54ea\u4e2a\u5b50\u667a\u80fd\u4f53"},{"location":"zh/adavance-guide/multi-agent/#_2","title":"\u5c01\u88c5\u5355\u4e2a\u667a\u80fd\u4f53\u4e3a\u5de5\u5177","text":"<p>\u5c01\u88c5\u5355\u4e2a\u667a\u80fd\u4f53\u53ea\u9700\u4e09\u6b65\uff1a</p> <ol> <li>\u5bfc\u5165 <code>wrap_agent_as_tool</code></li> <li>\u628a\u667a\u80fd\u4f53\u5b9e\u4f8b\u4f5c\u4e3a\u53c2\u6570\u4f20\u5165</li> <li>\u83b7\u5f97\u53ef\u76f4\u63a5\u88ab\u5176\u4ed6\u667a\u80fd\u4f53\u8c03\u7528\u7684\u5de5\u5177\u5bf9\u8c61</li> </ol>"},{"location":"zh/adavance-guide/multi-agent/#_3","title":"\u51fd\u6570\u53c2\u6570\u8bf4\u660e","text":"\u53c2\u6570 \u8bf4\u660e <code>agent</code> \u667a\u80fd\u4f53\u5b9e\u4f8b\uff0c\u5fc5\u987b\u5df2\u5b9a\u4e49 <code>name</code> \u5c5e\u6027\u3002\u7c7b\u578b: <code>CompiledStateGraph</code>\u5fc5\u586b: \u662f <code>tool_name</code> \u5de5\u5177\u540d\u79f0\uff0c\u9ed8\u8ba4\u4e3a <code>transfer_to_{agent_name}</code>\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u5426 <code>tool_description</code> \u5de5\u5177\u63cf\u8ff0\uff0c\u9ed8\u8ba4\u4e3a <code>This tool transforms input to {agent_name}</code>\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u5426 <code>pre_input_hooks</code> \u667a\u80fd\u4f53\u8fd0\u884c\u524d\u7684\u94a9\u5b50\u51fd\u6570\u3002\u7c7b\u578b: <code>tuple[Callable, Callable] | Callable</code>\u5fc5\u586b: \u5426 <code>post_output_hooks</code> \u667a\u80fd\u4f53\u8fd0\u884c\u540e\u7684\u94a9\u5b50\u51fd\u6570\u3002\u7c7b\u578b: <code>tuple[Callable, Callable] | Callable</code>\u5fc5\u586b: \u5426"},{"location":"zh/adavance-guide/multi-agent/#_4","title":"\u4f7f\u7528\u793a\u4f8b","text":"<p>\u4e0b\u9762\u6211\u4eec\u4ee5 <code>supervisor</code> \u667a\u80fd\u4f53\u4e3a\u4f8b\uff0c\u4ecb\u7ecd\u5982\u4f55\u901a\u8fc7 <code>wrap_agent_as_tool</code> \u5c06\u5b50\u667a\u80fd\u4f53\u5c01\u88c5\u4e3a\u5de5\u5177\u3002</p> <p>\u9996\u5148\u5b9e\u73b0\u4e24\u4e2a\u5b50\u667a\u80fd\u4f53\uff0c\u4e00\u4e2a\u7528\u4e8e\u53d1\u9001\u90ae\u4ef6\uff0c\u4e00\u4e2a\u7528\u4e8e\u65e5\u7a0b\u67e5\u8be2\u548c\u5b89\u6392\u3002</p>"},{"location":"zh/adavance-guide/multi-agent/#_5","title":"\u90ae\u4ef6\u667a\u80fd\u4f53","text":"<pre><code>from langchain_core.tools import tool\nfrom langchain_dev_utils.chat_models import register_model_provider\nfrom langchain_dev_utils.agents import create_agent, wrap_agent_as_tool \n\nregister_model_provider(\n    \"vllm\",\n    \"openai-compatible\",\n    base_url=\"http://localhost:8000/v1\",\n)\n\n\n@tool\ndef send_email(\n    to: list[str],  # \u7535\u5b50\u90ae\u4ef6\u5730\u5740\n    subject: str,\n    body: str,\n    cc: list[str] = [],\n) -&gt; str:\n    \"\"\"\u901a\u8fc7\u7535\u5b50\u90ae\u4ef6API\u53d1\u9001\u90ae\u4ef6\u3002\u8981\u6c42\u6b63\u786e\u683c\u5f0f\u7684\u5730\u5740\u3002\"\"\"\n    # \u5b58\u6839\uff1a\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u8fd9\u91cc\u4f1a\u8c03\u7528SendGrid\u3001Gmail API\u7b49\n    return f\"\u90ae\u4ef6\u5df2\u53d1\u9001\u81f3 {', '.join(to)} - \u4e3b\u9898: {subject}\"\n\n\nEMAIL_AGENT_PROMPT = (\n    \"\u4f60\u662f\u4e00\u4e2a\u7535\u5b50\u90ae\u4ef6\u52a9\u624b\u3002\"\n    \"\u6839\u636e\u81ea\u7136\u8bed\u8a00\u8bf7\u6c42\u64b0\u5199\u4e13\u4e1a\u90ae\u4ef6\u3002\"\n    \"\u63d0\u53d6\u6536\u4ef6\u4eba\u4fe1\u606f\u5e76\u5236\u4f5c\u6070\u5f53\u7684\u4e3b\u9898\u884c\u548c\u6b63\u6587\u5185\u5bb9\u3002\"\n    \"\u4f7f\u7528 send_email \u6765\u53d1\u9001\u90ae\u4ef6\u3002\"\n    \"\u59cb\u7ec8\u5728\u6700\u7ec8\u56de\u590d\u4e2d\u786e\u8ba4\u5df2\u53d1\u9001\u7684\u5185\u5bb9\u3002\"\n)\n\nemail_agent = create_agent(\n    \"vllm:qwen3-4b\",\n    tools=[send_email],\n    system_prompt=EMAIL_AGENT_PROMPT,\n    name=\"email_agent\",\n)\n</code></pre>"},{"location":"zh/adavance-guide/multi-agent/#_6","title":"\u65e5\u7a0b\u667a\u80fd\u4f53","text":"<pre><code>@tool\ndef create_calendar_event(\n    title: str,\n    start_time: str,  # ISO\u683c\u5f0f: \"2024-01-15T14:00:00\"\n    end_time: str,  # ISO\u683c\u5f0f: \"2024-01-15T15:00:00\"\n    attendees: list[str],  # \u7535\u5b50\u90ae\u4ef6\u5730\u5740\n    location: str = \"\",\n) -&gt; str:\n    \"\"\"\u521b\u5efa\u65e5\u5386\u4e8b\u4ef6\u3002\u8981\u6c42\u7cbe\u786e\u7684ISO\u65e5\u671f\u65f6\u95f4\u683c\u5f0f\u3002\"\"\"\n    # \u5b58\u6839\uff1a\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u8fd9\u91cc\u4f1a\u8c03\u7528Google Calendar API\u3001Outlook API\u7b49\n    return f\"\u4e8b\u4ef6\u5df2\u521b\u5efa\uff1a{title} \u4ece {start_time} \u5230 {end_time}\uff0c\u5171\u6709 {len(attendees)} \u4f4d\u53c2\u4e0e\u8005\"\n\n\n@tool\ndef get_available_time_slots(\n    attendees: list[str],\n    date: str,  # ISO\u683c\u5f0f: \"2024-01-15\"\n    duration_minutes: int,\n) -&gt; list[str]:\n    \"\"\"\u5728\u7279\u5b9a\u65e5\u671f\u67e5\u8be2\u53c2\u4e0e\u8005\u7684\u65e5\u5386\u53ef\u7528\u65f6\u95f4\u3002\"\"\"\n    # \u5b58\u6839\uff1a\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u8fd9\u91cc\u4f1a\u67e5\u8be2\u65e5\u5386API\n    return [\"09:00\", \"14:00\", \"16:00\"]\n\n\nCALENDAR_AGENT_PROMPT = (\n    \"\u4f60\u662f\u4e00\u4e2a\u65e5\u5386\u65e5\u7a0b\u5b89\u6392\u52a9\u624b\u3002\"\n    \"\u5c06\u81ea\u7136\u8bed\u8a00\u7684\u65e5\u7a0b\u5b89\u6392\u8bf7\u6c42\uff08\u4f8b\u5982'\u4e0b\u5468\u4e8c\u4e0b\u53482\u70b9'\uff09\u89e3\u6790\u4e3a\u6b63\u786e\u7684ISO\u65e5\u671f\u65f6\u95f4\u683c\u5f0f\u3002\"\n    \"\u9700\u8981\u65f6\u4f7f\u7528 get_available_time_slots \u6765\u68c0\u67e5\u53ef\u7528\u65f6\u95f4\u3002\"\n    \"\u4f7f\u7528 create_calendar_event \u6765\u5b89\u6392\u4e8b\u4ef6\u3002\"\n    \"\u59cb\u7ec8\u5728\u6700\u7ec8\u56de\u590d\u4e2d\u786e\u8ba4\u5df2\u5b89\u6392\u7684\u5185\u5bb9\u3002\"\n)\n\ncalendar_agent = create_agent(\n    \"vllm:qwen3-4b\",\n    tools=[create_calendar_event, get_available_time_slots],\n    system_prompt=CALENDAR_AGENT_PROMPT,\n    name=\"calendar_agent\",\n)\n</code></pre> <p>\u63a5\u4e0b\u6765\uff0c\u4f7f\u7528 <code>wrap_agent_as_tool</code> \u5c06\u8fd9\u4e24\u4e2a\u5b50\u667a\u80fd\u4f53\u5c01\u88c5\u4e3a\u5de5\u5177\u3002</p> <pre><code>schedule_event = wrap_agent_as_tool(\n    calendar_agent,\n    tool_name=\"schedule_event\",\n    tool_description=(\n        \"\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u5b89\u6392\u65e5\u5386\u4e8b\u4ef6\u3002\"\n        \"\u5728\u7528\u6237\u60f3\u8981\u521b\u5efa\u3001\u4fee\u6539\u6216\u68c0\u67e5\u65e5\u5386\u7ea6\u4f1a\u65f6\u4f7f\u7528\u6b64\u529f\u80fd\u3002\"\n        \"\u80fd\u591f\u5904\u7406\u65e5\u671f/\u65f6\u95f4\u89e3\u6790\u3001\u67e5\u8be2\u53ef\u7528\u65f6\u95f4\u548c\u521b\u5efa\u4e8b\u4ef6\u3002\"\n        \"\u8f93\u5165\uff1a\u81ea\u7136\u8bed\u8a00\u65e5\u5386\u5b89\u6392\u8bf7\u6c42\uff08\u4f8b\u5982'\u4e0e\u8bbe\u8ba1\u56e2\u961f\u4e0b\u4e2a\u661f\u671f\u4e8c\u4e0b\u53482\u70b9\u7684\u4f1a\u8bae'\uff09\"\n    ),\n)\nmanage_email = wrap_agent_as_tool(\n    email_agent,\n    tool_name=\"manage_email\",\n    tool_description=(\n        \"\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u53d1\u9001\u7535\u5b50\u90ae\u4ef6\u3002\"\n        \"\u5728\u7528\u6237\u60f3\u8981\u53d1\u9001\u901a\u77e5\u3001\u63d0\u9192\u6216\u4efb\u4f55\u7535\u5b50\u90ae\u4ef6\u901a\u4fe1\u65f6\u4f7f\u7528\u6b64\u529f\u80fd\u3002\"\n        \"\u80fd\u591f\u63d0\u53d6\u6536\u4ef6\u4eba\u4fe1\u606f\u3001\u4e3b\u9898\u751f\u6210\u548c\u7535\u5b50\u90ae\u4ef6\u64b0\u5199\u3002\"\n        \"\u8f93\u5165\uff1a\u81ea\u7136\u8bed\u8a00\u7535\u5b50\u90ae\u4ef6\u8bf7\u6c42\uff08\u4f8b\u5982'\u5411\u4ed6\u4eec\u53d1\u9001\u4f1a\u8bae\u63d0\u9192'\uff09\"\n    ),\n)\n</code></pre> <p>\u6700\u7ec8\u521b\u5efa\u4e00\u4e2a <code>supervisor_agent</code>\uff0c\u5b83\u53ef\u4ee5\u8c03\u7528\u8fd9\u4e24\u4e2a\u5de5\u5177\u3002</p> <pre><code>SUPERVISOR_PROMPT = (\n    \"\u4f60\u662f\u4e00\u4e2a\u6709\u7528\u7684\u4e2a\u4eba\u52a9\u624b\u3002\"\n    \"\u4f60\u53ef\u4ee5\u5b89\u6392\u65e5\u5386\u4e8b\u4ef6\u5e76\u53d1\u9001\u7535\u5b50\u90ae\u4ef6\u3002\"\n    \"\u5c06\u7528\u6237\u8bf7\u6c42\u5206\u89e3\u4e3a\u9002\u5f53\u7684\u5de5\u5177\u8c03\u7528\uff0c\u5e76\u534f\u8c03\u7ed3\u679c\u3002\"\n    \"\u5f53\u8bf7\u6c42\u6d89\u53ca\u591a\u4e2a\u64cd\u4f5c\u65f6\uff0c\u8bf7\u4f7f\u7528\u591a\u4e2a\u5de5\u5177\u6309\u987a\u5e8f\u64cd\u4f5c\u3002\"\n)\n\n\nsupervisor_agent = create_agent(\n    \"vllm:qwen3-4b\",\n    tools=[schedule_event, manage_email],\n    system_prompt=SUPERVISOR_PROMPT,\n)\n\nprint(\n    supervisor_agent.invoke({\"messages\": [HumanMessage(content=\"\u67e5\u8be2\u660e\u5929\u7684\u7a7a\u95f2\u65f6\u95f4\")]})\n)\nprint(\n    supervisor_agent.invoke(\n        {\"messages\": [HumanMessage(content=\"\u7ed9test@123.com\u53d1\u9001\u90ae\u4ef6\u4f1a\u8bae\u63d0\u9192\")]}\n    )\n)\n</code></pre> <p>\u63d0\u793a</p> <p>\u4e0a\u8ff0\u793a\u4f8b\u4e2d\uff0c\u6211\u4eec\u662f\u4ece <code>langchain_dev_utils.agents</code> \u4e2d\u5bfc\u5165\u4e86 <code>create_agent</code> \u51fd\u6570\uff0c\u800c\u4e0d\u662f <code>langchain.agents</code>\u3002\u8fd9\u662f\u56e0\u4e3a\u672c\u5e93\u4e5f\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e0e\u5b98\u65b9 <code>create_agent</code> \u51fd\u6570\u529f\u80fd\u5b8c\u5168\u76f8\u540c\u7684\u51fd\u6570\uff0c\u53ea\u662f\u6269\u5145\u4e86\u901a\u8fc7\u5b57\u7b26\u4e32\u6307\u5b9a\u6a21\u578b\u7684\u529f\u80fd\u3002\u8fd9\u4f7f\u5f97\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 <code>register_model_provider</code> \u6ce8\u518c\u7684\u6a21\u578b\uff0c\u800c\u65e0\u9700\u521d\u59cb\u5316\u6a21\u578b\u5b9e\u4f8b\u540e\u4f20\u5165\u3002</p>"},{"location":"zh/adavance-guide/multi-agent/#_7","title":"\u5c01\u88c5\u591a\u4e2a\u667a\u80fd\u4f53\u4e3a\u5355\u4e00\u5de5\u5177","text":"<p>\u5c06\u591a\u4e2a\u667a\u80fd\u4f53\u5c01\u88c5\u4e3a\u5355\u4e00\u5de5\u5177\u53ea\u9700\u4e09\u6b65\uff1a</p> <ol> <li>\u5bfc\u5165 <code>wrap_all_agents_as_tool</code></li> <li>\u628a\u591a\u4e2a\u667a\u80fd\u4f53\u5b9e\u4f8b\u4f5c\u4e3a\u5217\u8868\u4e00\u6b21\u6027\u4f20\u5165</li> <li>\u83b7\u5f97\u53ef\u76f4\u63a5\u88ab\u5176\u4ed6\u667a\u80fd\u4f53\u8c03\u7528\u7684\u7edf\u4e00\u5de5\u5177\u5bf9\u8c61</li> </ol>"},{"location":"zh/adavance-guide/multi-agent/#_8","title":"\u51fd\u6570\u53c2\u6570\u8bf4\u660e","text":"\u53c2\u6570 \u8bf4\u660e <code>agents</code> \u667a\u80fd\u4f53\u5b9e\u4f8b\u5217\u8868\u3002\u7c7b\u578b: <code>list[CompiledStateGraph]</code>\u5fc5\u586b: \u662f <code>tool_name</code> \u5de5\u5177\u540d\u79f0\uff0c\u9ed8\u8ba4\u4e3a <code>task</code>\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u5426 <code>tool_description</code> \u5de5\u5177\u63cf\u8ff0\uff0c\u9ed8\u8ba4\u5305\u542b\u6240\u6709\u53ef\u7528\u667a\u80fd\u4f53\u4fe1\u606f\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u5426 <code>pre_input_hooks</code> \u667a\u80fd\u4f53\u8fd0\u884c\u524d\u7684\u94a9\u5b50\u51fd\u6570\u3002\u7c7b\u578b: <code>tuple[Callable, Callable] | Callable</code>\u5fc5\u586b: \u5426 <code>post_output_hooks</code> \u667a\u80fd\u4f53\u8fd0\u884c\u540e\u7684\u94a9\u5b50\u51fd\u6570\u3002\u7c7b\u578b: <code>tuple[Callable, Callable] | Callable</code>\u5fc5\u586b: \u5426"},{"location":"zh/adavance-guide/multi-agent/#_9","title":"\u4f7f\u7528\u793a\u4f8b","text":"<p>\u5bf9\u4e8e\u4e0a\u4e00\u4e2a\u793a\u4f8b\u7684 <code>calendar_agent</code> \u548c <code>email_agent</code>\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06\u5b83\u4eec\u5c01\u88c5\u4e3a\u4e00\u4e2a\u5de5\u5177 <code>call_subagent</code>\uff1a</p> <pre><code>call_subagent_tool = wrap_all_agents_as_tool(\n    [calendar_agent, email_agent],\n    tool_name=\"call_subagent\",\n    tool_description=(\n        \"\u8c03\u7528\u5b50\u667a\u80fd\u4f53\u6267\u884c\u4efb\u52a1\u3002\"\n        \"\u53ef\u4ee5\u4f7f\u7528\u7684\u667a\u80fd\u4f53\u6709\uff1a\"\n        \"- calendar_agent\uff1a\u7528\u4e8e\u5b89\u6392\u65e5\u5386\u4e8b\u4ef6\"\n        \"- email_agent\uff1a\u7528\u4e8e\u53d1\u9001\u7535\u5b50\u90ae\u4ef6\"\n    ),\n)\n\nMAIN_AGENT_PROMPT = (\n    \"\u4f60\u662f\u4e00\u4e2a\u6709\u7528\u7684\u4e2a\u4eba\u52a9\u624b\u3002\"\n    \"\u4f60\u53ef\u4ee5\u4f7f\u7528**call_subagent**\u5de5\u5177\u8c03\u7528\u5b50\u667a\u80fd\u4f53\u6267\u884c\u4efb\u52a1\u3002\"\n    \"\u5c06\u7528\u6237\u8bf7\u6c42\u5206\u89e3\u4e3a\u9002\u5f53\u7684\u5de5\u5177\u8c03\u7528\uff0c\u5e76\u534f\u8c03\u7ed3\u679c\u3002\"\n    \"\u5f53\u8bf7\u6c42\u6d89\u53ca\u591a\u4e2a\u64cd\u4f5c\u65f6\uff0c\u8bf7\u4f7f\u7528\u591a\u4e2a\u5de5\u5177\u6309\u987a\u5e8f\u64cd\u4f5c\u3002\"\n)\n\nmain_agent = create_agent(\n    \"vllm:qwen3-4b\",\n    tools=[call_subagent_tool],\n    system_prompt=MAIN_AGENT_PROMPT,\n)\n</code></pre> <p>\u63d0\u793a</p> <p>\u9664\u4e86\u4f7f\u7528\u672c\u5e93\u63d0\u4f9b\u7684 <code>wrap_all_agents_as_tool</code> \u5c06\u591a\u4e2a\u667a\u80fd\u4f53\u5c01\u88c5\u4e3a\u5355\u4e00\u5de5\u5177\u5916\uff0c\u4f60\u8fd8\u53ef\u4ee5\u4f7f\u7528 <code>deepagents</code> \u5e93\u63d0\u4f9b\u7684 <code>SubAgentMiddleware</code> \u4e2d\u95f4\u4ef6\u5b9e\u73b0\u7c7b\u4f3c\u7684\u6548\u679c\u3002</p>"},{"location":"zh/adavance-guide/multi-agent/#_10","title":"\u94a9\u5b50\u51fd\u6570","text":"<p>\u672c\u5e93\u5185\u7f6e\u4e86\u7075\u6d3b\u7684\u94a9\u5b50\uff08hook\uff09\u673a\u5236\uff0c\u5141\u8bb8\u5728\u5b50\u667a\u80fd\u4f53\u8fd0\u884c\u524d\u540e\u63d2\u5165\u81ea\u5b9a\u4e49\u903b\u8f91\u3002\u8be5\u673a\u5236\u540c\u65f6\u9002\u7528\u4e8e <code>wrap_agent_as_tool</code> \u4e0e <code>wrap_all_agents_as_tool</code>\uff0c\u4e0b\u6587\u4ee5 <code>wrap_agent_as_tool</code> \u4e3a\u4f8b\u8fdb\u884c\u8bf4\u660e\u3002</p>"},{"location":"zh/adavance-guide/multi-agent/#1-pre_input_hooks","title":"1. pre_input_hooks","text":"<p>\u5728\u667a\u80fd\u4f53\u8fd0\u884c\u524d\u5bf9\u8f93\u5165\u8fdb\u884c\u9884\u5904\u7406\u3002\u53ef\u7528\u4e8e\u8f93\u5165\u589e\u5f3a\u3001\u4e0a\u4e0b\u6587\u6ce8\u5165\u3001\u683c\u5f0f\u6821\u9a8c\u3001\u6743\u9650\u68c0\u67e5\u7b49\u3002</p>"},{"location":"zh/adavance-guide/multi-agent/#_11","title":"\u652f\u6301\u7684\u4f20\u5165\u7c7b\u578b","text":"\u7c7b\u578b \u8bf4\u660e \u5355\u4e2a\u540c\u6b65\u51fd\u6570 \u540c\u65f6\u7528\u4e8e\u540c\u6b65\uff08<code>invoke</code>\uff09\u548c\u5f02\u6b65\uff08<code>ainvoke</code>\uff09\u8c03\u7528\u8def\u5f84\uff08\u5f02\u6b65\u8def\u5f84\u4e2d\u4e0d\u4f1a <code>await</code>\uff0c\u76f4\u63a5\u8c03\u7528\uff09 \u4e8c\u5143\u7ec4 <code>(sync_func, async_func)</code> \u7b2c\u4e00\u4e2a\u51fd\u6570\u7528\u4e8e\u540c\u6b65\u8c03\u7528\u8def\u5f84\uff1b\u7b2c\u4e8c\u4e2a\u51fd\u6570\uff08\u5fc5\u987b\u662f <code>async def</code>\uff09\u7528\u4e8e\u5f02\u6b65\u8c03\u7528\u8def\u5f84\uff0c\u5e76\u4f1a\u88ab <code>await</code>"},{"location":"zh/adavance-guide/multi-agent/#_12","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def pre_input_hook(request: str, runtime: ToolRuntime) -&gt; str | dict[str, Any]:\n    \"\"\"\n    \u53c2\u6570:\n        request: \u539f\u59cb\u5de5\u5177\u8c03\u7528\u8f93\u5165\n        runtime: langchain \u7684 ToolRuntime\n\n    \u8fd4\u56de:\n        \u5904\u7406\u540e\u7684\u8f93\u5165\uff0c\u4f5c\u4e3a agent \u7684\u5b9e\u9645\u8f93\u5165\uff08\u9700\u8981\u662f str \u6216 dict\uff09\n    \"\"\"\n</code></pre> <p>\u6ce8\u610f\uff1a</p> <ul> <li> <p>\u94a9\u5b50\u51fd\u6570\u7684\u8fd4\u56de\u503c\u5fc5\u987b\u662f str \u6216 dict\uff0c\u5426\u5219\u4f1a\u5f15\u53d1 ValueError\u3002</p> </li> <li> <p>\u82e5\u8fd4\u56de dict\uff0c\u5219\u4f1a\u88ab\u76f4\u63a5\u4f5c\u4e3a agent \u7684\u5b9e\u9645\u8f93\u5165\u3002</p> </li> <li> <p>\u82e5\u8fd4\u56de str\uff0c\u5219\u4f1a\u88ab\u5305\u88c5\u4e3a <code>HumanMessage(content=...)</code>\uff0c\u6700\u7ec8\u4ee5 <code>{\"messages\": [HumanMessage(content=...)]}</code> \u4f5c\u4e3a agent \u7684\u5b9e\u9645\u8f93\u5165\u3002</p> </li> <li> <p>\u82e5\u672a\u63d0\u4f9b <code>pre_input_hooks</code>\uff0c\u5219\u76f4\u63a5\u5c06\u539f\u59cb\u8f93\u5165\u4ee5 <code>{\"messages\": [HumanMessage(content=request)]}</code> \u4f5c\u4e3a agent \u7684\u5b9e\u9645\u8f93\u5165\u3002</p> </li> </ul>"},{"location":"zh/adavance-guide/multi-agent/#_13","title":"\u4f7f\u7528\u793a\u4f8b","text":"<p>\u4f8b\u5982\uff0c\u5728\u8c03\u7528\u5b50\u667a\u80fd\u4f53\u4e4b\u524d\uff0c\u53ef\u4ee5\u4f7f\u7528\u6a21\u578b\u5bf9\u4e3b\u667a\u80fd\u4f53\u7684\u5bf9\u8bdd\u5386\u53f2\u8fdb\u884c\u6458\u8981\uff0c\u4ece\u800c\u4e3a\u5b50\u667a\u80fd\u4f53\u63d0\u4f9b\u66f4\u7cbe\u51c6\u7684\u4efb\u52a1\u4e0a\u4e0b\u6587\u3002</p> <pre><code>from langchain.tools import ToolRuntime\nfrom langchain_core.messages import SystemMessage\nfrom langchain_dev_utils.agents import wrap_agent_as_tool\n\n\ndef process_input(request: str, runtime: ToolRuntime) -&gt; str:\n    messages = runtime.state.get(\"messages\", [])\n\n    new_messages = [\n        SystemMessage(\n            content=\"\"\"\u8bf7\u57fa\u4e8e\u4ee5\u4e0b\u5bf9\u8bdd\u5386\u53f2\uff0c\u751f\u6210\u7b80\u6d01\u51c6\u786e\u7684\u6458\u8981\u3002\n            \u6458\u8981\u5e94\u5305\u542b\uff1a\n            1) \u5bf9\u8bdd\u4e3b\u9898\uff1b\n            2) \u5173\u952e\u5185\u5bb9\uff1b\n            3) \u5f53\u524d\u72b6\u6001\u6216\u8fdb\u5c55\u3002\n            \u6458\u8981\u957f\u5ea6\u63a7\u5236\u5728200\u5b57\u4ee5\u5185\u3002\"\"\"\n        ),\n        *messages,\n    ]\n\n    if messages:\n        summary = model.invoke(new_messages)\n\n        return (\n            \"&lt;history_summary&gt;\\n\"\n            + summary.content\n            + \"\\n&lt;/history_summary&gt;\\n\"\n            + \"&lt;task_description&gt;\\n\"\n            + request\n            + \"\\n&lt;/task_description&gt;\"\n        )\n    return \"&lt;task_description&gt;\\n\" + request + \"\\n&lt;/task_description&gt;\"\n\n\nasync def process_input_async(request: str, runtime: ToolRuntime) -&gt; str:\n    messages = runtime.state.get(\"messages\", [])\n\n    new_messages = [\n        SystemMessage(\n            content=\"\"\"\u8bf7\u57fa\u4e8e\u4ee5\u4e0b\u5bf9\u8bdd\u5386\u53f2\uff0c\u751f\u6210\u7b80\u6d01\u51c6\u786e\u7684\u6458\u8981\u3002\n            \u6458\u8981\u5e94\u5305\u542b\uff1a\n            1) \u5bf9\u8bdd\u4e3b\u9898\uff1b\n            2) \u5173\u952e\u4fe1\u606f\u70b9\uff1b\n            3) \u5f53\u524d\u72b6\u6001\u6216\u8fdb\u5c55\u3002\n            \u6458\u8981\u957f\u5ea6\u63a7\u5236\u5728200\u5b57\u4ee5\u5185\u3002\"\"\"\n        ),\n        *messages,\n    ]\n\n    if messages:\n        summary = await model.ainvoke(new_messages)\n\n        return (\n            \"&lt;history_summary&gt;\\n\"\n            + summary.content\n            + \"\\n&lt;/history_summary&gt;\\n\"\n            + \"&lt;task_description&gt;\\n\"\n            + request\n            + \"\\n&lt;/task_description&gt;\"\n        )\n    return \"&lt;task_description&gt;\\n\" + request + \"\\n&lt;/task_description&gt;\"\n\n\n# \u4f7f\u7528\ncall_agent_tool = wrap_agent_as_tool(\n    agent, pre_input_hooks=(process_input, process_input_async)\n)\n</code></pre>"},{"location":"zh/adavance-guide/multi-agent/#2-post_output_hooks","title":"2. post_output_hooks","text":"<p>\u5728\u667a\u80fd\u4f53\u8fd0\u884c\u5b8c\u6210\u540e\uff0c\u5bf9\u5176\u8fd4\u56de\u7684\u5b8c\u6574\u6d88\u606f\u5217\u8868\u8fdb\u884c\u540e\u5904\u7406\uff0c\u4ee5\u751f\u6210\u5de5\u5177\u7684\u6700\u7ec8\u8fd4\u56de\u503c\u3002\u53ef\u7528\u4e8e\u7ed3\u679c\u63d0\u53d6\u3001\u7ed3\u6784\u5316\u8f6c\u6362\u7b49\u3002</p>"},{"location":"zh/adavance-guide/multi-agent/#_14","title":"\u652f\u6301\u7684\u4f20\u5165\u7c7b\u578b","text":"\u7c7b\u578b \u8bf4\u660e \u5355\u4e2a\u51fd\u6570 \u540c\u65f6\u7528\u4e8e\u540c\u6b65\u548c\u5f02\u6b65\u8def\u5f84\uff08\u5f02\u6b65\u8def\u5f84\u4e2d\u4e0d <code>await</code>\uff09 \u4e8c\u5143\u7ec4 <code>(sync_func, async_func)</code> \u7b2c\u4e00\u4e2a\u7528\u4e8e\u540c\u6b65\u8def\u5f84\uff1b\u7b2c\u4e8c\u4e2a\uff08<code>async def</code>\uff09\u7528\u4e8e\u5f02\u6b65\u8def\u5f84\uff0c\u5e76\u4f1a\u88ab <code>await</code>"},{"location":"zh/adavance-guide/multi-agent/#_15","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def post_output_hook(request: str, response: dict[str, Any], runtime: ToolRuntime) -&gt; Union[str, Command]:\n    \"\"\"\n    \u53c2\u6570:\n        request: \u672a\u7ecf\u5904\u7406\u7684\u539f\u59cb\u8f93\u5165\n        response: agent \u8fd4\u56de\u7684\u5b8c\u6574\u54cd\u5e94\n        runtime: langchain \u7684 ToolRuntime\n\n    \u8fd4\u56de:\n        \u80fd\u591f\u88ab\u5e8f\u5217\u5316\u4e3a\u5b57\u7b26\u4e32\u7684\u503c\uff0c\u6216\u8005\u662f Command \u5bf9\u8c61\n    \"\"\"\n</code></pre> <p>\u6ce8\u610f\uff1a</p> <ul> <li> <p>\u94a9\u5b50\u51fd\u6570\u7684\u8fd4\u56de\u503c\u5fc5\u987b\u662f\u53ef\u4ee5\u88ab\u5e8f\u5217\u5316\u4e3a\u5b57\u7b26\u4e32\u7684\u503c\u6216\u8005 <code>Command</code> \u5bf9\u8c61\u3002</p> </li> <li> <p>\u94a9\u5b50\u51fd\u6570\u7684\u4e24\u4e2a\u5165\u53c2\uff0c<code>request</code> \u662f\u672a\u7ecf\u5904\u7406\u7684\u539f\u59cb\u8f93\u5165\uff0c<code>response</code> \u662f agent \u8fd4\u56de\u7684\u5b8c\u6574\u54cd\u5e94\uff08\u5373 <code>agent.invoke(input)</code> \u7684\u8fd4\u56de\u503c\uff09\u3002</p> </li> <li> <p>\u82e5\u672a\u63d0\u4f9b <code>post_output_hooks</code>\uff0c\u5219\u4f1a\u5c06 agent \u7684\u6700\u7ec8\u54cd\u5e94\u76f4\u63a5\u4f5c\u4e3a\u5de5\u5177\u7684\u8fd4\u56de\u503c\uff08\u5373 <code>response[\"messages\"][-1].content</code>\uff09\u3002</p> </li> </ul>"},{"location":"zh/adavance-guide/multi-agent/#_16","title":"\u4f7f\u7528\u793a\u4f8b","text":"<p>\u4f8b\u5982\uff0c\u5b50\u667a\u80fd\u4f53\u6267\u884c\u5b8c\u6bd5\u540e\uff0c\u9664\u4e86\u66f4\u65b0 <code>messages</code> \u952e\u5916\uff0c\u53ef\u80fd\u8fd8\u4f1a\u66f4\u65b0\u5176\u5b83\u72b6\u6001\u952e\u3002\u5982\u679c\u9700\u8981\u5c06\u8fd9\u4e9b\u989d\u5916\u7684\u72b6\u6001\u952e\u4fdd\u5b58\u5230\u4e3b\u667a\u80fd\u4f53\u7684\u72b6\u6001\u4e2d\uff0c\u53ef\u4ee5\u4f7f\u7528 <code>Command</code> \u5bf9\u8c61\u8fdb\u884c\u8fd4\u56de\u3002</p> <pre><code>from typing import Any\nfrom langchain.tools import ToolRuntime\nfrom langchain_core.messages import ToolMessage\nfrom langgraph.types import Command\n\n\ndef process_output_sync(\n    request: str, response: dict[str, Any], runtime: ToolRuntime\n) -&gt; Command:\n    return Command(\n        update={\n            \"messages\": [\n                ToolMessage(\n                    content=response[\"messages\"][-1].content,\n                    tool_call_id=runtime.tool_call_id,\n                )\n            ],\n            \"example_state_key\": response[\"example_state_key\"],\n        }\n    )\n\n\nasync def process_output_async(\n    request: str, response: dict[str, Any], runtime: ToolRuntime\n) -&gt; Command:\n    return Command(\n        update={\n            \"messages\": [\n                ToolMessage(\n                    content=response[\"messages\"][-1].content,\n                    tool_call_id=runtime.tool_call_id,\n                )\n            ],\n            \"example_state_key\": response[\"example_state_key\"],\n        }\n    )\n\n\n# \u4f7f\u7528\ncall_agent_tool = wrap_agent_as_tool(\n    agent, post_output_hooks=(process_output_sync, process_output_async)\n)\n</code></pre>"},{"location":"zh/adavance-guide/openai-compatible/","title":"OpenAI \u517c\u5bb9 API \u6a21\u578b\u63d0\u4f9b\u5546\u96c6\u6210","text":"<p>\u524d\u63d0\u6761\u4ef6</p> <p>\u4f7f\u7528\u6b64\u529f\u80fd\u65f6\uff0c\u5fc5\u987b\u5b89\u88c5 standard \u7248\u672c\u7684 <code>langchain-dev-utils</code> \u5e93\u3002\u5177\u4f53\u53ef\u4ee5\u53c2\u8003\u5b89\u88c5\u90e8\u5206\u7684\u4ecb\u7ecd\u3002</p>"},{"location":"zh/adavance-guide/openai-compatible/#_1","title":"\u6982\u8ff0","text":"<p>\u8bb8\u591a\u6a21\u578b\u63d0\u4f9b\u5546\u90fd\u63d0\u4f9b OpenAI \u517c\u5bb9 API \u670d\u52a1\uff0c\u4f8b\u5982 vLLM\u3001OpenRouter \u548c Together AI \u7b49\u3002\u672c\u5e93\u63d0\u4f9b\u4e00\u5957 OpenAI \u517c\u5bb9 API \u96c6\u6210\u65b9\u6848\uff0c\u8986\u76d6\u5bf9\u8bdd\u6a21\u578b\u4e0e\u5d4c\u5165\u6a21\u578b\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u300c\u63d0\u4f9b\u5546\u5df2\u63d0\u4f9b OpenAI \u517c\u5bb9 API\uff0c\u4f46\u5c1a\u65e0\u5bf9\u5e94 LangChain \u96c6\u6210\u300d\u7684\u573a\u666f\u3002</p> <p>\u672c\u5e93\u63d0\u4f9b\u4e86\u4e24\u4e2a\u5de5\u5177\u51fd\u6570\uff0c\u7528\u4e8e\u521b\u5efa\u5bf9\u8bdd\u6a21\u578b\u96c6\u6210\u7c7b\u4e0e\u5d4c\u5165\u6a21\u578b\u96c6\u6210\u7c7b\uff1a</p> \u51fd\u6570\u540d \u8bf4\u660e <code>create_openai_compatible_model</code> \u521b\u5efa\u5bf9\u8bdd\u6a21\u578b\u96c6\u6210\u7c7b <code>create_openai_compatible_embedding</code> \u521b\u5efa\u5d4c\u5165\u6a21\u578b\u96c6\u6210\u7c7b <p>\u8bf4\u660e</p> <p>\u672c\u5e93\u63d0\u4f9b\u7684\u4e24\u4e2a\u5de5\u5177\u51fd\u6570\u7684\u6700\u521d\u7075\u611f\u501f\u9274\u81ea JavaScript \u751f\u6001\u7684 @ai-sdk/openai-compatible\u3002</p> <p>\u4ee5\u4e0b\u5c06\u4ee5\u63a5\u5165 vLLM \u4e3a\u4f8b\uff0c\u5c55\u793a\u5982\u4f55\u4f7f\u7528\u672c\u529f\u80fd\u3002</p> vLLM \u4ecb\u7ecd <p>vLLM \u662f\u5e38\u7528\u7684\u5927\u6a21\u578b\u63a8\u7406\u6846\u67b6\uff0c\u9002\u5408\u672c\u5730\u6216\u81ea\u5efa\u73af\u5883\u4e0b\u7684\u9ad8\u6027\u80fd\u63a8\u7406\u670d\u52a1\u3002\u5b83\u53ef\u4ee5\u5c06\u5927\u6a21\u578b\u90e8\u7f72\u4e3a OpenAI \u517c\u5bb9\u7684 API\uff0c\u4fbf\u4e8e\u590d\u7528\u73b0\u6709\u7684 SDK \u4e0e\u8c03\u7528\u65b9\u5f0f\uff1b\u540c\u65f6\u652f\u6301\u5bf9\u8bdd\u6a21\u578b\u4e0e\u5d4c\u5165\u6a21\u578b\u7684\u90e8\u7f72\uff0c\u4ee5\u53ca\u591a\u6a21\u578b\u670d\u52a1\u3001\u5de5\u5177\u8c03\u7528\u4e0e\u63a8\u7406\u8f93\u51fa\u7b49\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u5bf9\u8bdd\u3001\u5de5\u5177\u8c03\u7528\u4e0e\u591a\u6a21\u6001\u7b49\u573a\u666f\u3002</p> <p>\u4ee5\u4e0b\u793a\u4f8b\u5747\u4e3a\u540e\u7eed\u5185\u5bb9\u4e2d\u4f1a\u7528\u5230\u7684\u6a21\u578b\u90e8\u7f72\u547d\u4ee4\uff1a</p> <p>Qwen3-4B\uff1a</p> <pre><code>vllm serve Qwen/Qwen3-4B \\\n--reasoning-parser qwen3 \\\n--enable-auto-tool-choice --tool-call-parser hermes \\\n--host 0.0.0.0 --port 8000 \\\n--served-model-name qwen3-4b\n</code></pre> <p>GLM-4.7-Flash\uff1a</p> <pre><code>vllm serve zai-org/GLM-4.7-Flash \\\n --tensor-parallel-size 4 \\\n --speculative-config.method mtp \\\n --speculative-config.num_speculative_tokens 1 \\\n --tool-call-parser glm47 \\\n --reasoning-parser glm45 \\\n --enable-auto-tool-choice \\\n --served-model-name glm-4.7-flash\n</code></pre> <p>Qwen3-VL-2B-Instruct\uff1a</p> <pre><code>vllm serve Qwen/Qwen3-VL-2B-Instruct \\\n--trust-remote-code \\\n--host 0.0.0.0 --port 8000 \\\n--served-model-name qwen3-vl-2b\n</code></pre> <p>Qwen3-Embedding-4B\uff1a</p> <p><pre><code>vllm serve Qwen/Qwen3-Embedding-4B \\\n--task embed \\\n--served-model-name qwen3-embedding-4b \\\n--host 0.0.0.0 --port 8000\n</code></pre> \u670d\u52a1\u5730\u5740\u4e3a <code>http://localhost:8000/v1</code>\u3002</p>"},{"location":"zh/adavance-guide/openai-compatible/#_2","title":"\u5bf9\u8bdd\u6a21\u578b\u7684\u521b\u5efa\u4e0e\u4f7f\u7528","text":""},{"location":"zh/adavance-guide/openai-compatible/#_3","title":"\u521b\u5efa\u5bf9\u8bdd\u6a21\u578b\u7c7b","text":"<p>\u4f7f\u7528 <code>create_openai_compatible_model</code> \u51fd\u6570\u53ef\u4ee5\u521b\u5efa\u5bf9\u8bdd\u6a21\u578b\u96c6\u6210\u7c7b\u3002\u8be5\u51fd\u6570\u63a5\u53d7\u4ee5\u4e0b\u53c2\u6570\uff1a</p> \u53c2\u6570 \u8bf4\u660e <code>model_provider</code> \u6a21\u578b\u63d0\u4f9b\u5546\u540d\u79f0\uff0c\u4f8b\u5982 <code>vllm</code>\u3002\u5fc5\u987b\u4ee5\u5b57\u6bcd\u6216\u6570\u5b57\u5f00\u5934\uff0c\u53ea\u80fd\u5305\u542b\u5b57\u6bcd\u3001\u6570\u5b57\u548c\u4e0b\u5212\u7ebf\uff0c\u957f\u5ea6\u4e0d\u8d85\u8fc7 20 \u4e2a\u5b57\u7b26\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u662f <code>base_url</code> \u6a21\u578b\u63d0\u4f9b\u5546\u9ed8\u8ba4 API \u5730\u5740\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u5426 <code>compatibility_options</code> \u517c\u5bb9\u6027\u9009\u9879\u914d\u7f6e\u3002\u7c7b\u578b: <code>dict</code>\u5fc5\u586b: \u5426 <code>model_profiles</code> \u8be5\u63d0\u4f9b\u5546\u5404\u6a21\u578b\u7684 profile \u914d\u7f6e\u5b57\u5178\u3002\u7c7b\u578b: <code>dict</code>\u5fc5\u586b: \u5426 <code>chat_model_cls_name</code> \u5bf9\u8bdd\u6a21\u578b\u7c7b\u540d\uff08\u9700\u7b26\u5408 Python \u7c7b\u540d\u89c4\u8303\uff09\u3002\u9ed8\u8ba4\u503c\u4e3a <code>Chat{model_provider}</code>\uff08\u5176\u4e2d <code>{model_provider}</code> \u9996\u5b57\u6bcd\u5927\u5199\uff09\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u5426 <p>\u5176\u4e2d\uff0c<code>compatibility_options</code> \u662f\u4e00\u4e2a\u5b57\u5178\uff0c\u7528\u4e8e\u58f0\u660e\u8be5\u63d0\u4f9b\u5546\u5bf9 OpenAI API \u7684\u90e8\u5206\u7279\u6027\u7684\u652f\u6301\u60c5\u51b5\uff0c\u4ee5\u63d0\u9ad8\u517c\u5bb9\u6027\u548c\u7a33\u5b9a\u6027\u3002</p> <p>\u76ee\u524d\u652f\u6301\u4ee5\u4e0b\u914d\u7f6e\u9879\uff1a</p> \u914d\u7f6e\u9879 \u8bf4\u660e <code>supported_tool_choice</code> \u652f\u6301\u7684 <code>tool_choice</code> \u7b56\u7565\u5217\u8868\u3002\u7c7b\u578b: <code>list[str]</code>\u9ed8\u8ba4\u503c: <code>[\"auto\"]</code> <code>supported_response_format</code> \u652f\u6301\u7684 <code>response_format</code> \u683c\u5f0f\u5217\u8868\uff08<code>json_schema</code>\u3001<code>json_object</code>\uff09\u3002\u7c7b\u578b: <code>list[str]</code>\u9ed8\u8ba4\u503c: <code>[]</code> <code>reasoning_keep_policy</code> \u5386\u53f2\u6d88\u606f\u4e2d <code>reasoning_content</code> \u5b57\u6bb5\u7684\u4fdd\u7559\u7b56\u7565\u3002\u7c7b\u578b: <code>str</code>\u9ed8\u8ba4\u503c: <code>\"never\"</code> <code>include_usage</code> \u662f\u5426\u5728\u6d41\u5f0f\u8fd4\u56de\u7ed3\u679c\u4e2d\u5305\u542b <code>usage</code> \u4fe1\u606f\u3002\u7c7b\u578b: <code>bool</code>\u9ed8\u8ba4\u503c: <code>True</code> <p>\u8865\u5145</p> <p>\u7531\u4e8e\u540c\u4e00\u6a21\u578b\u63d0\u4f9b\u5546\u7684\u4e0d\u540c\u6a21\u578b\u5bf9 <code>tool_choice</code>\u3001<code>response_format</code> \u7b49\u53c2\u6570\u7684\u652f\u6301\u60c5\u51b5\u5b58\u5728\u5dee\u5f02\uff0c\u8fd9\u56db\u4e2a\u517c\u5bb9\u6027\u9009\u9879\u4e3a\u7c7b\u7684\u5b9e\u4f8b\u5c5e\u6027\u3002\u56e0\u6b64\uff0c\u521b\u5efa\u5bf9\u8bdd\u6a21\u578b\u7c7b\u65f6\u53ef\u4ee5\u4f20\u5165\u503c\u4f5c\u4e3a\u5168\u5c40\u9ed8\u8ba4\u503c\uff08\u4ee3\u8868\u8be5\u63d0\u4f9b\u5546\u5927\u90e8\u5206\u6a21\u578b\u652f\u6301\u7684\u914d\u7f6e\uff09\uff0c\u540e\u7eed\u5982\u9700\u9488\u5bf9\u7279\u5b9a\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u53ef\u5728\u5b9e\u4f8b\u5316\u65f6\u8986\u76d6\u540c\u540d\u53c2\u6570\u3002</p> <p>\u63d0\u793a</p> <p>\u672c\u5e93\u4f1a\u57fa\u4e8e\u7528\u6237\u4f20\u5165\u7684\u53c2\u6570\uff0c\u4f7f\u7528\u5185\u7f6e\u7684 <code>BaseChatOpenAICompatible</code> \u6784\u5efa\u9762\u5411\u7279\u5b9a\u63d0\u4f9b\u5546\u7684\u5bf9\u8bdd\u6a21\u578b\u7c7b\u3002\u8be5\u7c7b\u7ee7\u627f\u81ea <code>langchain-openai</code> \u7684 <code>BaseChatOpenAI</code>\uff0c\u5e76\u5728\u4ee5\u4e0b\u65b9\u9762\u8fdb\u884c\u4e86\u589e\u5f3a\uff1a</p> <ul> <li>\u652f\u6301\u66f4\u591a\u683c\u5f0f\u7684\u63a8\u7406\u5185\u5bb9\uff1a\u9664 OpenAI \u5b98\u65b9\u683c\u5f0f\u5916\uff0c\u4e5f\u652f\u6301\u4ee5 <code>reasoning_content</code> \u53c2\u6570\u8fd4\u56de\u7684\u63a8\u7406\u5185\u5bb9\u683c\u5f0f\u3002</li> <li>\u652f\u6301 <code>video</code> \u7c7b\u578b\u7684 content_block\uff1a\u8865\u9f50 <code>ChatOpenAI</code> \u5728\u89c6\u9891\u7c7b\u578b\u7684 <code>content_block</code> \u4e0a\u7684\u80fd\u529b\u7f3a\u53e3\u3002</li> <li>\u81ea\u52a8\u9009\u62e9\u66f4\u5408\u9002\u7684\u7ed3\u6784\u5316\u8f93\u51fa\u65b9\u5f0f\uff1a\u6839\u636e\u63d0\u4f9b\u5546\u5b9e\u9645\u652f\u6301\u60c5\u51b5\uff0c\u5728 <code>function_calling</code> \u4e0e <code>json_schema</code> \u4e4b\u95f4\u81ea\u52a8\u9009\u62e9\u66f4\u4f18\u65b9\u6848\u3002</li> <li>\u901a\u8fc7 <code>compatibility_options</code> \u7cbe\u7ec6\u9002\u914d\u5dee\u5f02\uff1a\u6309\u9700\u914d\u7f6e\u5bf9 <code>tool_choice</code>\u3001<code>response_format</code> \u7b49\u53c2\u6570\u7684\u652f\u6301\u5dee\u5f02\u3002</li> </ul> <p>\u4f7f\u7528\u5982\u4e0b\u4ee3\u7801\u521b\u5efa\u5bf9\u8bdd\u6a21\u578b\u7c7b\uff1a</p> <pre><code>from langchain_dev_utils.chat_models.adapters import create_openai_compatible_model\n\nChatVLLM = create_openai_compatible_model(\n    model_provider=\"vllm\",\n    base_url=\"http://localhost:8000/v1\",\n    chat_model_cls_name=\"ChatVLLM\",\n)\n\nmodel = ChatVLLM(model=\"qwen3-4b\")\nprint(model.invoke(\"\u4f60\u597d\"))\n</code></pre> <p>\u5728\u521b\u5efa\u5bf9\u8bdd\u6a21\u578b\u7c7b\u65f6\uff0c<code>base_url</code> \u53c2\u6570\u53ef\u4ee5\u7701\u7565\u3002\u672a\u4f20\u5165\u65f6\uff0c\u672c\u5e93\u4f1a\u9ed8\u8ba4\u8bfb\u53d6\u5bf9\u5e94\u73af\u5883\u53d8\u91cf\uff0c\u4f8b\u5982\uff1a</p> <pre><code>export VLLM_API_BASE=http://localhost:8000/v1\n</code></pre> <p>\u6b64\u65f6\u4ee3\u7801\u53ef\u4ee5\u7701\u7565 <code>base_url</code>\uff1a</p> <pre><code>from langchain_dev_utils.chat_models.adapters import create_openai_compatible_model\n\nChatVLLM = create_openai_compatible_model(\n    model_provider=\"vllm\",\n    chat_model_cls_name=\"ChatVLLM\",\n)\n\nmodel = ChatVLLM(model=\"qwen3-4b\")\nprint(model.invoke(\"\u4f60\u597d\"))\n</code></pre> <p>\u6ce8\u610f\uff1a\u4e0a\u8ff0\u4ee3\u7801\u6210\u529f\u8fd0\u884c\u7684\u524d\u63d0\u662f\u5df2\u914d\u7f6e\u73af\u5883\u53d8\u91cf <code>VLLM_API_KEY</code>\u3002\u867d\u7136 vLLM \u672c\u8eab\u4e0d\u8981\u6c42 API Key\uff0c\u4f46\u5bf9\u8bdd\u6a21\u578b\u7c7b\u521d\u59cb\u5316\u65f6\u9700\u8981\u4f20\u5165\uff0c\u56e0\u6b64\u8bf7\u5148\u8bbe\u7f6e\u8be5\u53d8\u91cf\uff0c\u4f8b\u5982\uff1a</p> <pre><code>export VLLM_API_KEY=vllm_api_key\n</code></pre> <p>\u63d0\u793a</p> <p>\u521b\u5efa\u7684\u5bf9\u8bdd\u6a21\u578b\u7c7b\uff08\u5d4c\u5165\u6a21\u578b\u7c7b\u4e5f\u9075\u5faa\u6b64\u547d\u540d\u89c4\u5219\uff09\u73af\u5883\u53d8\u91cf\u7684\u547d\u540d\u89c4\u5219\uff1a</p> <ul> <li> <p>API \u5730\u5740\uff1a<code>${PROVIDER_NAME}_API_BASE</code>\uff08\u5168\u5927\u5199\uff0c\u4e0b\u5212\u7ebf\u5206\u9694\uff09\u3002</p> </li> <li> <p>API Key\uff1a<code>${PROVIDER_NAME}_API_KEY</code>\uff08\u5168\u5927\u5199\uff0c\u4e0b\u5212\u7ebf\u5206\u9694\uff09\u3002</p> </li> </ul>"},{"location":"zh/adavance-guide/openai-compatible/#_4","title":"\u4f7f\u7528\u5bf9\u8bdd\u6a21\u578b\u7c7b","text":""},{"location":"zh/adavance-guide/openai-compatible/#_5","title":"\u666e\u901a\u8c03\u7528","text":"<p>\u901a\u8fc7 <code>invoke</code> \u65b9\u6cd5\u53ef\u4ee5\u8fdb\u884c\u666e\u901a\u8c03\u7528\uff0c\u8fd4\u56de\u6a21\u578b\u54cd\u5e94\u3002</p> <pre><code>from langchain_core.messages import HumanMessage\n\nmodel = ChatVLLM(model=\"qwen3-4b\")\nresponse = model.invoke([HumanMessage(\"Hello\")])\nprint(response)\n</code></pre> <p>\u540c\u65f6\u4e5f\u652f\u6301 <code>ainvoke</code> \u8fdb\u884c\u5f02\u6b65\u8c03\u7528\uff1a</p> <pre><code>from langchain_core.messages import HumanMessage\n\nmodel = ChatVLLM(model=\"qwen3-4b\")\nresponse = await model.ainvoke([HumanMessage(\"Hello\")])\nprint(response)\n</code></pre>"},{"location":"zh/adavance-guide/openai-compatible/#_6","title":"\u6d41\u5f0f\u8c03\u7528","text":"<p>\u901a\u8fc7 <code>stream</code> \u65b9\u6cd5\u53ef\u4ee5\u8fdb\u884c\u6d41\u5f0f\u8c03\u7528\uff0c\u7528\u4e8e\u6d41\u5f0f\u8fd4\u56de\u6a21\u578b\u54cd\u5e94\u3002</p> <pre><code>from langchain_core.messages import HumanMessage\n\nmodel = ChatVLLM(model=\"qwen3-4b\")\nfor chunk in model.stream([HumanMessage(\"Hello\")]):\n    print(chunk)\n</code></pre> <p>\u4ee5\u53ca\u901a\u8fc7 <code>astream</code> \u8fdb\u884c\u5f02\u6b65\u6d41\u5f0f\u8c03\u7528\uff1a</p> <pre><code>from langchain_core.messages import HumanMessage\n\nmodel = ChatVLLM(model=\"qwen3-4b\")\nasync for chunk in model.astream([HumanMessage(\"Hello\")]):\n    print(chunk)\n</code></pre> \u6d41\u5f0f\u8f93\u51fa\u9009\u9879 <p>\u53ef\u4ee5\u901a\u8fc7 <code>stream_options={\"include_usage\": True}</code> \u5728\u6d41\u5f0f\u54cd\u5e94\u672b\u5c3e\u9644\u52a0 token \u4f7f\u7528\u60c5\u51b5\uff08<code>prompt_tokens</code> \u548c <code>completion_tokens</code>\uff09\u3002 \u672c\u5e93\u9ed8\u8ba4\u5f00\u542f\u8be5\u9009\u9879\uff1b\u82e5\u9700\u5173\u95ed\uff0c\u53ef\u5728\u521b\u5efa\u6a21\u578b\u7c7b\u6216\u5b9e\u4f8b\u5316\u65f6\u4f20\u5165\u517c\u5bb9\u6027\u9009\u9879 <code>include_usage=False</code>\u3002</p>"},{"location":"zh/adavance-guide/openai-compatible/#_7","title":"\u5de5\u5177\u8c03\u7528","text":"<p>\u5982\u679c\u6a21\u578b\u672c\u8eab\u652f\u6301\u5de5\u5177\u8c03\u7528\uff0c\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 <code>bind_tools</code> \u8fdb\u884c\u5de5\u5177\u8c03\u7528\uff1a</p> <pre><code>from langchain_core.messages import HumanMessage\nfrom langchain_core.tools import tool\nimport datetime\n\n@tool\ndef get_current_time() -&gt; str:\n    \"\"\"\u83b7\u53d6\u5f53\u524d\u65f6\u95f4\u6233\"\"\"\n    return str(datetime.datetime.now().timestamp())\n\nmodel = ChatVLLM(model=\"qwen3-4b\").bind_tools([get_current_time])\nresponse = model.invoke([HumanMessage(\"\u83b7\u53d6\u5f53\u524d\u65f6\u95f4\u6233\")])\nprint(response)\n</code></pre> \u5e76\u884c\u5de5\u5177\u8c03\u7528 <p>\u5982\u679c\u6a21\u578b\u652f\u6301\u5e76\u884c\u5de5\u5177\u8c03\u7528\uff0c\u53ef\u4ee5\u5728 <code>bind_tools</code> \u4e2d\u4f20\u9012 <code>parallel_tool_calls=True</code> \u5f00\u542f\u5e76\u884c\u5de5\u5177\u8c03\u7528\uff08\u90e8\u5206\u6a21\u578b\u63d0\u4f9b\u5546\u9ed8\u8ba4\u5f00\u542f\uff0c\u5219\u65e0\u9700\u663e\u5f0f\u4f20\u53c2\uff09\u3002</p> <p>\u4f8b\u5982\uff1a</p> <pre><code>from langchain_core.messages import HumanMessage\nfrom langchain_core.tools import tool\n\n\n@tool\ndef get_current_weather(location: str) -&gt; str:\n    \"\"\"\u83b7\u53d6\u5f53\u524d\u5929\u6c14\"\"\"\n    return f\"\u5f53\u524d{location}\u7684\u5929\u6c14\u662f\u6674\u6717\"\n\nmodel = ChatVLLM(model=\"qwen3-4b\").bind_tools(\n    [get_current_weather], parallel_tool_calls=True\n)\nresponse = model.invoke([HumanMessage(\"\u83b7\u53d6\u6d1b\u6749\u77f6\u548c\u4f26\u6566\u7684\u5929\u6c14\")])\nprint(response)\n</code></pre> \u5f3a\u5236\u5de5\u5177\u8c03\u7528 <p>\u901a\u8fc7 <code>tool_choice</code> \u53c2\u6570\uff0c\u53ef\u4ee5\u63a7\u5236\u6a21\u578b\u5728\u54cd\u5e94\u65f6\u662f\u5426\u8c03\u7528\u5de5\u5177\u4ee5\u53ca\u8c03\u7528\u54ea\u4e2a\u5de5\u5177\uff0c\u4ee5\u63d0\u5347\u51c6\u786e\u6027\u3001\u53ef\u9760\u6027\u548c\u53ef\u63a7\u6027\u3002\u5e38\u89c1\u53d6\u503c\u6709\uff1a</p> <ul> <li><code>\"auto\"</code>\uff1a\u6a21\u578b\u81ea\u4e3b\u51b3\u5b9a\u662f\u5426\u8c03\u7528\u5de5\u5177\uff08\u9ed8\u8ba4\u884c\u4e3a\uff09\uff1b</li> <li><code>\"none\"</code>\uff1a\u7981\u6b62\u8c03\u7528\u5de5\u5177\uff1b</li> <li><code>\"required\"</code>\uff1a\u5f3a\u5236\u8c03\u7528\u81f3\u5c11\u4e00\u4e2a\u5de5\u5177\uff1b</li> <li>\u6307\u5b9a\u5177\u4f53\u5de5\u5177\uff08\u5728 OpenAI \u517c\u5bb9 API \u4e2d\uff0c\u5177\u4f53\u4e3a <code>{\"type\": \"function\", \"function\": {\"name\": \"xxx\"}}</code>\uff09\u3002</li> </ul> <p>\u4e0d\u540c\u63d0\u4f9b\u5546\u5bf9<code>tool_choice</code>\u7684\u652f\u6301\u8303\u56f4\u4e0d\u540c\u3002\u4e3a\u89e3\u51b3\u5dee\u5f02\uff0c\u672c\u5e93\u5f15\u5165\u517c\u5bb9\u6027\u914d\u7f6e\u9879 <code>supported_tool_choice</code>\uff0c\u9ed8\u8ba4\u503c\u4e3a <code>[\"auto\"]</code>\uff0c\u6b64\u65f6<code>bind_tools</code> \u4e2d\u4f20\u5165\u7684 <code>tool_choice</code> \u53ea\u80fd\u4e3a <code>auto</code>\uff0c\u5176\u4ed6\u53d6\u503c\u4f1a\u88ab\u8fc7\u6ee4\u3002</p> <p>\u82e5\u9700\u652f\u6301\u4f20\u9012\u5176\u4ed6 <code>tool_choice</code> \u53d6\u503c\uff0c\u5fc5\u987b\u914d\u7f6e\u652f\u6301\u9879\u3002\u914d\u7f6e\u503c\u4e3a\u5b57\u7b26\u4e32\u5217\u8868\uff0c\u6bcf\u4e2a\u5b57\u7b26\u4e32\u7684\u53ef\u9009\u503c\uff1a</p> <ul> <li><code>\"auto\"</code>, <code>\"none\"</code>, <code>\"required\"</code>\uff1a\u5bf9\u5e94\u6807\u51c6\u7b56\u7565\uff1b</li> <li><code>\"specific\"</code>\uff1a\u672c\u5e93\u7279\u6709\u6807\u8bc6\uff0c\u8868\u793a\u652f\u6301\u6307\u5b9a\u5177\u4f53\u5de5\u5177\u3002</li> </ul> <p>\u4f8b\u5982 vLLM \u652f\u6301\u5168\u90e8\u7b56\u7565\uff1a</p> <pre><code>from langchain_dev_utils.chat_models.adapters import create_openai_compatible_model\n\nChatVLLM = create_openai_compatible_model(\n    model_provider=\"vllm\",\n    chat_model_cls_name=\"ChatVLLM\",\n    compatibility_options={\n        \"supported_tool_choice\": [\"auto\", \"required\", \"none\", \"specific\"]\n    },\n)\n\nmodel = ChatVLLM(model=\"qwen3-4b\").bind_tools(\n    [get_current_weather], tool_choice=\"required\"\n)\n</code></pre>"},{"location":"zh/adavance-guide/openai-compatible/#_8","title":"\u7ed3\u6784\u5316\u8f93\u51fa","text":"<pre><code>from langchain_core.messages import HumanMessage\nfrom pydantic import BaseModel\n\nclass User(BaseModel):\n    name: str\n    age: int\n\nmodel = ChatVLLM(model=\"qwen3-4b\").with_structured_output(User)\nresponse = model.invoke([HumanMessage(\"\u4f60\u597d\uff0c\u6211\u53eb\u5f20\u4e09\uff0c\u4eca\u5e7425\u5c81\")])\nprint(response)\n</code></pre> \u9ed8\u8ba4\u7684\u7ed3\u6784\u5316\u8f93\u51fa\u65b9\u6cd5 <p>\u76ee\u524d\u5e38\u89c1\u7684\u7ed3\u6784\u5316\u8f93\u51fa\u65b9\u6cd5\u6709\u4e09\u79cd\uff1a<code>json_schema</code>\u3001<code>function_calling</code>\u3001<code>json_mode</code>\u3002\u5176\u4e2d\uff0c\u6548\u679c\u6700\u597d\u7684\u662f<code>json_schema</code>\uff0c\u6545\u672c\u5e93\u7684 <code>with_structured_output</code> \u4f1a\u4f18\u5148\u4f7f\u7528<code>json_schema</code>\u4f5c\u4e3a\u7ed3\u6784\u5316\u8f93\u51fa\u65b9\u6cd5\uff1b\u5f53\u63d0\u4f9b\u5546\u4e0d\u652f\u6301\u65f6\uff0c\u624d\u4f1a\u81ea\u52a8\u964d\u7ea7\u4e3a <code>function_calling</code>\u3002\u4e0d\u540c\u7684\u6a21\u578b\u63d0\u4f9b\u5546\u5bf9\u4e8e\u7ed3\u6784\u5316\u8f93\u51fa\u7684\u652f\u6301\u7a0b\u5ea6\u6709\u6240\u4e0d\u540c\u3002\u672c\u5e93\u901a\u8fc7\u517c\u5bb9\u6027\u914d\u7f6e\u9879 <code>supported_response_format</code> \u58f0\u660e\u63d0\u4f9b\u5546\u652f\u6301\u7684\u7ed3\u6784\u5316\u8f93\u51fa\u65b9\u6cd5\u3002\u9ed8\u8ba4\u503c\u4e3a <code>[]</code>\uff0c\u8868\u793a\u65e2\u4e0d\u652f\u6301 <code>json_schema</code> \u4e5f\u4e0d\u652f\u6301 <code>json_mode</code>\u3002\u6b64\u65f6 <code>with_structured_output(method=...)</code> \u4f1a\u56fa\u5b9a\u4f7f\u7528 <code>function_calling</code>\uff1b\u5373\u4f7f\u4f20\u5165 <code>json_schema</code> / <code>json_mode</code> \u4e5f\u4f1a\u81ea\u52a8\u8f6c\u5316\u4e3a <code>function_calling</code>\uff0c\u5982\u679c\u60f3\u8981\u4f7f\u7528\u5bf9\u5e94\u7684\u7ed3\u6784\u5316\u8f93\u51fa\u65b9\u6cd5\uff0c\u9700\u8981\u663e\u5f0f\u4f20\u5165\u54cd\u5e94\u7684\u53c2\u6570\uff08\u5c24\u5176\u662f<code>json_schema</code>)\u3002</p> <p>\u4f8b\u5982\uff0cvLLM \u90e8\u7f72\u7684\u6a21\u578b\u652f\u6301 <code>json_schema</code> \u7ed3\u6784\u5316\u8f93\u51fa\u65b9\u6cd5\uff0c\u5219\u53ef\u4ee5\u5728\u6ce8\u518c\u65f6\u8fdb\u884c\u58f0\u660e\uff1a</p> <pre><code>from langchain_dev_utils.chat_models.adapters import create_openai_compatible_model\n\nChatVLLM = create_openai_compatible_model(\n    model_provider=\"vllm\",\n    chat_model_cls_name=\"ChatVLLM\",\n    compatibility_options={\"supported_response_format\": [\"json_schema\"]},\n)\n\nmodel = ChatVLLM(model=\"qwen3-4b\")\n</code></pre> <p>\u6ce8\u610f</p> <p><code>supported_response_format</code>\u76ee\u524d\u4ec5\u5f71\u54cd<code>model.with_structured_output</code>\u65b9\u6cd5\u3002\u5bf9\u4e8e<code>create_agent</code>\u4e2d\u7684\u7ed3\u6784\u5316\u8f93\u51fa\uff0c\u82e5\u9700\u8981\u4f7f\u7528<code>json_schema</code>\u7684\u5b9e\u73b0\u65b9\u5f0f\uff0c\u4f60\u9700\u8981\u786e\u4fdd\u5bf9\u5e94\u6a21\u578b\u7684<code>profile</code>\u4e2d\u5305\u542b<code>structured_output</code>\u5b57\u6bb5\uff0c\u4e14\u503c\u4e3a<code>True</code>\u3002</p>"},{"location":"zh/adavance-guide/openai-compatible/#_9","title":"\u4f20\u9012\u989d\u5916\u53c2\u6570","text":"<p>\u7531\u4e8e\u8be5\u7c7b\u7ee7\u627f\u81ea <code>BaseChatOpenAI</code>\uff0c\u56e0\u6b64\u652f\u6301\u4f20\u9012 <code>BaseChatOpenAI</code> \u7684\u6a21\u578b\u53c2\u6570\uff0c\u4f8b\u5982 <code>temperature</code>\u3001<code>extra_body</code> \u7b49\u3002</p> <p>\u4f8b\u5982\uff0c\u4f7f\u7528 <code>extra_body</code> \u4f20\u9012\u989d\u5916\u53c2\u6570\uff08\u6b64\u5904\u4e3a\u5173\u95ed\u601d\u8003\u6a21\u5f0f\uff09\uff1a</p> <pre><code>from langchain_core.messages import HumanMessage\n\nmodel = ChatVLLM(\n    model=\"qwen3-4b\",\n    extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},\n)\nresponse = model.invoke([HumanMessage(\"Hello\")])\nprint(response)\n</code></pre>"},{"location":"zh/adavance-guide/openai-compatible/#_10","title":"\u4f20\u9012\u591a\u6a21\u6001\u6570\u636e","text":"<p>\u652f\u6301\u4f20\u9012\u591a\u6a21\u6001\u6570\u636e\u3002\u4f60\u53ef\u4ee5\u4f7f\u7528 OpenAI \u517c\u5bb9\u7684\u591a\u6a21\u6001\u6570\u636e\u683c\u5f0f\uff0c\u6216\u76f4\u63a5\u4f7f\u7528 LangChain \u4e2d\u7684 <code>content_block</code>\u3002</p> <p>\u4f20\u9012\u56fe\u7247\u7c7b\u6570\u636e\uff1a</p> <p><pre><code>from langchain_core.messages import HumanMessage\nmessages = [\n    HumanMessage(\n        content_blocks=[\n            {\n                \"type\": \"image\",\n                \"url\": \"https://example.com/image.png\",\n            },\n            {\"type\": \"text\", \"text\": \"\u63cf\u8ff0\u8fd9\u5f20\u56fe\u7247\"},\n        ]\n    )\n]\n\nmodel = ChatVLLM(model=\"qwen3-vl-2b\")\nresponse = model.invoke(messages)\nprint(response)\n</code></pre> \u4f20\u9012\u89c6\u9891\u7c7b\u6570\u636e\uff1a</p> <pre><code>from langchain_core.messages import HumanMessage\n\nmessages = [\n    HumanMessage(\n        content_blocks=[\n            {\n                \"type\": \"video\",\n                \"url\": \"https://example.com/video.mp4\",\n            },\n            {\"type\": \"text\", \"text\": \"\u63cf\u8ff0\u8fd9\u6bb5\u89c6\u9891\"},\n        ]\n    )\n]\n\nmodel = ChatVLLM(model=\"qwen3-vl-2b\")\nresponse = model.invoke(messages)\nprint(response)\n</code></pre>"},{"location":"zh/adavance-guide/openai-compatible/#_11","title":"\u4f7f\u7528\u63a8\u7406\u6a21\u578b","text":"<p>\u672c\u5e93\u521b\u5efa\u7684\u6a21\u578b\u7c7b\u7684\u4e00\u5927\u7279\u70b9\u5c31\u662f\u8fdb\u4e00\u6b65\u9002\u914d\u4e86\u66f4\u591a\u7684\u63a8\u7406\u6a21\u578b\u3002</p> <p>\u4f8b\u5982\uff1a</p> <pre><code>from langchain_core.messages import HumanMessage\n\nmodel = ChatVLLM(model=\"qwen3-4b\")\nresponse = model.invoke(\"\u4e3a\u4ec0\u4e48\u9e66\u9e49\u7684\u7fbd\u6bdb\u5982\u6b64\u9c9c\u8273\uff1f\")\nreasoning_steps = [b for b in response.content_blocks if b[\"type\"] == \"reasoning\"]\nprint(\" \".join(step[\"reasoning\"] for step in reasoning_steps))\n</code></pre> \u4e0d\u540c\u63a8\u7406\u6a21\u5f0f\u7684\u652f\u6301 <p>\u4e0d\u540c\u6a21\u578b\u7684\u63a8\u7406\u6a21\u5f0f\u4e0d\u5c3d\u76f8\u540c\uff08\u8fd9\u70b9\u5728Agent\u5f00\u53d1\u4e2d\u5c24\u4e3a\u91cd\u8981\uff09\uff1a\u6709\u4e9b\u9700\u8981\u5728\u672c\u6b21\u8c03\u7528\u4e2d\u663e\u5f0f\u4f20\u9012 <code>reasoning_content</code> \u5b57\u6bb5\uff0c\u6709\u4e9b\u5219\u65e0\u9700\u3002\u672c\u5e93\u63d0\u4f9b <code>reasoning_keep_policy</code> \u517c\u5bb9\u6027\u914d\u7f6e\u4ee5\u9002\u914d\u8fd9\u4e9b\u5dee\u5f02\u3002</p> <p>\u8be5\u914d\u7f6e\u9879\u652f\u6301\u4ee5\u4e0b\u53d6\u503c\uff1a</p> <ul> <li> <p><code>never</code>\uff1a\u5728\u5386\u53f2\u6d88\u606f\u4e2d\u4e0d\u4fdd\u7559\u4efb\u4f55\u63a8\u7406\u5185\u5bb9\uff08\u9ed8\u8ba4)\uff1b</p> </li> <li> <p><code>current</code>\uff1a\u4ec5\u4fdd\u7559\u5f53\u524d\u5bf9\u8bdd\u4e2d\u7684 <code>reasoning_content</code> \u5b57\u6bb5\uff1b</p> </li> <li> <p><code>all</code>\uff1a\u4fdd\u7559\u6240\u6709\u5bf9\u8bdd\u4e2d\u7684 <code>reasoning_content</code> \u5b57\u6bb5\u3002</p> </li> </ul> <pre><code>graph LR\n    A[reasoning_content \u4fdd\u7559\u7b56\u7565] --&gt; B{\u53d6\u503c?};\n    B --&gt;|never| C[\u4e0d\u5305\u542b\u4efb\u4f55&lt;br&gt;reasoning_content];\n    B --&gt;|current| D[\u4ec5\u5305\u542b\u5f53\u524d\u5bf9\u8bdd\u7684&lt;br&gt;reasoning_content&lt;br&gt;\u9002\u914d\u4ea4\u9519\u5f0f\u601d\u8003\u6a21\u5f0f];\n    B --&gt;|all| E[\u5305\u542b\u6240\u6709\u5bf9\u8bdd\u7684&lt;br&gt;reasoning_content];\n    C --&gt; F[\u53d1\u9001\u7ed9\u6a21\u578b];\n    D --&gt; F;\n    E --&gt; F;</code></pre> <p>\u4f8b\u5982\uff0c\u7528\u6237\u5148\u63d0\u95ee\"\u7ebd\u7ea6\u5929\u6c14\u5982\u4f55\uff1f\"\uff0c\u968f\u540e\u8ffd\u95ee\"\u4f26\u6566\u5929\u6c14\u5982\u4f55\uff1f\"\uff0c\u5f53\u524d\u6b63\u8981\u8fdb\u884c\u7b2c\u4e8c\u8f6e\u5bf9\u8bdd\uff0c\u4e14\u5373\u5c06\u8fdb\u884c\u6700\u540e\u4e00\u6b21\u6a21\u578b\u8c03\u7528\u3002</p> <ul> <li>\u53d6\u503c\u4e3a<code>never</code>\u65f6</li> </ul> <p>\u6700\u7ec8\u4f20\u9012\u7ed9\u6a21\u578b\u7684 messages \u4e2d\u4e0d\u4f1a\u6709\u4efb\u4f55 <code>reasoning_content</code> \u5b57\u6bb5\uff0c\u6a21\u578b\u6536\u5230\u7684 messages \u4e3a\uff1a</p> <pre><code>messages = [\n    {\"content\": \"\u67e5\u7ebd\u7ea6\u5929\u6c14\u5982\u4f55\uff1f\", \"role\": \"user\"},\n    {\"content\": \"\", \"role\": \"assistant\", \"tool_calls\": [...]},\n    {\"content\": \"\u591a\u4e91 7~13\u00b0C\", \"role\": \"tool\", \"tool_call_id\": \"...\"},\n    {\"content\": \"\u7ebd\u7ea6\u4eca\u5929\u5929\u6c14\u4e3a\u591a\u4e91\uff0c7~13\u00b0C\u3002\", \"role\": \"assistant\"},\n    {\"content\": \"\u67e5\u4f26\u6566\u5929\u6c14\u5982\u4f55\uff1f\", \"role\": \"user\"},\n    {\"content\": \"\", \"role\": \"assistant\", \"tool_calls\": [...]},\n    {\"content\": \"\u96e8\u5929\uff0c14~20\u00b0C\", \"role\": \"tool\", \"tool_call_id\": \"...\"},\n]\n</code></pre> <ul> <li>\u53d6\u503c\u4e3a<code>current</code>\u65f6</li> </ul> <p>\u4ec5\u4fdd\u7559\u5f53\u524d\u5bf9\u8bdd\u4e2d\u7684 <code>reasoning_content</code> \u5b57\u6bb5\u3002\u8be5\u7b56\u7565\u9002\u7528\u4e8e\u4ea4\u9519\u5f0f\u601d\u8003\uff08Interleaved Thinking\uff09\u573a\u666f\uff0c\u5373\u6a21\u578b\u5728\u663e\u5f0f\u63a8\u7406\u4e0e\u5de5\u5177\u8c03\u7528\u4e4b\u95f4\u4ea4\u66ff\u8fdb\u884c\uff0c\u6b64\u65f6\u9700\u8981\u5c06\u5f53\u524d\u8f6e\u6b21\u7684\u63a8\u7406\u5185\u5bb9\u8fdb\u884c\u4fdd\u7559\u3002\u6a21\u578b\u6536\u5230\u7684 messages \u4e3a\uff1a <pre><code>messages = [\n    {\"content\": \"\u67e5\u7ebd\u7ea6\u5929\u6c14\u5982\u4f55\uff1f\", \"role\": \"user\"},\n    {\"content\": \"\", \"role\": \"assistant\", \"tool_calls\": [...]},\n    {\"content\": \"\u591a\u4e91 7~13\u00b0C\", \"role\": \"tool\", \"tool_call_id\": \"...\"},\n    {\"content\": \"\u7ebd\u7ea6\u4eca\u5929\u5929\u6c14\u4e3a\u591a\u4e91\uff0c7~13\u00b0C\u3002\", \"role\": \"assistant\"},\n    {\"content\": \"\u67e5\u4f26\u6566\u5929\u6c14\u5982\u4f55\uff1f\", \"role\": \"user\"},\n    {\n        \"content\": \"\",\n        \"reasoning_content\": \"\u67e5\u4f26\u6566\u5929\u6c14\uff0c\u9700\u8981\u76f4\u63a5\u8c03\u7528\u5929\u6c14\u5de5\u5177\u3002\",  # \u4ec5\u4fdd\u7559\u672c\u8f6e\u5bf9\u8bdd\u7684 reasoning_content\n        \"role\": \"assistant\",\n        \"tool_calls\": [...],\n    },\n    {\"content\": \"\u96e8\u5929\uff0c14~20\u00b0C\", \"role\": \"tool\", \"tool_call_id\": \"...\"},\n]\n</code></pre></p> <ul> <li>\u53d6\u503c\u4e3a<code>all</code>\u65f6</li> </ul> <p>\u4fdd\u7559\u6240\u6709\u5bf9\u8bdd\u4e2d\u7684 <code>reasoning_content</code> \u5b57\u6bb5\u3002\u6a21\u578b\u6536\u5230\u7684 messages \u4e3a\uff1a <pre><code>messages = [\n    {\"content\": \"\u67e5\u7ebd\u7ea6\u5929\u6c14\u5982\u4f55\uff1f\", \"role\": \"user\"},\n    {\n        \"content\": \"\",\n        \"reasoning_content\": \"\u67e5\u7ebd\u7ea6\u5929\u6c14\uff0c\u9700\u8981\u76f4\u63a5\u8c03\u7528\u5929\u6c14\u5de5\u5177\u3002\",  # \u4fdd\u7559 reasoning_content\n        \"role\": \"assistant\",\n        \"tool_calls\": [...],\n    },\n    {\"content\": \"\u591a\u4e91 7~13\u00b0C\", \"role\": \"tool\", \"tool_call_id\": \"...\"},\n    {\n        \"content\": \"\u7ebd\u7ea6\u4eca\u5929\u5929\u6c14\u4e3a\u591a\u4e91\uff0c7~13\u00b0C\u3002\",\n        \"reasoning_content\": \"\u76f4\u63a5\u8fd4\u56de\u7ebd\u7ea6\u5929\u6c14\u7ed3\u679c\u3002\",  # \u4fdd\u7559 reasoning_content\n        \"role\": \"assistant\",\n    },\n    {\"content\": \"\u67e5\u4f26\u6566\u5929\u6c14\u5982\u4f55\uff1f\", \"role\": \"user\"},\n    {\n        \"content\": \"\",\n        \"reasoning_content\": \"\u67e5\u4f26\u6566\u5929\u6c14\uff0c\u9700\u8981\u76f4\u63a5\u8c03\u7528\u5929\u6c14\u5de5\u5177\u3002\",  # \u4fdd\u7559 reasoning_content\n        \"role\": \"assistant\",\n        \"tool_calls\": [...],\n    },\n    {\"content\": \"\u96e8\u5929\uff0c14~20\u00b0C\", \"role\": \"tool\", \"tool_call_id\": \"...\"},\n]\n</code></pre></p> <p>\u6ce8\u610f\uff1a\u82e5\u672c\u8f6e\u5bf9\u8bdd\u4e0d\u6d89\u53ca\u5de5\u5177\u8c03\u7528\uff0c\u5219<code>current</code>\u4e0e<code>never</code>\u6548\u679c\u76f8\u540c\u3002</p> <p>\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u867d\u7136\u8be5\u53c2\u6570\u5c5e\u4e8e\u517c\u5bb9\u6027\u914d\u7f6e\u9879\uff0c\u4f46\u540c\u4e00\u63d0\u4f9b\u5546\u7684\u4e0d\u540c\u6a21\u578b\u3001\u751a\u81f3\u540c\u4e00\u6a21\u578b\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u5bf9 <code>reasoning_content</code> \u7684\u4fdd\u7559\u7b56\u7565\u8981\u6c42\u4e5f\u53ef\u80fd\u4e0d\u540c\uff0c\u56e0\u6b64\u5efa\u8bae\u5728\u5b9e\u4f8b\u5316\u65f6\u663e\u5f0f\u6307\u5b9a\uff0c\u521b\u5efa\u7c7b\u65f6\u65e0\u9700\u8d4b\u503c\u3002</p> <p>\u4f8b\u5982\uff0c\u4ee5 GLM-4.7-Flash \u6a21\u578b\u4e3a\u4f8b\uff0c\u7531\u4e8e\u5176\u652f\u6301\u4ea4\u9519\u5f0f\u601d\u8003\uff08Interleaved Thinking\uff09\u6a21\u5f0f\uff0c\u4e00\u822c\u9700\u8981\u5728\u5b9e\u4f8b\u5316\u65f6\u5c06 <code>reasoning_keep_policy</code> \u8bbe\u7f6e\u4e3a <code>current</code>\uff0c\u4ee5\u4fbf\u4ec5\u4fdd\u7559\u5f53\u524d\u8f6e\u6b21\u7684 <code>reasoning_content</code>\u3002\u4f8b\u5982\uff1a</p> <p><pre><code>from langchain_core.messages import HumanMessage\n\nmodel = ChatVLLM(model=\"glm-4.7-flash\", reasoning_keep_policy=\"current\")\nagent = create_agent(\n    model=model,\n    tools=[get_current_weather],\n)\nresponse = agent.invoke({\"messages\": [HumanMessage(content=\"\u67e5\u7ebd\u7ea6\u5929\u6c14\u5982\u4f55\uff1f\")]})\nprint(response)\n</code></pre> \u540c\u65f6\uff0cGLM-4.7-Flash \u6a21\u578b\u4e5f\u652f\u6301\u53e6\u4e00\u79cd\u601d\u8003\u6a21\u5f0f\uff0c\u88ab\u79f0\u4e4b\u4e3aPreserved Thinking\u3002\u6b64\u65f6\u9700\u8981\u4fdd\u7559\u5386\u53f2\u6d88\u606f\u4e2d\u7684\u6240\u6709 <code>reasoning_content</code> \u5b57\u6bb5\uff0c\u53ef\u4ee5\u5c06 <code>reasoning_keep_policy</code> \u8bbe\u7f6e\u4e3a <code>all</code>\u3002\u4f8b\u5982\uff1a</p> <pre><code>from langchain_core.messages import HumanMessage\n\nmodel = ChatVLLM(\n    model=\"glm-4.7-flash\",\n    reasoning_keep_policy=\"all\",\n    extra_body={\"chat_template_kwargs\": {\"clear_thinking\": False}},\n)\n\nagent = create_agent(\n    model=model,\n    tools=[get_current_weather],\n)\nresponse = agent.invoke({\"messages\": [HumanMessage(content=\"\u67e5\u7ebd\u7ea6\u5929\u6c14\u5982\u4f55\uff1f\")]})\nprint(response)\n</code></pre>"},{"location":"zh/adavance-guide/openai-compatible/#model-profiles","title":"Model profiles","text":"<p>\u53ef\u4ee5\u901a\u8fc7 <code>model.profile</code> \u83b7\u53d6\u6a21\u578b\u7684 profile\u3002\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u8fd4\u56de\u7a7a\u5b57\u5178\u3002</p> <p>\u4f60\u4e5f\u53ef\u4ee5\u5728\u5b9e\u4f8b\u5316\u65f6\u663e\u5f0f\u4f20\u5165 <code>profile</code> \u53c2\u6570\u6307\u5b9a\u6a21\u578b profile\u3002</p> <p>\u4f8b\u5982\uff1a <pre><code>from langchain_core.messages import HumanMessage\n\ncustom_profile = {\n    \"max_input_tokens\": 100_000,\n    \"tool_calling\": True,\n    \"structured_output\": True,\n    # ...\n}\nmodel = ChatVLLM(model=\"qwen3-4b\", profile=custom_profile)\nprint(model.profile)\n</code></pre> \u6216\u8005\u76f4\u63a5\u5728\u521b\u5efa\u65f6\u4f20\u5165\u6a21\u578b\u63d0\u4f9b\u5546\u7684\u6240\u6709\u6a21\u578b\u7684<code>profile</code>\u53c2\u6570\u3002</p> <p>\u4f8b\u5982\uff1a <pre><code>from langchain_dev_utils.chat_models.adapters import create_openai_compatible_model\n\nmodel_profiles = {\n    \"qwen3-4b\": {\n        \"max_input_tokens\": 131072,\n        \"max_output_tokens\": 8192,\n        \"image_inputs\": False,\n        \"audio_inputs\": False,\n        \"video_inputs\": False,\n        \"image_outputs\": False,\n        \"audio_outputs\": False,\n        \"video_outputs\": False,\n        \"reasoning_output\": True,\n        \"tool_calling\": True,\n    }\n    # \u6b64\u5904\u8fd8\u53ef\u4ee5\u5199\u66f4\u591a\u7684model profile\n}\n\nChatVLLM = create_openai_compatible_model(\n    model_provider=\"vllm\",\n    base_url=\"http://localhost:8000/v1\",\n    model_profiles=model_profiles,\n)\n\nmodel = ChatVLLM(\n    model=\"qwen3-4b\",\n)\nprint(model.profile)\n</code></pre></p>"},{"location":"zh/adavance-guide/openai-compatible/#openai-responses-api","title":"\u652f\u6301 OpenAI \u6700\u65b0\u7684 Responses API","text":"<p>\u8be5\u6a21\u578b\u7c7b\u4e5f\u652f\u6301 OpenAI \u6700\u65b0\u7684 <code>responses</code> API\uff08\u53c2\u6570\u540d\u4e3a <code>use_responses_api</code>\uff09\u3002\u76ee\u524d\u4ec5\u5c11\u91cf\u63d0\u4f9b\u5546\u652f\u6301\u8be5\u98ce\u683c\u63a5\u53e3\uff1b\u82e5\u4f60\u7684\u63d0\u4f9b\u5546\u652f\u6301\uff0c\u53ef\u901a\u8fc7 <code>use_responses_api=True</code> \u5f00\u542f\u3002</p> <p>\u4f8b\u5982 vLLM \u652f\u6301 <code>responses</code> API\uff0c\u5219\u53ef\u4ee5\u8fd9\u6837\u4f7f\u7528\uff1a</p> <pre><code>from langchain_core.messages import HumanMessage\n\nmodel = ChatVLLM(model=\"qwen3-4b\", use_responses_api=True)\nresponse = model.invoke([HumanMessage(content=\"\u4f60\u597d\")])\nprint(response)\n</code></pre> <p>\u6ce8\u610f</p> <p>\u8be5\u529f\u80fd\u6682\u672a\u4fdd\u8bc1\u5b8c\u5168\u652f\u6301\uff0c\u53ef\u4ee5\u7528\u4e8e\u7b80\u5355\u6d4b\u8bd5\uff0c\u4f46\u4e0d\u8981\u7528\u4e8e\u751f\u4ea7\u73af\u5883\u3002</p> <p>\u6ce8\u610f</p> <p>\u672c\u5e93\u76ee\u524d\u65e0\u6cd5\u4fdd\u8bc1 100% \u9002\u914d\u6240\u6709 OpenAI \u517c\u5bb9\u63a5\u53e3\uff08\u5c3d\u7ba1\u53ef\u4ee5\u4f7f\u7528\u517c\u5bb9\u6027\u914d\u7f6e\u6765\u63d0\u5347\u517c\u5bb9\u6027\uff09\u3002\u82e5\u6a21\u578b\u63d0\u4f9b\u5546\u5df2\u6709\u5b98\u65b9\u6216\u793e\u533a\u96c6\u6210\u7c7b\uff0c\u8bf7\u4f18\u5148\u91c7\u7528\u8be5\u96c6\u6210\u7c7b\u3002\u5982\u9047\u5230\u4efb\u4f55\u517c\u5bb9\u6027\u95ee\u9898\uff0c\u6b22\u8fce\u5728\u672c\u5e93 GitHub \u4ed3\u5e93\u63d0\u4ea4 issue\u3002</p>"},{"location":"zh/adavance-guide/openai-compatible/#_12","title":"\u5d4c\u5165\u6a21\u578b\u7684\u521b\u5efa\u4e0e\u4f7f\u7528","text":""},{"location":"zh/adavance-guide/openai-compatible/#_13","title":"\u521b\u5efa\u5d4c\u5165\u6a21\u578b\u7c7b","text":"<p>\u4e0e\u5bf9\u8bdd\u6a21\u578b\u7c7b\u7c7b\u4f3c\uff0c\u53ef\u4ee5\u4f7f\u7528 <code>create_openai_compatible_embedding</code> \u521b\u5efa\u5d4c\u5165\u6a21\u578b\u96c6\u6210\u7c7b\u3002\u8be5\u51fd\u6570\u63a5\u53d7\u4ee5\u4e0b\u53c2\u6570\uff1a</p> \u53c2\u6570 \u8bf4\u660e <code>embedding_provider</code> \u5d4c\u5165\u6a21\u578b\u63d0\u4f9b\u5546\u540d\u79f0\uff0c\u4f8b\u5982 <code>vllm</code>\u3002\u5fc5\u987b\u4ee5\u5b57\u6bcd\u6216\u6570\u5b57\u5f00\u5934\uff0c\u53ea\u80fd\u5305\u542b\u5b57\u6bcd\u3001\u6570\u5b57\u548c\u4e0b\u5212\u7ebf\uff0c\u957f\u5ea6\u4e0d\u8d85\u8fc7 20 \u4e2a\u5b57\u7b26\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u662f <code>base_url</code> \u6a21\u578b\u63d0\u4f9b\u5546\u9ed8\u8ba4 API \u5730\u5740\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u5426 <code>embedding_model_cls_name</code> \u5d4c\u5165\u6a21\u578b\u7c7b\u540d\uff08\u9700\u7b26\u5408 Python \u7c7b\u540d\u89c4\u8303\uff09\u3002\u9ed8\u8ba4\u503c\u4e3a <code>{Provider}Embeddings</code>\uff08\u5176\u4e2d <code>{Provider}</code> \u4e3a\u9996\u5b57\u6bcd\u5927\u5199\u7684\u63d0\u4f9b\u5546\u540d\u79f0\uff09\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u5426 <p>\u540c\u6837\uff0c\u6211\u4eec\u4f7f\u7528 <code>create_openai_compatible_embedding</code> \u6765\u96c6\u6210 vLLM \u7684\u5d4c\u5165\u6a21\u578b\u3002</p> <pre><code>from langchain_dev_utils.embeddings.adapters import create_openai_compatible_embedding\n\nVLLMEmbeddings = create_openai_compatible_embedding(\n    embedding_provider=\"vllm\",\n    base_url=\"http://localhost:8000/v1\",\n    embedding_model_cls_name=\"VLLMEmbeddings\",\n)\n\nembedding = VLLMEmbeddings(model=\"qwen3-embedding-4b\")\nprint(embedding.embed_query(\"\u4f60\u597d\"))\n</code></pre> <p><code>base_url</code> \u4e5f\u53ef\u4ee5\u7701\u7565\u3002\u672a\u4f20\u5165\u65f6\uff0c\u672c\u5e93\u4f1a\u9ed8\u8ba4\u8bfb\u53d6\u73af\u5883\u53d8\u91cf <code>VLLM_API_BASE</code>\uff1a</p> <pre><code>export VLLM_API_BASE=\"http://localhost:8000/v1\"\n</code></pre> <p>\u6b64\u65f6\u4ee3\u7801\u53ef\u4ee5\u7701\u7565 <code>base_url</code>\uff1a</p> <pre><code>from langchain_dev_utils.embeddings.adapters import create_openai_compatible_embedding\n\nVLLMEmbeddings = create_openai_compatible_embedding(\n    embedding_provider=\"vllm\",\n    embedding_model_cls_name=\"VLLMEmbeddings\",\n)\n\nembedding = VLLMEmbeddings(model=\"qwen3-embedding-4b\")\nprint(embedding.embed_query(\"\u4f60\u597d\"))\n</code></pre> <p>\u6ce8\u610f\uff1a\u4e0a\u8ff0\u4ee3\u7801\u6210\u529f\u8fd0\u884c\u7684\u524d\u63d0\u662f\u5df2\u914d\u7f6e\u73af\u5883\u53d8\u91cf <code>VLLM_API_KEY</code>\u3002\u867d\u7136 vLLM \u672c\u8eab\u4e0d\u8981\u6c42 API Key\uff0c\u4f46\u5d4c\u5165\u6a21\u578b\u7c7b\u521d\u59cb\u5316\u65f6\u9700\u8981\u4f20\u5165\uff0c\u56e0\u6b64\u8bf7\u5148\u8bbe\u7f6e\u8be5\u53d8\u91cf\uff0c\u4f8b\u5982\uff1a</p> <pre><code>export VLLM_API_KEY=vllm_api_key\n</code></pre>"},{"location":"zh/adavance-guide/openai-compatible/#_14","title":"\u4f7f\u7528\u5d4c\u5165\u6a21\u578b\u7c7b","text":"<p>\u8fd9\u91cc\u4f7f\u7528\u524d\u9762\u521b\u5efa\u597d\u7684 <code>VLLMEmbeddings</code> \u7c7b\u6765\u521d\u59cb\u5316\u5d4c\u5165\u6a21\u578b\u5b9e\u4f8b\u3002</p>"},{"location":"zh/adavance-guide/openai-compatible/#_15","title":"\u5411\u91cf\u5316\u67e5\u8be2","text":"<pre><code>embedding = VLLMEmbeddings(model=\"qwen3-embedding-4b\")\nprint(embedding.embed_query(\"\u4f60\u597d\"))\n</code></pre> <p>\u540c\u6837\uff0c\u4e5f\u652f\u6301\u5f02\u6b65\u8c03\u7528\uff1a</p> <pre><code>embedding = VLLMEmbeddings(model=\"qwen3-embedding-4b\")\nres = await embedding.aembed_query(\"\u4f60\u597d\")\nprint(res)\n</code></pre>"},{"location":"zh/adavance-guide/openai-compatible/#_16","title":"\u5411\u91cf\u5316\u5b57\u7b26\u4e32\u5217\u8868","text":"<p><pre><code>documents = [\"\u4f60\u597d\", \"\u4f60\u597d\uff0c\u6211\u662f\u5f20\u4e09\"]\nembedding = VLLMEmbeddings(model=\"qwen3-embedding-4b\")\nprint(embedding.embed_documents(documents))\n</code></pre> \u540c\u6837\uff0c\u4e5f\u652f\u6301\u5f02\u6b65\u8c03\u7528\uff1a</p> <pre><code>documents = [\"\u4f60\u597d\", \"\u4f60\u597d\uff0c\u6211\u662f\u5f20\u4e09\"]\nembedding = VLLMEmbeddings(model=\"qwen3-embedding-4b\")\nres = await embedding.aembed_documents(documents)\nprint(res)\n</code></pre> <p>\u5d4c\u5165\u6a21\u578b\u517c\u5bb9\u6027\u8bf4\u660e</p> <p>\u517c\u5bb9 OpenAI \u7684\u5d4c\u5165 API \u901a\u5e38\u8868\u73b0\u51fa\u8f83\u597d\u7684\u517c\u5bb9\u6027\uff0c\u4f46\u4ecd\u9700\u6ce8\u610f\u4ee5\u4e0b\u5dee\u5f02\u70b9\uff1a</p> <ol> <li> <p><code>check_embedding_ctx_length</code>\uff1a\u4ec5\u5728\u4f7f\u7528\u5b98\u65b9 OpenAI \u5d4c\u5165\u670d\u52a1\u65f6\u8bbe\u4e3a <code>True</code>\uff1b\u5176\u4f59\u5d4c\u5165\u6a21\u578b\u4e00\u5f8b\u8bbe\u4e3a <code>False</code>\u3002</p> </li> <li> <p><code>dimensions</code>\uff1a\u82e5\u6a21\u578b\u652f\u6301\u81ea\u5b9a\u4e49\u5411\u91cf\u7ef4\u5ea6\uff08\u5982 1024\u30014096\uff09\uff0c\u53ef\u76f4\u63a5\u4f20\u5165\u8be5\u53c2\u6570\u3002</p> </li> <li> <p><code>chunk_size</code>\uff1a\u4e3a\u5355\u6b21API\u8c03\u7528\u4e2d\u80fd\u5904\u7406\u7684\u6587\u672c\u6570\u91cf\u4e0a\u9650\u3002\u4f8b\u5982<code>chunk_size</code>\u5927\u5c0f\u4e3a10\uff0c\u610f\u5473\u7740\u4e00\u6b21\u8bf7\u6c42\u6700\u591a\u53ef\u4f20\u516510\u4e2a\u6587\u672c\u8fdb\u884c\u5411\u91cf\u5316\u3002</p> </li> <li> <p>\u5355\u6587\u672c token \u4e0a\u9650\uff1a\u65e0\u6cd5\u901a\u8fc7\u53c2\u6570\u63a7\u5236\uff0c\u9700\u5728\u9884\u5904\u7406\u5206\u5757\u9636\u6bb5\u81ea\u884c\u4fdd\u8bc1\u3002</p> </li> </ol> <p>\u6ce8\u610f\uff1a\u4f7f\u7528\u672c\u529f\u80fd\u521b\u5efa\u7684\u5bf9\u8bdd\u6a21\u578b\u7c7b\u548c\u5d4c\u5165\u6a21\u578b\u7c7b\uff0c\u5747\u652f\u6301\u4f20\u5165 <code>BaseChatOpenAI</code> \u4e0e <code>OpenAIEmbeddings</code> \u7684\u53c2\u6570\uff0c\u4f8b\u5982 <code>temperature</code>\u3001<code>extra_body</code>\u3001<code>dimensions</code> \u7b49\u3002</p> <p>\u6ce8\u610f</p> <p>\u4e0e\u6a21\u578b\u7ba1\u7406\u7c7b\u4f3c\uff0c\u4e0a\u8ff0\u4e24\u4e2a\u51fd\u6570\u5e95\u5c42\u4f7f\u7528 <code>pydantic.create_model</code> \u521b\u5efa\u6a21\u578b\u7c7b\uff0c\u4f1a\u5e26\u6765\u4e00\u5b9a\u7684\u6027\u80fd\u5f00\u9500\u3002\u6b64\u5916\uff0c<code>create_openai_compatible_model</code> \u4f7f\u7528\u5168\u5c40\u5b57\u5178\u8bb0\u5f55\u5404\u6a21\u578b\u63d0\u4f9b\u5546\u7684 <code>profiles</code>\uff0c\u4e3a\u4e86\u907f\u514d\u591a\u7ebf\u7a0b\u5e76\u53d1\u95ee\u9898\uff0c\u5efa\u8bae\u5728\u9879\u76ee\u542f\u52a8\u9636\u6bb5\u521b\u5efa\u597d\u96c6\u6210\u7c7b\uff0c\u540e\u7eed\u907f\u514d\u52a8\u6001\u521b\u5efa\u3002</p>"},{"location":"zh/adavance-guide/openai-compatible/#_17","title":"\u4e0e\u6a21\u578b\u7ba1\u7406\u529f\u80fd\u96c6\u6210","text":"<p>\u672c\u5e93\u5df2\u5c06\u6b64\u529f\u80fd\u65e0\u7f1d\u63a5\u5165\u6a21\u578b\u7ba1\u7406\u529f\u80fd\u3002\u6ce8\u518c\u5bf9\u8bdd\u6a21\u578b\u65f6\uff0c\u53ea\u9700\u5c06 <code>chat_model</code> \u8bbe\u4e3a <code>\"openai-compatible\"</code>\uff1b\u6ce8\u518c\u5d4c\u5165\u6a21\u578b\u65f6\uff0c\u5c06 <code>embeddings_model</code> \u8bbe\u4e3a <code>\"openai-compatible\"</code> \u5373\u53ef\u3002</p>"},{"location":"zh/adavance-guide/openai-compatible/#_18","title":"\u5bf9\u8bdd\u6a21\u578b\u7c7b\u6ce8\u518c","text":"<p>\u5177\u4f53\u4ee3\u7801\u5982\u4e0b\uff1a</p> <p>\u65b9\u5f0f\u4e00\uff1a\u663e\u5f0f\u4f20\u53c2</p> <pre><code>from langchain_dev_utils.chat_models import register_model_provider\n\nregister_model_provider(\n    provider_name=\"vllm\",\n    chat_model=\"openai-compatible\",\n    base_url=\"http://localhost:8000/v1\"\n)\n</code></pre> <p>\u65b9\u5f0f\u4e8c\uff1a\u901a\u8fc7\u73af\u5883\u53d8\u91cf\uff08\u63a8\u8350\u7528\u4e8e\u914d\u7f6e\u7ba1\u7406\uff09</p> <pre><code>from langchain_dev_utils.chat_models import register_model_provider\n\nregister_model_provider(\n    provider_name=\"vllm\",\n    chat_model=\"openai-compatible\"\n    # \u81ea\u52a8\u8bfb\u53d6 VLLM_API_BASE\n)\n</code></pre> <p>\u540c\u65f6\uff0c<code>create_openai_compatible_model</code>\u51fd\u6570\u4e2d\u7684<code>base_url</code>\u3001<code>compatibility_options</code>\u3001<code>model_profiles</code>\u53c2\u6570\u4e5f\u652f\u6301\u4f20\u5165\u3002\u53ea\u9700\u8981\u5728<code>register_model_provider</code>\u51fd\u6570\u4e2d\u4f20\u5165\u5bf9\u5e94\u7684\u53c2\u6570\u5373\u53ef\u3002</p> <p>\u4f8b\u5982\uff1a</p> <pre><code>from langchain_dev_utils.chat_models import register_model_provider\n\nregister_model_provider(\n    provider_name=\"vllm\",\n    chat_model=\"openai-compatible\",\n    base_url=\"http://localhost:8000/v1\",\n    compatibility_options={\n        \"supported_tool_choice\": [\"auto\", \"none\", \"required\", \"specific\"],\n        \"supported_response_format\": [\"json_schema\"]\n    },\n    model_profiles=model_profiles,\n)\n</code></pre>"},{"location":"zh/adavance-guide/openai-compatible/#_19","title":"\u5d4c\u5165\u6a21\u578b\u7c7b\u6ce8\u518c","text":"<p>\u4e0e\u5bf9\u8bdd\u6a21\u578b\u7c7b\u6ce8\u518c\u7c7b\u4f3c\uff1a</p> <p>\u65b9\u5f0f\u4e00\uff1a\u663e\u5f0f\u4f20\u53c2</p> <pre><code>from langchain_dev_utils.embeddings import register_embeddings_provider\n\nregister_embeddings_provider(\n    provider_name=\"vllm\",\n    embeddings_model=\"openai-compatible\",\n    base_url=\"http://localhost:8000/v1\",\n)\n</code></pre> <p>\u65b9\u5f0f\u4e8c\uff1a\u73af\u5883\u53d8\u91cf\uff08\u63a8\u8350\uff09</p> <pre><code>export VLLM_API_BASE=http://localhost:8000/v1\n</code></pre> <pre><code>from langchain_dev_utils.embeddings import register_embeddings_provider\n\nregister_embeddings_provider(\n    provider_name=\"vllm\",\n    embeddings_model=\"openai-compatible\"\n)\n</code></pre> <p>\u6700\u4f73\u5b9e\u8df5</p> <p>\u63a5\u5165 OpenAI \u517c\u5bb9 API \u65f6\uff0c\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 <code>langchain-openai</code> \u7684 <code>ChatOpenAI</code> \u6216 <code>OpenAIEmbeddings</code>\uff0c\u5e76\u901a\u8fc7 <code>base_url</code> \u4e0e <code>api_key</code> \u6307\u5411\u4f60\u7684\u63d0\u4f9b\u5546\u670d\u52a1\u3002\u8be5\u65b9\u5f0f\u8db3\u591f\u7b80\u5355\uff0c\u9002\u7528\u4e8e\u6bd4\u8f83\u7b80\u5355\u7684\u573a\u666f\uff08\u5c24\u5176\u662f\u4f7f\u7528\u7684\u662f\u666e\u901a\u7684\u5bf9\u8bdd\u6a21\u578b\u800c\u4e0d\u662f\u63a8\u7406\u6a21\u578b\uff09\u3002</p> <p>\u4f46\u662f\u4f1a\u5b58\u5728\u4ee5\u4e0b\u95ee\u9898\uff1a</p> <ol> <li> <p>\u65e0\u6cd5\u663e\u793a\u975e OpenAI \u5b98\u65b9\u63a8\u7406\u6a21\u578b\u7684\u601d\u7ef4\u94fe\uff08\u5373<code>reasoning_content</code>\u8fd4\u56de\u7684\u5185\u5bb9\uff09</p> </li> <li> <p>\u4e0d\u652f\u6301 <code>video</code> \u7c7b\u578b\u7684 content_block</p> </li> <li> <p>\u7ed3\u6784\u5316\u8f93\u51fa\u7684\u9ed8\u8ba4\u7b56\u7565\u8986\u76d6\u7387\u8f83\u4f4e</p> </li> </ol> <p>\u5f53\u4f60\u9047\u5230\u4e0a\u8ff0\u5dee\u5f02\u65f6\uff0c\u53ef\u4ee5\u4f7f\u7528\u672c\u5e93\u63d0\u4f9b\u7684 OpenAI \u517c\u5bb9\u96c6\u6210\u7c7b\u8fdb\u884c\u9002\u914d\u3002\u5bf9\u4e8e\u5d4c\u5165\u6a21\u578b\uff0c\u517c\u5bb9\u6027\u901a\u5e38\u66f4\u597d\uff1a\u591a\u6570\u60c5\u51b5\u4e0b\u76f4\u63a5\u4f7f\u7528 <code>OpenAIEmbeddings</code> \u5e76\u5c06 <code>check_embedding_ctx_length=False</code> \u5373\u53ef\u3002</p>"},{"location":"zh/adavance-guide/pipeline/","title":"\u72b6\u6001\u56fe\u7f16\u6392","text":""},{"location":"zh/adavance-guide/pipeline/#_2","title":"\u6982\u8ff0","text":"<p>\u5728 LangGraph \u4e2d\uff0c\u72b6\u6001\u56fe\u7f16\u6392\u662f\u6784\u5efa\u590d\u6742 AI \u5e94\u7528\u7684\u5173\u952e\u80fd\u529b\u3002\u901a\u8fc7\u6309\u7279\u5b9a\u6a21\u5f0f\u7ec4\u5408\u591a\u4e2a\u72b6\u6001\u56fe\uff0c\u53ef\u4ee5\u5f62\u6210\u5f3a\u5927\u4e14\u903b\u8f91\u6e05\u6670\u7684\u5de5\u4f5c\u6d41\u3002</p> <p>\u672c\u5e93\u63d0\u4f9b\u4e24\u79cd\u6700\u5e38\u89c1\u7684\u7f16\u6392\u65b9\u5f0f\uff1a</p> \u7f16\u6392\u65b9\u5f0f \u529f\u80fd\u63cf\u8ff0 \u9002\u7528\u573a\u666f \u987a\u5e8f\u7f16\u6392 \u6309\u987a\u5e8f\u7ec4\u5408\u591a\u4e2a\u72b6\u6001\u56fe\uff0c\u5f62\u6210\u987a\u5e8f\u5de5\u4f5c\u6d41 \u4efb\u52a1\u9700\u6309\u6b65\u9aa4\u6267\u884c\u4e14\u4f9d\u8d56\u524d\u4e00\u6b65\u8f93\u51fa \u5e76\u884c\u7f16\u6392 \u5e76\u884c\u7ec4\u5408\u591a\u4e2a\u72b6\u6001\u56fe\uff0c\u5f62\u6210\u5e76\u884c\u5de5\u4f5c\u6d41 \u591a\u4e2a\u4efb\u52a1\u76f8\u4e92\u72ec\u7acb\uff0c\u53ef\u540c\u65f6\u6267\u884c\u4ee5\u63d0\u9ad8\u6548\u7387"},{"location":"zh/adavance-guide/pipeline/#_3","title":"\u987a\u5e8f\u7f16\u6392","text":"<p>\u987a\u5e8f\u7f16\u6392\uff08Sequential Pipeline\uff09\u5c06\u590d\u6742\u4efb\u52a1\u62c6\u89e3\u4e3a\u8fde\u7eed\u3001\u6709\u5e8f\u7684\u5b50\u4efb\u52a1\uff0c\u5e76\u4ea4\u7531\u4e0d\u540c\u7684\u4e13\u95e8\u5316\u667a\u80fd\u4f53\u4f9d\u6b21\u5904\u7406\u3002</p> <p>\u4f7f\u7528 <code>create_sequential_pipeline</code> \u53ef\u5c06\u591a\u4e2a\u72b6\u6001\u56fe\u4ee5\u987a\u5e8f\u65b9\u5f0f\u7ec4\u5408\u3002</p>"},{"location":"zh/adavance-guide/pipeline/#_4","title":"\u5178\u578b\u5e94\u7528\u573a\u666f","text":"<p>\u4ee5\u7528\u6237\u8d2d\u4e70\u5546\u54c1\u4e3a\u4f8b\uff0c\u5178\u578b\u6d41\u7a0b\u5982\u4e0b\uff1a</p> <pre><code>graph LR\n    Start([\u7528\u6237\u4e0b\u5355\u8bf7\u6c42])\n    Inv[\u5e93\u5b58\u786e\u8ba4]\n    Ord[\u521b\u5efa\u8ba2\u5355]\n    Pay[\u5b8c\u6210\u652f\u4ed8]\n    Del[\u786e\u8ba4\u53d1\u8d27]\n    End([\u8ba2\u5355\u5b8c\u6210])\n\n    Start --&gt; Inv --&gt; Ord --&gt; Pay --&gt; Del --&gt; End</code></pre> <p>\u8be5\u6d41\u7a0b\u73af\u73af\u76f8\u6263\uff0c\u987a\u5e8f\u4e0d\u53ef\u98a0\u5012\u3002</p> <p>\u5176\u4e2d\uff0c\u5e93\u5b58\u786e\u8ba4\u3001\u521b\u5efa\u8ba2\u5355\u3001\u5b8c\u6210\u652f\u4ed8\u3001\u786e\u8ba4\u53d1\u8d27\u56db\u4e2a\u6b65\u9aa4\u5206\u522b\u7531\u4e13\u95e8\u667a\u80fd\u4f53\u5904\u7406\u3002\u901a\u8fc7 <code>create_sequential_pipeline</code> \u5c06\u56db\u4e2a\u72b6\u6001\u56fe\u987a\u5e8f\u7f16\u6392\uff0c\u5373\u53ef\u5f62\u6210\u9ad8\u5ea6\u81ea\u52a8\u5316\u3001\u804c\u8d23\u6e05\u6670\u7684\u5546\u54c1\u8d2d\u4e70\u5de5\u4f5c\u6d41\u3002</p>"},{"location":"zh/adavance-guide/pipeline/#_5","title":"\u57fa\u7840\u793a\u4f8b","text":"<p>\u4e0b\u9762\u793a\u4f8b\u5c55\u793a\u5982\u4f55\u7528 <code>create_sequential_pipeline</code> \u6784\u5efa\u5546\u54c1\u8d2d\u4e70\u7684\u987a\u5e8f\u5de5\u4f5c\u6d41\uff1a</p> <p>\u5148\u521b\u5efa\u5bf9\u8bdd\u6a21\u578b\u5bf9\u8c61\u3002\u8fd9\u91cc\u4ee5\u63a5\u5165\u672c\u5730 vllm \u90e8\u7f72\u7684 <code>qwen3-4b</code> \u4e3a\u4f8b\uff0c\u5176\u63a5\u53e3\u4e0e OpenAI \u517c\u5bb9\uff0c\u56e0\u6b64\u53ef\u76f4\u63a5\u7528 <code>create_openai_compatible_model</code> \u6784\u5efa\u6a21\u578b\u7c7b\u3002</p> <p><pre><code>from langchain_dev_utils.chat_models.adapters import create_openai_compatible_model\n\nChatVLLM = create_openai_compatible_model(\n    model_provider=\"vllm\",\n    base_url=\"http://localhost:8000/v1\",\n    chat_model_cls_name=\"ChatVLLM\",\n)\n</code></pre> \u518d\u5b9e\u4f8b\u5316\u4e00\u4e2a <code>ChatVLLM</code> \u5bf9\u8c61\uff0c\u4f9b\u540e\u7eed\u667a\u80fd\u4f53\u8c03\u7528\u3002</p> <p><pre><code>model = ChatVLLM(model=\"qwen3-4b\")\n</code></pre> \u968f\u540e\u521b\u5efa\u76f8\u5173\u5de5\u5177\uff0c\u4f8b\u5982\u67e5\u8be2\u5e93\u5b58\u3001\u521b\u5efa\u8ba2\u5355\u3001\u8fdb\u884c\u652f\u4ed8\u7b49\u3002</p> \u5de5\u5177\u7684\u5b9e\u73b0\u53c2\u8003 <pre><code>from langchain_core.tools import tool\n\n@tool\ndef check_inventory(product_name: str) -&gt; dict:\n    \"\"\"\u67e5\u8be2\u5e93\u5b58\"\"\"\n    return {\"product_name\": product_name, \"in_stock\": True, \"available\": 42}\n\n@tool\ndef create_order(product_name: str, quantity: int) -&gt; str:\n    \"\"\"\u521b\u5efa\u8ba2\u5355\"\"\"\n    return f\"\u5df2\u521b\u5efa\u8ba2\u5355 ORD-10001\uff0c\u5546\u54c1\uff1a{product_name}\uff0c\u6570\u91cf\uff1a{quantity}\u3002\"\n\n@tool\ndef pay_order(order_id: str) -&gt; str:\n    \"\"\"\u652f\u4ed8\u8ba2\u5355\"\"\"\n    return f\"\u8ba2\u5355 {order_id} \u652f\u4ed8\u6210\u529f\u3002\"\n\n@tool\ndef confirm_delivery(order_id: str, address: str) -&gt; str:\n    \"\"\"\u786e\u8ba4\u53d1\u8d27\"\"\"\n    return f\"\u8ba2\u5355 {order_id} \u5df2\u5b89\u6392\u53d1\u8d27\uff0c\u6536\u8d27\u5730\u5740\uff1a{address}\u3002\"\n</code></pre> <p>\u7136\u540e\u521b\u5efa\u5bf9\u5e94\u7684\u56db\u4e2a\u5b50\u667a\u80fd\u4f53\u3002</p> <pre><code>from langchain.agents import create_agent\n\ninventory_agent = create_agent(\n    model=model,\n    tools=[check_inventory],\n    system_prompt=\"\u4f60\u662f\u5e93\u5b58\u52a9\u624b\uff0c\u8d1f\u8d23\u786e\u8ba4\u5546\u54c1\u662f\u5426\u6709\u8d27\u3002\",\n    name=\"inventory_agent\",\n)\n\norder_agent = create_agent(\n    model=model,\n    tools=[create_order],\n    system_prompt=\"\u4f60\u662f\u4e0b\u5355\u52a9\u624b\uff0c\u8d1f\u8d23\u521b\u5efa\u8ba2\u5355\u3002\",\n    name=\"order_agent\",\n)\n\npayment_agent = create_agent(\n    model=model,\n    tools=[pay_order],\n    system_prompt=\"\u4f60\u662f\u652f\u4ed8\u52a9\u624b\uff0c\u8d1f\u8d23\u5b8c\u6210\u652f\u4ed8\u3002\",\n    name=\"payment_agent\",\n)\n\ndelivery_agent = create_agent(\n    model=model,\n    tools=[confirm_delivery],\n    system_prompt=\"\u4f60\u662f\u53d1\u8d27\u52a9\u624b\uff0c\u8d1f\u8d23\u786e\u8ba4\u53d1\u8d27\u4fe1\u606f\u3002\",\n    name=\"delivery_agent\",\n)\n</code></pre> <p>\u6700\u540e\u4f7f\u7528 <code>create_sequential_pipeline</code> \u5c06\u8fd9\u56db\u4e2a\u667a\u80fd\u4f53\u6309\u987a\u5e8f\u7f16\u6392\u3002</p> <p><pre><code>from langchain_dev_utils.pipeline import create_sequential_pipeline\nfrom langchain.agents import AgentState\n\ngraph = create_sequential_pipeline(\n    sub_graphs=[\n        inventory_agent,\n        order_agent,\n        payment_agent,\n        delivery_agent,\n    ],\n    state_schema=AgentState,\n)\n</code></pre> \u8fd0\u884c\u6d4b\u8bd5\uff1a</p> <pre><code>response = graph.invoke(\n    {\n        \"messages\": [\n            HumanMessage(\"\u6211\u8981\u4e70\u4e00\u526f\u65e0\u7ebf\u8033\u673a\uff0c\u6570\u91cf2\uff0c\u8bf7\u4e0b\u5355\uff0c\u6536\u8d27\u5730\u5740X\u5e02X\u533aX\u8defX\u53f7\")\n        ]\n    }\n)\nprint(response)\n</code></pre>"},{"location":"zh/adavance-guide/pipeline/#_6","title":"\u4e0a\u4e0b\u6587\u5de5\u7a0b\u4f18\u5316","text":"<p>\u8be5\u793a\u4f8b\u4f1a\u628a\u524d\u9762\u6240\u6709\u667a\u80fd\u4f53\u7684\u5b8c\u6574\u4e0a\u4e0b\u6587\u4f9d\u6b21\u4f20\u7ed9\u5f53\u524d\u667a\u80fd\u4f53\uff0c\u53ef\u80fd\u9020\u6210\u4e0a\u4e0b\u6587\u81a8\u80c0\uff0c\u5f71\u54cd\u6027\u80fd\u548c\u6548\u679c\u3002</p> <p>\u53ef\u91c7\u7528\u4ee5\u4e0b\u65b9\u6848\u7cbe\u7b80\u4e0a\u4e0b\u6587\uff1a</p> \u65b9\u6848 \u63cf\u8ff0 \u4f18\u70b9 \u4f7f\u7528\u4e2d\u95f4\u4ef6 \u4f7f\u7528 <code>create_agent</code> \u914d\u5408\u4e2d\u95f4\u4ef6\uff0c\u4ec5\u62bd\u53d6\u5e76\u4f20\u9012\u5fc5\u8981\u4fe1\u606f \u5b9e\u73b0\u7b80\u5355\uff0c\u4ee3\u7801\u6539\u52a8\u5c0f \u81ea\u5b9a\u4e49\u72b6\u6001\u56fe \u57fa\u4e8e <code>LangGraph</code> \u81ea\u5b9a\u4e49\u72b6\u6001\u56fe\uff0c\u663e\u5f0f\u63a7\u5236\u72b6\u6001\u5b57\u6bb5\u4e0e\u6d88\u606f\u6d41\u52a8 \u7075\u6d3b\u6027\u9ad8\uff0c\u53ef\u7cbe\u786e\u63a7\u5236 \u70b9\u51fb\u67e5\u770b\u5229\u7528\u4e2d\u95f4\u4ef6\u89e3\u51b3\u7684\u53c2\u8003\u4ee3\u7801 <pre><code>from typing import Any\n\nfrom langchain.agents.middleware import AgentMiddleware\nfrom langchain_core.messages import RemoveMessage\nfrom langgraph.runtime import Runtime\n\nfrom langchain_dev_utils.agents.middleware import format_prompt\n\n\nclass PurchaseState(AgentState, total=False):\n    stock: str\n    order: str\n    payment: str\n    delivery: str\n\n\nclass ClearAgentContextMiddleware(AgentMiddleware):\n    state_schema = PurchaseState\n\n    def __init__(self, result_save_key: str) -&gt; None:\n        super().__init__()\n        self.result_save_key = result_save_key\n\n    def after_agent(\n        self, state: PurchaseState, runtime: Runtime\n    ) -&gt; dict[str, Any] | None:\n        final_message = state[\"messages\"][-1]\n        update_key = self.result_save_key\n        return {\n            \"messages\": [\n                RemoveMessage(id=msg.id or \"\") for msg in state[\"messages\"][1:]\n            ],\n            update_key: final_message.content,\n        }\n\n\ninventory_agent = create_agent(\n    model=model,\n    tools=[check_inventory],\n    system_prompt=\"\u4f60\u662f\u5e93\u5b58\u52a9\u624b\uff0c\u8d1f\u8d23\u786e\u8ba4\u5546\u54c1\u662f\u5426\u6709\u8d27\u3002\u6700\u7ec8\u8bf7\u8f93\u51fa\u5e93\u5b58\u67e5\u8be2\u7ed3\u679c\u3002\",\n    name=\"inventory_agent\",\n    state_schema=PurchaseState,\n    middleware=[format_prompt, ClearAgentContextMiddleware(\"stock\")],\n)\n\norder_agent = create_agent(\n    model=model,\n    tools=[create_order],\n    system_prompt=(\n        \"\u4f60\u662f\u4e0b\u5355\u52a9\u624b\uff0c\u8d1f\u8d23\u521b\u5efa\u8ba2\u5355\u3002\\n\"\n        \"\u5e93\u5b58\u7ed3\u679c\uff1a{stock}\\n\"\n        \"\u8bf7\u57fa\u4e8e\u5e93\u5b58\u7ed3\u679c\u521b\u5efa\u8ba2\u5355\uff0c\u5e76\u8f93\u51fa\u8ba2\u5355\u53f7\u3002\"\n    ),\n    name=\"order_agent\",\n    state_schema=PurchaseState,\n    middleware=[format_prompt, ClearAgentContextMiddleware(\"order\")],\n)\n\npayment_agent = create_agent(\n    model=model,\n    tools=[pay_order],\n    system_prompt=(\n        \"\u4f60\u662f\u652f\u4ed8\u52a9\u624b\uff0c\u8d1f\u8d23\u5b8c\u6210\u652f\u4ed8\u3002\\n\"\n        \"\u8ba2\u5355\u7ed3\u679c\uff1a{order}\\n\"\n        \"\u8bf7\u4ece\u8ba2\u5355\u7ed3\u679c\u4e2d\u83b7\u53d6\u8ba2\u5355\u53f7\u5e76\u5b8c\u6210\u652f\u4ed8\u3002\"\n    ),\n    name=\"payment_agent\",\n    state_schema=PurchaseState,\n    middleware=[format_prompt, ClearAgentContextMiddleware(\"payment\")],\n)\n\ndelivery_agent = create_agent(\n    model=model,\n    tools=[confirm_delivery],\n    system_prompt=(\n        \"\u4f60\u662f\u53d1\u8d27\u52a9\u624b\uff0c\u8d1f\u8d23\u786e\u8ba4\u53d1\u8d27\u4fe1\u606f\u3002\\n\"\n        \"\u8ba2\u5355\u7ed3\u679c\uff1a{order}\\n\"\n        \"\u652f\u4ed8\u7ed3\u679c\uff1a{payment}\\n\"\n        \"\u8bf7\u786e\u8ba4\u53d1\u8d27\u5e76\u590d\u8ff0\u6536\u8d27\u5730\u5740\u3002\"\n    ),\n    name=\"delivery_agent\",\n    state_schema=PurchaseState,\n    middleware=[format_prompt, ClearAgentContextMiddleware(\"delivery\")],\n)\n\ngraph = create_sequential_pipeline(\n    sub_graphs=[\n        inventory_agent,\n        order_agent,\n        payment_agent,\n        delivery_agent,\n    ],\n    state_schema=PurchaseState,\n)\n\nresponse = graph.invoke(\n    {\n        \"messages\": [\n            HumanMessage(\"\u6211\u8981\u4e70\u4e00\u526f\u65e0\u7ebf\u8033\u673a\uff0c\u6570\u91cf2\uff0c\u8bf7\u4e0b\u5355\uff0c\u6536\u8d27\u5730\u5740X\u5e02X\u533aX\u8defX\u53f7\")\n        ]\n    }\n)\nprint(response)\n</code></pre> <p>\u5b9e\u73b0\u8bf4\u660e\uff1a</p> <ol> <li> <p>\u6269\u5c55\u72b6\u6001\u6a21\u5f0f\uff1a\u5728\u667a\u80fd\u4f53\u7684 State Schema \u4e2d\u65b0\u589e <code>stock</code>\u3001<code>order</code>\u3001<code>payment</code>\u3001<code>delivery</code> \u56db\u4e2a\u5b57\u6bb5\uff0c\u7528\u4e8e\u4fdd\u5b58\u5404\u667a\u80fd\u4f53\u7684\u6700\u7ec8\u8f93\u51fa\u3002</p> </li> <li> <p>\u81ea\u5b9a\u4e49\u4e2d\u95f4\u4ef6\uff1a\u521b\u5efa <code>ClearAgentContextMiddleware</code> \u4e2d\u95f4\u4ef6\uff0c\u5728\u6bcf\u4e2a\u667a\u80fd\u4f53\u7ed3\u675f\u540e\u5148\u7528 <code>RemoveMessage</code> \u6e05\u7406\u4e0a\u4e0b\u6587\uff0c\u518d\u5c06\u6700\u7ec8\u7ed3\u679c\uff08<code>final_message.content</code>\uff09\u5199\u5165\u5bf9\u5e94\u5b57\u6bb5\u3002</p> </li> <li> <p>\u52a8\u6001\u63d0\u793a\u683c\u5f0f\u5316\uff1a\u4f7f\u7528 <code>format_prompt</code> \u4e2d\u95f4\u4ef6\uff0c\u5728\u8fd0\u884c\u65f6\u6309\u9700\u5c06\u524d\u7f6e\u8f93\u51fa\u62fc\u5165 <code>system_prompt</code>\u3002</p> </li> </ol> <p>\u63d0\u793a</p> <p>\u5bf9\u4e8e\u4e32\u884c\u7ec4\u5408\u7684\u56fe\uff0cLangGraph \u7684 <code>StateGraph</code> \u63d0\u4f9b <code>add_sequence</code> \u4f5c\u4e3a\u7b80\u5199\uff0c\u66f4\u9002\u5408\u8282\u70b9\u4e3a\u51fd\u6570\uff08\u800c\u975e\u5b50\u56fe\uff09\u7684\u573a\u666f\u3002</p> <pre><code>graph = StateGraph(AgentState)\ngraph.add_sequence([(\"graph1\", graph1), (\"graph2\", graph2), (\"graph3\", graph3)])\ngraph.add_edge(\"__start__\", \"graph1\")\ngraph = graph.compile()\n</code></pre> <p>\u4f46\u4e0a\u8ff0\u5199\u6cd5\u4ecd\u7565\u663e\u7e41\u7410\uff0c\u66f4\u63a8\u8350\u4f7f\u7528 <code>create_sequential_pipeline</code>\uff0c\u4e00\u884c\u4ee3\u7801\u5373\u53ef\u5feb\u901f\u6784\u5efa\u4e32\u884c\u6267\u884c\u56fe\u3002</p>"},{"location":"zh/adavance-guide/pipeline/#_7","title":"\u5e76\u884c\u7f16\u6392","text":"<p>\u5e76\u884c\u7f16\u6392\uff08Parallel Pipeline\uff09\u5c06\u591a\u4e2a\u72b6\u6001\u56fe\u5e76\u884c\u7ec4\u5408\uff0c\u5e76\u53d1\u6267\u884c\u5404\u4efb\u52a1\uff0c\u4ece\u800c\u63d0\u9ad8\u6267\u884c\u6548\u7387\u3002</p> <p>\u4f7f\u7528 <code>create_parallel_pipeline</code> \u53ef\u5c06\u591a\u4e2a\u72b6\u6001\u56fe\u4ee5\u5e76\u884c\u65b9\u5f0f\u7ec4\u5408\uff0c\u5b9e\u73b0\u5e76\u884c\u6267\u884c\u3002</p>"},{"location":"zh/adavance-guide/pipeline/#_8","title":"\u5178\u578b\u5e94\u7528\u573a\u666f","text":"<p>\u5728\u5546\u54c1\u8d2d\u4e70\u573a\u666f\u4e2d\uff0c\u7528\u6237\u53ef\u80fd\u540c\u65f6\u9700\u8981\u591a\u79cd\u67e5\u8be2\uff0c\u4f8b\u5982\u5546\u54c1\u8be6\u60c5\u3001\u5e93\u5b58\u3001\u4f18\u60e0\u4e0e\u8fd0\u8d39\u4f30\u7b97\uff0c\u53ef\u5e76\u884c\u6267\u884c\u3002</p> <p>\u6d41\u7a0b\u5982\u4e0b\uff1a</p> <pre><code>graph LR\n    Start([\u7528\u6237\u8bf7\u6c42])\n\n    subgraph Parallel [\u5e76\u884c\u6267\u884c]\n        direction TB\n        Prod[\u5546\u54c1\u8be6\u60c5\u67e5\u8be2]\n        Inv[\u5e93\u5b58\u67e5\u8be2]\n        Prom[\u4f18\u60e0\u8ba1\u7b97]\n        Ship[\u8fd0\u8d39\u4f30\u7b97]\n    end\n\n    End([\u805a\u5408\u7ed3\u679c])\n\n    Start --&gt; Prod\n    Start --&gt; Inv\n    Start --&gt; Prom\n    Start --&gt; Ship\n\n    Prod --&gt; End\n    Inv --&gt; End\n    Prom --&gt; End\n    Ship --&gt; End</code></pre>"},{"location":"zh/adavance-guide/pipeline/#_9","title":"\u57fa\u7840\u793a\u4f8b","text":"<p>\u5148\u521b\u5efa\u51e0\u4e2a\u5de5\u5177\u3002</p> \u5de5\u5177\u7684\u5b9e\u73b0\u53c2\u8003 <pre><code>@tool\ndef get_product_detail(product_name: str) -&gt; dict:\n    \"\"\"\u67e5\u8be2\u5546\u54c1\u8be6\u60c5\"\"\"\n    return {\n        \"product_name\": product_name,\n        \"sku\": \"SKU-10001\",\n        \"price\": 299,\n        \"highlights\": [\"\u4e3b\u52a8\u964d\u566a\", \"\u84dd\u72595.3\", \"30\u5c0f\u65f6\u7eed\u822a\"],\n    }\n\n@tool\ndef check_inventory(product_name: str) -&gt; dict:\n    \"\"\"\u67e5\u8be2\u5e93\u5b58\"\"\"\n    return {\"product_name\": product_name, \"in_stock\": True, \"available\": 42}\n\n@tool\ndef calculate_promotions(product_name: str, quantity: int) -&gt; dict:\n    \"\"\"\u8ba1\u7b97\u4f18\u60e0\"\"\"\n    return {\n        \"product_name\": product_name,\n        \"quantity\": quantity,\n        \"discounts\": [\"\u6ee1300\u51cf30\", \"\u4f1a\u545895\u6298\"],\n        \"estimated_discount\": 45,\n    }\n\n@tool\ndef estimate_shipping(address: str) -&gt; dict:\n    \"\"\"\u4f30\u7b97\u8fd0\u8d39\u548c\u65f6\u6548\"\"\"\n    return {\n        \"address\": address,\n        \"fee\": 12,\n        \"eta_days\": 2,\n    }\n</code></pre> <p>\u4ee5\u53ca\u5bf9\u5e94\u7684\u5b50\u667a\u80fd\u4f53\uff1a</p> <pre><code>product_agent = create_agent(\n    model,\n    tools=[get_product_detail],\n    system_prompt=\"\u4f60\u662f\u5546\u54c1\u52a9\u7406\uff0c\u8d1f\u8d23\u89e3\u6790\u7528\u6237\u9700\u6c42\u5e76\u67e5\u8be2\u5546\u54c1\u8be6\u60c5\u3002\",\n    name=\"product_agent\",\n    state_schema=AgentState,\n)\n\ninventory_agent = create_agent(\n    model,\n    tools=[check_inventory],\n    system_prompt=\"\u4f60\u662f\u5e93\u5b58\u52a9\u7406\uff0c\u8d1f\u8d23\u6839\u636eSKU\u67e5\u8be2\u5e93\u5b58\u3002\",\n    name=\"inventory_agent\",\n    state_schema=AgentState,\n)\n\npromotion_agent = create_agent(\n    model,\n    tools=[calculate_promotions],\n    system_prompt=\"\u4f60\u662f\u4f18\u60e0\u52a9\u7406\uff0c\u8d1f\u8d23\u8ba1\u7b97\u5f53\u524d\u53ef\u7528\u4f18\u60e0\u548c\u9884\u8ba1\u6298\u6263\u3002\",\n    name=\"promotion_agent\",\n    state_schema=AgentState,\n)\n\nshipping_agent = create_agent(\n    model,\n    tools=[estimate_shipping],\n    system_prompt=\"\u4f60\u662f\u914d\u9001\u52a9\u7406\uff0c\u8d1f\u8d23\u4f30\u7b97\u8fd0\u8d39\u548c\u65f6\u6548\u3002\",\n    name=\"shipping_agent\",\n    state_schema=AgentState,\n)\n</code></pre> <p>\u7528 <code>create_parallel_pipeline</code> \u7ec4\u5408\u5b50\u667a\u80fd\u4f53\u3002</p> <p><pre><code>from langchain_dev_utils.pipeline import create_parallel_pipeline\n\ngraph = create_parallel_pipeline(\n    sub_graphs=[\n        product_agent,\n        inventory_agent,\n        promotion_agent,\n        shipping_agent,\n    ],\n    state_schema=AgentState,\n)\n</code></pre> \u8fd0\u884c\u6d4b\u8bd5\uff1a</p> <pre><code>response = graph.invoke(\n    {\"messages\": [HumanMessage(\"\u6211\u60f3\u4e70\u4e00\u526f\u65e0\u7ebf\u8033\u673a\uff0c\u6570\u91cf2\uff0c\u6536\u8d27\u5730\u5740X\u5e02X\u533aX\u8defX\u53f7\")]}\n)\nprint(response)\n</code></pre>"},{"location":"zh/adavance-guide/pipeline/#_10","title":"\u4f7f\u7528\u5206\u652f\u51fd\u6570\u6307\u5b9a\u5e76\u884c\u6267\u884c\u7684\u5b50\u56fe","text":"<p>\u5728\u591a\u6570\u573a\u666f\u4e0b\uff0c\u4e0d\u5e0c\u671b\u6240\u6709\u5b50\u56fe\u90fd\u5e76\u884c\u6267\u884c\uff0c\u800c\u662f\u6309\u6761\u4ef6\u5e76\u884c\u90e8\u5206\u5b50\u56fe\u3002\u6b64\u65f6\u9700\u4f7f\u7528 <code>branches_fn</code> \u6307\u5b9a\u5206\u652f\u51fd\u6570\u3002\u5206\u652f\u51fd\u6570\u9700\u8fd4\u56de <code>Send</code> \u5217\u8868\uff0c\u6bcf\u4e2a <code>Send</code> \u5305\u542b\u5b50\u56fe\u540d\u79f0\u4e0e\u8f93\u5165\u3002</p>"},{"location":"zh/adavance-guide/pipeline/#_11","title":"\u5e94\u7528\u573a\u666f","text":"<p><code>Router</code> \u662f\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5178\u578b\u67b6\u6784\uff1a\u7531\u8def\u7531\u6a21\u578b\u6839\u636e\u7528\u6237\u8bf7\u6c42\u8fdb\u884c\u9700\u6c42\u5206\u6790\u4e0e\u4efb\u52a1\u62c6\u89e3\uff0c\u518d\u5206\u53d1\u7ed9\u82e5\u5e72\u4e1a\u52a1\u667a\u80fd\u4f53\u6267\u884c\u3002\u5728\u8ba2\u5355\u67e5\u8be2\u573a\u666f\u4e2d\uff0c\u7528\u6237\u53ef\u80fd\u540c\u65f6\u5173\u5fc3\u8ba2\u5355\u72b6\u6001\u3001\u5546\u54c1\u4fe1\u606f\u6216\u9000\u6b3e\uff0c\u6b64\u65f6\u53ef\u7531\u8def\u7531\u6a21\u578b\u5c06\u8bf7\u6c42\u5206\u914d\u7ed9\u8ba2\u5355\u3001\u5546\u54c1\u3001\u9000\u6b3e\u7b49\u667a\u80fd\u4f53\u3002</p> <p>\u5148\u7f16\u5199\u5de5\u5177\u3002</p> \u5de5\u5177\u7684\u5b9e\u73b0\u53c2\u8003 <pre><code>@tool\ndef list_orders() -&gt; dict:\n    \"\"\"\u67e5\u8be2\u7528\u6237\u8ba2\u5355\u5217\u8868\"\"\"\n    return {\n        \"orders\": [\n            {\n                \"order_id\": \"ORD-20250101-0001\",\n                \"status\": \"\u5df2\u53d1\u8d27\",\n                \"items\": [{\"product_name\": \"\u65e0\u7ebf\u8033\u673a\", \"qty\": 1}],\n                \"created_at\": \"2025-01-01 10:02:11\",\n            },\n            {\n                \"order_id\": \"ORD-20241215-0234\",\n                \"status\": \"\u5df2\u5b8c\u6210\",\n                \"items\": [{\"product_name\": \"\u673a\u68b0\u952e\u76d8\", \"qty\": 1}],\n                \"created_at\": \"2024-12-15 21:18:03\",\n            },\n        ],\n    }\n\n@tool\ndef get_order_detail(order_id: str) -&gt; dict:\n    \"\"\"\u67e5\u8be2\u8ba2\u5355\u8be6\u60c5\"\"\"\n    return {\n        \"status\": \"\u5df2\u53d1\u8d27\",\n        \"receiver\": {\"name\": \"\u5f20\u4e09\", \"phone\": \"138****0000\"},\n        \"items\": [\n            {\n                \"product_id\": \"P-10001\",\n                \"product_name\": \"\u65e0\u7ebf\u8033\u673a\",\n                \"qty\": 1,\n                \"price\": 299,\n            }\n        ],\n    }\n\n@tool\ndef get_shipping_trace(tracking_no: str) -&gt; dict:\n    \"\"\"\u67e5\u8be2\u7269\u6d41\u8f68\u8ff9\"\"\"\n    return {\n        \"events\": [\n            {\"time\": \"2025-01-02 09:10\", \"status\": \"\u5feb\u4ef6\u5df2\u63fd\u6536\"},\n            {\"time\": \"2025-01-02 18:45\", \"status\": \"\u5feb\u4ef6\u8fd0\u8f93\u4e2d\"},\n            {\"time\": \"2025-01-03 11:20\", \"status\": \"\u5feb\u4ef6\u5df2\u5230\u8fbe\u6d3e\u9001\u7ad9\"},\n        ],\n    }\n\n@tool\ndef search_products(query: str) -&gt; dict:\n    \"\"\"\u641c\u7d22\u4ea7\u54c1\"\"\"\n    return {\n        \"results\": [\n            {\n                \"product_id\": \"P-10001\",\n                \"name\": \"\u65e0\u7ebf\u8033\u673a Pro\",\n                \"price\": 299,\n                \"highlights\": [\"\u964d\u566a\", \"\u84dd\u72595.3\", \"\u7eed\u822a30\u5c0f\u65f6\"],\n            },\n            {\n                \"product_id\": \"P-10002\",\n                \"name\": \"\u65e0\u7ebf\u8033\u673a Lite\",\n                \"price\": 199,\n                \"highlights\": [\"\u8f7b\u91cf\", \"\u4f4e\u5ef6\u8fdf\", \"\u7eed\u822a24\u5c0f\u65f6\"],\n            },\n        ],\n    }\n\n@tool\ndef get_product_detail(product_id: str) -&gt; dict:\n    \"\"\"\u67e5\u8be2\u4ea7\u54c1\u8be6\u60c5\"\"\"\n    return {\n        \"product_id\": product_id,\n        \"name\": \"\u65e0\u7ebf\u8033\u673a Pro\",\n        \"price\": 299,\n        \"specs\": {\"color\": [\"\u9ed1\", \"\u767d\"], \"warranty_months\": 12},\n        \"description\": \"\u4e3b\u6253\u964d\u566a\u4e0e\u957f\u7eed\u822a\u7684\u771f\u65e0\u7ebf\u8033\u673a\u3002\",\n    }\n\n\n@tool\ndef check_inventory(product_name: str) -&gt; dict:\n    \"\"\"\u67e5\u8be2\u5e93\u5b58\"\"\"\n    return {\"product_name\": product_name, \"in_stock\": True, \"available\": 42}\n\n@tool\ndef create_refund(order_id: str, reason: str) -&gt; dict:\n    \"\"\"\u53d1\u8d77\u9000\u6b3e\"\"\"\n    return {\n        \"refund_id\": \"RFD-20250103-0009\",\n        \"status\": \"\u5df2\u63d0\u4ea4\",\n        \"reason\": reason,\n        \"estimated_days\": 3,\n    }\n\n@tool\ndef get_refund_status(refund_id: str) -&gt; dict:\n    \"\"\"\u67e5\u8be2\u9000\u6b3e\u72b6\u6001\"\"\"\n    return {\n        \"refund_id\": refund_id,\n        \"status\": \"\u5904\u7406\u4e2d\",\n        \"progress\": [\n            {\"time\": \"2025-01-03 12:05\", \"status\": \"\u5df2\u63d0\u4ea4\"},\n            {\"time\": \"2025-01-03 12:20\", \"status\": \"\u5ba2\u670d\u5ba1\u6838\u4e2d\"},\n        ],\n        \"estimated_days\": 2,\n    }\n\n@tool\ndef refund_policy() -&gt; dict:\n    \"\"\"\u67e5\u770b\u9000\u6b3e\u653f\u7b56\"\"\"\n    return {\n        \"window_days\": 7,\n        \"requirements\": [\"\u5546\u54c1\u5b8c\u597d\", \"\u914d\u4ef6\u9f50\u5168\", \"\u63d0\u4f9b\u8ba2\u5355\u53f7\"],\n        \"notes\": [\"\u90e8\u5206\u6d3b\u52a8\u5546\u54c1\u4e0d\u652f\u6301\u65e0\u7406\u7531\u9000\u6b3e\", \"\u5230\u8d26\u65f6\u95f4\u89c6\u652f\u4ed8\u6e20\u9053\u800c\u5b9a\"],\n    }\n</code></pre> <p>\u7136\u540e\u521b\u5efa\u5bf9\u5e94\u7684\u5b50\u667a\u80fd\u4f53\u3002</p> <pre><code>ORDER_AGENT_PROMPT = (\n    \"\u4f60\u662f\u8ba2\u5355\u7ba1\u7406\u52a9\u624b\u3002\\n\"\n    \"\u4f60\u53ef\u4ee5\u4f7f\u7528\u5de5\u5177\u6765\u67e5\u8be2\u8ba2\u5355\u5217\u8868\u3001\u8ba2\u5355\u8be6\u60c5\u3001\u7269\u6d41\u8f68\u8ff9\u3002\\n\"\n    \"\u4f18\u5148\u4f7f\u7528\u5de5\u5177\u83b7\u53d6\u4fe1\u606f\uff0c\u518d\u57fa\u4e8e\u5de5\u5177\u7ed3\u679c\u7ed9\u51fa\u7ed3\u8bba\u3002\\n\"\n    \"\u8f93\u51fa\u8981\u6c42\uff1a\u7528\u4e2d\u6587\u56de\u7b54\uff0c\u7ed3\u6784\u6e05\u6670\uff0c\u5fc5\u8981\u65f6\u7528\u6761\u76ee\u5217\u51fa\u8ba2\u5355\u4fe1\u606f\u3002\\n\"\n)\n\norder_agent = create_agent(\n    model,\n    system_prompt=ORDER_AGENT_PROMPT,\n    tools=[list_orders, get_order_detail, get_shipping_trace],\n    name=\"order_agent\",\n)\n\n\nPRODUCT_AGENT_PROMPT = (\n    \"\u4f60\u662f\u4ea7\u54c1\u7ba1\u7406\u52a9\u624b\u3002\\n\"\n    \"\u4f60\u53ef\u4ee5\u4f7f\u7528\u5de5\u5177\u6765\u641c\u7d22\u4ea7\u54c1\u3001\u67e5\u770b\u4ea7\u54c1\u8be6\u60c5\u3001\u67e5\u8be2\u5e93\u5b58\u3002\\n\"\n    \"\u4f18\u5148\u4f7f\u7528\u5de5\u5177\u83b7\u53d6\u4fe1\u606f\uff0c\u518d\u57fa\u4e8e\u5de5\u5177\u7ed3\u679c\u7ed9\u51fa\u5efa\u8bae\u3002\\n\"\n    \"\u5f53\u7528\u6237\u7684\u9700\u6c42\u4e0d\u660e\u786e\u65f6\uff0c\u5148\u63d0\u51fa\u4e00\u4e2a\u6f84\u6e05\u95ee\u9898\uff08\u4f8b\u5982\u54c1\u7c7b/\u9884\u7b97/\u7528\u9014\uff09\u3002\\n\"\n    \"\u8f93\u51fa\u8981\u6c42\uff1a\u7528\u4e2d\u6587\u56de\u7b54\uff0c\u7ed9\u51fa\u53ef\u6267\u884c\u7684\u4e0b\u4e00\u6b65\u5efa\u8bae\u3002\\n\"\n)\n\n\nproduct_agent = create_agent(\n    model,\n    system_prompt=PRODUCT_AGENT_PROMPT,\n    tools=[search_products, get_product_detail, check_inventory],\n    name=\"product_agent\",\n)\n\n\nREFUND_AGENT_PROMPT = (\n    \"\u4f60\u662f\u9000\u6b3e\u7ba1\u7406\u52a9\u624b\u3002\\n\"\n    \"\u4f60\u53ef\u4ee5\u4f7f\u7528\u5de5\u5177\u6765\u53d1\u8d77\u9000\u6b3e\u3001\u67e5\u8be2\u9000\u6b3e\u72b6\u6001\u3001\u67e5\u770b\u9000\u6b3e\u653f\u7b56\u3002\\n\"\n    \"\u4f18\u5148\u4f7f\u7528\u5de5\u5177\u83b7\u53d6\u4fe1\u606f\uff1b\u82e5\u7528\u6237\u7f3a\u5c11\u5173\u952e\u5b57\u6bb5\uff08\u4f8b\u5982\u8ba2\u5355\u53f7\uff09\uff0c\u5148\u8ffd\u95ee\u3002\\n\"\n    \"\u8f93\u51fa\u8981\u6c42\uff1a\u7528\u4e2d\u6587\u56de\u7b54\uff0c\u660e\u786e\u544a\u77e5\u9000\u6b3e\u8fdb\u5ea6/\u6240\u9700\u6750\u6599/\u9884\u8ba1\u65f6\u95f4\u3002\\n\"\n)\n\n\nrefund_agent = create_agent(\n    model,\n    system_prompt=REFUND_AGENT_PROMPT,\n    tools=[create_refund, get_refund_status, refund_policy],\n    name=\"refund_agent\",\n)\n</code></pre> <p>\u518d\u7f16\u5199\u5206\u652f\u51fd\u6570\uff1a\u7531\u8def\u7531\u6a21\u578b\u6839\u636e\u8bf7\u6c42\u8fd4\u56de\u8981\u6267\u884c\u7684\u667a\u80fd\u4f53\u540d\u79f0\u53ca\u5bf9\u5e94\u4efb\u52a1\u63cf\u8ff0\u3002</p> <pre><code>from typing import Literal, cast\nfrom typing_extensions import TypedDict\n\n\nfrom langchain_core.messages import SystemMessage\nfrom langgraph.types import Send\nfrom pydantic import BaseModel, Field\n\n\nclass RouterInput(TypedDict):\n    query: str\n\n\nclass RouterState(AgentState):\n    query: str\n\n\nROUTER_SYSTEM_PROMPT = (\n    \"\u4f60\u662f\u4e00\u4e2aRouter\u6a21\u578b\uff0c\u53ea\u8d1f\u8d23\u628a\u7528\u6237\u95ee\u9898\u62c6\u5206\u5e76\u5206\u53d1\u5230\u5408\u9002\u7684\u4e1a\u52a1\u5b50\u667a\u80fd\u4f53\u3002\\n\"\n    \"\u53ef\u9009\u7684\u4e1a\u52a1\u57df\u53ea\u6709\uff1aorder\uff08\u8ba2\u5355\uff09\u3001product\uff08\u4ea7\u54c1\uff09\u3001refund\uff08\u9000\u6b3e\uff09\u3002\\n\"\n    \"\u4f60\u5fc5\u987b\u8f93\u51fa\u4e00\u4e2a classifications \u5217\u8868\uff08\u7528\u4e8e\u5e76\u884c\u8c03\u7528\u591a\u4e2a\u5b50\u667a\u80fd\u4f53\uff09\u3002\\n\"\n    \"\u89c4\u5219\uff1a\\n\"\n    \"1) source \u5fc5\u987b\u662f\u4e0a\u8ff0\u4e09\u4e2a\u4e4b\u4e00\uff1b\\n\"\n    \"2) query \u5fc5\u987b\u662f\u53d1\u7ed9\u8be5\u5b50\u667a\u80fd\u4f53\u7684\u3001\u53ef\u76f4\u63a5\u6267\u884c\u7684\u4efb\u52a1\u63cf\u8ff0\uff1b\\n\"\n    \"3) \u5982\u679c\u7528\u6237\u4e00\u53e5\u8bdd\u4e2d\u540c\u65f6\u6d89\u53ca\u591a\u4e2a\u4e1a\u52a1\u57df\uff08\u4f8b\u5982\u2018\u67e5\u8ba2\u5355\u2019+\u2018\u770b\u4ea7\u54c1\u2019+\u2018\u95ee\u9000\u6b3e\u2019\uff09\uff0c\u5fc5\u987b\u62c6\u6210\u591a\u4e2a classification\uff0c\u4ee5\u4fbf\u5e76\u884c\u6267\u884c\uff1b\\n\"\n    \"4) \u5982\u679c\u65e0\u6cd5\u5224\u65ad\uff0c\u4f18\u5148\u9009\u62e9 product\uff0c\u5e76\u628a\u95ee\u9898\u539f\u6837\u4ea4\u7ed9\u5b83\u3002\\n\"\n    \"\u793a\u4f8bA\uff1a\u7528\u6237\uff1a\u2018\u67e5\u4e00\u4e0bORD-1\u7269\u6d41\uff0c\u5e76\u770b\u770b\u8fd9\u6b3e\u8033\u673a\u6709\u6ca1\u6709\u8d27\u2019 -&gt; \u8fd4\u56de2\u6761\uff1aorder(\u67e5\u8be2\u7269\u6d41)+product(\u67e5\u8be2\u5e93\u5b58)\u3002\\n\"\n    \"\u793a\u4f8bB\uff1a\u7528\u6237\uff1a\u2018\u6211\u60f3\u9000ORD-1\uff0c\u9000\u6b3e\u591a\u4e45\u5230\u8d26\u2019 -&gt; \u8fd4\u56de1\u6761\uff1arefund(\u53d1\u8d77/\u67e5\u8be2\u9000\u6b3e)\u3002\\n\"\n    \"\u793a\u4f8bC\uff1a\u7528\u6237\uff1a\u2018\u6211\u60f3\u77e5\u9053\u8fd9\u6b3e\u8033\u673a\u7684\u89c4\u683c\u2019 -&gt; \u8fd4\u56de1\u6761\uff1aproduct(\u67e5\u8be2\u8be6\u60c5)\u3002\\n\"\n)\n\n\nclass Classification(TypedDict):\n    \"\"\"\u4e00\u6b21\u8def\u7531\u51b3\u7b56\uff1a\u8c03\u7528\u54ea\u4e2a\u667a\u80fd\u4f53\u5e76\u9644\u5e26\u4ec0\u4e48\u67e5\u8be2\u3002\"\"\"\n\n    source: Literal[\"order\", \"refund\", \"product\"]\n    query: str\n\n\nclass ClassificationResult(BaseModel):\n    \"\"\"\u5c06\u7528\u6237\u67e5\u8be2\u5206\u7c7b\u4e3a\u9762\u5411\u667a\u80fd\u4f53\u7684\u5b50\u95ee\u9898\u7684\u7ed3\u679c\u3002\"\"\"\n\n    classifications: list[Classification] = Field(\n        description=\"\u8981\u8c03\u7528\u7684\u667a\u80fd\u4f53\u5217\u8868\u53ca\u5176\u5bf9\u5e94\u7684\u5b50\u95ee\u9898\"\n    )\n\n\ndef branch_fn(state: RouterState) -&gt; list[Send]:\n    structured_llm = model.with_structured_output(ClassificationResult)\n\n    query = state.get(\"query\")\n    classification_result = cast(\n        ClassificationResult,\n        structured_llm.invoke(\n            [\n                SystemMessage(ROUTER_SYSTEM_PROMPT),\n                HumanMessage(query),\n            ]\n        ),\n    )\n\n    classifications = classification_result.classifications or []\n    if not classifications:\n        classifications = [{\"source\": \"product\", \"query\": query}]\n\n    sends: list[Send] = []\n    for res in classifications:\n        source = res.get(\"source\")\n        if source not in {\"order\", \"refund\", \"product\"}:\n            source = \"product\"\n        sub_query = (res.get(\"query\") or query).strip() or query\n        sends.append(Send(f\"{source}_agent\", {\"messages\": [HumanMessage(sub_query)]}))\n    return sends\n</code></pre> <p>\u6700\u7ec8\u4f7f\u7528 <code>create_parallel_agent</code> \u521b\u5efa\u5e76\u884c\u667a\u80fd\u4f53\uff0c\u5e76\u4f20\u5165\u5206\u652f\u51fd\u6570\u3002</p> <p>\u8fd0\u884c\u6d4b\u8bd5\uff1a</p> <pre><code>response_single = graph.invoke(\n    {\n        \"query\": \"\u4f60\u597d\uff0c\u6211\u8981\u67e5\u8be2\u4e00\u4e0b\u4e4b\u524d\u8d2d\u4e70\u7684\u4ea7\u54c1\",\n    }\n)\nprint(response_single)\n\nresponse_parallel = graph.invoke(\n    {\n        \"query\": \"\u63a8\u8350\u4e00\u6b3e\u9002\u5408\u901a\u52e4\u7684\u65e0\u7ebf\u8033\u673a\u5e76\u770b\u770b\u5e93\u5b58\uff1b\u540c\u65f6\uff0c\u544a\u8bc9\u6211\u4f60\u4eec\u5546\u54c1\u7684\u9000\u6b3e\u653f\u7b56\uff1f\",\n    }\n)\nprint(response_parallel)\n</code></pre> <p>\u63d0\u793a</p> <ul> <li>\u672a\u4f20\u5165 <code>branches_fn</code> \u53c2\u6570\u65f6\uff1a\u6240\u6709\u5b50\u56fe\u90fd\u4f1a\u5e76\u884c\u6267\u884c</li> <li>\u4f20\u5165 <code>branches_fn</code> \u53c2\u6570\u65f6\uff1a\u6267\u884c\u54ea\u4e9b\u5b50\u56fe\u7531\u8be5\u51fd\u6570\u7684\u8fd4\u56de\u503c\u51b3\u5b9a</li> </ul>"},{"location":"zh/api-reference/agent/","title":"Agent \u6a21\u5757 API \u53c2\u8003\u6587\u6863","text":""},{"location":"zh/api-reference/agent/#create_agent","title":"create_agent","text":"<p>\u521b\u5efa\u4e00\u4e2a\u667a\u80fd\u4f53\uff0c\u63d0\u4f9b\u4e0e langchain \u5b98\u65b9 <code>create_agent</code> \u5b8c\u5168\u76f8\u540c\u7684\u529f\u80fd\uff0c\u4f46\u62d3\u5c55\u4e86\u5b57\u7b26\u4e32\u6307\u5b9a\u6a21\u578b\u3002</p>"},{"location":"zh/api-reference/agent/#_1","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def create_agent(  # noqa: PLR0915\n    model: str,\n    tools: Sequence[BaseTool | Callable | dict[str, Any]] | None = None,\n    *,\n    system_prompt: str | SystemMessage | None = None,\n    response_format: ResponseFormat[ResponseT] | type[ResponseT] | None = None,\n    middleware: Sequence[AgentMiddleware[StateT_co, ContextT]] = (),\n    state_schema: type[AgentState[ResponseT]] | None = None,\n    context_schema: type[ContextT] | None = None,\n    checkpointer: Checkpointer | None = None,\n    store: BaseStore | None = None,\n    interrupt_before: list[str] | None = None,\n    interrupt_after: list[str] | None = None,\n    debug: bool = False,\n    name: str | None = None,\n    cache: BaseCache | None = None,\n) -&gt; CompiledStateGraph[\n    AgentState[ResponseT], ContextT, _InputAgentState, _OutputAgentState[ResponseT]\n]:\n</code></pre>"},{"location":"zh/api-reference/agent/#_2","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 model str \u662f - \u53ef\u7531 <code>load_chat_model</code> \u52a0\u8f7d\u7684\u6a21\u578b\u6807\u8bc6\u7b26\u5b57\u7b26\u4e32\u3002\u53ef\u6307\u5b9a\u4e3a \"provider:model-name\" \u683c\u5f0f tools Sequence[BaseTool | Callable | dict[str, Any]] | None \u5426 None \u667a\u80fd\u4f53\u53ef\u7528\u7684\u5de5\u5177\u5217\u8868 system_prompt str | SystemMessage | None \u5426 None \u667a\u80fd\u4f53\u7684\u81ea\u5b9a\u4e49\u7cfb\u7edf\u63d0\u793a\u8bcd middleware Sequence[AgentMiddleware[AgentState[ResponseT], ContextT]] \u5426 () \u667a\u80fd\u4f53\u7684\u4e2d\u95f4\u4ef6 response_format ResponseFormat[ResponseT] | type[ResponseT] | None \u5426 None \u667a\u80fd\u4f53\u7684\u54cd\u5e94\u683c\u5f0f state_schema type[AgentState[ResponseT]] | None \u5426 None \u667a\u80fd\u4f53\u7684\u72b6\u6001\u6a21\u5f0f context_schema type[ContextT] | None \u5426 None \u667a\u80fd\u4f53\u7684\u4e0a\u4e0b\u6587\u6a21\u5f0f checkpointer Checkpointer | None \u5426 None \u72b6\u6001\u6301\u4e45\u5316\u7684\u68c0\u67e5\u70b9 store BaseStore | None \u5426 None \u6570\u636e\u6301\u4e45\u5316\u7684\u5b58\u50a8 interrupt_before list[str] | None \u5426 None \u6267\u884c\u524d\u8981\u4e2d\u65ad\u7684\u8282\u70b9 interrupt_after list[str] | None \u5426 None \u6267\u884c\u540e\u8981\u4e2d\u65ad\u7684\u8282\u70b9 debug bool \u5426 False \u542f\u7528\u8c03\u8bd5\u6a21\u5f0f name str | None \u5426 None \u667a\u80fd\u4f53\u540d\u79f0 cache BaseCache | None \u5426 None \u7f13\u5b58"},{"location":"zh/api-reference/agent/#_3","title":"\u6ce8\u610f\u4e8b\u9879","text":"<p>\u6b64\u51fd\u6570\u63d0\u4f9b\u4e0e <code>langchain</code> \u5b98\u65b9 <code>create_agent</code> \u5b8c\u5168\u76f8\u540c\u7684\u529f\u80fd\uff0c\u4f46\u62d3\u5c55\u4e86\u6a21\u578b\u9009\u62e9\u3002\u4e3b\u8981\u533a\u522b\u5728\u4e8e <code>model</code> \u53c2\u6570\u5fc5\u987b\u662f\u53ef\u7531 <code>load_chat_model</code> \u51fd\u6570\u52a0\u8f7d\u7684\u5b57\u7b26\u4e32\uff0c\u5141\u8bb8\u4f7f\u7528\u6ce8\u518c\u7684\u6a21\u578b\u63d0\u4f9b\u8005\u8fdb\u884c\u66f4\u7075\u6d3b\u7684\u6a21\u578b\u9009\u62e9\u3002</p>"},{"location":"zh/api-reference/agent/#_4","title":"\u793a\u4f8b","text":"<pre><code>agent = create_agent(model=\"vllm:qwen3-4b\", tools=[get_current_time])\n</code></pre>"},{"location":"zh/api-reference/agent/#wrap_agent_as_tool","title":"wrap_agent_as_tool","text":"<p>\u5c06\u667a\u80fd\u4f53\u5305\u88c5\u4e3a\u5de5\u5177\u3002</p>"},{"location":"zh/api-reference/agent/#_5","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def wrap_agent_as_tool(\n    agent: CompiledStateGraph,\n    tool_name: Optional[str] = None,\n    tool_description: Optional[str] = None,\n    pre_input_hooks: Optional[\n        tuple[\n            Callable[[str, ToolRuntime], str | dict[str, Any]],\n            Callable[[str, ToolRuntime], Awaitable[str | dict[str, Any]]],\n        ]\n        | Callable[[str, ToolRuntime], str | dict[str, Any]]\n    ] = None,\n    post_output_hooks: Optional[\n        tuple[\n            Callable[[str, dict[str, Any], ToolRuntime], Any],\n            Callable[[str, dict[str, Any], ToolRuntime], Awaitable[Any]],\n        ]\n        | Callable[[str, dict[str, Any], ToolRuntime], Any]\n    ] = None,\n) -&gt; BaseTool:\n</code></pre>"},{"location":"zh/api-reference/agent/#_6","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 agent CompiledStateGraph \u662f - \u667a\u80fd\u4f53 tool_name Optional[str] \u5426 None \u5de5\u5177\u540d\u79f0 tool_description Optional[str] \u5426 None \u5de5\u5177\u63cf\u8ff0 pre_input_hooks - \u5426 None Agent \u8f93\u5165\u9884\u5904\u7406\u51fd\u6570 post_output_hooks - \u5426 None Agent \u8f93\u51fa\u540e\u5904\u7406\u51fd\u6570"},{"location":"zh/api-reference/agent/#_7","title":"\u793a\u4f8b","text":"<pre><code>tool = wrap_agent_as_tool(agent)\n</code></pre>"},{"location":"zh/api-reference/agent/#wrap_all_agents_as_tool","title":"wrap_all_agents_as_tool","text":"<p>\u5c06\u6240\u6709\u667a\u80fd\u4f53\u5305\u88c5\u4e3a\u5355\u4e2a\u5de5\u5177\u3002</p>"},{"location":"zh/api-reference/agent/#_8","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def wrap_all_agents_as_tool(\n    agents: list[CompiledStateGraph],\n    tool_name: Optional[str] = None,\n    tool_description: Optional[str] = None,\n    pre_input_hooks: Optional[\n        tuple[\n            Callable[[str, ToolRuntime], str | dict[str, Any]],\n            Callable[[str, ToolRuntime], Awaitable[str | dict[str, Any]]],\n        ]\n        | Callable[[str, ToolRuntime], str | dict[str, Any]]\n    ] = None,\n    post_output_hooks: Optional[\n        tuple[\n            Callable[[str, dict[str, Any], ToolRuntime], Any],\n            Callable[[str, dict[str, Any], ToolRuntime], Awaitable[Any]],\n        ]\n        | Callable[[str, dict[str, Any], ToolRuntime], Any]\n    ] = None,\n) -&gt; BaseTool:\n</code></pre>"},{"location":"zh/api-reference/agent/#_9","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 agents list[CompiledStateGraph] \u662f - \u667a\u80fd\u4f53\u5217\u8868(\u81f3\u5c11\u5305\u542b2\u4e2a\uff0c\u4e14\u6bcf\u4e2a\u667a\u80fd\u4f53\u5fc5\u987b\u6709\u552f\u4e00\u7684\u540d\u79f0) tool_name Optional[str] \u5426 None \u5de5\u5177\u540d\u79f0 tool_description Optional[str] \u5426 None \u5de5\u5177\u63cf\u8ff0 pre_input_hooks - \u5426 None Agent \u8f93\u5165\u9884\u5904\u7406\u51fd\u6570 post_output_hooks - \u5426 None Agent \u8f93\u51fa\u540e\u5904\u7406\u51fd\u6570"},{"location":"zh/api-reference/agent/#_10","title":"\u793a\u4f8b","text":"<pre><code>tool = wrap_all_agents_as_tool([time_agent, weather_agent])\n</code></pre>"},{"location":"zh/api-reference/agent/#summarizationmiddleware","title":"SummarizationMiddleware","text":"<p>\u7528\u4e8e\u667a\u80fd\u4f53\u4e0a\u4e0b\u6587\u6458\u8981\u7684\u4e2d\u95f4\u4ef6\u3002</p>"},{"location":"zh/api-reference/agent/#_11","title":"\u7c7b\u5b9a\u4e49","text":"<pre><code>class SummarizationMiddleware(_SummarizationMiddleware):\n    def __init__(\n        self,\n        model: str,\n        *,\n        trigger: ContextSize | list[ContextSize] | None = None,\n        keep: ContextSize = (\"messages\", _DEFAULT_MESSAGES_TO_KEEP),\n        token_counter: TokenCounter = count_tokens_approximately,\n        summary_prompt: str = DEFAULT_SUMMARY_PROMPT,\n        trim_tokens_to_summarize: int | None = _DEFAULT_TRIM_TOKEN_LIMIT,\n        **deprecated_kwargs: Any,\n    ) -&gt; None\n</code></pre>"},{"location":"zh/api-reference/agent/#_12","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 model str \u662f - \u53ef\u7531 <code>load_chat_model</code> \u52a0\u8f7d\u7684\u6a21\u578b\u6807\u8bc6\u7b26\u5b57\u7b26\u4e32\u3002\u53ef\u6307\u5b9a\u4e3a \"provider:model-name\" \u683c\u5f0f trigger ContextSize | list[ContextSize] | None \u5426 None \u89e6\u53d1\u6458\u8981\u7684\u4e0a\u4e0b\u6587\u5927\u5c0f keep ContextSize \u5426 (\"messages\", _DEFAULT_MESSAGES_TO_KEEP) \u4fdd\u7559\u7684\u4e0a\u4e0b\u6587\u5927\u5c0f token_counter TokenCounter \u5426 count_tokens_approximately token \u8ba1\u6570\u5668 summary_prompt str \u5426 DEFAULT_SUMMARY_PROMPT \u6458\u8981\u63d0\u793a\u8bcd trim_tokens_to_summarize int | None \u5426 _DEFAULT_TRIM_TOKEN_LIMIT \u6458\u8981\u524d\u8981\u622a\u53d6\u7684 token \u6570"},{"location":"zh/api-reference/agent/#_13","title":"\u793a\u4f8b","text":"<pre><code>summarization_middleware = SummarizationMiddleware(model=\"vllm:qwen3-4b\")\n</code></pre>"},{"location":"zh/api-reference/agent/#llmtoolselectormiddleware","title":"LLMToolSelectorMiddleware","text":"<p>\u7528\u4e8e\u667a\u80fd\u4f53\u5de5\u5177\u9009\u62e9\u7684\u4e2d\u95f4\u4ef6\u3002</p>"},{"location":"zh/api-reference/agent/#_14","title":"\u7c7b\u5b9a\u4e49","text":"<pre><code>class LLMToolSelectorMiddleware(_LLMToolSelectorMiddleware):\n    def __init__(\n        self,\n        *,\n        model: str,\n        system_prompt: Optional[str] = None,\n        max_tools: Optional[int] = None,\n        always_include: Optional[list[str]] = None,\n    ) -&gt; None\n</code></pre>"},{"location":"zh/api-reference/agent/#_15","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 model str \u662f - \u53ef\u7531 <code>load_chat_model</code> \u52a0\u8f7d\u7684\u6a21\u578b\u6807\u8bc6\u7b26\u5b57\u7b26\u4e32\u3002\u53ef\u6307\u5b9a\u4e3a \"provider:model-name\" \u683c\u5f0f system_prompt Optional[str] \u5426 None \u7cfb\u7edf\u63d0\u793a\u8bcd max_tools Optional[int] \u5426 None \u6700\u5927\u5de5\u5177\u6570 always_include Optional[list[str]] \u5426 None \u603b\u662f\u5305\u542b\u7684\u5de5\u5177"},{"location":"zh/api-reference/agent/#_16","title":"\u793a\u4f8b","text":"<pre><code>llm_tool_selector_middleware = LLMToolSelectorMiddleware(model=\"vllm:qwen3-4b\")\n</code></pre>"},{"location":"zh/api-reference/agent/#planmiddleware","title":"PlanMiddleware","text":"<p>\u7528\u4e8e\u667a\u80fd\u4f53\u8ba1\u5212\u7ba1\u7406\u7684\u4e2d\u95f4\u4ef6\u3002</p>"},{"location":"zh/api-reference/agent/#_17","title":"\u7c7b\u5b9a\u4e49","text":"<pre><code>class PlanMiddleware(AgentMiddleware):\n    state_schema = PlanState\n    def __init__(\n        self,\n        *,\n        system_prompt: Optional[str] = None,\n        custom_plan_tool_descriptions: Optional[PlanToolDescription] = None,\n        use_read_plan_tool: bool = True,\n    ) -&gt; None:\n</code></pre>"},{"location":"zh/api-reference/agent/#_18","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 system_prompt Optional[str] \u5426 None \u7cfb\u7edf\u63d0\u793a\u8bcd custom_plan_tool_descriptions Optional[PlanToolDescription] \u5426 None \u81ea\u5b9a\u4e49\u8ba1\u5212\u5de5\u5177\u7684\u63cf\u8ff0 use_read_plan_tool bool \u5426 True \u662f\u5426\u4f7f\u7528\u8bfb\u8ba1\u5212\u5de5\u5177"},{"location":"zh/api-reference/agent/#_19","title":"\u793a\u4f8b","text":"<pre><code>plan_middleware = PlanMiddleware()\n</code></pre>"},{"location":"zh/api-reference/agent/#modelfallbackmiddleware","title":"ModelFallbackMiddleware","text":"<p>\u7528\u4e8e\u667a\u80fd\u4f53\u6a21\u578b\u56de\u9000\u7684\u4e2d\u95f4\u4ef6\u3002</p>"},{"location":"zh/api-reference/agent/#_20","title":"\u7c7b\u5b9a\u4e49","text":"<pre><code>class ModelFallbackMiddleware(_ModelFallbackMiddleware):\n    def __init__(\n        self,\n        first_model: str,\n        *additional_models: str,\n    ) -&gt; None\n</code></pre>"},{"location":"zh/api-reference/agent/#_21","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 first_model str \u662f - \u53ef\u7531 <code>load_chat_model</code> \u52a0\u8f7d\u7684\u6a21\u578b\u6807\u8bc6\u7b26\u5b57\u7b26\u4e32\u3002\u53ef\u6307\u5b9a\u4e3a \"provider:model-name\" \u683c\u5f0f additional_models str \u5426 - \u5907\u7528\u6a21\u578b\u5217\u8868"},{"location":"zh/api-reference/agent/#_22","title":"\u793a\u4f8b","text":"<pre><code>model_fallback_middleware = ModelFallbackMiddleware(\n    \"vllm:qwen3-4b\",\n    \"vllm:qwen3-8b\"\n)\n</code></pre>"},{"location":"zh/api-reference/agent/#llmtoolemulator","title":"LLMToolEmulator","text":"<p>\u7528\u4e8e\u4f7f\u7528\u5927\u6a21\u578b\u6765\u6a21\u62df\u5de5\u5177\u8c03\u7528\u7684\u4e2d\u95f4\u4ef6\u3002</p>"},{"location":"zh/api-reference/agent/#_23","title":"\u7c7b\u5b9a\u4e49","text":"<pre><code>class LLMToolEmulator(_LLMToolEmulator):\n    def __init__(\n        self,\n        *,\n        model: str,\n        tools: list[str | BaseTool] | None = None,\n    ) -&gt; None\n</code></pre>"},{"location":"zh/api-reference/agent/#_24","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 model str \u662f - \u53ef\u7531 <code>load_chat_model</code> \u52a0\u8f7d\u7684\u6a21\u578b\u6807\u8bc6\u7b26\u5b57\u7b26\u4e32\u3002\u53ef\u6307\u5b9a\u4e3a \"provider:model-name\" \u683c\u5f0f tools list[str | BaseTool] | None \u5426 None \u5de5\u5177\u5217\u8868"},{"location":"zh/api-reference/agent/#_25","title":"\u793a\u4f8b","text":"<pre><code>llm_tool_emulator = LLMToolEmulator(model=\"vllm:qwen3-4b\", tools=[get_current_time])\n</code></pre>"},{"location":"zh/api-reference/agent/#modelroutermiddleware","title":"ModelRouterMiddleware","text":"<p>\u7528\u4e8e\u6839\u636e\u8f93\u5165\u5185\u5bb9\u52a8\u6001\u8def\u7531\u5230\u5408\u9002\u6a21\u578b\u7684\u4e2d\u95f4\u4ef6\u3002</p>"},{"location":"zh/api-reference/agent/#_26","title":"\u7c7b\u5b9a\u4e49","text":"<pre><code>class ModelRouterMiddleware(AgentMiddleware):\n    state_schema = ModelRouterState\n    def __init__(\n        self,\n        router_model: str | BaseChatModel,\n        model_list: list[ModelDict],\n        router_prompt: Optional[str] = None,\n    ) -&gt; None\n</code></pre>"},{"location":"zh/api-reference/agent/#_27","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 router_model str | BaseChatModel \u662f - \u7528\u4e8e\u8def\u7531\u7684\u6a21\u578b\uff0c\u63a5\u6536\u5b57\u7b26\u4e32\u7c7b\u578b\uff08\u4f7f\u7528<code>load_chat_model</code>\u52a0\u8f7d\uff09\u6216\u8005\u76f4\u63a5\u4f20\u5165 ChatModel model_list list[ModelDict] \u662f - \u6a21\u578b\u5217\u8868\uff0c\u6bcf\u4e2a\u6a21\u578b\u9700\u8981\u5305\u542b <code>model_name</code> \u548c <code>model_description</code> \u4e24\u4e2a\u952e\uff0c\u540c\u65f6\u4e5f\u53ef\u4ee5\u9009\u62e9\u6027\u5730\u5305\u542b <code>tools</code>\u3001<code>model_kwargs</code>\u3001<code>model_instance</code>\u3001<code>model_system_prompt</code> \u8fd9\u56db\u4e2a\u952e router_prompt Optional[str] \u5426 None \u8def\u7531\u6a21\u578b\u7684\u63d0\u793a\u8bcd\uff0c\u5982\u679c\u4e3a None \u5219\u4f7f\u7528\u9ed8\u8ba4\u7684\u63d0\u793a\u8bcd"},{"location":"zh/api-reference/agent/#_28","title":"\u793a\u4f8b","text":"<pre><code>model_router_middleware = ModelRouterMiddleware(\n    router_model=\"vllm:qwen3-4b\",\n    model_list=[\n        {\n            \"model_name\": \"vllm:qwen3-4b\",\n            \"model_description\": \"\u9002\u5408\u666e\u901a\u4efb\u52a1\uff0c\u5982\u5bf9\u8bdd\u3001\u6587\u672c\u751f\u6210\u7b49\"\n        },\n        {\n            \"model_name\": \"vllm:qwen3-8b\",\n            \"model_description\": \"\u9002\u5408\u590d\u6742\u4efb\u52a1\uff0c\u5982\u4ee3\u7801\u751f\u6210\u3001\u6570\u636e\u5206\u6790\u7b49\",\n        },\n    ]\n)\n</code></pre>"},{"location":"zh/api-reference/agent/#handoffagentmiddleware","title":"HandoffAgentMiddleware","text":"<p>\u7528\u4e8e\u5b9e\u73b0\u591a\u667a\u80fd\u4f53\u5207\u6362\uff08handoffs\uff09\u7684\u4e2d\u95f4\u4ef6\u3002</p>"},{"location":"zh/api-reference/agent/#_29","title":"\u7c7b\u5b9a\u4e49","text":"<pre><code>class HandoffAgentMiddleware(AgentMiddleware):\n    state_schema = MultiAgentState\n    def __init__(\n        self,\n        agents_config: dict[str, AgentConfig],\n        custom_handoffs_tool_descriptions: Optional[dict[str, str]] = None,\n        handoffs_tool_overrides: Optional[dict[str, BaseTool]] = None,\n    ) -&gt; None:\n</code></pre>"},{"location":"zh/api-reference/agent/#_30","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 agents_config dict[str, AgentConfig] \u662f - \u667a\u80fd\u4f53\u914d\u7f6e\u5b57\u5178\uff0c\u952e\u4e3a\u667a\u80fd\u4f53\u540d\u79f0\uff0c\u503c\u4e3a\u667a\u80fd\u4f53\u914d\u7f6e custom_handoffs_tool_descriptions Optional[dict[str, str]] \u5426 None \u81ea\u5b9a\u4e49\u4ea4\u63a5\u5230\u5176\u5b83\u667a\u80fd\u4f53\u7684\u5de5\u5177\u63cf\u8ff0 handoffs_tool_overrides Optional[dict[str, BaseTool]] \u5426 None \u81ea\u5b9a\u4e49\u4ea4\u63a5\u5230\u5176\u5b83\u667a\u80fd\u4f53\u7684\u5de5\u5177"},{"location":"zh/api-reference/agent/#_31","title":"\u793a\u4f8b","text":"<pre><code>handoffs_agent_middleware = HandoffsAgentMiddleware({\n    \"time_agent\":{\n        \"model\":\"vllm:qwen3-4b\",\n        \"prompt\":\"\u4f60\u662f\u4e00\u4e2a\u65f6\u95f4\u667a\u80fd\u4f53\uff0c\u8d1f\u8d23\u56de\u7b54\u65f6\u95f4\u76f8\u5173\u7684\u95ee\u9898\u3002\",\n        \"tools\":[get_current_time, transfer_to_default_agent],\n        \"handoffs\":[\"default_agent\"]\n    },\n    \"default_agent\":{\n        \"model\":\"vllm:qwen3-8b\",\n        \"prompt\":\"\u4f60\u662f\u4e00\u4e2a\u590d\u6742\u4efb\u52a1\u667a\u80fd\u4f53\uff0c\u8d1f\u8d23\u56de\u7b54\u590d\u6742\u4efb\u52a1\u76f8\u5173\u7684\u95ee\u9898\u3002\",\n        \"default\":True,\n        \"handoffs\":[\"time_agent\"]\n    }\n})\n</code></pre>"},{"location":"zh/api-reference/agent/#toolcallrepairmiddleware","title":"ToolCallRepairMiddleware","text":"<p>\u7528\u4e8e\u4fee\u590d\u65e0\u6548\u5de5\u5177\u8c03\u7528\u7684\u4e2d\u95f4\u4ef6\u3002</p>"},{"location":"zh/api-reference/agent/#_32","title":"\u7c7b\u5b9a\u4e49","text":"<pre><code>class ToolCallRepairMiddleware(AgentMiddleware):\n</code></pre>"},{"location":"zh/api-reference/agent/#_33","title":"\u793a\u4f8b","text":"<pre><code>tool_call_repair_middleware = ToolCallRepairMiddleware()\n</code></pre>"},{"location":"zh/api-reference/agent/#format_prompt","title":"format_prompt","text":"<p>\u7528\u4e8e\u683c\u5f0f\u5316\u63d0\u793a\u8bcd\u7684\u4e2d\u95f4\u4ef6\u3002</p>"},{"location":"zh/api-reference/agent/#_34","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>@dynamic_prompt\ndef format_prompt(request: ModelRequest) -&gt; str\n</code></pre>"},{"location":"zh/api-reference/agent/#planstate","title":"PlanState","text":"<p>\u7528\u4e8e Plan \u7684\u72b6\u6001 Schema\u3002</p>"},{"location":"zh/api-reference/agent/#_35","title":"\u7c7b\u5b9a\u4e49","text":"<pre><code>class Plan(TypedDict):\n    content: str\n    status: Literal[\"pending\", \"in_progress\", \"done\"]\n\n\nclass PlanState(AgentState):\n    plan: NotRequired[list[Plan]]\n</code></pre>"},{"location":"zh/api-reference/agent/#_36","title":"\u5c5e\u6027","text":"\u5c5e\u6027 \u7c7b\u578b \u63cf\u8ff0 plan NotRequired[list[Plan]] \u8ba1\u5212\u5217\u8868 plan.content str \u8ba1\u5212\u5185\u5bb9 plan.status Literal[\"pending\", \"in_progress\", \"done\"] \u8ba1\u5212\u72b6\u6001\uff0c\u53d6\u503c\u4e3a<code>pending</code>\u3001<code>in_progress</code>\u3001<code>done</code>"},{"location":"zh/api-reference/agent/#modeldict","title":"ModelDict","text":"<p>\u6a21\u578b\u5217\u8868\u7684\u7c7b\u578b\u3002</p>"},{"location":"zh/api-reference/agent/#_37","title":"\u7c7b\u5b9a\u4e49","text":"<pre><code>class ModelDict(TypedDict):\n    model_name: str\n    model_description: str\n    tools: NotRequired[list[BaseTool | dict[str, Any]]]\n    model_kwargs: NotRequired[dict[str, Any]]\n    model_instance: NotRequired[BaseChatModel]\n    model_system_prompt: NotRequired[str]\n</code></pre>"},{"location":"zh/api-reference/agent/#_38","title":"\u5c5e\u6027","text":"\u5c5e\u6027 \u7c7b\u578b \u5fc5\u586b \u63cf\u8ff0 model_name str \u662f \u6a21\u578b\u540d\u79f0 model_description str \u662f \u6a21\u578b\u63cf\u8ff0 tools NotRequired[list[BaseTool | dict[str, Any]]] \u5426 \u6a21\u578b\u53ef\u7528\u7684\u5de5\u5177 model_kwargs NotRequired[dict[str, Any]] \u5426 \u4f20\u9012\u7ed9\u6a21\u578b\u7684\u989d\u5916\u53c2\u6570 model_instance NotRequired[BaseChatModel] \u5426 \u6a21\u578b\u5b9e\u4f8b model_system_prompt NotRequired[str] \u5426 \u6a21\u578b\u7684\u7cfb\u7edf\u63d0\u793a\u8bcd"},{"location":"zh/api-reference/agent/#selectmodel","title":"SelectModel","text":"<p>\u7528\u4e8e\u9009\u62e9\u6a21\u578b\u7684\u5de5\u5177\u7c7b\u3002</p>"},{"location":"zh/api-reference/agent/#_39","title":"\u7c7b\u5b9a\u4e49","text":"<pre><code>class SelectModel(BaseModel):\n    \"\"\"Tool for model selection - Must call this tool to return the finally selected model\"\"\"\n\n    model_name: str = Field(\n        ...,\n        description=\"Selected model name (must be the full model name, for example, openai:gpt-4o)\",\n    )\n</code></pre>"},{"location":"zh/api-reference/agent/#_40","title":"\u5c5e\u6027","text":"\u5c5e\u6027 \u7c7b\u578b \u5fc5\u586b \u63cf\u8ff0 model_name str \u662f \u9009\u62e9\u7684\u6a21\u578b\u540d\u79f0\uff08\u5fc5\u987b\u662f\u5b8c\u6574\u7684\u6a21\u578b\u540d\u79f0\uff0c\u4f8b\u5982\uff0copenai:gpt-4o\uff09"},{"location":"zh/api-reference/agent/#multiagentstate","title":"MultiAgentState","text":"<p>\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u5207\u6362\u7684\u72b6\u6001 Schema\u3002</p>"},{"location":"zh/api-reference/agent/#_41","title":"\u7c7b\u5b9a\u4e49","text":"<pre><code>class MultiAgentState(AgentState):\n    active_agent: NotRequired[str]\n</code></pre>"},{"location":"zh/api-reference/agent/#_42","title":"\u5c5e\u6027","text":"\u5c5e\u6027 \u7c7b\u578b \u63cf\u8ff0 active_agent NotRequired[str] \u5f53\u524d\u6fc0\u6d3b\u7684\u667a\u80fd\u4f53\u540d\u79f0"},{"location":"zh/api-reference/agent/#agentconfig","title":"AgentConfig","text":"<p>\u667a\u80fd\u4f53\u914d\u7f6e\u7684\u7c7b\u578b\u3002</p>"},{"location":"zh/api-reference/agent/#_43","title":"\u7c7b\u5b9a\u4e49","text":"<pre><code>class AgentConfig(TypedDict):\n    model: NotRequired[str | BaseChatModel]\n    prompt: str | SystemMessage\n    tools: NotRequired[list[BaseTool | dict[str, Any]]]\n    default: NotRequired[bool]\n    handoffs: list[str] | Literal[\"all\"]\n</code></pre>"},{"location":"zh/api-reference/agent/#_44","title":"\u5c5e\u6027","text":"\u5c5e\u6027 \u7c7b\u578b \u5fc5\u586b \u63cf\u8ff0 model NotRequired[str | BaseChatModel] \u5426 \u6a21\u578b\u540d\u79f0\u6216\u6a21\u578b\u5b9e\u4f8b prompt str | SystemMessage \u662f \u667a\u80fd\u4f53\u7684\u63d0\u793a\u8bcd tools list[BaseTool | dict[str, Any]] \u662f \u667a\u80fd\u4f53\u53ef\u7528\u7684\u5de5\u5177 default NotRequired[bool] \u5426 \u662f\u5426\u4e3a\u9ed8\u8ba4\u667a\u80fd\u4f53 handoffs list[str] | Literal[\"all\"] \u662f \u53ef\u4ee5\u4ea4\u63a5\u5230\u7684\u667a\u80fd\u4f53\u540d\u79f0\u5217\u8868\uff0c\u6216\"all\"\u8868\u793a\u6240\u6709\u667a\u80fd\u4f53"},{"location":"zh/api-reference/chat_model/","title":"ChatModel \u6a21\u5757 API \u53c2\u8003\u6587\u6863","text":""},{"location":"zh/api-reference/chat_model/#register_model_provider","title":"register_model_provider","text":"<p>\u6ce8\u518c\u804a\u5929\u6a21\u578b\u7684\u63d0\u4f9b\u8005\u3002</p>"},{"location":"zh/api-reference/chat_model/#_1","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def register_model_provider(\n    provider_name: str,\n    chat_model: ChatModelType,\n    base_url: Optional[str] = None,\n    model_profiles: Optional[dict[str, dict[str, Any]]] = None,\n    compatibility_options: Optional[CompatibilityOptions] = None,\n) -&gt; None:\n</code></pre>"},{"location":"zh/api-reference/chat_model/#_2","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 provider_name str \u662f - \u81ea\u5b9a\u4e49\u63d0\u4f9b\u5546\u540d\u79f0 chat_model ChatModelType \u662f - ChatModel \u7c7b\u6216\u652f\u6301\u7684\u63d0\u4f9b\u8005\u5b57\u7b26\u4e32\u7c7b\u578b base_url Optional[str] \u5426 None \u63d0\u4f9b\u5546\u7684 BaseURL model_profiles Optional[dict[str, dict[str, Any]]] \u5426 None \u63d0\u4f9b\u5546\u6240\u652f\u6301\u7684\u6a21\u578b\u7684profile\uff0c\u683c\u5f0f\u4e3a <code>{model_name: model_profile}</code> compatibility_options Optional[CompatibilityOptions] \u5426 None \u517c\u5bb9\u6027\u9009\u9879"},{"location":"zh/api-reference/chat_model/#_3","title":"\u793a\u4f8b","text":"<pre><code>register_model_provider(\"fakechat\",FakeChatModel)\nregister_model_provider(\"vllm\", \"openai-compatible\", base_url=\"http://localhost:8000/v1\")\n</code></pre>"},{"location":"zh/api-reference/chat_model/#batch_register_model_provider","title":"batch_register_model_provider","text":"<p>\u6279\u91cf\u6ce8\u518c\u6a21\u578b\u63d0\u4f9b\u8005\u3002</p>"},{"location":"zh/api-reference/chat_model/#_4","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def batch_register_model_provider(\n    providers: list[ChatModelProvider],\n) -&gt; None:\n</code></pre>"},{"location":"zh/api-reference/chat_model/#_5","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 providers list[ChatModelProvider] \u662f - \u63d0\u4f9b\u8005\u914d\u7f6e\u5217\u8868"},{"location":"zh/api-reference/chat_model/#_6","title":"\u793a\u4f8b","text":"<pre><code>batch_register_model_provider([\n    {\"provider_name\": \"fakechat\", \"chat_model\": FakeChatModel},\n    {\"provider_name\": \"vllm\", \"chat_model\": \"openai-compatible\", \"base_url\": \"http://localhost:8000/v1\"},\n])\n</code></pre>"},{"location":"zh/api-reference/chat_model/#load_chat_model","title":"load_chat_model","text":"<p>\u4ece\u5df2\u6ce8\u518c\u7684\u63d0\u4f9b\u8005\u52a0\u8f7d\u804a\u5929\u6a21\u578b\u3002</p>"},{"location":"zh/api-reference/chat_model/#_7","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def load_chat_model(\n    model: str,\n    *,\n    model_provider: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; BaseChatModel:\n</code></pre>"},{"location":"zh/api-reference/chat_model/#_8","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 model str \u662f - \u6a21\u578b\u540d\u79f0\uff0c\u683c\u5f0f\u4e3a <code>model_name</code> \u6216 <code>provider_name:model_name</code> model_provider Optional[str] \u5426 None \u6a21\u578b\u63d0\u4f9b\u8005\u540d\u79f0 **kwargs Any \u5426 - \u989d\u5916\u7684\u6a21\u578b\u53c2\u6570"},{"location":"zh/api-reference/chat_model/#_9","title":"\u793a\u4f8b","text":"<pre><code>model = load_chat_model(\"vllm:qwen3-4b\")\n</code></pre>"},{"location":"zh/api-reference/chat_model/#create_openai_compatible_model","title":"create_openai_compatible_model","text":"<p>\u521b\u5efa\u4e00\u4e2a OpenAI \u517c\u5bb9\u7684\u804a\u5929\u6a21\u578b\u7c7b\u3002</p>"},{"location":"zh/api-reference/chat_model/#_10","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def create_openai_compatible_model(\n    model_provider: str,\n    base_url: Optional[str] = None,\n    compatibility_options: Optional[CompatibilityOptions] = None,\n    model_profiles: Optional[dict[str, dict[str, Any]]] = None,\n    chat_model_cls_name: Optional[str] = None,\n) -&gt; type[BaseChatModel]:\n</code></pre>"},{"location":"zh/api-reference/chat_model/#_11","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 model_provider str \u662f - \u6a21\u578b\u63d0\u4f9b\u8005\u540d\u79f0 base_url Optional[str] \u5426 None \u6a21\u578b\u63d0\u4f9b\u8005\u7684 BaseURL compatibility_options Optional[CompatibilityOptions] \u5426 None \u517c\u5bb9\u6027\u9009\u9879 model_profiles Optional[dict[str, dict[str, Any]]] \u5426 None \u6a21\u578b\u63d0\u4f9b\u8005\u6240\u652f\u6301\u7684\u6a21\u578b\u7684profile\uff0c\u683c\u5f0f\u4e3a <code>{model_name: model_profile}</code> chat_model_cls_name Optional[str] \u5426 None \u81ea\u5b9a\u4e49\u7684\u804a\u5929\u6a21\u578b\u7c7b\u540d"},{"location":"zh/api-reference/chat_model/#_12","title":"\u8fd4\u56de\u503c","text":"\u7c7b\u578b \u63cf\u8ff0 type[BaseChatModel] \u52a8\u6001\u521b\u5efa\u7684 OpenAI \u517c\u5bb9\u804a\u5929\u6a21\u578b\u7c7b"},{"location":"zh/api-reference/chat_model/#_13","title":"\u793a\u4f8b","text":"<pre><code>ChatVLLM = create_openai_compatible_model(\n    model_provider=\"vllm\",\n    base_url=\"http://localhost:8000/v1\",\n    chat_model_cls_name=\"ChatVLLM\",\n)\n</code></pre>"},{"location":"zh/api-reference/chat_model/#chatmodeltype","title":"ChatModelType","text":"<p>\u6ce8\u518c\u6a21\u578b\u63d0\u4f9b\u5546\u65f6<code>chat_model</code>\u53c2\u6570\u652f\u6301\u7684\u7c7b\u578b\u3002</p>"},{"location":"zh/api-reference/chat_model/#_14","title":"\u7c7b\u578b\u5b9a\u4e49","text":"<pre><code>ChatModelType = Union[type[BaseChatModel], Literal[\"openai-compatible\"]]\n</code></pre>"},{"location":"zh/api-reference/chat_model/#toolchoicetype","title":"ToolChoiceType","text":"<p><code>tool_choice</code>\u53c2\u6570\u652f\u6301\u7684\u7c7b\u578b\u3002</p>"},{"location":"zh/api-reference/chat_model/#_15","title":"\u7c7b\u578b\u5b9a\u4e49","text":"<pre><code>ToolChoiceType = list[Literal[\"auto\", \"none\", \"required\", \"specific\"]]\n</code></pre>"},{"location":"zh/api-reference/chat_model/#responseformattype","title":"ResponseFormatType","text":"<p><code>response_format</code>\u652f\u6301\u7684\u7c7b\u578b\u3002</p>"},{"location":"zh/api-reference/chat_model/#_16","title":"\u7c7b\u578b\u5b9a\u4e49","text":"<pre><code>ResponseFormatType = list[Literal[\"json_schema\", \"json_mode\"]]\n</code></pre>"},{"location":"zh/api-reference/chat_model/#reasoningkeeppolicy","title":"ReasoningKeepPolicy","text":"<p>messages\u5217\u8868\u4e2dreasoning_content\u5b57\u6bb5\u7684\u4fdd\u7559\u7b56\u7565\u3002</p>"},{"location":"zh/api-reference/chat_model/#_17","title":"\u7c7b\u578b\u5b9a\u4e49","text":"<pre><code>ReasoningKeepPolicy = Literal[\"never\", \"current\", \"all\"]\n</code></pre>"},{"location":"zh/api-reference/chat_model/#compatibilityoptions","title":"CompatibilityOptions","text":"<p>\u6a21\u578b\u63d0\u4f9b\u5546\u7684\u517c\u5bb9\u6027\u9009\u9879\u3002</p>"},{"location":"zh/api-reference/chat_model/#_18","title":"\u7c7b\u5b9a\u4e49","text":"<pre><code>class CompatibilityOptions(TypedDict):\n    supported_tool_choice: NotRequired[ToolChoiceType]\n    supported_response_format: NotRequired[ResponseFormatType]\n    reasoning_keep_policy: NotRequired[ReasoningKeepPolicy]\n    include_usage: NotRequired[bool]\n</code></pre>"},{"location":"zh/api-reference/chat_model/#_19","title":"\u5b57\u6bb5\u8bf4\u660e","text":"\u5b57\u6bb5 \u7c7b\u578b \u5fc5\u586b \u63cf\u8ff0 supported_tool_choice NotRequired[ToolChoiceType] \u5426 \u652f\u6301\u7684 <code>tool_choice</code> \u7b56\u7565\u5217\u8868 supported_response_format NotRequired[ResponseFormatType] \u5426 \u652f\u6301\u7684 <code>response_format</code> \u65b9\u6cd5\u5217\u8868 reasoning_keep_policy NotRequired[ReasoningKeepPolicy] \u5426 \u4f20\u7ed9\u6a21\u578b\u7684\u5386\u53f2\u6d88\u606f\uff08messages\uff09\u4e2d <code>reasoning_content</code> \u5b57\u6bb5\u7684\u4fdd\u7559\u7b56\u7565\u3002\u53ef\u9009\u503c\u6709<code>never</code>\u3001<code>current</code>\u3001<code>all</code> include_usage NotRequired[bool] \u5426 \u662f\u5426\u5728\u6700\u540e\u4e00\u6761\u6d41\u5f0f\u8fd4\u56de\u7ed3\u679c\u4e2d\u5305\u542b <code>usage</code> \u4fe1\u606f"},{"location":"zh/api-reference/chat_model/#chatmodelprovider","title":"ChatModelProvider","text":"<p>\u804a\u5929\u6a21\u578b\u63d0\u4f9b\u8005\u914d\u7f6e\u7c7b\u578b\u3002</p>"},{"location":"zh/api-reference/chat_model/#_20","title":"\u7c7b\u5b9a\u4e49","text":"<pre><code>class ChatModelProvider(TypedDict):\n    provider_name: str\n    chat_model: ChatModelType\n    base_url: NotRequired[str]\n    model_profiles: NotRequired[dict[str, dict[str, Any]]]\n    compatibility_options: NotRequired[CompatibilityOptions]\n</code></pre>"},{"location":"zh/api-reference/chat_model/#_21","title":"\u5b57\u6bb5\u8bf4\u660e","text":"\u5b57\u6bb5 \u7c7b\u578b \u5fc5\u586b \u63cf\u8ff0 provider_name str \u662f \u63d0\u4f9b\u8005\u540d\u79f0 chat_model ChatModelType \u662f \u652f\u6301\u4f20\u5165\u5bf9\u8bdd\u6a21\u578b\u7c7b\u6216\u5b57\u7b26\u4e32\uff08\u76ee\u524d\u53ea\u652f\u6301<code>openai-compatible</code>\uff09 base_url NotRequired[str] \u5426 \u57fa\u7840 URL model_profiles NotRequired[dict[str, dict[str, Any]]] \u5426 \u63d0\u4f9b\u5546\u6240\u652f\u6301\u7684\u6a21\u578b\u7684profile\uff0c\u683c\u5f0f\u4e3a <code>{model_name: model_profile}</code> compatibility_options NotRequired[CompatibilityOptions] \u5426 \u6a21\u578b\u63d0\u4f9b\u5546\u517c\u5bb9\u6027\u9009\u9879"},{"location":"zh/api-reference/embeddings/","title":"Embeddings \u6a21\u5757 API \u53c2\u8003\u6587\u6863","text":""},{"location":"zh/api-reference/embeddings/#register_embeddings_provider","title":"register_embeddings_provider","text":"<p>\u6ce8\u518c\u5d4c\u5165\u6a21\u578b\u7684\u63d0\u4f9b\u8005\u3002</p>"},{"location":"zh/api-reference/embeddings/#_1","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def register_embeddings_provider(\n    provider_name: str,\n    embeddings_model: EmbeddingsType,\n    base_url: Optional[str] = None,\n) -&gt; None:\n</code></pre>"},{"location":"zh/api-reference/embeddings/#_2","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 provider_name str \u662f - \u81ea\u5b9a\u4e49\u63d0\u4f9b\u8005\u540d\u79f0 embeddings_model EmbeddingsType \u662f - \u5d4c\u5165\u6a21\u578b\u7c7b\u6216\u652f\u6301\u7684\u63d0\u4f9b\u8005\u5b57\u7b26\u4e32\u7c7b\u578b base_url Optional[str] \u5426 None \u63d0\u4f9b\u8005\u7684 BaseURL"},{"location":"zh/api-reference/embeddings/#_3","title":"\u793a\u4f8b","text":"<pre><code>register_embeddings_provider(\"fakeembeddings\", FakeEmbeddings)\nregister_embeddings_provider(\"vllm\", \"openai-compatible\", base_url=\"http://localhost:8000/v1\")\n</code></pre>"},{"location":"zh/api-reference/embeddings/#batch_register_embeddings_provider","title":"batch_register_embeddings_provider","text":"<p>\u6279\u91cf\u6ce8\u518c\u5d4c\u5165\u6a21\u578b\u63d0\u4f9b\u8005\u3002</p>"},{"location":"zh/api-reference/embeddings/#_4","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def batch_register_embeddings_provider(\n    providers: list[EmbeddingProvider]\n) -&gt; None:\n</code></pre>"},{"location":"zh/api-reference/embeddings/#_5","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 providers list[EmbeddingProvider] \u662f - \u63d0\u4f9b\u8005\u914d\u7f6e\u5217\u8868"},{"location":"zh/api-reference/embeddings/#_6","title":"\u793a\u4f8b","text":"<pre><code>batch_register_embeddings_provider([\n    {\"provider_name\": \"fakeembeddings\", \"embeddings_model\": FakeEmbeddings},\n    {\"provider_name\": \"vllm\", \"embeddings_model\": \"openai-compatible\", \"base_url\": \"http://localhost:8000/v1\"},\n])\n</code></pre>"},{"location":"zh/api-reference/embeddings/#load_embeddings","title":"load_embeddings","text":"<p>\u4ece\u5df2\u6ce8\u518c\u7684\u63d0\u4f9b\u8005\u52a0\u8f7d\u5d4c\u5165\u6a21\u578b\u3002</p>"},{"location":"zh/api-reference/embeddings/#_7","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def load_embeddings(\n    model: str,\n    *,\n    provider: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; Embeddings:\n</code></pre>"},{"location":"zh/api-reference/embeddings/#_8","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 model str \u662f - \u6a21\u578b\u540d\u79f0\uff0c\u683c\u5f0f\u4e3a <code>model_name</code> \u6216 <code>provider_name:model_name</code> provider Optional[str] \u5426 None \u6a21\u578b\u63d0\u4f9b\u8005\u540d\u79f0 **kwargs Any \u5426 - \u989d\u5916\u7684\u6a21\u578b\u53c2\u6570"},{"location":"zh/api-reference/embeddings/#_9","title":"\u793a\u4f8b","text":"<pre><code>embeddings = load_embeddings(\"vllm:qwen3-embedding-4b\")\n</code></pre>"},{"location":"zh/api-reference/embeddings/#create_openai_compatible_embedding","title":"create_openai_compatible_embedding","text":"<p>\u521b\u5efa\u4e00\u4e2a OpenAI \u517c\u5bb9\u7684\u5d4c\u5165\u6a21\u578b\u7c7b\u3002</p>"},{"location":"zh/api-reference/embeddings/#_10","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def create_openai_compatible_embedding(\n    embedding_provider: str,\n    base_url: Optional[str] = None,\n    embedding_model_cls_name: Optional[str] = None,\n) -&gt; type[Embeddings]:\n</code></pre>"},{"location":"zh/api-reference/embeddings/#_11","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 embedding_provider str \u662f - \u5d4c\u5165\u6a21\u578b\u63d0\u4f9b\u8005\u540d\u79f0 base_url Optional[str] \u5426 None \u6a21\u578b\u63d0\u4f9b\u8005\u7684 BaseURL embedding_model_cls_name Optional[str] \u5426 None \u81ea\u5b9a\u4e49\u7684\u5d4c\u5165\u6a21\u578b\u7c7b\u540d"},{"location":"zh/api-reference/embeddings/#_12","title":"\u8fd4\u56de\u503c","text":"\u7c7b\u578b \u63cf\u8ff0 type[Embeddings] \u52a8\u6001\u521b\u5efa\u7684 OpenAI \u517c\u5bb9\u5d4c\u5165\u6a21\u578b\u7c7b"},{"location":"zh/api-reference/embeddings/#_13","title":"\u793a\u4f8b","text":""},{"location":"zh/api-reference/embeddings/#embeddingstype","title":"EmbeddingsType","text":"<p>\u6ce8\u518c\u5d4c\u5165\u63d0\u4f9b\u5546\u65f6<code>embeddings_model</code>\u53c2\u6570\u652f\u6301\u7684\u7c7b\u578b\u3002</p>"},{"location":"zh/api-reference/embeddings/#_14","title":"\u7c7b\u578b\u5b9a\u4e49","text":"<pre><code>EmbeddingsType = Union[type[Embeddings], Literal[\"openai-compatible\"]]\n</code></pre>"},{"location":"zh/api-reference/embeddings/#embeddingprovider","title":"EmbeddingProvider","text":"<p>\u5d4c\u5165\u6a21\u578b\u63d0\u4f9b\u8005\u914d\u7f6e\u7c7b\u578b\u3002</p>"},{"location":"zh/api-reference/embeddings/#_15","title":"\u7c7b\u5b9a\u4e49","text":"<pre><code>class EmbeddingProvider(TypedDict):\n    provider_name: str\n    embeddings_model: EmbeddingsType\n    base_url: NotRequired[str]\n</code></pre>"},{"location":"zh/api-reference/embeddings/#_16","title":"\u5b57\u6bb5\u8bf4\u660e","text":"\u5b57\u6bb5 \u7c7b\u578b \u5fc5\u586b \u63cf\u8ff0 provider_name str \u662f \u63d0\u4f9b\u8005\u540d\u79f0 embeddings_model EmbeddingsType \u662f \u5d4c\u5165\u6a21\u578b\u7c7b\u6216\u5b57\u7b26\u4e32 base_url NotRequired[str] \u5426 \u57fa\u7840 URL"},{"location":"zh/api-reference/message_convert/","title":"Message Convert \u6a21\u5757 API \u53c2\u8003\u6587\u6863","text":""},{"location":"zh/api-reference/message_convert/#convert_reasoning_content_for_ai_message","title":"convert_reasoning_content_for_ai_message","text":"<p>\u5c06\u601d\u7ef4\u94fe\u5408\u5e76\u5230\u6700\u7ec8\u56de\u590d\u4e2d\u3002</p>"},{"location":"zh/api-reference/message_convert/#_1","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def convert_reasoning_content_for_ai_message(\n    model_response: AIMessage,\n    think_tag: Tuple[str, str] = (\"&lt;think&gt;\", \"&lt;/think&gt;\"),\n) -&gt; AIMessage\n</code></pre>"},{"location":"zh/api-reference/message_convert/#_2","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 model_response AIMessage \u662f - \u5305\u542b\u63a8\u7406\u5185\u5bb9\u7684 AI \u6d88\u606f think_tag Tuple[str, str] \u5426 <code>(\"&lt;think&gt;\",\"&lt;/think&gt;\")</code> \u63a8\u7406\u5185\u5bb9\u7684\u5f00\u59cb\u548c\u7ed3\u675f\u6807\u7b7e"},{"location":"zh/api-reference/message_convert/#_3","title":"\u793a\u4f8b","text":"<pre><code>response = convert_reasoning_content_for_ai_message(\n    response, think_tag=(\"&lt;start&gt;\", \"&lt;end&gt;\")\n)\n</code></pre>"},{"location":"zh/api-reference/message_convert/#convert_reasoning_content_for_chunk_iterator","title":"convert_reasoning_content_for_chunk_iterator","text":"<p>\u4e3a\u6d41\u5f0f\u6d88\u606f\u5757\u5408\u5e76\u63a8\u7406\u5185\u5bb9\u3002</p>"},{"location":"zh/api-reference/message_convert/#_4","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def convert_reasoning_content_for_chunk_iterator(\n    model_response: Iterator[AIMessageChunk | AIMessage],\n    think_tag: Tuple[str, str] = (\"&lt;think&gt;\", \"&lt;/think&gt;\"),\n) -&gt; Iterator[AIMessageChunk | AIMessage]\n</code></pre>"},{"location":"zh/api-reference/message_convert/#_5","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 model_response Iterator[AIMessageChunk | AIMessage] \u662f - \u6d88\u606f\u5757\u7684\u8fed\u4ee3\u5668 think_tag Tuple[str, str] \u5426 <code>(\"&lt;think&gt;\",\"&lt;/think&gt;\")</code> \u63a8\u7406\u5185\u5bb9\u7684\u5f00\u59cb\u548c\u7ed3\u675f\u6807\u7b7e"},{"location":"zh/api-reference/message_convert/#_6","title":"\u793a\u4f8b","text":"<pre><code>for chunk in convert_reasoning_content_for_chunk_iterator(\n    model.stream(\"Hello\"), think_tag=(\"&lt;think&gt;\", \"&lt;/think&gt;\")\n):\n    print(chunk.content, end=\"\", flush=True)\n</code></pre>"},{"location":"zh/api-reference/message_convert/#aconvert_reasoning_content_for_chunk_iterator","title":"aconvert_reasoning_content_for_chunk_iterator","text":"<p><code>convert_reasoning_content_for_chunk_iterator</code> \u7684\u5f02\u6b65\u7248\u672c\u3002</p>"},{"location":"zh/api-reference/message_convert/#_7","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>async def aconvert_reasoning_content_for_chunk_iterator(\n    model_response: AsyncIterator[AIMessageChunk | AIMessage],\n    think_tag: Tuple[str, str] = (\"&lt;think&gt;\", \"&lt;/think&gt;\"),\n) -&gt; AsyncIterator[AIMessageChunk | AIMessage]\n</code></pre>"},{"location":"zh/api-reference/message_convert/#_8","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 model_response AsyncIterator[AIMessageChunk | AIMessage] \u662f - \u6d88\u606f\u5757\u7684\u5f02\u6b65\u8fed\u4ee3\u5668 think_tag Tuple[str, str] \u5426 <code>(\"&lt;think&gt;\",\"&lt;/think&gt;\")</code> \u63a8\u7406\u5185\u5bb9\u7684\u5f00\u59cb\u548c\u7ed3\u675f\u6807\u7b7e"},{"location":"zh/api-reference/message_convert/#_9","title":"\u793a\u4f8b","text":"<pre><code>async for chunk in aconvert_reasoning_content_for_chunk_iterator(\n    model.astream(\"Hello\"), think_tag=(\"&lt;think&gt;\", \"&lt;/think&gt;\")\n):\n    print(chunk.content, end=\"\", flush=True)\n</code></pre>"},{"location":"zh/api-reference/message_convert/#merge_ai_message_chunk","title":"merge_ai_message_chunk","text":"<p>\u5c06\u6d41\u5f0f\u8f93\u51fa\u7684 chunks \u5408\u5e76\u4e3a\u4e00\u4e2a AIMessage\u3002</p>"},{"location":"zh/api-reference/message_convert/#_10","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def merge_ai_message_chunk(\n    chunks: Sequence[AIMessageChunk]\n) -&gt; AIMessage\n</code></pre>"},{"location":"zh/api-reference/message_convert/#_11","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 chunks Sequence[AIMessageChunk] \u662f - \u5f85\u5408\u5e76\u7684\u6d88\u606f\u5757\u5217\u8868"},{"location":"zh/api-reference/message_convert/#_12","title":"\u793a\u4f8b","text":"<pre><code>chunks = list(model.stream(\"Hello\"))\nmerged = merge_ai_message_chunk(chunks)\n</code></pre>"},{"location":"zh/api-reference/message_convert/#format_sequence","title":"format_sequence","text":"<p>\u5c06 BaseMessage\u3001Document \u6216\u5b57\u7b26\u4e32\u5217\u8868\u683c\u5f0f\u5316\u4e3a\u5355\u4e2a\u5b57\u7b26\u4e32\u3002</p>"},{"location":"zh/api-reference/message_convert/#_13","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def format_sequence(\n    inputs: List[Union[BaseMessage, Document, str]],\n    separator: str = \"-\",\n    with_num: bool = False\n) -&gt; str\n</code></pre>"},{"location":"zh/api-reference/message_convert/#_14","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 inputs List[Union[BaseMessage, Document, str]] \u662f - \u5f85\u683c\u5f0f\u5316\u7684\u9879\u76ee\u5217\u8868 separator str \u5426 \"-\" \u5206\u9694\u7b26\u5b57\u7b26\u4e32 with_num bool \u5426 False \u662f\u5426\u6dfb\u52a0\u6570\u5b57\u524d\u7f00"},{"location":"zh/api-reference/message_convert/#_15","title":"\u793a\u4f8b","text":"<pre><code>formatted = format_sequence(messages, separator=\"\\n\", with_num=True)\n</code></pre>"},{"location":"zh/api-reference/pipeline/","title":"Pipeline \u6a21\u5757 API \u53c2\u8003\u6587\u6863","text":""},{"location":"zh/api-reference/pipeline/#create_sequential_pipeline","title":"create_sequential_pipeline","text":"<p>\u5c06\u591a\u4e2a\u72b6\u6001\u76f8\u540c\u7684\u5b50\u56fe\u4ee5\u4e32\u884c\u65b9\u5f0f\u7ec4\u5408\u3002</p>"},{"location":"zh/api-reference/pipeline/#_1","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def create_sequential_pipeline(\n    sub_graphs: list[SubGraph],\n    state_schema: type[StateT],\n    graph_name: Optional[str] = None,\n    context_schema: type[ContextT] | None = None,\n    input_schema: type[InputT] | None = None,\n    output_schema: type[OutputT] | None = None,\n    checkpointer: Checkpointer | None = None,\n    store: BaseStore | None = None,\n    cache: BaseCache | None = None,\n) -&gt; CompiledStateGraph[StateT, ContextT, InputT, OutputT]:\n</code></pre>"},{"location":"zh/api-reference/pipeline/#_2","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 sub_graphs list[SubGraph] \u662f - \u8981\u7ec4\u5408\u7684\u72b6\u6001\u56fe\u5217\u8868 state_schema type[StateT] \u662f - \u6700\u7ec8\u751f\u6210\u56fe\u7684 State Schema graph_name Optional[str] \u5426 None \u6700\u7ec8\u751f\u6210\u56fe\u7684\u540d\u79f0 context_schema type[ContextT] | None \u5426 None \u6700\u7ec8\u751f\u6210\u56fe\u7684 Context Schema input_schema type[InputT] | None \u5426 None \u6700\u7ec8\u751f\u6210\u56fe\u7684\u8f93\u5165 Schema output_schema type[OutputT] | None \u5426 None \u6700\u7ec8\u751f\u6210\u56fe\u7684\u8f93\u51fa Schema checkpointer Checkpointer | None \u5426 None \u6700\u7ec8\u751f\u6210\u56fe\u7684 Checkpointer store BaseStore | None \u5426 None \u6700\u7ec8\u751f\u6210\u56fe\u7684 Store cache BaseCache | None \u5426 None \u6700\u7ec8\u751f\u6210\u56fe\u7684 Cache"},{"location":"zh/api-reference/pipeline/#_3","title":"\u793a\u4f8b","text":"<pre><code>create_sequential_pipeline(\n    sub_graphs=[graph1, graph2],\n    state_schema=State,\n    graph_name=\"sequential_pipeline\",\n    context_schema=Context,\n    input_schema=Input,\n    output_schema=Output,\n)\n</code></pre>"},{"location":"zh/api-reference/pipeline/#create_parallel_pipeline","title":"create_parallel_pipeline","text":"<p>\u5c06\u591a\u4e2a\u72b6\u6001\u76f8\u540c\u7684\u5b50\u56fe\u4ee5\u5e76\u884c\u65b9\u5f0f\u7ec4\u5408\u3002</p>"},{"location":"zh/api-reference/pipeline/#_4","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def create_parallel_pipeline(\n    sub_graphs: list[SubGraph],\n    state_schema: type[StateT],\n    graph_name: Optional[str] = None,\n    branches_fn: Optional[\n        Union[\n            Callable[..., list[Send]],\n            Callable[..., Awaitable[list[Send]]],\n        ]\n    ] = None,\n    context_schema: type[ContextT] | None = None,\n    input_schema: type[InputT] | None = None,\n    output_schema: type[OutputT] | None = None,\n    checkpointer: Checkpointer | None = None,\n    store: BaseStore | None = None,\n    cache: BaseCache | None = None,\n) -&gt; CompiledStateGraph[StateT, ContextT, InputT, OutputT]:\n</code></pre>"},{"location":"zh/api-reference/pipeline/#_5","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 sub_graphs list[SubGraph] \u662f - \u8981\u7ec4\u5408\u7684\u72b6\u6001\u56fe\u5217\u8868 state_schema type[StateT] \u662f - \u6700\u7ec8\u751f\u6210\u56fe\u7684 State Schema graph_name Optional[str] \u5426 None \u6700\u7ec8\u751f\u6210\u56fe\u7684\u540d\u79f0 branches_fn Optional[Union[Callable[..., list[Send]], Callable[..., Awaitable[list[Send]]]]] \u5426 None \u5e76\u884c\u5206\u652f\u51fd\u6570\uff0c\u8fd4\u56de Send \u5217\u8868\u63a7\u5236\u5e76\u884c\u6267\u884c context_schema type[ContextT] | None \u5426 None \u6700\u7ec8\u751f\u6210\u56fe\u7684 Context Schema input_schema type[InputT] | None \u5426 None \u6700\u7ec8\u751f\u6210\u56fe\u7684\u8f93\u5165 Schema output_schema type[OutputT] | None \u5426 None \u6700\u7ec8\u751f\u6210\u56fe\u7684\u8f93\u51fa Schema checkpointer Checkpointer | None \u5426 None \u6700\u7ec8\u751f\u6210\u56fe\u7684 Checkpointer store BaseStore | None \u5426 None \u6700\u7ec8\u751f\u6210\u56fe\u7684 Store cache BaseCache | None \u5426 None \u6700\u7ec8\u751f\u6210\u56fe\u7684 Cache"},{"location":"zh/api-reference/pipeline/#_6","title":"\u793a\u4f8b","text":"<pre><code>create_parallel_pipeline(\n    sub_graphs=[graph1, graph2],\n    state_schema=State,\n    graph_name=\"parallel_pipeline\",\n    branches_fn=lambda state: [Send(\"graph1\", state), Send(\"graph2\", state)],\n    context_schema=Context,\n    input_schema=Input,\n    output_schema=Output,\n)\n</code></pre>"},{"location":"zh/api-reference/tool_calling/","title":"Tool Calling \u6a21\u5757 API \u53c2\u8003\u6587\u6863","text":""},{"location":"zh/api-reference/tool_calling/#has_tool_calling","title":"has_tool_calling","text":"<p>\u68c0\u67e5\u6d88\u606f\u662f\u5426\u5305\u542b\u5de5\u5177\u8c03\u7528\u3002</p>"},{"location":"zh/api-reference/tool_calling/#_1","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def has_tool_calling(\n    message: AIMessage\n) -&gt; bool\n</code></pre>"},{"location":"zh/api-reference/tool_calling/#_2","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 message AIMessage \u662f - \u5f85\u68c0\u67e5\u7684\u6d88\u606f"},{"location":"zh/api-reference/tool_calling/#_3","title":"\u793a\u4f8b","text":"<pre><code>if has_tool_calling(response):\n    # \u5904\u7406\u5de5\u5177\u8c03\u7528\n    pass\n</code></pre>"},{"location":"zh/api-reference/tool_calling/#parse_tool_calling","title":"parse_tool_calling","text":"<p>\u4ece\u6d88\u606f\u4e2d\u89e3\u6790\u5de5\u5177\u8c03\u7528\u53c2\u6570\u3002</p>"},{"location":"zh/api-reference/tool_calling/#_4","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def parse_tool_calling(\n    message: AIMessage, first_tool_call_only: bool = False\n) -&gt; Union[tuple[str, dict], list[tuple[str, dict]]]\n</code></pre>"},{"location":"zh/api-reference/tool_calling/#_5","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 message AIMessage \u662f - \u5f85\u89e3\u6790\u7684\u6d88\u606f first_tool_call_only bool \u5426 False \u662f\u5426\u4ec5\u8fd4\u56de\u7b2c\u4e00\u4e2a\u5de5\u5177\u8c03\u7528"},{"location":"zh/api-reference/tool_calling/#_6","title":"\u793a\u4f8b","text":"<pre><code># \u83b7\u53d6\u6240\u6709\u5de5\u5177\u8c03\u7528\ntool_calls = parse_tool_calling(response)\n\n# \u4ec5\u83b7\u53d6\u7b2c\u4e00\u4e2a\u5de5\u5177\u8c03\u7528\nname, args = parse_tool_calling(response, first_tool_call_only=True)\n</code></pre>"},{"location":"zh/api-reference/tool_calling/#human_in_the_loop","title":"human_in_the_loop","text":"<p>\u4e3a\u540c\u6b65\u5de5\u5177\u51fd\u6570\u6dfb\u52a0\"\u4eba\u5728\u56de\u8def\"\u4eba\u5de5\u5ba1\u6838\u80fd\u529b\u7684\u88c5\u9970\u5668\u3002</p>"},{"location":"zh/api-reference/tool_calling/#_7","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def human_in_the_loop(\n    func: Optional[Callable] = None,\n    *,\n    handler: Optional[HumanInterruptHandler] = None\n) -&gt; Union[Callable[[Callable], BaseTool], BaseTool]\n</code></pre>"},{"location":"zh/api-reference/tool_calling/#_8","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 func Optional[Callable] \u5426 None \u5f85\u88c5\u9970\u7684\u540c\u6b65\u51fd\u6570\uff08\u88c5\u9970\u5668\u8bed\u6cd5\u7cd6\uff09 handler Optional[HumanInterruptHandler] \u5426 None \u81ea\u5b9a\u4e49\u4e2d\u65ad\u5904\u7406\u51fd\u6570"},{"location":"zh/api-reference/tool_calling/#_9","title":"\u793a\u4f8b","text":"<pre><code>@human_in_the_loop\ndef get_current_time():\n    \"\"\"\u83b7\u53d6\u5f53\u524d\u65f6\u95f4\"\"\"\n    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n</code></pre>"},{"location":"zh/api-reference/tool_calling/#human_in_the_loop_async","title":"human_in_the_loop_async","text":"<p>\u4e3a\u5f02\u6b65\u5de5\u5177\u51fd\u6570\u6dfb\u52a0\"\u4eba\u5728\u56de\u8def\"\u4eba\u5de5\u5ba1\u6838\u80fd\u529b\u7684\u88c5\u9970\u5668\u3002</p>"},{"location":"zh/api-reference/tool_calling/#_10","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def human_in_the_loop_async(\n    func: Optional[Callable] = None,\n    *,\n    handler: Optional[HumanInterruptHandler] = None\n) -&gt; Union[Callable[[Callable], BaseTool], BaseTool]\n</code></pre>"},{"location":"zh/api-reference/tool_calling/#_11","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 func Optional[Callable] \u5426 None \u5f85\u88c5\u9970\u7684\u5f02\u6b65\u51fd\u6570\uff08\u88c5\u9970\u5668\u8bed\u6cd5\u7cd6\uff09 handler Optional[HumanInterruptHandler] \u5426 None \u81ea\u5b9a\u4e49\u4e2d\u65ad\u5904\u7406\u51fd\u6570"},{"location":"zh/api-reference/tool_calling/#_12","title":"\u793a\u4f8b","text":"<pre><code>@human_in_the_loop_async\nasync def get_current_time():\n    \"\"\"\u83b7\u53d6\u5f53\u524d\u65f6\u95f4\"\"\"\n    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n</code></pre>"},{"location":"zh/api-reference/tool_calling/#interruptparams","title":"InterruptParams","text":"<p>\u4f20\u9012\u7ed9\u4e2d\u65ad\u5904\u7406\u51fd\u6570\u7684\u53c2\u6570\u7c7b\u578b\u3002</p>"},{"location":"zh/api-reference/tool_calling/#_13","title":"\u7c7b\u5b9a\u4e49","text":"<pre><code>class InterruptParams(TypedDict):\n    tool_call_name: str\n    tool_call_args: Dict[str, Any]\n    tool: BaseTool\n</code></pre>"},{"location":"zh/api-reference/tool_calling/#_14","title":"\u5b57\u6bb5\u8bf4\u660e","text":"\u5b57\u6bb5 \u7c7b\u578b \u5fc5\u586b \u63cf\u8ff0 tool_call_name str \u662f \u5de5\u5177\u8c03\u7528\u540d\u79f0 tool_call_args Dict[str, Any] \u662f \u5de5\u5177\u8c03\u7528\u53c2\u6570 tool BaseTool \u662f \u5de5\u5177\u5b9e\u4f8b"},{"location":"zh/api-reference/tool_calling/#humaninterrupthandler","title":"HumanInterruptHandler","text":"<p>\u4e2d\u65ad\u5904\u7406\u5668\u51fd\u6570\u7684\u7c7b\u578b\u522b\u540d\u3002</p>"},{"location":"zh/api-reference/tool_calling/#_15","title":"\u7c7b\u578b\u5b9a\u4e49","text":"<pre><code>HumanInterruptHandler = Callable[[InterruptParams], Any]\n</code></pre>"},{"location":"zh/getting-started-guide/chat/","title":"\u5bf9\u8bdd\u6a21\u578b\u7ba1\u7406","text":""},{"location":"zh/getting-started-guide/chat/#_2","title":"\u6982\u8ff0","text":"<p>LangChain \u7684 <code>init_chat_model</code> \u51fd\u6570\u4ec5\u652f\u6301\u6709\u9650\u7684\u6a21\u578b\u63d0\u4f9b\u5546\u3002\u672c\u5e93\u63d0\u4f9b\u66f4\u7075\u6d3b\u7684\u5bf9\u8bdd\u6a21\u578b\u7ba1\u7406\u65b9\u6848\uff0c\u652f\u6301\u81ea\u5b9a\u4e49\u6a21\u578b\u63d0\u4f9b\u5546\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u63a5\u5165\u672a\u5185\u7f6e\u652f\u6301\u7684\u6a21\u578b\u670d\u52a1\uff08\u5982 vLLM\u7b49\uff09\u7684\u573a\u666f\u3002</p>"},{"location":"zh/getting-started-guide/chat/#_3","title":"\u6ce8\u518c\u6a21\u578b\u63d0\u4f9b\u5546","text":"<p>\u6ce8\u518c\u5bf9\u8bdd\u6a21\u578b\u63d0\u4f9b\u5546\u9700\u8c03\u7528 <code>register_model_provider</code>\u3002\u5bf9\u4e8e\u4e0d\u540c\u7684\u60c5\u51b5\uff0c\u6ce8\u518c\u6b65\u9aa4\u7565\u6709\u4e0d\u540c\u3002</p>"},{"location":"zh/getting-started-guide/chat/#langchain","title":"\u5df2\u6709 LangChain \u5bf9\u8bdd\u6a21\u578b\u7c7b","text":"<p>\u82e5\u6a21\u578b\u63d0\u4f9b\u5546\u5df2\u6709\u73b0\u6210\u4e14\u5408\u9002\u7684 LangChain \u96c6\u6210\uff08\u8be6\u89c1\u5bf9\u8bdd\u6a21\u578b\u7c7b\u96c6\u6210\uff09\uff0c\u8bf7\u5c06\u76f8\u5e94\u7684\u96c6\u6210\u5bf9\u8bdd\u6a21\u578b\u7c7b\u4f5c\u4e3a chat_model \u53c2\u6570\u4f20\u5165\u3002</p>"},{"location":"zh/getting-started-guide/chat/#_4","title":"\u53c2\u6570\u8bf4\u660e","text":"\u53c2\u6570 \u8bf4\u660e <code>provider_name</code> \u6a21\u578b\u63d0\u4f9b\u5546\u540d\u79f0\uff0c\u7528\u4e8e\u540e\u7eed\u5728 <code>load_chat_model</code> \u4e2d\u5f15\u7528\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u662f <code>chat_model</code> LangChain \u5bf9\u8bdd\u6a21\u578b\u7c7b\u3002\u7c7b\u578b: <code>type[BaseChatModel]</code>\u5fc5\u586b: \u662f <code>base_url</code> API \u57fa\u7840\u5730\u5740\uff0c\u901a\u5e38\u65e0\u9700\u624b\u52a8\u8bbe\u7f6e\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u5426 <code>model_profiles</code> \u6a21\u578b\u914d\u7f6e\u4fe1\u606f\u5b57\u5178\u3002\u7c7b\u578b: <code>dict</code>\u5fc5\u586b: \u5426"},{"location":"zh/getting-started-guide/chat/#_5","title":"\u4ee3\u7801\u793a\u4f8b","text":"<pre><code>from langchain_core.language_models.fake_chat_models import FakeChatModel\nfrom langchain_dev_utils.chat_models import register_model_provider\n\nregister_model_provider(\n    provider_name=\"fake_provider\",\n    chat_model=FakeChatModel,\n)\n</code></pre>"},{"location":"zh/getting-started-guide/chat/#_6","title":"\u4f7f\u7528\u8bf4\u660e","text":"<ul> <li><code>FakeChatModel</code> \u4ec5\u7528\u4e8e\u6d4b\u8bd5\u3002\u5b9e\u9645\u4f7f\u7528\u4e2d\u5fc5\u987b\u4f20\u5165\u5177\u5907\u771f\u5b9e\u529f\u80fd\u7684 <code>ChatModel</code> \u7c7b\u3002</li> <li><code>provider_name</code> \u4ee3\u8868\u6a21\u578b\u63d0\u4f9b\u5546\u7684\u540d\u79f0\uff0c\u7528\u4e8e\u540e\u7eed\u5728 <code>load_chat_model</code> \u4e2d\u5f15\u7528\u3002</li> </ul> <p>\u6ce8\u610f</p> <p><code>provider_name</code> \u5fc5\u987b\u4ee5\u5b57\u6bcd\u6216\u6570\u5b57\u5f00\u5934\uff0c\u53ea\u80fd\u5305\u542b\u5b57\u6bcd\u3001\u6570\u5b57\u548c\u4e0b\u5212\u7ebf\uff0c\u957f\u5ea6\u4e0d\u8d85\u8fc7 20 \u4e2a\u5b57\u7b26\u3002</p>"},{"location":"zh/getting-started-guide/chat/#_7","title":"\u53ef\u9009\u53c2\u6570\u8bf4\u660e","text":"<p>base_url</p> <p>\u6b64\u53c2\u6570\u901a\u5e38\u65e0\u9700\u8bbe\u7f6e\uff08\u56e0\u4e3a\u5bf9\u8bdd\u6a21\u578b\u7c7b\u5185\u90e8\u4e00\u822c\u5df2\u5b9a\u4e49\u9ed8\u8ba4\u7684 API \u5730\u5740\uff09\uff0c\u4ec5\u5f53\u9700\u8981\u8986\u76d6\u5bf9\u8bdd\u6a21\u578b\u7c7b\u9ed8\u8ba4\u5730\u5740\u65f6\u624d\u4f20\u5165 <code>base_url</code>\uff0c\u4e14\u4ec5\u5bf9\u5b57\u6bb5\u540d\u4e3a <code>api_base</code> \u6216 <code>base_url</code>\uff08\u542b\u522b\u540d\uff09\u7684\u5c5e\u6027\u751f\u6548\u3002</p> <p>model_profiles</p> <p>\u5982\u679c\u4f60\u7684 LangChain \u96c6\u6210\u5bf9\u8bdd\u6a21\u578b\u7c7b\u5df2\u5168\u9762\u652f\u6301 <code>profile</code> \u53c2\u6570\uff08\u5373\u53ef\u4ee5\u901a\u8fc7 <code>model.profile</code> \u76f4\u63a5\u8bbf\u95ee\u6a21\u578b\u7684\u76f8\u5173\u5c5e\u6027\uff0c\u4f8b\u5982 <code>max_input_tokens</code>\u3001<code>tool_calling</code> \u7b49\uff09\uff0c\u5219\u65e0\u9700\u989d\u5916\u8bbe\u7f6e <code>model_profiles</code>\u3002</p> <p>\u5982\u679c\u901a\u8fc7 <code>model.profile</code> \u8bbf\u95ee\u65f6\u8fd4\u56de\u7684\u662f\u4e00\u4e2a\u7a7a\u5b57\u5178 <code>{}</code>\uff0c\u8bf4\u660e\u8be5 LangChain \u5bf9\u8bdd\u6a21\u578b\u7c7b\u53ef\u80fd\u6682\u65f6\u672a\u652f\u6301 <code>profile</code> \u53c2\u6570\uff0c\u6b64\u65f6\u53ef\u4ee5\u624b\u52a8\u63d0\u4f9b <code>model_profiles</code>\u3002</p> <p><code>model_profiles</code> \u662f\u4e00\u4e2a\u5b57\u5178\uff0c\u5176\u6bcf\u4e00\u4e2a\u952e\u4e3a\u6a21\u578b\u540d\u79f0\uff0c\u503c\u4e3a\u5bf9\u5e94\u6a21\u578b\u7684 profile \u914d\u7f6e:</p> <pre><code>{\n    \"model_name_1\": {\n        \"max_input_tokens\": 100_000,\n        \"tool_calling\": True,\n        \"structured_output\": True,\n        # ... \u5176\u4ed6\u53ef\u9009\u5b57\u6bb5\n    },\n    \"model_name_2\": {\n        \"max_input_tokens\": 32768,\n        \"image_inputs\": True,\n        \"tool_calling\": False,\n        # ... \u5176\u4ed6\u53ef\u9009\u5b57\u6bb5\n    },\n    # \u53ef\u4ee5\u6709\u4efb\u610f\u591a\u4e2a\u6a21\u578b\u914d\u7f6e\n}\n</code></pre> <p>\u63d0\u793a</p> <p>\u63a8\u8350\u4f7f\u7528 <code>langchain-model-profiles</code> \u5e93\u6765\u83b7\u53d6\u4f60\u6240\u7528\u6a21\u578b\u63d0\u4f9b\u5546\u7684 profiles\u3002</p>"},{"location":"zh/getting-started-guide/chat/#langchain-openai-api","title":"\u672a\u6709 LangChain \u5bf9\u8bdd\u6a21\u578b\u7c7b\uff0c\u4f46\u6a21\u578b\u63d0\u4f9b\u5546\u652f\u6301 OpenAI \u517c\u5bb9 API","text":"<p>\u8fd9\u79cd\u60c5\u51b5\u4e0b\u7684\u53c2\u6570\u8bf4\u660e\u5982\u4e0b\uff1a</p>"},{"location":"zh/getting-started-guide/chat/#_8","title":"\u53c2\u6570\u8bf4\u660e","text":"\u53c2\u6570 \u8bf4\u660e <code>provider_name</code> \u6a21\u578b\u63d0\u4f9b\u5546\u540d\u79f0\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u662f <code>chat_model</code> \u56fa\u5b9a\u53d6\u503c <code>\"openai-compatible\"</code>\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u662f <code>base_url</code> API \u57fa\u7840\u5730\u5740\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u5426 <code>model_profiles</code> \u6a21\u578b\u914d\u7f6e\u4fe1\u606f\u5b57\u5178\u3002\u7c7b\u578b: <code>dict</code>\u5fc5\u586b: \u5426 <code>compatibility_options</code> \u517c\u5bb9\u6027\u9009\u9879\u914d\u7f6e\u3002\u7c7b\u578b: <code>dict</code>\u5fc5\u586b: \u5426"},{"location":"zh/getting-started-guide/chat/#_9","title":"\u4ee3\u7801\u793a\u4f8b","text":"<p>\u65b9\u5f0f\u4e00\uff1a\u663e\u5f0f\u4f20\u53c2</p> <pre><code>register_model_provider(\n    provider_name=\"vllm\",\n    chat_model=\"openai-compatible\",\n    base_url=\"http://localhost:8000/v1\"\n)\n</code></pre> <p>\u65b9\u5f0f\u4e8c\uff1a\u901a\u8fc7\u73af\u5883\u53d8\u91cf\uff08\u63a8\u8350\u7528\u4e8e\u914d\u7f6e\u7ba1\u7406\uff09</p> <pre><code>export VLLM_API_BASE=http://localhost:8000/v1\n</code></pre> <pre><code>register_model_provider(\n    provider_name=\"vllm\",\n    chat_model=\"openai-compatible\"\n    # \u81ea\u52a8\u8bfb\u53d6 VLLM_API_BASE\n)\n</code></pre> <p>\u6ce8\u610f\uff1a\u5173\u4e8e\u8fd9\u90e8\u5206\u66f4\u591a\u7684\u7ec6\u8282\uff0c\u8bf7\u53c2\u8003OpenAI \u517c\u5bb9 API \u96c6\u6210\u3002</p>"},{"location":"zh/getting-started-guide/chat/#_10","title":"\u6279\u91cf\u6ce8\u518c","text":"<p>\u82e5\u9700\u6ce8\u518c\u591a\u4e2a\u63d0\u4f9b\u5546\uff0c\u53ef\u4f7f\u7528 <code>batch_register_model_provider</code> \u907f\u514d\u91cd\u590d\u8c03\u7528\u3002</p>"},{"location":"zh/getting-started-guide/chat/#_11","title":"\u53c2\u6570\u8bf4\u660e","text":"\u53c2\u6570 \u8bf4\u660e <code>providers</code> \u63d0\u4f9b\u5546\u914d\u7f6e\u5217\u8868\uff0c\u6bcf\u4e2a\u5b57\u5178\u5305\u542b\u6ce8\u518c\u53c2\u6570\u3002\u7c7b\u578b: <code>list[dict]</code>\u5fc5\u586b: \u662f"},{"location":"zh/getting-started-guide/chat/#_12","title":"\u4ee3\u7801\u793a\u4f8b","text":"<pre><code>from langchain_dev_utils.chat_models import batch_register_model_provider\nfrom langchain_core.language_models.fake_chat_models import FakeChatModel\n\nbatch_register_model_provider(\n    providers=[\n        {\n            \"provider_name\": \"fake_provider\",\n            \"chat_model\": FakeChatModel,\n        },\n        {\n            \"provider_name\": \"vllm\",\n            \"chat_model\": \"openai-compatible\",\n            \"base_url\": \"http://localhost:8000/v1\",\n        },\n    ]\n)\n</code></pre> <p>\u6ce8\u610f</p> <p>\u4e24\u4e2a\u6ce8\u518c\u51fd\u6570\u5747\u57fa\u4e8e\u5168\u5c40\u5b57\u5178\u5b9e\u73b0\u3002\u4e3a\u907f\u514d\u591a\u7ebf\u7a0b\u95ee\u9898\uff0c\u5fc5\u987b\u5728\u5e94\u7528\u542f\u52a8\u9636\u6bb5\u5b8c\u6210\u6240\u6709\u6ce8\u518c\uff0c\u7981\u6b62\u8fd0\u884c\u65f6\u52a8\u6001\u6ce8\u518c\u3002  </p> <p>\u6b64\u5916\uff0c\u6ce8\u518c\u65f6\u82e5\u5c06 <code>chat_model</code> \u8bbe\u4e3a <code>openai-compatible</code>\uff0c\u5185\u90e8\u4f1a\u901a\u8fc7 <code>pydantic.create_model</code> \u52a8\u6001\u521b\u5efa\u65b0\u7684\u6a21\u578b\u7c7b\uff08\u4ee5 <code>BaseChatOpenAICompatible</code> \u4e3a\u57fa\u7c7b\uff0c\u751f\u6210\u5bf9\u5e94\u7684\u5bf9\u8bdd\u6a21\u578b\u96c6\u6210\u7c7b\uff09\uff0c\u6b64\u8fc7\u7a0b\u6d89\u53ca Python \u5143\u7c7b\u64cd\u4f5c\u548c pydantic \u9a8c\u8bc1\u903b\u8f91\u521d\u59cb\u5316\uff0c\u5b58\u5728\u4e00\u5b9a\u6027\u80fd\u5f00\u9500\uff0c\u56e0\u6b64\u8bf7\u907f\u514d\u5728\u8fd0\u884c\u671f\u9891\u7e41\u6ce8\u518c\u3002</p>"},{"location":"zh/getting-started-guide/chat/#_13","title":"\u52a0\u8f7d\u5bf9\u8bdd\u6a21\u578b","text":"<p>\u4f7f\u7528 <code>load_chat_model</code> \u51fd\u6570\u52a0\u8f7d\u5bf9\u8bdd\u6a21\u578b\uff08\u521d\u59cb\u5316\u5bf9\u8bdd\u6a21\u578b\u5b9e\u4f8b\uff09\u3002</p>"},{"location":"zh/getting-started-guide/chat/#_14","title":"\u53c2\u6570\u8bf4\u660e","text":"\u53c2\u6570 \u8bf4\u660e <code>model</code> \u6a21\u578b\u540d\u79f0\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u662f <code>model_provider</code> \u6a21\u578b\u63d0\u4f9b\u5546\u540d\u79f0\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u5426 <p>\u9664\u6b64\u4e4b\u5916\uff0c\u8fd8\u53ef\u4ee5\u4f20\u5165\u4efb\u610f\u6570\u91cf\u7684\u5173\u952e\u5b57\u53c2\u6570\uff0c\u7528\u4e8e\u4f20\u9012\u5bf9\u8bdd\u6a21\u578b\u7c7b\u7684\u989d\u5916\u53c2\u6570\u3002</p>"},{"location":"zh/getting-started-guide/chat/#_15","title":"\u53c2\u6570\u89c4\u5219","text":"<ul> <li>\u82e5\u672a\u4f20 <code>model_provider</code>\uff0c\u5219 <code>model</code> \u5fc5\u987b\u4e3a <code>provider_name:model_name</code> \u683c\u5f0f\uff1b</li> <li>\u82e5\u4f20 <code>model_provider</code>\uff0c\u5219 <code>model</code> \u5fc5\u987b\u4ec5\u4e3a <code>model_name</code>\u3002</li> </ul>"},{"location":"zh/getting-started-guide/chat/#_16","title":"\u4ee3\u7801\u793a\u4f8b","text":"<pre><code># \u65b9\u5f0f\u4e00\uff1amodel \u5305\u542b provider \u4fe1\u606f\nmodel = load_chat_model(\"vllm:qwen3-4b\")\n\n# \u65b9\u5f0f\u4e8c\uff1a\u5355\u72ec\u6307\u5b9a provider\nmodel = load_chat_model(\"qwen3-4b\", model_provider=\"vllm\")\n</code></pre>"},{"location":"zh/getting-started-guide/chat/#_17","title":"\u6a21\u578b\u65b9\u6cd5\u548c\u53c2\u6570","text":"<p>\u5bf9\u4e8e\u652f\u6301\u7684\u6a21\u578b\u65b9\u6cd5\u548c\u53c2\u6570\uff0c\u9700\u8981\u53c2\u8003\u5bf9\u5e94\u7684\u5bf9\u8bdd\u6a21\u578b\u7c7b\u7684\u4f7f\u7528\u8bf4\u660e\u3002\u5982\u679c\u91c7\u7528\u7684\u662f\u7b2c\u4e8c\u79cd\u60c5\u51b5\uff0c\u5219\u652f\u6301\u6240\u6709\u7684<code>BaseChatOpenAI</code> \u7c7b\u7684\u65b9\u6cd5\u548c\u53c2\u6570\u3002</p>"},{"location":"zh/getting-started-guide/chat/#_18","title":"\u517c\u5bb9\u5b98\u65b9\u63d0\u4f9b\u5546","text":"<p>\u5bf9\u4e8e LangChain \u5b98\u65b9\u5df2\u652f\u6301\u7684\u63d0\u4f9b\u5546\uff08\u5982 <code>openai</code>\uff09\uff0c\u53ef\u76f4\u63a5\u4f7f\u7528 <code>load_chat_model</code> \u65e0\u9700\u6ce8\u518c\uff1a</p> <pre><code>model = load_chat_model(\"openai:gpt-4o-mini\")\n# \u6216\nmodel = load_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n</code></pre> <p>\u6700\u4f73\u5b9e\u8df5</p> <p>\u5bf9\u4e8e\u672c\u6a21\u5757\u7684\u4f7f\u7528\uff0c\u53ef\u4ee5\u6839\u636e\u4e0b\u9762\u4e09\u79cd\u60c5\u51b5\u8fdb\u884c\u9009\u62e9\uff1a</p> <ol> <li> <p>\u82e5\u63a5\u5165\u7684\u6240\u6709\u6a21\u578b\u63d0\u4f9b\u5546\u5747\u88ab\u5b98\u65b9 <code>init_chat_model</code> \u652f\u6301\uff0c\u8bf7\u76f4\u63a5\u4f7f\u7528\u5b98\u65b9\u51fd\u6570\uff0c\u4ee5\u83b7\u5f97\u6700\u4f73\u517c\u5bb9\u6027\u548c\u7a33\u5b9a\u6027\u3002</p> </li> <li> <p>\u82e5\u63a5\u5165\u7684\u90e8\u5206\u6a21\u578b\u63d0\u4f9b\u5546\u4e3a\u975e\u5b98\u65b9\u652f\u6301\uff0c\u53ef\u4f7f\u7528\u672c\u6a21\u5757\u7684\u529f\u80fd\uff0c\u5148\u5229\u7528<code>register_model_provider</code>\u6ce8\u518c\u6a21\u578b\u63d0\u4f9b\u5546\uff0c\u7136\u540e\u4f7f\u7528<code>load_chat_model</code>\u52a0\u8f7d\u6a21\u578b\u3002</p> </li> <li> <p>\u82e5\u63a5\u5165\u7684\u6a21\u578b\u63d0\u4f9b\u5546\u6682\u65e0\u9002\u5408\u7684\u96c6\u6210\uff0c\u4f46\u63d0\u4f9b\u5546\u63d0\u4f9b\u4e86 OpenAI \u517c\u5bb9\u7684 API\uff08\u5982 vLLM\uff09\uff0c\u5219\u63a8\u8350\u4f7f\u7528\u672c\u6a21\u5757\u7684\u529f\u80fd\uff0c\u5148\u5229\u7528<code>register_model_provider</code>\u6ce8\u518c\u6a21\u578b\u63d0\u4f9b\u5546\uff08chat_model\u4f20\u5165<code>openai-compatible</code>\uff09\uff0c\u7136\u540e\u4f7f\u7528<code>load_chat_model</code>\u52a0\u8f7d\u6a21\u578b\u3002</p> </li> </ol>"},{"location":"zh/getting-started-guide/embedding/","title":"\u5d4c\u5165\u6a21\u578b\u7ba1\u7406","text":""},{"location":"zh/getting-started-guide/embedding/#_2","title":"\u6982\u8ff0","text":"<p>LangChain \u7684 <code>init_embeddings</code> \u51fd\u6570\u4ec5\u652f\u6301\u6709\u9650\u7684\u5d4c\u5165\u6a21\u578b\u63d0\u4f9b\u5546\u3002\u672c\u5e93\u63d0\u4f9b\u66f4\u7075\u6d3b\u7684\u5d4c\u5165\u6a21\u578b\u7ba1\u7406\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u63a5\u5165\u672a\u5185\u7f6e\u652f\u6301\u7684\u5d4c\u5165\u670d\u52a1\uff08\u5982 vLLM \u7b49\uff09\u7684\u573a\u666f\u3002</p>"},{"location":"zh/getting-started-guide/embedding/#_3","title":"\u6ce8\u518c\u5d4c\u5165\u6a21\u578b\u63d0\u4f9b\u5546","text":"<p>\u6ce8\u518c\u5d4c\u5165\u6a21\u578b\u63d0\u4f9b\u5546\u9700\u8c03\u7528 <code>register_embeddings_provider</code>\u3002\u6839\u636e <code>embeddings_model</code> \u7c7b\u578b\u4e0d\u540c\uff0c\u6ce8\u518c\u65b9\u5f0f\u7565\u6709\u5dee\u5f02\u3002</p>"},{"location":"zh/getting-started-guide/embedding/#langchain","title":"\u5df2\u6709 LangChain \u5d4c\u5165\u6a21\u578b\u7c7b","text":"<p>\u82e5\u5d4c\u5165\u6a21\u578b\u63d0\u4f9b\u5546\u5df2\u6709\u73b0\u6210\u4e14\u5408\u9002\u7684 LangChain \u96c6\u6210\uff08\u8be6\u89c1 \u5d4c\u5165\u6a21\u578b\u96c6\u6210\u5217\u8868\uff09\uff0c\u8bf7\u5c06\u76f8\u5e94\u7684\u5d4c\u5165\u6a21\u578b\u7c7b\u76f4\u63a5\u4f20\u5165 <code>embeddings_model</code> \u53c2\u6570\u3002</p>"},{"location":"zh/getting-started-guide/embedding/#_4","title":"\u53c2\u6570\u8bf4\u660e","text":"\u53c2\u6570 \u8bf4\u660e <code>provider_name</code> \u6a21\u578b\u63d0\u4f9b\u5546\u540d\u79f0\uff0c\u7528\u4e8e\u540e\u7eed\u5728 <code>load_embeddings</code> \u4e2d\u5f15\u7528\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u662f <code>embeddings_model</code> LangChain \u5d4c\u5165\u6a21\u578b\u7c7b\u3002\u7c7b\u578b: <code>type[Embeddings]</code>\u5fc5\u586b: \u662f <code>base_url</code> API \u57fa\u7840\u5730\u5740\uff0c\u901a\u5e38\u65e0\u9700\u624b\u52a8\u8bbe\u7f6e\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u5426"},{"location":"zh/getting-started-guide/embedding/#_5","title":"\u4ee3\u7801\u793a\u4f8b","text":"<pre><code>from langchain_core.embeddings.fake import FakeEmbeddings\nfrom langchain_dev_utils.embeddings import register_embeddings_provider\n\nregister_embeddings_provider(\n    provider_name=\"fake_provider\",\n    embeddings_model=FakeEmbeddings,\n)\n</code></pre>"},{"location":"zh/getting-started-guide/embedding/#_6","title":"\u4f7f\u7528\u8bf4\u660e","text":"<ul> <li><code>FakeEmbeddings</code> \u4ec5\u7528\u4e8e\u6d4b\u8bd5\u3002\u5b9e\u9645\u4f7f\u7528\u4e2d\u5fc5\u987b\u4f20\u5165\u5177\u5907\u771f\u5b9e\u529f\u80fd\u7684 <code>Embeddings</code> \u7c7b\u3002</li> <li><code>provider_name</code> \u4ee3\u8868\u6a21\u578b\u63d0\u4f9b\u5546\u7684\u540d\u79f0\uff0c\u7528\u4e8e\u540e\u7eed\u5728 <code>load_embeddings</code> \u4e2d\u5f15\u7528\u3002\u540d\u79f0\u53ef\u81ea\u5b9a\u4e49\uff0c\u4f46\u4e0d\u8981\u5305\u542b <code>:</code>\u3001<code>-</code> \u7b49\u7279\u6b8a\u5b57\u7b26\u3002</li> </ul> <p>\u6ce8\u610f</p> <p><code>provider_name</code> \u5fc5\u987b\u4ee5\u5b57\u6bcd\u6216\u6570\u5b57\u5f00\u5934\uff0c\u53ea\u80fd\u5305\u542b\u5b57\u6bcd\u3001\u6570\u5b57\u548c\u4e0b\u5212\u7ebf\uff0c\u957f\u5ea6\u4e0d\u8d85\u8fc7 20 \u4e2a\u5b57\u7b26\u3002</p>"},{"location":"zh/getting-started-guide/embedding/#_7","title":"\u53ef\u9009\u53c2\u6570\u8bf4\u660e","text":"<p>base_url</p> <p>\u6b64\u53c2\u6570\u901a\u5e38\u65e0\u9700\u8bbe\u7f6e\uff08\u56e0\u4e3a\u5d4c\u5165\u6a21\u578b\u7c7b\u5185\u90e8\u4e00\u822c\u5df2\u5b9a\u4e49\u9ed8\u8ba4\u7684 API \u5730\u5740\uff09\uff0c\u4ec5\u5f53\u9700\u8981\u8986\u76d6\u5d4c\u5165\u6a21\u578b\u7c7b\u9ed8\u8ba4\u5730\u5740\u65f6\u624d\u4f20\u5165 <code>base_url</code>\uff0c\u4e14\u4ec5\u5bf9\u5b57\u6bb5\u540d\u4e3a <code>api_base</code> \u6216 <code>base_url</code>\uff08\u542b\u522b\u540d\uff09\u7684\u5c5e\u6027\u751f\u6548\u3002</p>"},{"location":"zh/getting-started-guide/embedding/#langchain-openai-api","title":"\u672a\u6709 LangChain \u5d4c\u5165\u6a21\u578b\u7c7b\uff0c\u4f46\u63d0\u4f9b\u5546\u652f\u6301 OpenAI \u517c\u5bb9 API","text":"<p>\u8fd9\u79cd\u60c5\u51b5\u4e0b\u7684\u53c2\u6570\u8bf4\u660e\u5982\u4e0b\uff1a</p>"},{"location":"zh/getting-started-guide/embedding/#_8","title":"\u53c2\u6570\u8bf4\u660e","text":"\u53c2\u6570 \u8bf4\u660e <code>provider_name</code> \u6a21\u578b\u63d0\u4f9b\u5546\u540d\u79f0\uff0c\u7528\u4e8e\u540e\u7eed\u5728 <code>load_embeddings</code> \u4e2d\u5f15\u7528\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u662f <code>embeddings_model</code> \u56fa\u5b9a\u53d6\u503c <code>\"openai-compatible\"</code>\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u662f <code>base_url</code> API \u57fa\u7840\u5730\u5740\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u5426"},{"location":"zh/getting-started-guide/embedding/#_9","title":"\u4ee3\u7801\u793a\u4f8b","text":"<p>\u65b9\u5f0f\u4e00\uff1a\u663e\u5f0f\u4f20\u53c2</p> <pre><code>register_embeddings_provider(\n    provider_name=\"vllm\",\n    embeddings_model=\"openai-compatible\",\n    base_url=\"http://localhost:8000/v1\"\n)\n</code></pre> <p>\u65b9\u5f0f\u4e8c\uff1a\u73af\u5883\u53d8\u91cf\uff08\u63a8\u8350\uff09</p> <pre><code>export VLLM_API_BASE=http://localhost:8000/v1\n</code></pre> <pre><code>register_embeddings_provider(\n    provider_name=\"vllm\",\n    embeddings_model=\"openai-compatible\"\n    # \u81ea\u52a8\u8bfb\u53d6 VLLM_API_BASE\n)\n</code></pre> <p>\u6ce8\u610f\uff1a\u5173\u4e8e\u8fd9\u90e8\u5206\u66f4\u591a\u7684\u7ec6\u8282\uff0c\u8bf7\u53c2\u8003OpenAI \u517c\u5bb9 API \u96c6\u6210\u3002</p>"},{"location":"zh/getting-started-guide/embedding/#_10","title":"\u6279\u91cf\u6ce8\u518c","text":"<p>\u82e5\u9700\u6ce8\u518c\u591a\u4e2a\u63d0\u4f9b\u5546\uff0c\u53ef\u4f7f\u7528 <code>batch_register_embeddings_provider</code>\u3002</p>"},{"location":"zh/getting-started-guide/embedding/#_11","title":"\u53c2\u6570\u8bf4\u660e","text":"\u53c2\u6570 \u8bf4\u660e <code>providers</code> \u63d0\u4f9b\u5546\u914d\u7f6e\u5217\u8868\uff0c\u6bcf\u4e2a\u5b57\u5178\u5305\u542b\u6ce8\u518c\u53c2\u6570\u3002\u7c7b\u578b: <code>list[dict]</code>\u5fc5\u586b: \u662f"},{"location":"zh/getting-started-guide/embedding/#_12","title":"\u4ee3\u7801\u793a\u4f8b","text":"<pre><code>from langchain_dev_utils.embeddings import batch_register_embeddings_provider\nfrom langchain_core.embeddings.fake import FakeEmbeddings\n\nbatch_register_embeddings_provider(\n    providers=[\n        {\n            \"provider_name\": \"fake_provider\",\n            \"embeddings_model\": FakeEmbeddings,\n        },\n        {\n            \"provider_name\": \"vllm\",\n            \"embeddings_model\": \"openai-compatible\",\n            \"base_url\": \"http://localhost:8000/v1\",\n        },\n    ]\n)\n</code></pre> <p>\u6ce8\u610f</p> <p>\u4e24\u4e2a\u6ce8\u518c\u51fd\u6570\u5747\u57fa\u4e8e\u5168\u5c40\u5b57\u5178\u5b9e\u73b0\u3002\u5fc5\u987b\u5728\u5e94\u7528\u542f\u52a8\u9636\u6bb5\u5b8c\u6210\u6240\u6709\u6ce8\u518c\uff0c\u7981\u6b62\u8fd0\u884c\u65f6\u52a8\u6001\u6ce8\u518c\uff0c\u4ee5\u907f\u514d\u591a\u7ebf\u7a0b\u95ee\u9898\u3002 </p> <p>\u6b64\u5916\uff0c\u6ce8\u518c\u65f6\u82e5\u5c06 <code>embeddings_model</code> \u8bbe\u4e3a <code>openai-compatible</code>\uff0c\u5185\u90e8\u4f1a\u901a\u8fc7 <code>pydantic.create_model</code> \u52a8\u6001\u521b\u5efa\u65b0\u7684\u6a21\u578b\u7c7b\uff08\u4ee5 <code>BaseEmbeddingOpenAICompatible</code> \u4e3a\u57fa\u7c7b\uff0c\u751f\u6210\u5bf9\u5e94\u7684\u5d4c\u5165\u6a21\u578b\u96c6\u6210\u7c7b\uff09\uff0c\u6b64\u8fc7\u7a0b\u6d89\u53ca Python \u5143\u7c7b\u64cd\u4f5c\u548c pydantic \u9a8c\u8bc1\u903b\u8f91\u521d\u59cb\u5316\uff0c\u5b58\u5728\u4e00\u5b9a\u6027\u80fd\u5f00\u9500\uff0c\u56e0\u6b64\u8bf7\u907f\u514d\u5728\u8fd0\u884c\u671f\u9891\u7e41\u6ce8\u518c\u3002</p>"},{"location":"zh/getting-started-guide/embedding/#_13","title":"\u52a0\u8f7d\u5d4c\u5165\u6a21\u578b","text":"<p>\u4f7f\u7528 <code>load_embeddings</code> \u521d\u59cb\u5316\u5d4c\u5165\u6a21\u578b\u5b9e\u4f8b\u3002</p>"},{"location":"zh/getting-started-guide/embedding/#_14","title":"\u53c2\u6570\u8bf4\u660e","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u8bf4\u660e <code>model</code> <code>str</code> \u662f - \u6a21\u578b\u540d\u79f0 <code>provider</code> <code>str</code> \u5426 <code>None</code> \u6a21\u578b\u63d0\u4f9b\u5546\u540d\u79f0 <p>\u9664\u6b64\u4e4b\u5916\uff0c\u8fd8\u53ef\u4ee5\u4f20\u5165\u4efb\u610f\u6570\u91cf\u7684\u5173\u952e\u5b57\u53c2\u6570\uff0c\u7528\u4e8e\u4f20\u9012\u5d4c\u5165\u6a21\u578b\u7c7b\u7684\u989d\u5916\u53c2\u6570\u3002</p>"},{"location":"zh/getting-started-guide/embedding/#_15","title":"\u53c2\u6570\u89c4\u5219","text":"<ul> <li>\u82e5\u672a\u4f20 <code>provider</code>\uff0c\u5219 <code>model</code> \u5fc5\u987b\u4e3a <code>provider_name:embeddings_name</code> \u683c\u5f0f\uff1b</li> <li>\u82e5\u4f20 <code>provider</code>\uff0c\u5219 <code>model</code> \u4ec5\u4e3a <code>embeddings_name</code>\u3002</li> </ul>"},{"location":"zh/getting-started-guide/embedding/#_16","title":"\u4ee3\u7801\u793a\u4f8b","text":"<pre><code># \u65b9\u5f0f\u4e00\uff1amodel \u5305\u542b provider \u4fe1\u606f\nembedding = load_embeddings(\"vllm:qwen3-embedding-4b\")\n\n# \u65b9\u5f0f\u4e8c\uff1a\u5355\u72ec\u6307\u5b9a provider\nembedding = load_embeddings(\"qwen3-embedding-4b\", provider=\"vllm\")\n</code></pre>"},{"location":"zh/getting-started-guide/embedding/#_17","title":"\u6a21\u578b\u65b9\u6cd5\u548c\u53c2\u6570","text":"<p>\u5bf9\u4e8e\u652f\u6301\u7684\u6a21\u578b\u65b9\u6cd5\u548c\u53c2\u6570\uff0c\u9700\u8981\u53c2\u8003\u5bf9\u5e94\u7684\u5d4c\u5165\u6a21\u578b\u7c7b\u7684\u4f7f\u7528\u8bf4\u660e\u3002\u5982\u679c\u91c7\u7528\u7684\u662f\u7b2c\u4e8c\u79cd\u60c5\u51b5\uff0c\u5219\u652f\u6301\u6240\u6709\u7684<code>OpenAIEmbeddings</code> \u7c7b\u7684\u65b9\u6cd5\u548c\u53c2\u6570\u3002</p>"},{"location":"zh/getting-started-guide/embedding/#_18","title":"\u517c\u5bb9\u5b98\u65b9\u63d0\u4f9b\u5546","text":"<p>\u5bf9\u4e8e LangChain \u5b98\u65b9\u5df2\u652f\u6301\u7684\u63d0\u4f9b\u5546\uff08\u5982 <code>openai</code>\uff09\uff0c\u53ef\u76f4\u63a5\u4f7f\u7528 <code>load_embeddings</code> \u65e0\u9700\u6ce8\u518c\uff1a</p> <pre><code>model = load_embeddings(\"openai:text-embedding-3-large\")\n# \u6216\nmodel = load_embeddings(\"text-embedding-3-large\", provider=\"openai\")\n</code></pre> <p>\u6700\u4f73\u5b9e\u8df5</p> <p>\u5bf9\u4e8e\u672c\u6a21\u5757\u7684\u4f7f\u7528\uff0c\u53ef\u4ee5\u6839\u636e\u4e0b\u9762\u4e09\u79cd\u60c5\u51b5\u8fdb\u884c\u9009\u62e9\uff1a</p> <ol> <li> <p>\u82e5\u63a5\u5165\u7684\u6240\u6709\u5d4c\u5165\u6a21\u578b\u63d0\u4f9b\u5546\u5747\u88ab\u5b98\u65b9 <code>init_embeddings</code> \u652f\u6301\uff0c\u8bf7\u76f4\u63a5\u4f7f\u7528\u5b98\u65b9\u51fd\u6570\uff0c\u4ee5\u83b7\u5f97\u6700\u4f73\u517c\u5bb9\u6027\u3002</p> </li> <li> <p>\u82e5\u63a5\u5165\u7684\u90e8\u5206\u5d4c\u5165\u6a21\u578b\u63d0\u4f9b\u5546\u4e3a\u975e\u5b98\u65b9\u652f\u6301\uff0c\u53ef\u5229\u7528\u672c\u6a21\u5757\u7684\u6ce8\u518c\u4e0e\u52a0\u8f7d\u673a\u5236\uff0c\u5148\u5229\u7528<code>register_embeddings_provider</code>\u6ce8\u518c\u6a21\u578b\u63d0\u4f9b\u5546\uff0c\u7136\u540e\u4f7f\u7528<code>load_embeddings</code>\u52a0\u8f7d\u6a21\u578b\u3002</p> </li> <li> <p>\u82e5\u63a5\u5165\u7684\u5d4c\u5165\u6a21\u578b\u63d0\u4f9b\u5546\u6682\u65e0\u9002\u5408\u7684\u96c6\u6210\uff0c\u4f46\u63d0\u4f9b\u5546\u63d0\u4f9b\u4e86 OpenAI \u517c\u5bb9\u7684 API\uff08\u5982 vLLM\uff09\uff0c\u5219\u63a8\u8350\u5229\u7528\u672c\u6a21\u5757\u7684\u529f\u80fd\uff0c\u5148\u5229\u7528<code>register_embeddings_provider</code>\u6ce8\u518c\u6a21\u578b\u63d0\u4f9b\u5546\uff08embeddings_model\u4f20\u5165<code>openai-compatible</code>\uff09\uff0c\u7136\u540e\u4f7f\u7528<code>load_embeddings</code>\u52a0\u8f7d\u6a21\u578b\u3002</p> </li> </ol>"},{"location":"zh/getting-started-guide/format/","title":"\u683c\u5f0f\u5316\u5e8f\u5217","text":""},{"location":"zh/getting-started-guide/format/#_2","title":"\u6982\u8ff0","text":"<p>\u7528\u4e8e\u5c06\u7531 Document\u3001Message \u6216\u5b57\u7b26\u4e32\u7ec4\u6210\u7684\u5217\u8868\u683c\u5f0f\u5316\u4e3a\u5355\u4e2a\u6587\u672c\u5b57\u7b26\u4e32\u3002\u5177\u4f53\u51fd\u6570\u4e3a <code>format_sequence</code>\u3002</p>"},{"location":"zh/getting-started-guide/format/#_3","title":"\u4f7f\u7528\u793a\u4f8b","text":""},{"location":"zh/getting-started-guide/format/#_4","title":"\u4ee3\u7801\u793a\u4f8b","text":"<pre><code>from langchain_core.documents import Document\nfrom langchain_core.messages import AIMessage\nfrom langchain_dev_utils.message_convert import format_sequence\n\nformated1 = format_sequence(\n    [\n        AIMessage(content=\"Hello1\"),\n        AIMessage(content=\"Hello2\"),\n        AIMessage(content=\"Hello3\"),\n    ]\n)\nprint(formated1)\n</code></pre>"},{"location":"zh/getting-started-guide/format/#_5","title":"\u8f93\u51fa\u7ed3\u679c","text":"<pre><code>-Hello1\n-Hello2\n-Hello3\n</code></pre>"},{"location":"zh/getting-started-guide/format/#_6","title":"\u4ee3\u7801\u793a\u4f8b","text":"<pre><code>format2 = format_sequence(\n    [\n        Document(page_content=\"content1\"),\n        Document(page_content=\"content2\"),\n        Document(page_content=\"content3\"),\n    ],\n    separator=\"&gt;\",\n)\nprint(format2)\n</code></pre>"},{"location":"zh/getting-started-guide/format/#_7","title":"\u8f93\u51fa\u7ed3\u679c","text":"<pre><code>&gt;content1\n&gt;content2\n&gt;content3\n</code></pre>"},{"location":"zh/getting-started-guide/format/#_8","title":"\u4ee3\u7801\u793a\u4f8b","text":"<pre><code>format3 = format_sequence(\n    [\n        \"str1\",\n        \"str2\",\n        \"str3\",\n    ],\n    separator=\"&gt;\",\n    with_num=True,\n)\nprint(format3)\n</code></pre>"},{"location":"zh/getting-started-guide/format/#_9","title":"\u8f93\u51fa\u7ed3\u679c","text":"<pre><code>&gt;1. str1\n&gt;2. str2\n&gt;3. str3\n</code></pre>"},{"location":"zh/getting-started-guide/installation/","title":"\u5b89\u88c5","text":"<p><code>langchain-dev-utils</code>\u652f\u6301\u4f7f\u7528<code>pip</code>\u3001<code>poetry</code>\u3001<code>uv</code>\u7b49\u591a\u79cd\u5305\u7ba1\u7406\u5668\u8fdb\u884c\u5b89\u88c5\u3002</p> <p>\u5b89\u88c5\u57fa\u7840\u7248\u672c\u7684<code>langchain-dev-utils</code>\uff1a</p> pippoetryuv <pre><code>pip install -U langchain-dev-utils\n</code></pre> <pre><code>poetry add langchain-dev-utils\n</code></pre> <pre><code>uv add langchain-dev-utils\n</code></pre> <p>\u5b89\u88c5\u5b8c\u6574\u529f\u80fd\u7248\u672c\u7684<code>langchain-dev-utils</code>\uff1a</p> pippoetryuv <pre><code>pip install -U \"langchain-dev-utils[standard]\"\n</code></pre> <pre><code>poetry add \"langchain-dev-utils[standard]\"\n</code></pre> <pre><code>uv add langchain-dev-utils[standard]\n</code></pre>"},{"location":"zh/getting-started-guide/installation/#_2","title":"\u9a8c\u8bc1\u5b89\u88c5","text":"<p>\u5b89\u88c5\u540e\uff0c\u9a8c\u8bc1\u5305\u662f\u5426\u6b63\u786e\u5b89\u88c5\uff1a</p> <pre><code>import langchain_dev_utils\nprint(langchain_dev_utils.__version__)\n</code></pre>"},{"location":"zh/getting-started-guide/installation/#_3","title":"\u4f9d\u8d56\u9879","text":"<p>\u8be5\u5305\u4f1a\u81ea\u52a8\u5b89\u88c5\u4ee5\u4e0b\u4f9d\u8d56\u9879\uff1a</p> <ul> <li><code>langchain</code></li> <li><code>langgraph</code> (\u5b89\u88c5<code>langchain</code>\u65f6\u4f1a\u540c\u65f6\u4e5f\u4f1a\u5b89\u88c5)</li> </ul> <p>\u5982\u679c\u662f standard \u7248\u672c\uff0c\u8fd8\u4f1a\u5b89\u88c5\u4ee5\u4e0b\u4f9d\u8d56\u9879\uff1a</p> <ul> <li><code>langchain-openai</code>\uff08\u7528\u4e8e\u6a21\u578b\u7ba1\u7406\uff09</li> <li><code>json-repair</code>(\u7528\u4e8e\u4e2d\u95f4\u4ef6\u7684\u5de5\u5177\u8c03\u7528\u9519\u8bef\u4fee\u590d)</li> </ul>"},{"location":"zh/getting-started-guide/message/","title":"\u6d88\u606f\u5904\u7406","text":""},{"location":"zh/getting-started-guide/message/#_2","title":"\u6982\u8ff0","text":"<p>\u4e3b\u8981\u529f\u80fd\u5305\u62ec\uff1a</p> <ul> <li>\u5408\u5e76\u63a8\u7406\u5185\u5bb9\u81f3\u6700\u7ec8\u56de\u590d</li> <li>\u5408\u5e76\u6d41\u5f0f\u8f93\u51fa\u7684 Chunks</li> </ul>"},{"location":"zh/getting-started-guide/message/#_3","title":"\u5408\u5e76\u63a8\u7406\u5185\u5bb9\u81f3\u6700\u7ec8\u56de\u590d","text":"<p>\u7528\u4e8e\u5c06\u63a8\u7406\u5185\u5bb9\uff08<code>reasoning_content</code>\uff09\u5408\u5e76\u81f3\u6700\u7ec8\u56de\u590d\uff08<code>content</code>\uff09\u3002</p>"},{"location":"zh/getting-started-guide/message/#_4","title":"\u529f\u80fd\u8bf4\u660e","text":"\u51fd\u6570 \u8bf4\u660e <code>convert_reasoning_content_for_ai_message</code> \u5c06 AIMessage \u4e2d\u7684\u63a8\u7406\u5185\u5bb9\u5408\u5e76\u5230\u5185\u5bb9\u5b57\u6bb5\uff08\u7528\u4e8e\u6a21\u578b\u7684 invoke \u548c ainvoke\uff09 <code>convert_reasoning_content_for_chunk_iterator</code> \u5c06\u6d41\u5f0f\u54cd\u5e94\u4e2d\u7684\u63a8\u7406\u5185\u5bb9\u5408\u5e76\u5230\u5185\u5bb9\u5b57\u6bb5\uff08\u7528\u4e8e\u6a21\u578b\u7684 stream\uff09 <code>aconvert_reasoning_content_for_chunk_iterator</code> <code>convert_reasoning_content_for_chunk_iterator</code> \u7684\u5f02\u6b65\u7248\u672c\uff0c\u7528\u4e8e\u5f02\u6b65\u6d41\u5f0f\u5904\u7406\uff08\u7528\u4e8e\u6a21\u578b\u7684 astream\uff09"},{"location":"zh/getting-started-guide/message/#_5","title":"\u4ee3\u7801\u793a\u4f8b","text":"<pre><code>from langchain_dev_utils.message_convert import (\n    convert_reasoning_content_for_ai_message,\n    convert_reasoning_content_for_chunk_iterator,\n)\n\nresponse = model.invoke(\"\u4f60\u597d\")\nconverted_response = convert_reasoning_content_for_ai_message(\n    response, think_tag=(\"&lt;start&gt;\", \"&lt;end&gt;\")\n)\nprint(converted_response.content)\n\nfor chunk in convert_reasoning_content_for_chunk_iterator(\n    model.stream(\"\u4f60\u597d\"), think_tag=(\"&lt;start&gt;\", \"&lt;end&gt;\")\n):\n    print(chunk.content, end=\"\", flush=True)\n</code></pre>"},{"location":"zh/getting-started-guide/message/#chunks","title":"\u5408\u5e76\u6d41\u5f0f\u8f93\u51fa\u7684 Chunks","text":"<p>\u63d0\u4f9b\u5c06\u591a\u4e2a\u56e0\u4e3a\u6d41\u5f0f\u8f93\u51fa\u800c\u4ea7\u751f\u7684 AIMessageChunk \u5408\u5e76\u4e3a\u5355\u4e2a AIMessage \u7684\u5de5\u5177\u51fd\u6570\u3002</p>"},{"location":"zh/getting-started-guide/message/#_6","title":"\u6838\u5fc3\u51fd\u6570","text":"\u51fd\u6570 \u8bf4\u660e <code>merge_ai_message_chunk</code> \u5408\u5e76 AI \u6d88\u606f\u5757"},{"location":"zh/getting-started-guide/message/#_7","title":"\u4ee3\u7801\u793a\u4f8b","text":"<pre><code>from langchain_dev_utils.message_convert import merge_ai_message_chunk\n\nchunks = []\nfor chunk in model.stream(\"\u4f60\u597d\"):\n    chunks.append(chunk)\n\nmerged_message = merge_ai_message_chunk(chunks)\nprint(merged_message)\n</code></pre>"},{"location":"zh/getting-started-guide/tool/","title":"\u5de5\u5177\u8c03\u7528\u5904\u7406","text":""},{"location":"zh/getting-started-guide/tool/#_2","title":"\u6982\u8ff0","text":"<p>\u63d0\u4f9b\u68c0\u6d4b\u4ee5\u53ca\u89e3\u6790\u5de5\u5177\u8c03\u7528\u53c2\u6570\u7684\u5b9e\u7528\u5de5\u5177\u3002</p>"},{"location":"zh/getting-started-guide/tool/#_3","title":"\u68c0\u6d4b\u5de5\u5177\u8c03\u7528","text":"<p>\u68c0\u6d4b\u6d88\u606f\u662f\u5426\u5305\u542b\u5de5\u5177\u8c03\u7528\uff0c\u6838\u5fc3\u51fd\u6570\u662f <code>has_tool_calling</code>\u3002</p>"},{"location":"zh/getting-started-guide/tool/#_4","title":"\u4ee3\u7801\u793a\u4f8b","text":"<pre><code>import datetime\nfrom langchain_core.tools import tool\nfrom langchain_dev_utils.tool_calling import has_tool_calling\n\n@tool\ndef get_current_time() -&gt; str:\n    \"\"\"\u83b7\u53d6\u5f53\u524d\u65f6\u95f4\u6233\"\"\"\n    return str(datetime.datetime.now().timestamp())\n\nresponse = model.bind_tools([get_current_time]).invoke(\"\u73b0\u5728\u51e0\u70b9\u4e86\uff1f\")\nprint(has_tool_calling(response))\n</code></pre>"},{"location":"zh/getting-started-guide/tool/#_5","title":"\u89e3\u6790\u5de5\u5177\u8c03\u7528\u53c2\u6570","text":"<p>\u63d0\u4f9b\u4e00\u4e2a\u5b9e\u7528\u51fd\u6570\u6765\u89e3\u6790\u5de5\u5177\u8c03\u7528\u53c2\u6570\uff0c\u4ece\u6d88\u606f\u4e2d\u63d0\u53d6\u53c2\u6570\u4fe1\u606f\uff0c\u6838\u5fc3\u51fd\u6570\u662f <code>parse_tool_calling</code>\u3002</p>"},{"location":"zh/getting-started-guide/tool/#_6","title":"\u4ee3\u7801\u793a\u4f8b","text":"<pre><code>import datetime\nfrom langchain_core.tools import tool\nfrom langchain_dev_utils.tool_calling import has_tool_calling, parse_tool_calling\n\n@tool\ndef get_current_time() -&gt; str:\n    \"\"\"\u83b7\u53d6\u5f53\u524d\u65f6\u95f4\u6233\"\"\"\n    return str(datetime.datetime.now().timestamp())\n\nresponse = model.bind_tools([get_current_time]).invoke(\"\u73b0\u5728\u51e0\u70b9\u4e86\uff1f\")\n\nif has_tool_calling(response):\n    name, args = parse_tool_calling(\n        response, first_tool_call_only=True\n    )\n    print(name, args)\n</code></pre>"}]}