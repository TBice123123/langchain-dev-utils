{"config":{"lang":["en","zh"],"separator":"[\\s\\u200b\\u3000\\-\u3001\u3002\uff0c\uff0e\uff1f\uff01\uff1b]+","pipeline":["stemmer"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"\ud83e\udd9c\ufe0f\ud83e\uddf0 langchain-dev-utils","text":"<p> \ud83d\ude80 An efficient toolkit designed specifically for LangChain and LangGraph developers </p> <p> </p>"},{"location":"#why-choose-langchain-dev-utils","title":"Why choose langchain-dev-utils?","text":"<p>Tired of writing repetitive code in LangChain development? <code>langchain-dev-utils</code> is exactly the solution you need! This lightweight yet powerful toolkit is designed to enhance the development experience of LangChain and LangGraph, helping you:</p> <ul> <li>Boost development efficiency - Reduce boilerplate code, allowing you to focus on core functionality</li> <li>Simplify complex workflows - Easily manage multi-model, multi-tool, and multi-agent applications</li> <li>Enhance code quality - Improve consistency and readability, reduce maintenance costs</li> <li>Accelerate prototype development - Quickly implement ideas, iterate and validate faster</li> </ul>"},{"location":"#core-features","title":"Core Features","text":"<ul> <li> <p> Unified Model Management</p> <p>Specify model providers through strings, easily switch and combine different models.</p> </li> <li> <p> Built-in OpenAI-Compatible Integration Class</p> <p>Built-in OpenAI-Compatible API integration class, improving model compatibility through explicit configuration.</p> </li> <li> <p> Flexible Message Processing</p> <p>Supports chain-of-thought concatenation, streaming processing, and message formatting</p> </li> <li> <p> Powerful Tool Calling</p> <p>Built-in tool calling detection, parameter parsing, and human review functions</p> </li> <li> <p> Efficient Agent Development</p> <p>Simplifies the agent creation process and expands more common middleware</p> </li> <li> <p> Convenient State Graph Building</p> <p>Provides pre-built two functions for easily constructing sequential and parallel state graphs.</p> </li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>1. Install <code>langchain-dev-utils</code></p> <pre><code>pip install -U \"langchain-dev-utils[standard]\"\n</code></pre> <p>2. Get Started</p> <pre><code>from langchain.tools import tool\nfrom langchain_core.messages import HumanMessage\nfrom langchain_dev_utils.chat_models import register_model_provider, load_chat_model\nfrom langchain_dev_utils.agents import create_agent\n\n# Register model provider\nregister_model_provider(\"vllm\", \"openai-compatible\", base_url=\"http://localhost:8000/v1\")\n\n@tool\ndef get_current_weather(location: str) -&gt; str:\n    \"\"\"Get the current weather for the specified location\"\"\"\n    return f\"25 degrees, {location}\"\n\n# Dynamically load model using string\nmodel = load_chat_model(\"vllm:qwen3-4b\")\nresponse = model.invoke(\"Hello\")\nprint(response)\n\n# Create agent\nagent = create_agent(\"vllm:qwen3-4b\", tools=[get_current_weather])\nresponse = agent.invoke({\"messages\": [HumanMessage(content=\"What's the weather like in New York today?\")]})\nprint(response)\n</code></pre>"},{"location":"#github-repository","title":"GitHub Repository","text":"<p>Visit the GitHub Repository to view source code and issues.</p>"},{"location":"adavance-guide/graph/","title":"Predefined StateGraph Builder Functions","text":""},{"location":"adavance-guide/graph/#overview","title":"Overview","text":"<p>LangGraph is an official orchestration framework by LangChain for building complex workflows. However, in real-world business scenarios, using LangGraph directly often requires writing a significant amount of boilerplate code (node naming, edge connection, graph compilation, etc.).</p> <p>To reduce the barrier to entry, this library provides two predefined functions for quickly constructing state graphs for sequential or parallel execution. Developers only need to focus on implementing business nodes, and the function handles the orchestration automatically.</p> <p>The two functions are:</p> Function Name Description Use Case create_sequential_graph Combines multiple nodes in sequence to form a sequential execution state graph Tasks that must be executed step-by-step and depend on the output of the previous step create_parallel_graph Combines multiple nodes in parallel to form a parallel execution state graph Multiple tasks are independent of each other and can be executed simultaneously to improve efficiency"},{"location":"adavance-guide/graph/#sequential-workflow","title":"Sequential Workflow","text":"<p>Sequential workflows are suitable for scenarios where \"tasks must be executed in a specific order, and the subsequent step depends on the output of the previous one.\" In LangGraph, each step typically corresponds to a state graph node.</p> <p>You can use <code>create_sequential_graph</code> to combine multiple nodes into a state graph in a fixed order.</p>"},{"location":"adavance-guide/graph/#typical-scenario","title":"Typical Scenario","text":"<p>Taking user product purchase as an example, the typical workflow is as follows:</p> <pre><code>graph LR\n    Start([User Order Request])\n    Inv[Inventory Check]\n    Ord[Create Order]\n    Pay[Process Payment]\n    Del[Confirm Shipment]\n    End([Order Complete])\n\n    Start --&gt; Inv --&gt; Ord --&gt; Pay --&gt; Del --&gt; End</code></pre> <p>This workflow is tightly linked, and the order cannot be reversed.</p> <p>These four stages (Inventory Check, Create Order, Process Payment, Confirm Shipment) can be abstracted as independent nodes, each executed by a dedicated agent. Using <code>create_sequential_graph</code>, you can connect these four nodes in sequence to form a highly automated product purchase workflow with clear responsibilities.</p> <p>The following example shows how to use <code>create_sequential_graph</code> to build a sequential product purchase workflow.</p> <p>First, create the chat model object. Here, we use the locally deployed <code>qwen3-4b</code> via vLLM as an example. Since its interface is compatible with OpenAI, we can directly use <code>create_openai_compatible_model</code> to construct the model class.</p> <p><pre><code>from langchain_dev_utils.chat_models.adapters import create_openai_compatible_model\n\nChatVLLM = create_openai_compatible_model(\n    model_provider=\"vllm\",\n    base_url=\"http://localhost:8000/v1\",\n    chat_model_cls_name=\"ChatVLLM\",\n)\n</code></pre> Then instantiate a <code>ChatVLLM</code> object for subsequent agent calls.</p> <p><pre><code>model = ChatVLLM(model=\"qwen3-4b\")\n</code></pre> Next, create relevant tools, such as checking inventory, creating orders, processing payments, etc.</p> Reference implementation for tools <pre><code>from langchain_core.tools import tool\n\n@tool\ndef check_inventory(product_name: str) -&gt; dict:\n    \"\"\"Check inventory\"\"\"\n    return {\"product_name\": product_name, \"in_stock\": True, \"available\": 42}\n\n@tool\ndef create_order(product_name: str, quantity: int) -&gt; str:\n    \"\"\"Create order\"\"\"\n    return f\"Order ORD-10001 created, product: {product_name}, quantity: {quantity}.\"\n\n@tool\ndef pay_order(order_id: str) -&gt; str:\n    \"\"\"Pay order\"\"\"\n    return f\"Order {order_id} payment successful.\"\n\n@tool\ndef confirm_delivery(order_id: str, address: str) -&gt; str:\n    \"\"\"Confirm shipment\"\"\"\n    return f\"Order {order_id} arranged for shipment, address: {address}.\"\n</code></pre> <p>Then create the corresponding four sub-agents and node functions that invoke these agents.</p> <pre><code>from langchain.agents import create_agent\n\ninventory_agent = create_agent(\n    model=model,\n    tools=[check_inventory],\n    system_prompt=\"You are an inventory assistant responsible for confirming if a product is in stock. Finally, please output the inventory check result.\",\n    name=\"inventory_agent\",\n)\n\norder_agent = create_agent(\n    model=model,\n    tools=[create_order],\n    system_prompt=\"You are an ordering assistant responsible for creating orders.\",\n    name=\"order_agent\"\n)\n\npayment_agent = create_agent(\n    model=model,\n    tools=[pay_order],\n    system_prompt=\"You are a payment assistant responsible for completing payments.\",\n    name=\"payment_agent\"\n)\n\ndelivery_agent = create_agent(\n    model=model,\n    tools=[confirm_delivery],\n    system_prompt=(\n        \"You are a shipping assistant responsible for confirming shipment info and arranging delivery.\"\n    ),\n    name=\"delivery_agent\",\n    state_schema=AgentState\n)\n\ndef inventory(state: AgentState):\n    response = inventory_agent.invoke({\"messages\": state[\"messages\"]})\n    return {\"messages\": [AIMessage(content=response[\"messages\"][-1].content)]}\n\ndef order(state: AgentState):\n    response = order_agent.invoke({\"messages\": state[\"messages\"]})\n    return {\"messages\": [AIMessage(content=response[\"messages\"][-1].content)]}\n\ndef payment(state: AgentState):\n    response = payment_agent.invoke({\"messages\": state[\"messages\"]})\n    return {\"messages\": [AIMessage(content=response[\"messages\"][-1].content)]}\n\ndef delivery(state: AgentState):\n    response = delivery_agent.invoke({\"messages\": state[\"messages\"]})\n    return {\"messages\": [AIMessage(content=response[\"messages\"][-1].content)]}\n</code></pre> <p>Note</p> <p>Although LangGraph allows adding agents (sub-graphs) directly as nodes to the graph, this causes the current agent's context to include the entire runtime context of previous agents, which violates best practices for context engineering management. Therefore, it is recommended to encapsulate agents within nodes and only output the final result.</p> <p>Finally, use <code>create_sequential_graph</code> to connect these four nodes in sequence into a state graph.</p> <p><pre><code>from langchain_dev_utils.graph import create_sequential_graph\n\ngraph = create_sequential_graph(\n    nodes=[\n        inventory,\n        order,\n        payment,\n        delivery,\n    ],\n    state_schema=AgentState\n)\n</code></pre> Running example:</p> <pre><code>response = graph.invoke(\n    {\n        \"messages\": [\n            HumanMessage(\"I want to buy a pair of wireless headphones, quantity 2, please place the order, shipping address is No. X, X Road, X District, X City\")\n        ]\n    }\n)\nprint(response)\n</code></pre>"},{"location":"adavance-guide/graph/#parallel-workflow","title":"Parallel Workflow","text":"<p>Parallel workflows are suitable for scenarios where \"multiple tasks are independent of each other and can be executed simultaneously,\" improving overall throughput or reducing end-to-end latency through concurrent execution.</p> <p>You can use <code>create_parallel_graph</code> to combine multiple nodes into a state graph in a parallel manner.</p>"},{"location":"adavance-guide/graph/#typical-scenario_1","title":"Typical Scenario","text":"<p>In a product purchase scenario, users might need multiple queries simultaneously, such as product details, inventory, promotions, and shipping estimation, which can be executed in parallel.</p> <p>The process is as follows:</p> <pre><code>graph LR\n    Start([User Request])\n\n    subgraph Parallel [Parallel Execution]\n        direction TB\n        Prod[Product Details Query]\n        Inv[Inventory Query]\n        Prom[Promotion Calculation]\n        Ship[Shipping Estimation]\n    end\n\n    End([Aggregate Results])\n\n    Start --&gt; Prod\n    Start --&gt; Inv\n    Start --&gt; Prom\n    Start --&gt; Ship\n\n    Prod --&gt; End\n    Inv --&gt; End\n    Prom --&gt; End\n    Ship --&gt; End</code></pre> <p>Next, we create a parallel workflow to implement the above process.</p> <p>First, create a few tools.</p> Reference implementation for tools <pre><code>@tool\ndef get_product_detail(product_name: str) -&gt; dict:\n    \"\"\"Query product details\"\"\"\n    return {\n        \"product_name\": product_name,\n        \"sku\": \"SKU-10001\",\n        \"price\": 299,\n        \"highlights\": [\"Active Noise Cancellation\", \"Bluetooth 5.3\", \"30-hour battery life\"],\n    }\n\n@tool\ndef check_inventory(product_name: str) -&gt; dict:\n    \"\"\"Check inventory\"\"\"\n    return {\"product_name\": product_name, \"in_stock\": True, \"available\": 42}\n\n@tool\ndef calculate_promotions(product_name: str, quantity: int) -&gt; dict:\n    \"\"\"Calculate promotions\"\"\"\n    return {\n        \"product_name\": product_name,\n        \"quantity\": quantity,\n        \"discounts\": [\"30 off 300\", \"Member 5% off\"],\n        \"estimated_discount\": 45,\n    }\n\n@tool\ndef estimate_shipping(address: str) -&gt; dict:\n    \"\"\"Estimate shipping fee and time\"\"\"\n    return {\n        \"address\": address,\n        \"fee\": 12,\n        \"eta_days\": 2,\n    }\n</code></pre> <p>And the corresponding sub-agents:</p> <pre><code>product_agent = create_agent(\n    model,\n    tools=[get_product_detail],\n    system_prompt=\"You are a product assistant responsible for parsing user needs and querying product details.\",\n    name=\"product_agent\",\n    state_schema=AgentState,\n)\n\ninventory_agent = create_agent(\n    model,\n    tools=[check_inventory],\n    system_prompt=\"You are an inventory assistant responsible for checking inventory based on SKU.\",\n    name=\"inventory_agent\",\n    state_schema=AgentState,\n)\n\npromotion_agent = create_agent(\n    model,\n    tools=[calculate_promotions],\n    system_prompt=\"You are a promotion assistant responsible for calculating current available promotions and estimated discounts.\",\n    name=\"promotion_agent\",\n    state_schema=AgentState,\n)\n\nshipping_agent = create_agent(\n    model,\n    tools=[estimate_shipping],\n    system_prompt=\"You are a shipping assistant responsible for estimating shipping fees and delivery times.\",\n    name=\"shipping_agent\",\n    state_schema=AgentState,\n)\n\ndef product(state: AgentState):\n    response = product_agent.invoke({\"messages\": state[\"messages\"]})\n    return {\"messages\": [AIMessage(content=response[\"messages\"][-1].content)]}\n\ndef inventory(state: AgentState):\n    response = inventory_agent.invoke({\"messages\": state[\"messages\"]})\n    return {\"messages\": [AIMessage(content=response[\"messages\"][-1].content)]}\n\ndef promotion(state: AgentState):\n    response = promotion_agent.invoke({\"messages\": state[\"messages\"]})\n    return {\"messages\": [AIMessage(content=response[\"messages\"][-1].content)]}\n\ndef shipping(state: AgentState):\n    response = shipping_agent.invoke({\"messages\": state[\"messages\"]})\n    return {\"messages\": [AIMessage(content=response[\"messages\"][-1].content)]}\n</code></pre> <p>Use <code>create_parallel_graph</code> to complete the orchestration of the parallel state graph.</p> <p><pre><code>from langchain_dev_utils.graph import create_parallel_graph\n\ngraph = create_parallel_graph(\n    nodes=[\n       product,\n       inventory,\n       promotion,\n       shipping,\n    ],\n    state_schema=AgentState,\n    graph_name=\"parallel_graph\",\n)\n</code></pre> Running example:</p> <pre><code>response = graph.invoke(\n    {\"messages\": [HumanMessage(\"I want to buy a pair of wireless headphones, quantity 2, shipping address No. X, X Road, X District, X City\")]}\n)\nprint(response)\n</code></pre>"},{"location":"adavance-guide/graph/#dynamic-parallelism-on-demand-parallelism","title":"Dynamic Parallelism (On-demand Parallelism)","text":"<p>In some cases, you may not want all nodes to participate in parallel execution, but rather \"selectively run a subset of nodes in parallel based on conditions.\" You can achieve this by specifying a <code>branches_fn</code> (branch function).</p> <p>The branch function needs to return a list of <code>Send</code> objects, where each <code>Send</code> contains the target node name and the input for that node.</p>"},{"location":"adavance-guide/graph/#router-multi-agent-architecture","title":"Router Multi-Agent Architecture","text":"<p>The on-demand parallelism feature can be used to implement the core part of a Router multi-agent architecture.</p> <p>In multi-agent systems, the <code>Router</code> architecture achieves efficient parallel processing by decomposing complex tasks and distributing them to specialized sub-agents. The architecture consists of three core steps:</p> <ol> <li> <p>Intent Recognition: A router model analyzes the user request, decomposes the task, and determines which agents to invoke.</p> </li> <li> <p>Parallel Execution: Multiple business agents process sub-tasks simultaneously.</p> </li> <li> <p>Result Synthesis: The responses from sub-agents are integrated into a final answer.</p> </li> </ol> <p>In an order inquiry scenario, users might care about order status, product information, or refund policies simultaneously. The system can invoke the order, product, and refund agents in parallel, then provide a unified reply.</p> <pre><code>graph LR\n    Start([User Request]) --&gt; Router([Intent Classifier])\n    Router --&gt;|Order Query| OrderAgent\n    Router --&gt;|Product Query| ProductAgent\n    Router --&gt;|Refund Query| RefundAgent\n    OrderAgent --&gt; Synthesize\n    ProductAgent --&gt; Synthesize\n    RefundAgent --&gt; Synthesize\n    Synthesize --&gt; End([Synthesized Reply])</code></pre> <p>1. Environment Setup and Tool Definition</p> <p>First, define the tools needed by the business agents.</p> Click to expand tool implementation code <pre><code>from langchain_core.tools import tool\n\n@tool\ndef list_orders() -&gt; dict:\n    \"\"\"Query user order list\"\"\"\n    return {\n        \"orders\": [\n            {\n                \"order_id\": \"ORD-20250101-0001\",\n                \"status\": \"Shipped\",\n                \"items\": [{\"product_name\": \"Wireless Headphones\", \"qty\": 1}],\n                \"created_at\": \"2025-01-01 10:02:11\",\n            },\n            {\n                \"order_id\": \"ORD-20241215-0234\",\n                \"status\": \"Completed\",\n                \"items\": [{\"product_name\": \"Mechanical Keyboard\", \"qty\": 1}],\n                \"created_at\": \"2024-12-15 21:18:03\",\n            },\n        ],\n    }\n\n@tool\ndef get_order_detail(order_id: str) -&gt; dict:\n    \"\"\"Query order details\"\"\"\n    return {\n        \"status\": \"Shipped\",\n        \"receiver\": {\"name\": \"Zhang San\", \"phone\": \"138****0000\"},\n        \"items\": [\n            {\n                \"product_id\": \"P-10001\",\n                \"product_name\": \"Wireless Headphones\",\n                \"qty\": 1,\n                \"price\": 299,\n            }\n        ],\n    }\n\n@tool\ndef get_shipping_trace(tracking_no: str) -&gt; dict:\n    \"\"\"Query shipping trajectory\"\"\"\n    return {\n        \"events\": [\n            {\"time\": \"2025-01-02 09:10\", \"status\": \"Package Picked Up\"},\n            {\"time\": \"2025-01-02 18:45\", \"status\": \"In Transit\"},\n            {\"time\": \"2025-01-03 11:20\", \"status\": \"Arrived at Delivery Station\"},\n        ],\n    }\n\n@tool\ndef search_products(query: str) -&gt; dict:\n    \"\"\"Search products\"\"\"\n    return {\n        \"results\": [\n            {\n                \"product_id\": \"P-10001\",\n                \"name\": \"Wireless Headphones Pro\",\n                \"price\": 299,\n                \"highlights\": [\"ANC\", \"Bluetooth 5.3\", \"30h Battery\"],\n            },\n            {\n                \"product_id\": \"P-10002\",\n                \"name\": \"Wireless Headphones Lite\",\n                \"price\": 199,\n                \"highlights\": [\"Lightweight\", \"Low Latency\", \"24h Battery\"],\n            },\n        ],\n    }\n\n@tool\ndef get_product_detail(product_id: str) -&gt; dict:\n    \"\"\"Query product details\"\"\"\n    return {\n        \"product_id\": product_id,\n        \"name\": \"Wireless Headphones Pro\",\n        \"price\": 299,\n        \"specs\": {\"color\": [\"Black\", \"White\"], \"warranty_months\": 12},\n        \"description\": \"True wireless headphones featuring noise cancellation and long battery life.\",\n    }\n\n@tool\ndef check_inventory(product_name: str) -&gt; dict:\n    \"\"\"Check inventory\"\"\"\n    return {\"product_name\": product_name, \"in_stock\": True, \"available\": 42}\n\n@tool\ndef create_refund(order_id: str, reason: str) -&gt; dict:\n    \"\"\"Initiate refund\"\"\"\n    return {\n        \"refund_id\": \"RFD-20250103-0009\",\n        \"status\": \"Submitted\",\n        \"reason\": reason,\n        \"estimated_days\": 3,\n    }\n\n@tool\ndef get_refund_status(refund_id: str) -&gt; dict:\n    \"\"\"Query refund status\"\"\"\n    return {\n        \"refund_id\": refund_id,\n        \"status\": \"Processing\",\n        \"progress\": [\n            {\"time\": \"2025-01-03 12:05\", \"status\": \"Submitted\"},\n            {\"time\": \"2025-01-03 12:20\", \"status\": \"CS Reviewing\"},\n        ],\n        \"estimated_days\": 2,\n    }\n\n@tool\ndef refund_policy() -&gt; dict:\n    \"\"\"View refund policy\"\"\"\n    return {\n        \"window_days\": 7,\n        \"requirements\": [\"Product intact\", \"All accessories included\", \"Provide order number\"],\n        \"notes\": [\"Some promotional items do not support no-reason returns\", \"Arrival time depends on payment channel\"],\n    }\n</code></pre> <p>2. Define State Schema</p> <p><code>RouterState</code> represents the state schema of the final graph, while <code>AgentInput</code> and <code>AgentOutput</code> define the input and output states of the sub-agent nodes respectively.</p> <pre><code>import operator\nfrom typing import Annotated, Literal\n\nfrom typing_extensions import TypedDict\n\n\nclass AgentInput(TypedDict):\n    \"\"\"Sub-agent input structure\"\"\"\n    query: str\n\n\nclass AgentOutput(TypedDict):\n    \"\"\"Sub-agent output structure\"\"\"\n    source: str\n    result: str\n\n\nclass Classification(TypedDict):\n    \"\"\"Routing classification result\"\"\"\n    source: Literal[\"order\", \"refund\", \"product\"]\n    query: str\n\n\nclass RouterState(TypedDict):\n    \"\"\"Global state schema\"\"\"\n    query: str\n    classifications: list[Classification]\n    results: Annotated[list[AgentOutput], operator.add] \n    final_answer: str\n</code></pre> <p>3. Create Sub-Agents</p> <p>Use LangChain's <code>create_agent</code> to quickly build three business agents, binding them with corresponding tools and prompts.</p> <pre><code>from langchain.agents import create_agent\nfrom langchain_core.messages import HumanMessage\n\nORDER_AGENT_PROMPT = (\n    \"You are an order management assistant.\\n\"\n    \"You can use tools to check order lists, order details, and shipping trajectories.\\n\"\n    \"Prioritize using tools to get information, then provide conclusions based on tool results.\\n\"\n    \"Output requirements: Answer in Chinese, clear structure, list order information with bullet points if necessary.\\n\"\n)\n\norder_agent = create_agent(\n    model,\n    system_prompt=ORDER_AGENT_PROMPT,\n    tools=[list_orders, get_order_detail, get_shipping_trace],\n    name=\"order_agent\",\n)\n\nPRODUCT_AGENT_PROMPT = (\n    \"You are a product management assistant.\\n\"\n    \"You can use tools to search products, view product details, and check inventory.\\n\"\n    \"Prioritize using tools to get information, then give suggestions based on tool results.\\n\"\n    \"When user needs are unclear, ask a clarification question first (e.g., category/budget/usage).\\n\"\n    \"Output requirements: Answer in Chinese, give actionable next steps.\\n\"\n)\n\nproduct_agent = create_agent(\n    model,\n    system_prompt=PRODUCT_AGENT_PROMPT,\n    tools=[search_products, get_product_detail, check_inventory],\n    name=\"product_agent\",\n)\n\nREFUND_AGENT_PROMPT = (\n    \"You are a refund management assistant.\\n\"\n    \"You can use tools to initiate refunds, check refund status, and view refund policies.\\n\"\n    \"Prioritize using tools to get information; if the user is missing key fields (e.g., order number), ask for them first.\\n\"\n    \"Output requirements: Answer in Chinese, clearly state refund progress/required materials/estimated time.\\n\"\n)\n\nrefund_agent = create_agent(\n    model,\n    system_prompt=REFUND_AGENT_PROMPT,\n    tools=[create_refund, get_refund_status, refund_policy],\n    name=\"refund_agent\",\n)\n</code></pre> <p>4. Encapsulate Agent Invocation Logic</p> <p>Encapsulate the agent invocation logic into node functions. Each function is responsible for invoking a specific agent and formatting the result into the <code>results</code> field.</p> <pre><code>def order(state: AgentInput):\n    response = order_agent.invoke({\"messages\": [HumanMessage(content=state[\"query\"])]})\n    return {\n        \"results\": [{\"source\": \"order\", \"result\": response[\"messages\"][-1].content}]\n    }\n\ndef product(state: AgentInput):     \n    response = product_agent.invoke(\n        {\"messages\": [HumanMessage(content=state[\"query\"])]}\n    )\n    return {\n        \"results\": [{\"source\": \"product\", \"result\": response[\"messages\"][-1].content}]\n    }\n\ndef refund(state: AgentInput):\n    response = refund_agent.invoke({\"messages\": [HumanMessage(content=state[\"query\"])]})\n    return {\n        \"results\": [{\"source\": \"refund\", \"result\": response[\"messages\"][-1].content}]\n    }\n</code></pre> <p>5. Implement Routing and Branching Logic</p> <p>Here, we use the on-demand parallelism feature. We define two nodes: 1.  <code>classify_query</code>: Uses an LLM to perform intent recognition, outputting the list of agents to invoke and the task content. 2.  <code>route_to_agents</code>: Generates a list of <code>Send</code> objects based on the classification results, deciding which nodes to execute in parallel.</p> <pre><code>from typing import cast\nfrom langchain_core.messages import SystemMessage\nfrom pydantic import BaseModel, Field\nfrom langgraph.constants import Send\n\nclass ClassificationResult(BaseModel):\n    classifications: list[Classification] = Field(\n        description=\"List of agents to invoke and their corresponding sub-questions\"\n    )\n\nROUTER_SYSTEM_PROMPT = (\n    \"You are a Router model, only responsible for splitting user questions and distributing them to appropriate business sub-agents.\\n\"\n    \"Available business domains are only: order (orders), product (products), refund (refunds).\\n\"\n    \"You must output a classifications list (used to invoke multiple sub-agents in parallel).\\n\"\n    \"Rules:\\n\"\n    \"1) source must be one of the three above;\\n\"\n    \"2) query must be a directly executable task description sent to that sub-agent;\\n\"\n    \"3) If a user sentence involves multiple business domains simultaneously (e.g., 'check order' + 'view product' + 'ask refund'), it must be split into multiple classifications for parallel execution;\\n\"\n    \"4) If unsure, prioritize product, and pass the question to it as is.\\n\"\n    \"Example A: User: 'Check shipping for ORD-1 and see if these headphones are in stock' -&gt; Return 2 items: order(check shipping) + product(check inventory).\\n\"\n    \"Example B: User: 'I want to return ORD-1, how long for refund' -&gt; Return 1 item: refund(initiate/check refund).\\n\"\n    \"Example C: User: 'I want to know the specs of this headphone' -&gt; Return 1 item: product(query details).\\n\"\n)\n\ndef classify_query(state: RouterState):\n    structured_llm = model.with_structured_output(ClassificationResult)\n\n    classify_result = cast(\n        ClassificationResult,\n        structured_llm.invoke(\n            [\n                SystemMessage(ROUTER_SYSTEM_PROMPT),\n                HumanMessage(state[\"query\"]),\n            ]\n        ),\n    )\n\n    return {\"classifications\": classify_result.classifications}\n\ndef route_to_agents(state: RouterState) -&gt; list[Send]:\n    \"\"\"Generate instructions for parallel execution based on classification results\"\"\"\n    return [Send(c[\"source\"], {\"query\": c[\"query\"]}) for c in state[\"classifications\"]]\n</code></pre> <p>6. Orchestrate Parallel Graph and Aggregation</p> <p>Use <code>create_parallel_graph</code> to create a parallel sub-graph. Here, the <code>branches_fn</code> parameter is passed to implement dynamic parallel execution based on conditions.</p> <pre><code>from langchain_dev_utils.graph import create_parallel_graph\n\nrouter_graph = create_parallel_graph(\n    nodes=[\n        order,\n        product,\n        refund,\n    ],\n    state_schema=RouterState,\n    branches_fn=route_to_agents, # Core logic: function determines which branches to run\n)\n</code></pre> <p>Next, write the aggregation node <code>synthesize_results</code> to integrate the results of parallel execution into a coherent answer.</p> <pre><code>SYNTHESIS_SYSTEM_PROMPT = (\n    \"Synthesize these results to answer the original question: {query}\\n\"\n    \"- Merge information from multiple sources, avoid redundancy\\n\"\n    \"- Highlight the most relevant and actionable information\\n\"\n    \"- Note any discrepancies between sources\\n\"\n    \"- Keep the answer concise and organized\\n\"\n)\n\ndef synthesize_results(state: RouterState) -&gt; dict:\n    if not state[\"results\"]:\n        return {\"final_answer\": \"No results found from any knowledge source.\"}\n\n    # Format outputs from each sub-agent\n    formatted = [\n        f\"**From {r['source'].title()}:**\\n{r['result']}\" for r in state[\"results\"]\n    ]\n\n    synthesis_response = model.invoke(\n        [\n            {\n                \"role\": \"system\",\n                \"content\": SYNTHESIS_SYSTEM_PROMPT.format(query=state[\"query\"]),\n            },\n            {\"role\": \"user\", \"content\": \"\\n\\n\".join(formatted)},\n        ]\n    )\n\n    return {\"final_answer\": synthesis_response.content}\n</code></pre> <p>7. Build the Final StateGraph</p> <p>Finally, use <code>create_sequential_graph</code> to connect \"Intent Classification -&gt; On-demand Parallel -&gt; Result Synthesis\" into a complete application flow.</p> <pre><code>from langchain_dev_utils.graph import create_sequential_graph\n\ngraph = create_sequential_graph(\n    nodes=[\n        classify_query,\n        router_graph,\n        synthesize_results,\n    ],\n    state_schema=RouterState,\n)\n</code></pre> <p>8. Running Examples</p> <pre><code># Example 1: Single intent (Product query)\nresponse = graph.invoke({\"query\": \"Hello, I want to check the product I purchased before\"})\nprint(response[\"final_answer\"])\n\n# Example 2: Mixed intent (Product query + Refund policy), will trigger parallel execution\nresponse = graph.invoke({\"query\": \"Recommend a wireless headset suitable for commuting and check stock; also, tell me your product refund policy?\"})\nprint(response[\"final_answer\"])\n</code></pre>"},{"location":"adavance-guide/multi-agent/","title":"Sub-Agent Tools (Agent as Tool)","text":""},{"location":"adavance-guide/multi-agent/#overview","title":"Overview","text":"<p>When building complex AI applications, multi-agent collaboration is a powerful architectural pattern. By assigning different responsibilities to specialized agents, you can achieve professional division of labor and efficient collaboration.</p> <p>There are multiple ways to implement multi-agent collaboration, and tool calling is a common and flexible approach. By encapsulating subagents as tools, a primary agent can dynamically delegate tasks to specialized subagents based on requirements.</p> <p>This library provides two pre-built functions to simplify this implementation:</p> Function Name Functional Description <code>wrap_agent_as_tool</code> Wraps a single agent instance into an independent tool <code>wrap_all_agents_as_tool</code> Wraps multiple agent instances into a unified tool, specifying which subagent to call via parameters"},{"location":"adavance-guide/multi-agent/#wrapping-a-single-agent-as-a-tool","title":"Wrapping a Single Agent as a Tool","text":"<p>Wrapping a single agent requires just three steps:</p> <ol> <li>Import <code>wrap_agent_as_tool</code></li> <li>Pass the agent instance as a parameter</li> <li>Obtain a tool object that can be directly called by other agents</li> </ol>"},{"location":"adavance-guide/multi-agent/#usage-example","title":"Usage Example","text":"<p>Below, we use a <code>supervisor</code> agent to demonstrate how to wrap subagents as tools using <code>wrap_agent_as_tool</code>.</p> <p>First, implement two subagents: one for sending emails and one for calendar queries and scheduling.</p> <p>First, the implementation of the email agent:</p> <pre><code>from langchain_core.tools import tool\nfrom langchain_dev_utils.chat_models import register_model_provider\nfrom langchain_dev_utils.agents import create_agent, wrap_agent_as_tool \n\nregister_model_provider(\n    \"vllm\",\n    \"openai-compatible\",\n    base_url=\"http://localhost:8000/v1\",\n)\n\n\n@tool\ndef send_email(\n    to: list[str],  # Email addresses\n    subject: str,\n    body: str,\n    cc: list[str] = [],\n) -&gt; str:\n    \"\"\"Sends an email via the Email API. Requires correctly formatted addresses.\"\"\"\n    # Stub: In a real application, this would call SendGrid, Gmail API, etc.\n    return f\"Email sent to {', '.join(to)} - Subject: {subject}\"\n\n\nEMAIL_AGENT_PROMPT = (\n    \"You are an email assistant.\"\n    \"Draft professional emails based on natural language requests.\"\n    \"Extract recipient information and create appropriate subject lines and body content.\"\n    \"Use send_email to send the email.\"\n    \"Always confirm what was sent in your final reply.\"\n)\n\nemail_agent = create_agent(\n    \"vllm:qwen3-4b\",\n    tools=[send_email],\n    system_prompt=EMAIL_AGENT_PROMPT,\n    name=\"email_agent\",\n)\n</code></pre> <p>And the implementation of the daily calendar agent:</p> <pre><code>@tool\ndef create_calendar_event(\n    title: str,\n    start_time: str,  # ISO format: \"2024-01-15T14:00:00\"\n    end_time: str,  # ISO format: \"2024-01-15T15:00:00\"\n    attendees: list[str],  # Email addresses\n    location: str = \"\",\n) -&gt; str:\n    \"\"\"Creates a calendar event. Requires precise ISO date-time format.\"\"\"\n    # Stub: In a real application, this would call Google Calendar API, Outlook API, etc.\n    return f\"Event created: {title} from {start_time} to {end_time} with {len(attendees)} participants\"\n\n\n@tool\ndef get_available_time_slots(\n    attendees: list[str],\n    date: str,  # ISO format: \"2024-01-15\"\n    duration_minutes: int,\n) -&gt; list[str]:\n    \"\"\"Queries calendar availability for attendees on a specific date.\"\"\"\n    # Stub: In a real application, this would query a calendar API\n    return [\"09:00\", \"14:00\", \"16:00\"]\n\n\nCALENDAR_AGENT_PROMPT = (\n    \"You are a calendar scheduling assistant.\"\n    \"Parse natural language scheduling requests (e.g., 'next Tuesday afternoon at 2pm') into the correct ISO date-time format.\"\n    \"Use get_available_time_slots to check availability when needed.\"\n    \"Use create_calendar_event to schedule the event.\"\n    \"Always confirm what was scheduled in your final reply.\"\n)\n\ncalendar_agent = create_agent(\n    \"vllm:qwen3-4b\",\n    tools=[create_calendar_event, get_available_time_slots],\n    system_prompt=CALENDAR_AGENT_PROMPT,\n    name=\"calendar_agent\",\n)\n</code></pre> <p>Next, use <code>wrap_agent_as_tool</code> to wrap these two subagents as tools.</p> <pre><code>schedule_event = wrap_agent_as_tool(\n    calendar_agent,\n    tool_name=\"schedule_event\",\n    tool_description=(\n        \"Schedule calendar events using natural language.\"\n        \"Use this when the user wants to create, modify, or check calendar appointments.\"\n        \"Capable of handling date/time parsing, checking available times, and creating events.\"\n        \"Input: Natural language calendar request (e.g., 'meeting with design team next Tuesday at 2pm')\"\n    ),\n)\nmanage_email = wrap_agent_as_tool(\n    email_agent,\n    tool_name=\"manage_email\",\n    tool_description=(\n        \"Send emails using natural language.\"\n        \"Use this when the user wants to send notifications, reminders, or any email communication.\"\n        \"Capable of extracting recipient information, subject generation, and email drafting.\"\n        \"Input: Natural language email request (e.g., 'send them a meeting reminder')\"\n    ),\n)\n</code></pre> <p>Note</p> <p>Both <code>tool_name</code> and <code>tool_description</code> for <code>wrap_agent_as_tool</code> are optional parameters. If omitted, the tool name defaults to <code>transfer_to_{agent_name}</code>, and the description defaults to <code>This tool transforms input to {agent_name}</code>. To allow the primary agent to identify and invoke the subagent more accurately, it is recommended to explicitly specify these two items (especially the description) to clearly convey the responsibilities and capabilities of the subagent.</p> <p>Finally, create a <code>supervisor_agent</code> that can call these two tools.</p> <pre><code>SUPERVISOR_PROMPT = (\n    \"You are a helpful personal assistant.\"\n    \"You can schedule calendar events and send emails.\"\n    \"Break down user requests into appropriate tool calls and coordinate the results.\"\n    \"When a request involves multiple operations, please use multiple tools sequentially.\"\n)\n\n\nsupervisor_agent = create_agent(\n    \"vllm:qwen3-4b\",\n    tools=[schedule_event, manage_email],\n    system_prompt=SUPERVISOR_PROMPT,\n)\n\nprint(\n    supervisor_agent.invoke({\"messages\": [HumanMessage(content=\"\u67e5\u8be2\u660e\u5929\u7684\u7a7a\u95f2\u65f6\u95f4\")]})\n)\nprint(\n    supervisor_agent.invoke(\n        {\"messages\": [HumanMessage(content=\"\u7ed9test@123.com\u53d1\u9001\u90ae\u4ef6\u4f1a\u8bae\u63d0\u9192\")]}\n    )\n)\n</code></pre> <p>Info</p> <p>In the example above, we imported <code>create_agent</code> from <code>langchain_dev_utils.agents</code> instead of <code>langchain.agents</code>. This is because this library provides a function with the exact same functionality as the official <code>create_agent</code>, but with the added feature of specifying the model via a string. This allows you to directly use models registered via <code>register_model_provider</code> without needing to initialize a model instance first.</p>"},{"location":"adavance-guide/multi-agent/#wrapping-multiple-agents-as-a-single-tool","title":"Wrapping Multiple Agents as a Single Tool","text":"<p>Wrapping multiple agents into a single tool requires just three steps:</p> <ol> <li>Import <code>wrap_all_agents_as_tool</code></li> <li>Pass multiple agent instances as a list at once</li> <li>Obtain a unified tool object that can be directly called by other agents</li> </ol>"},{"location":"adavance-guide/multi-agent/#usage-example_1","title":"Usage Example","text":"<p>For the <code>calendar_agent</code> and <code>email_agent</code> from the previous example, we can wrap them into a single tool <code>call_subagent</code>:</p> <pre><code>call_subagent_tool = wrap_all_agents_as_tool(\n    [calendar_agent, email_agent],\n    tool_name=\"call_subagent\",\n    tool_description=(\n        \"Call subagents to execute tasks.\"\n        \"Available agents are:\"\n        \"- calendar_agent: for scheduling calendar events\"\n        \"- email_agent: for sending emails\"\n    ),\n)\n\nMAIN_AGENT_PROMPT = (\n    \"You are a helpful personal assistant.\"\n    \"You can use the **call_subagent** tool to invoke subagents to perform tasks.\"\n    \"Break down user requests into appropriate tool calls and coordinate the results.\"\n    \"When a request involves multiple operations, please use multiple tools sequentially.\"\n)\n\nmain_agent = create_agent(\n    \"vllm:qwen3-4b\",\n    tools=[call_subagent_tool],\n    system_prompt=MAIN_AGENT_PROMPT,\n)\n</code></pre> <p>Note</p> <p>Both <code>tool_name</code> and <code>tool_description</code> for <code>wrap_all_agents_as_tool</code> are optional parameters. If omitted, the tool name defaults to <code>task</code>, and the description defaults to <code>Launch an ephemeral subagent for a task.\\nAvailable agents:\\n {available_agents}</code>. To ensure the primary agent accurately identifies and calls the subagent, it is recommended to explicitly fill in these two items, especially the description, to clearly convey the responsibilities and capabilities of each subagent.</p> <p>Info</p> <p>Besides using the <code>wrap_all_agents_as_tool</code> provided by this library to wrap multiple agents into a single tool, you can also use the <code>SubAgentMiddleware</code> provided by the <code>deepagents</code> library to achieve a similar effect.</p>"},{"location":"adavance-guide/multi-agent/#hook-function-mechanism","title":"Hook Function Mechanism","text":"<p>This library includes a flexible hook mechanism that allows you to insert custom logic before and after subagent execution. This mechanism applies to both <code>wrap_agent_as_tool</code> and <code>wrap_all_agents_as_tool</code>. The following explanation uses <code>wrap_agent_as_tool</code> as an example.</p> <p>The flow of hook function execution is shown in the diagram below:</p> <pre><code>graph LR\n    Start([Start]) --&gt; InHook{process_input exists?}\n\n    %% --- Input Hook ---\n    InHook -- Yes --&gt; DoInHook[Execute process_input]\n    InHook -- No --&gt; Raw[Use original request]\n\n    DoInHook --&gt; TypeCheck{Check Return Type}\n    Raw --&gt; TypeCheck\n\n    TypeCheck -- String --&gt; BuildDict[Construct dict containing messages]\n    TypeCheck -- Dict --&gt; UseDict[Use dict directly]\n\n    BuildDict --&gt; Core\n    UseDict --&gt; Core\n\n    %% --- Intermediate Execution ---\n    Core([Execute agent.invoke])\n\n    %% --- Output Hook ---\n    Core --&gt; OutHook{process_output exists?}\n\n    OutHook -- Yes --&gt; DoOutHook[Execute process_output]\n    OutHook -- No --&gt; ExtractContent[Extract text content from&lt;br/&gt;the last item in&lt;br/&gt; response messages]\n\n    DoOutHook --&gt; End([Return Result])\n    ExtractContent --&gt; End</code></pre>"},{"location":"adavance-guide/multi-agent/#1-pre_input_hooks","title":"1. pre_input_hooks","text":"<p>Preprocess the input before the agent runs. Useful for input enhancement, context injection, format validation, permission checks, etc.</p>"},{"location":"adavance-guide/multi-agent/#supported-input-types","title":"Supported Input Types","text":"Type Description Single sync function Used for both synchronous (<code>invoke</code>) and asynchronous (<code>ainvoke</code>) call paths (will not be awaited in async path, called directly) Tuple <code>(sync_func, async_func)</code> The first function is for the synchronous call path; the second function (must be <code>async def</code>) is for the asynchronous call path and will be awaited"},{"location":"adavance-guide/multi-agent/#function-signature","title":"Function Signature","text":"<pre><code>def pre_input_hook(request: str, runtime: ToolRuntime) -&gt; str | dict[str, Any]:\n    \"\"\"\n    Args:\n        request: Original tool call input\n        runtime: langchain's ToolRuntime\n\n    Returns:\n        Processed input, serving as the actual input for the agent (needs to be str or dict)\n    \"\"\"\n</code></pre> <p>Note:</p> <ul> <li> <p>The return value of the hook function must be str or dict, otherwise a ValueError will be raised.</p> </li> <li> <p>If a dict is returned, it will be used directly as the agent's actual input.</p> </li> <li> <p>If a str is returned, it will be wrapped as <code>HumanMessage(content=...)</code>, ultimately serving as the agent's actual input in the format <code>{\"messages\": [HumanMessage(content=...)]}</code>.</p> </li> <li> <p>If <code>pre_input_hooks</code> is not provided, the original input is used directly as the agent's actual input in the format <code>{\"messages\": [HumanMessage(content=request)]}</code>.</p> </li> </ul>"},{"location":"adavance-guide/multi-agent/#usage-example_2","title":"Usage Example","text":"<p>For example, passing additional session context to the SubAgent to provide more precise task context.</p> <pre><code>from langchain.tools import ToolRuntime\nfrom langchain_dev_utils.agents import wrap_agent_as_tool\n\n\ndef process_input(request: str, runtime: ToolRuntime) -&gt; str:\n    original_user_message = next(\n        message for message in runtime.state[\"messages\"] if message.type == \"human\"\n    )\n    prompt = (\n        \"You are assisting with the following user inquiry:\\n\\n\"\n        f\"{original_user_message.text}\\n\\n\"\n        \"You have been assigned the following sub-task:\\n\\n\"\n        f\"{request}\"\n    )\n    return prompt\n\n\n# Usage\ncall_agent_tool = wrap_agent_as_tool(agent, pre_input_hooks=process_input)\n</code></pre>"},{"location":"adavance-guide/multi-agent/#2-post_output_hooks","title":"2. post_output_hooks","text":"<p>Post-process the complete message list returned by the agent after execution to generate the tool's final return value. Useful for result extraction, structured transformation, etc.</p>"},{"location":"adavance-guide/multi-agent/#supported-input-types_1","title":"Supported Input Types","text":"Type Description Single function Used for both synchronous and asynchronous paths (will not be awaited in async path) Tuple <code>(sync_func, async_func)</code> The first is for the synchronous path; the second (<code>async def</code>) is for the asynchronous path and will be awaited"},{"location":"adavance-guide/multi-agent/#function-signature_1","title":"Function Signature","text":"<pre><code>def post_output_hook(request: str, response: dict[str, Any], runtime: ToolRuntime) -&gt; Union[str, Command]:\n    \"\"\"\n    Args:\n        request: Unprocessed original input\n        response: Complete response returned by the agent\n        runtime: langchain's ToolRuntime\n\n    Returns:\n        A value that can be serialized to a string, or a Command object\n    \"\"\"\n</code></pre> <p>Note:</p> <ul> <li> <p>The return value of the hook function must be a value that can be serialized to a string or a <code>Command</code> object.</p> </li> <li> <p>The hook function takes two arguments: <code>request</code> is the unprocessed original input, and <code>response</code> is the complete response returned by the agent (i.e., the return value of <code>agent.invoke(input)</code>).</p> </li> <li> <p>If <code>post_output_hooks</code> is not provided, the agent's final response will be used directly as the tool's return value (i.e., <code>response[\"messages\"][-1].text</code>).</p> </li> </ul>"},{"location":"adavance-guide/multi-agent/#usage-example_3","title":"Usage Example","text":"<p>For example, adding extra return content for the primary agent.</p> <pre><code>import json\n\ndef process_output(request: str, response: dict[str, Any], runtime: ToolRuntime) -&gt; str:\n    return json.dumps(\n        {\n            \"status\": \"success\",\n            \"event_id\": \"evt_123\",\n            \"summary\": response[\"messages\"][-1].text,\n        }\n    )\n\n\n# Usage\ncall_agent_tool = wrap_agent_as_tool(agent, post_output_hooks=process_output)\n</code></pre> <p>Tip</p> <p>For the <code>wrap_all_agents_as_tool</code> function, if you need to customize <code>pre_input_hooks</code> or <code>post_output_hooks</code> for different subagents, you can call <code>get_subagent_name(runtime)</code> inside the hook to get the current agent name and then process them separately by name. For example, assume you only need to customize <code>pre_input_hooks</code> for the <code>weather_agent</code> subagent (e.g., adding the current city and time). You can implement it like this:</p> <pre><code>from langchain_dev_utils.agents.wrap import get_subagent_name\nfrom datetime import datetime\n\ndef process_input(request: str, runtime: ToolRuntime):\n    subagent_name = get_subagent_name(runtime)\n    if subagent_name == \"weather_agent\":\n        city = runtime.state.get(\"city\", \"Unknown City\")\n        time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        return f\"Current city is: {city}, time is: {time}. Please complete the task based on the above information.\" + request \n    return request\n</code></pre>"},{"location":"adavance-guide/middleware/format/","title":"Format System Prompts","text":"<p>Placeholder variables in system prompts can be dynamically resolved through middleware. This library provides two implementations:</p> <ol> <li>Global Instance <code>format_prompt</code>: Pre-configured with f-string style formatting logic (e.g., <code>{name}</code>). Suitable for most scenarios and recommended for direct use.</li> <li>Middleware Class <code>FormatPromptMiddleware</code>: This class must be manually instantiated when jinja2 style formatting (e.g., <code>{{ name }}</code>) is required.</li> </ol> <p>Usage Notes</p> <p>Ensure you have installed <code>langchain-dev-utils[standard]</code> if you need to use jinja2 style templates. Please refer to the Installation Guide.</p>"},{"location":"adavance-guide/middleware/format/#variable-resolution-order","title":"Variable Resolution Order","text":"<p>The values for placeholder variables are resolved following a priority order from highest to lowest:</p> <ol> <li>First, check the <code>state</code>: The <code>state</code> dictionary is searched for a field matching the placeholder name.</li> <li>Then, check the <code>context</code>: If the field is not found in <code>state</code>, the lookup continues in the <code>context</code> object.</li> </ol> <p>This means values in <code>state</code> have higher priority and can override values with the same name in <code>context</code>.</p>"},{"location":"adavance-guide/middleware/format/#using-f-string-style-format_prompt","title":"Using f-string Style (<code>format_prompt</code>)","text":"<p><code>format_prompt</code> is the most commonly used global instance, employing Python's native f-string syntax. All examples below are based on this instance.</p>"},{"location":"adavance-guide/middleware/format/#getting-variables-only-from-state","title":"Getting Variables Only from <code>state</code>","text":"<p>This is the most basic usage, where all placeholder variables are provided by <code>state</code>.</p> <pre><code>from langchain_dev_utils.agents.middleware import format_prompt\nfrom langchain.agents import AgentState\n\nclass AssistantState(AgentState):\n    name: str\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    system_prompt=\"You are an intelligent assistant, and your name is {name}.\",\n    middleware=[format_prompt],\n    state_schema=AssistantState,\n)\n\n# When invoking, the 'name' value must be provided in state\nresponse = agent.invoke(\n    {\"messages\": [HumanMessage(content=\"Hello there\")], \"name\": \"assistant\"}\n)\nprint(response)\n</code></pre>"},{"location":"adavance-guide/middleware/format/#getting-variables-from-both-state-and-context","title":"Getting Variables from Both <code>state</code> and <code>context</code>","text":"<p>The following example demonstrates how to mix data from both <code>state</code> and <code>context</code>:</p> <pre><code>from dataclasses import dataclass\n\n@dataclass\nclass Context:\n    user: str\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    # {name} will be obtained from state, {user} will be obtained from context\n    system_prompt=\"You are an intelligent assistant, and your name is {name}. Your user is named {user}.\",\n    middleware=[format_prompt],\n    state_schema=AssistantState,\n    context_schema=Context,\n)\n\n# When invoking, provide 'name' for state and 'user' for context\nresponse = agent.invoke(\n    {\n        \"messages\": [HumanMessage(content=\"I'm going to New York for a few days, help me plan the itinerary\")],\n        \"name\": \"assistant\",\n    },\n    context=Context(user=\"Zhang San\"),\n)\nprint(response)\n</code></pre>"},{"location":"adavance-guide/middleware/format/#variable-override-example","title":"Variable Override Example","text":"<p>When a variable with the same name exists in both <code>state</code> and <code>context</code>, the value from <code>state</code> takes precedence.</p> <pre><code>from dataclasses import dataclass\n\n@dataclass\nclass Context:\n    # 'name' is defined in context\n    name: str\n    user: str\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    system_prompt=\"You are an intelligent assistant, and your name is {name}. Your user is named {user}.\",\n    middleware=[format_prompt],\n    state_schema=AssistantState, # 'name' is also defined in state\n    context_schema=Context,\n)\n\n# When invoking, values for 'name' are provided in both state and context\nresponse = agent.invoke(\n    {\n        \"messages\": [HumanMessage(content=\"What is your name?\")],\n        \"name\": \"assistant-1\",\n    },\n    context=Context(name=\"assistant-2\", user=\"Zhang San\"),\n)\n\n# The final system prompt will be: \"You are an intelligent assistant, and your name is assistant-1. Your user is named Zhang San.\"\n# Because state has higher priority\nprint(response)\n</code></pre>"},{"location":"adavance-guide/middleware/format/#using-jinja2-style-formatpromptmiddleware","title":"Using Jinja2 Style (<code>FormatPromptMiddleware</code>)","text":"<p>Use <code>FormatPromptMiddleware</code> and specify <code>template_format=\"jinja2\"</code> if your system prompts require more complex logic (such as loops, conditional statements) or if you prefer Jinja2 syntax.</p>"},{"location":"adavance-guide/middleware/format/#basic-example","title":"Basic Example","text":"<p>The example below demonstrates how to use Jinja2 syntax to dynamically generate prompts based on conditions.</p> <pre><code>from langchain_dev_utils.agents.middleware import FormatPromptMiddleware\nfrom dataclasses import dataclass\nfrom typing import Optional\n\n@dataclass\nclass Context:\n    user_role: Optional[str] = None  # User role, e.g., \"VIP\", \"Admin\"\n\n# Manually instantiate the middleware, specifying the format as jinja2\njinja2_formatter = FormatPromptMiddleware(template_format=\"jinja2\")\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    # Use {{ }} syntax\n    system_prompt=(\n        \"You are an intelligent assistant.\\n\"\n        \"{% if user_role == 'VIP' %}\"\n        \"Please provide premium, attentive service.\\n\"\n        \"{% elif user_role == 'Admin' %}\"\n        \"Demonstrate the authority and rigor of a system administrator.\\n\"\n        \"{% else %}\"\n        \"Provide standard user service.\\n\"\n        \"{% endif %}\"\n    ),\n    middleware=[jinja2_formatter],\n    context_schema=Context,\n)\n\n# Example 1: Regular user\nresponse = agent.invoke(\n    {\"messages\": [HumanMessage(content=\"Hello\")]},\n    context=Context(user_role=\"Guest\"),\n)\n\n# Example 2: VIP user\n# The system prompt will contain \"Please provide premium, attentive service.\"\nresponse = agent.invoke(\n    {\"messages\": [HumanMessage(content=\"Hello\")]},\n    context=Context(user_role=\"VIP\"),\n)\n</code></pre> <p>Jinja2 Template Caution</p> <p>When Jinja2 templating is enabled, <code>system_prompt</code> will be compiled into a Template object. Always hardcode the prompt skeleton in your codebase, injecting dynamic values solely via <code>state</code> or <code>context</code>. Never pass raw user input directly as the <code>system_prompt</code> argument to <code>create_agent</code>. (Regardless of this feature, <code>system_prompt</code> should always be fully controlled by developers and never directly accept external user input as the core system prompt.)</p>"},{"location":"adavance-guide/middleware/handoffs/","title":"Agent Handoff","text":"<p><code>HandoffAgentMiddleware</code> is a middleware used for flexibly switching between multiple sub-agents. It fully implements the official LangChain <code>handoffs</code> multi-agent collaboration scheme.</p>"},{"location":"adavance-guide/middleware/handoffs/#parameter-description","title":"Parameter Description","text":"Parameter Description <code>agents_config</code> A dictionary of agent configurations, where the key is the agent name and the value is the agent configuration dictionary.Type: <code>dict[str, AgentConfig]</code>Required: Yes <code>custom_handoffs_tool_descriptions</code> Custom descriptions for handoff tools, where the key is the agent name and the value is the corresponding handoff tool description.Type: <code>dict[str, str]</code>Required: No <code>handoffs_tool_overrides</code> Custom implementations for handoff tools, where the key is the agent name and the value is the corresponding handoff tool implementation.Type: <code>dict[str, BaseTool]</code>Required: No"},{"location":"adavance-guide/middleware/handoffs/#agents_config-configuration-description","title":"<code>agents_config</code> Configuration Description","text":"<p>Each agent is configured as a dictionary containing the following fields:</p> Field Description <code>model</code> Specifies the model used by this agent; if not passed, it inherits the model corresponding to the <code>model</code> parameter of <code>create_agent</code>. Supports strings (must be in <code>provider:model-name</code> format, e.g., <code>vllm:qwen3-4b</code>) or <code>BaseChatModel</code> instances.Type: <code>str</code> | <code>BaseChatModel</code>Required: No <code>prompt</code> The system prompt for the agent.Type: <code>str</code> | <code>SystemMessage</code>Required: Yes <code>tools</code> A list of tools the agent can call; if not passed, the agent only possesses relevant handoff tools.Type: <code>list[BaseTool]</code>Required: No <code>default</code> Whether to set as the default agent; defaults to <code>False</code>. There must be one and only one agent set to <code>True</code> in the entire configuration.Type: <code>bool</code>Required: No <code>handoffs</code> A list of names of other agents to which this agent can hand off. If set to <code>\"all\"</code>, it means this agent can hand off to all other agents.Type: <code>list[str]</code> | <code>str</code>Required: Yes <p>Note</p> <p>After using this middleware, the <code>tools</code> and <code>system_prompt</code> parameters of <code>create_agent</code> will be ignored, so there is no need to fill them in.</p> <p>For this paradigm of multi-agent implementation, a tool for handoffs is often required. This middleware utilizes the <code>handoffs</code> configuration of each agent to automatically create the corresponding handoff tool for each agent. If you wish to customize the description of the handoff tool, you can achieve this via the <code>custom_handoffs_tool_descriptions</code> parameter.</p>"},{"location":"adavance-guide/middleware/handoffs/#basic-usage","title":"Basic Usage","text":"<p>In this example, four agents will be used: <code>time_agent</code>, <code>weather_agent</code>, <code>code_agent</code>, and <code>default_agent</code>.</p> <p>Next, we need to create the corresponding agent configuration dictionary <code>agent_config</code>.</p> <pre><code>from langchain_dev_utils.agents.middleware.handoffs import AgentConfig\n\nagent_config: dict[str, AgentConfig] = {\n    \"time_agent\": {\n        \"model\": \"vllm:qwen3-8b\",\n        \"prompt\": \"You are a time assistant\",\n        \"tools\": [get_current_time],\n        \"handoffs\": [\"default_agent\"],  # This agent can only hand off to default_agent\n    },\n    \"weather_agent\": {\n        \"prompt\": \"You are a weather assistant\",\n        \"tools\": [get_current_weather, get_current_city],\n        \"handoffs\": [\"default_agent\"],\n    },\n    \"code_agent\": {\n        \"model\": load_chat_model(\"vllm:qwen3-coder-flash\"),\n        \"prompt\": \"You are a coding assistant\",\n        \"tools\": [\n            run_code,\n        ],\n        \"handoffs\": [\"default_agent\"],\n    },\n    \"default_agent\": {\n        \"prompt\": \"You are an assistant\",\n        \"default\": True, # Set as the default agent\n        \"handoffs\": \"all\",  # This agent can hand off to all other agents\n    },\n}\n</code></pre> <p>Finally, simply pass this configuration to <code>HandoffAgentMiddleware</code>.</p> <pre><code>from langchain_dev_utils.agents.middleware import HandoffAgentMiddleware\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    middleware=[HandoffAgentMiddleware(agents_config=agent_config)],\n)\n\nresponse = agent.invoke({\"messages\": [HumanMessage(content=\"What is the current time?\")]})\nprint(response)\n</code></pre>"},{"location":"adavance-guide/middleware/handoffs/#customizing-handoff-tool-descriptions","title":"Customizing Handoff Tool Descriptions","text":"<p>If you want to customize the description of the handoff tools, you can pass a second parameter <code>custom_handoffs_tool_descriptions</code>.</p> <pre><code>agent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    middleware=[\n        HandoffAgentMiddleware(\n            agents_config=agent_config,\n            custom_handoffs_tool_descriptions={\n                \"time_agent\": \"Use this tool to hand off to the time assistant to resolve time query issues\",\n                \"weather_agent\": \"Use this tool to hand off to the weather assistant to resolve weather query issues\",\n                \"code_agent\": \"Use this tool to hand off to the coding assistant to resolve code issues\",\n                \"default_agent\": \"Use this tool to hand off to the default assistant\",\n            },\n        )\n    ],\n)\n</code></pre>"},{"location":"adavance-guide/middleware/handoffs/#customizing-handoff-tool-implementation","title":"Customizing Handoff Tool Implementation","text":"<p>If you want to fully customize the implementation logic of the handoff tool, you can pass a third parameter <code>handoffs_tool_overrides</code>. Similar to the second parameter, it is also a dictionary where the key is the agent name and the value is the corresponding handoff tool implementation.</p> <p>A custom handoff tool must return a <code>Command</code> object, whose <code>update</code> attribute needs to contain the <code>messages</code> key (returning the tool response) and the <code>active_agent</code> key (whose value is the name of the agent to hand off to, used to switch the current agent).</p> <p>For example:</p> <pre><code>@tool\ndef transfer_to_code_agent(runtime: ToolRuntime) -&gt; Command:\n    \"\"\"This tool helps you hand off to the coding assistant\"\"\"\n    # You can add custom logic here\n    return Command(\n        update={\n            \"messages\": [\n                ToolMessage(\n                    content=\"transfer to code agent\",\n                    tool_call_id=runtime.tool_call_id,\n                )\n            ],\n            \"active_agent\": \"code_agent\",\n            # You can add other keys to update here\n        }\n    )\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    middleware=[\n        HandoffAgentMiddleware(\n            agents_config=agent_config,\n            handoffs_tool_overrides={\n                \"code_agent\": transfer_to_code_agent,\n            },\n        )\n    ],\n)\n</code></pre> <p><code>handoffs_tool_overrides</code> is used for highly customized implementations of handoff tools. If you only want to customize the description of the handoff tool, you should use <code>custom_handoffs_tool_descriptions</code>.</p>"},{"location":"adavance-guide/middleware/official/","title":"LangChain Built-in Middleware Extensions","text":"<p>This library enhances the following official middleware components, supporting direct model specification via strings\u2014provided the model has been registered via <code>register_model_provider</code>:</p> <ul> <li><code>SummarizationMiddleware</code></li> <li><code>LLMToolSelectorMiddleware</code></li> <li><code>ModelFallbackMiddleware</code></li> <li><code>LLMToolEmulator</code></li> </ul> <p>Simply import the middleware from this library to use them, with the same usage as the official versions:</p> <pre><code>from langchain_core.messages import AIMessage\nfrom langchain_dev_utils.agents.middleware import SummarizationMiddleware\nfrom langchain_dev_utils.chat_models import register_model_provider\n\n# Models must first be registered via register_model_provider\nregister_model_provider(\n    provider_name=\"vllm\",\n    chat_model=\"openai-compatible\",\n    base_url=\"http://localhost:8000/v1\",\n)\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    middleware=[\n        SummarizationMiddleware(\n            model=\"vllm:qwen3-4b\",\n            trigger=(\"tokens\", 50),\n            keep=(\"messages\", 1),\n        )\n    ],\n    system_prompt=\"You are an intelligent AI assistant capable of solving user problems\",\n)\nresponse = agent.invoke({\"messages\": messages})\nprint(response)\n</code></pre>"},{"location":"adavance-guide/middleware/overview/","title":"Overview","text":"<p>Middleware components are pluggable modules specifically designed for LangChain's pre-built Agents, aiming to achieve fine-grained control over the Agent's internal behavior. In addition to the built-in middleware provided by the LangChain framework, this library further enriches middleware support based on practical application scenarios.</p> <p>The middleware provided by this library includes:</p> <ul> <li><code>PlanMiddleware</code>: Task planning, breaking down complex tasks into ordered subtasks</li> <li><code>ModelRouterMiddleware</code>: Dynamically routing input to the most suitable model based on content</li> <li><code>HandoffAgentMiddleware</code>: Flexibly handing off tasks between multiple sub-agents</li> <li><code>ToolCallRepairMiddleware</code>: Automatically repairing invalid tool calls from LLMs</li> <li><code>FormatPromptMiddleware</code>: Dynamically formatting placeholders in system prompts</li> </ul> <p>Furthermore, this library extends the functionality of official middleware, enhancing model configuration usability by supporting model specification through string parameters:</p> <ul> <li>SummarizationMiddleware</li> <li>LLMToolSelectorMiddleware</li> <li>ModelFallbackMiddleware</li> <li>LLMToolEmulator</li> </ul>"},{"location":"adavance-guide/middleware/plan/","title":"Task Planning","text":"<p><code>PlanMiddleware</code> is a middleware used for structured decomposition and process management before executing complex tasks.</p> <p>Additional Notes</p> <p>Task planning is an efficient strategy for context engineering management. Before executing a task, the large language model first breaks down the overall task into multiple ordered subtasks, forming a task planning list (referred to as a plan in this library). It then executes each subtask sequentially, dynamically updating the task status after completing each step, until all subtasks are finished.</p>"},{"location":"adavance-guide/middleware/plan/#parameter-description","title":"Parameter Description","text":"Parameter Description <code>system_prompt</code> System prompt. If <code>None</code>, the default prompt will be used.Type: <code>str</code>Required: No <code>custom_plan_tool_descriptions</code> Custom descriptions for planning-related tools.Type: <code>dict</code>Required: No <code>use_read_plan_tool</code> Whether to enable the read plan tool.Type: <code>bool</code>Required: NoDefault Value: <code>True</code> <p>The keys for the <code>custom_plan_tool_descriptions</code> dictionary can be any of the following three values:</p> Key Description <code>write_plan</code> Description for the write plan tool <code>finish_sub_plan</code> Description for the finish sub-plan tool <code>read_plan</code> Description for the read plan tool"},{"location":"adavance-guide/middleware/plan/#usage-example","title":"Usage Example","text":"<pre><code>from langchain_dev_utils.agents.middleware import PlanMiddleware\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    middleware=[\n        PlanMiddleware(\n            custom_plan_tool_descriptions={\n                \"write_plan\": \"Used to write a plan, breaking down the task into multiple ordered subtasks.\",\n                \"finish_sub_plan\": \"Used to complete a subtask, updating its status to finished.\",\n                \"read_plan\": \"Used to query the current task planning list.\"\n            },\n            use_read_plan_tool=True,  # If you don't want to use the read plan tool, you can set this parameter to False\n        )\n    ],\n)\n\nresponse = agent.invoke(\n    {\"messages\": [HumanMessage(content=\"I'm going to New York for a few days, help me plan the itinerary\")]}\n)\nprint(response)\n</code></pre>"},{"location":"adavance-guide/middleware/plan/#tool-description","title":"Tool Description","text":"<p><code>PlanMiddleware</code> requires the use of two mandatory tools: <code>write_plan</code> and <code>finish_sub_plan</code>, while the <code>read_plan</code> tool is enabled by default. If not needed, you can set the <code>use_read_plan_tool</code> parameter to <code>False</code>.</p>"},{"location":"adavance-guide/middleware/plan/#comparison-with-the-official-to-do-list-middleware","title":"Comparison with the Official To-do List Middleware","text":"<p>This middleware is similar in function to the official To-do list middleware provided by LangChain, but differs in tool design:</p> Feature Official To-do List Middleware This Library's PlanMiddleware Number of Tools 1 (<code>write_todo</code>) 3 (<code>write_plan</code>, <code>finish_sub_plan</code>, <code>read_plan</code>) Functionality Focus Oriented towards to-do lists Specifically designed for planning lists Operation Method Adding and modifying are done through one tool Writing, modifying, and querying are handled by separate tools <p>Whether referred to as <code>todo</code> or <code>plan</code>, the essence is the same concept. The key distinction of this middleware from the official one is that it provides three dedicated tools:</p> <ul> <li><code>write_plan</code>: Used to write or update plan content</li> <li><code>finish_sub_plan</code>: Used to update the status of a subtask after its completion</li> <li><code>read_plan</code>: Used to query plan content</li> </ul>"},{"location":"adavance-guide/middleware/router/","title":"Model Router","text":"<p><code>ModelRouterMiddleware</code> is a middleware used to dynamically route to the most suitable model based on input content. It analyzes user requests using a \"router model\" and selects the model best suited for the current task from a predefined list.</p>"},{"location":"adavance-guide/middleware/router/#parameter-description","title":"Parameter Description","text":"Parameter Description <code>router_model</code> The model used to make routing decisions.Type: <code>str</code> | <code>BaseChatModel</code>Required: Yes <code>model_list</code> A list of model configurations.Type: <code>list[ModelDict]</code>Required: Yes <code>router_prompt</code> Custom prompt for the router model.Type: <code>str</code>Required: No"},{"location":"adavance-guide/middleware/router/#model_list-configuration-description","title":"<code>model_list</code> Configuration Description","text":"<p>Each model configuration is a dictionary containing the following fields:</p> Field Description <code>model_name</code> The unique identifier for the model, using the <code>provider:model-name</code> format.Type: <code>str</code>Required: Yes <code>model_description</code> A brief description of the model's capabilities or applicable scenarios.Type: <code>str</code>Required: Yes <code>tools</code> A whitelist of tools callable by this model. If not passed, the model defaults to having permission to use all tools.Type: <code>list[BaseTool]</code>Required: No <code>model_kwargs</code> Additional parameters when loading the model.Type: <code>dict</code>Required: No <code>model_system_prompt</code> System-level prompt for the model.Type: <code>str</code>Required: No <code>model_instance</code> An instantiated model object.Type: <code>BaseChatModel</code>Required: No <p>Explanation of the <code>model_instance</code> Field</p> <ul> <li>If provided: The instance is used directly, <code>model_name</code> serves only as an identifier, and <code>model_kwargs</code> is ignored; suitable for scenarios not using this library's chat model management features.</li> <li>If not provided: The model is loaded using <code>load_chat_model</code> based on <code>model_name</code> and <code>model_kwargs</code>.</li> <li>Naming format: In either case, it is recommended that <code>model_name</code> follows the <code>provider:model-name</code> format.</li> </ul>"},{"location":"adavance-guide/middleware/router/#usage-examples","title":"Usage Examples","text":"<p>Step 1: Define the Model List</p> <pre><code>from langchain_dev_utils.agents.middleware.model_router import ModelDict\n\nmodel_list: list[ModelDict] = [\n    {\n        \"model_name\": \"vllm:qwen3-8b\",\n        \"model_description\": \"Suitable for general tasks, such as conversation, text generation, etc.\",\n        \"model_kwargs\": {\n            \"temperature\": 0.7,\n            \"extra_body\": {\"chat_template_kwargs\": {\"enable_thinking\": False}},\n        },\n        \"model_system_prompt\": \"You are an assistant skilled at handling general tasks, such as conversation and text generation.\",\n    },\n    {\n        \"model_name\": \"vllm:qwen3-vl-2b\",\n        \"model_description\": \"Suitable for visual tasks\",\n        \"tools\": [],  # If the model does not require any tools, please set this field to an empty list []\n    },\n    {\n        \"model_name\": \"vllm:qwen3-coder-flash\",\n        \"model_description\": \"Suitable for code generation tasks\",\n        \"tools\": [run_python_code],  # Only allow the use of the run_python_code tool\n    },\n    {\n        \"model_name\": \"openai:gpt-4o\",\n        \"model_description\": \"Suitable for comprehensive and complex tasks\",\n        \"model_system_prompt\": \"You are an assistant skilled at handling comprehensive high-difficulty tasks\",\n        \"model_instance\": ChatOpenAI(\n            model=\"gpt-4o\"\n        ),  # Pass the instance directly; here model_name serves only as an identifier, model_kwargs is ignored\n    },\n]\n</code></pre> <p>Step 2: Create an Agent and Enable Middleware</p> <pre><code>from langchain_dev_utils.agents.middleware import ModelRouterMiddleware\nfrom langchain_core.messages import HumanMessage\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",  # This model serves as a placeholder; it is dynamically replaced by the middleware\n    tools=[get_current_time],\n    middleware=[\n        ModelRouterMiddleware(\n            router_model=\"vllm:qwen3-4b\",\n            model_list=model_list,\n        )\n    ],\n)\n\n# The routing middleware will automatically select the most suitable model based on the input content\nresponse = agent.invoke({\"messages\": [HumanMessage(content=\"Help me write a bubble sort code\")]})\nprint(response)\n</code></pre> <p>tools Parameter</p> <p>After using this middleware, the <code>tools</code> parameter of <code>create_agent</code> is treated as \"global supplementary tools\". These global tools are only appended to a model's available tool list if the <code>tools</code> field is not defined in that model's <code>model_list</code>; furthermore, these global tools must not be included in the <code>tools</code> field defined in <code>model_list</code>.</p> <p>With <code>ModelRouterMiddleware</code>, you can easily build a multi-model, multi-capability Agent that automatically selects the optimal model based on task type, improving response quality and efficiency.</p> <p>Parallel Execution</p> <p>Implementing model routing via middleware assigns only one task for execution at a time. If you wish to decompose a task into multiple sub-tasks executed in parallel by multiple models, please refer to Predefined StateGraph Construction Functions.</p>"},{"location":"adavance-guide/middleware/tool-call-repair/","title":"Tool Call Repair","text":"<p><code>ToolCallRepairMiddleware</code> is a middleware designed to automatically repair invalid tool calls from large language models.</p> <p>When large language models output tool call parameters that conform to a JSON Schema, they may sometimes generate malformed JSON content (often in the <code>arguments</code> field) due to model limitations. Such failed parsing attempts are flagged by LangChain and stored in the <code>invalid_tool_calls</code> field. <code>ToolCallRepairMiddleware</code> automatically detects this field and uses the <code>json-repair</code> library to attempt to fix the formatting, allowing the tool calls to execute normally.</p> <p>Usage Notes</p> <p>Before using this middleware, ensure <code>langchain-dev-utils[standard]</code> is installed. Refer to the Installation Guide for details.</p>"},{"location":"adavance-guide/middleware/tool-call-repair/#parameter-description","title":"Parameter Description","text":"<p>This middleware is designed for zero-configuration out-of-the-box use. No parameters are required during instantiation.</p>"},{"location":"adavance-guide/middleware/tool-call-repair/#usage-examples","title":"Usage Examples","text":""},{"location":"adavance-guide/middleware/tool-call-repair/#standard-usage","title":"Standard Usage","text":"<pre><code>from langchain_dev_utils.agents.middleware import ToolCallRepairMiddleware\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    tools=[run_python_code, get_current_time],\n    middleware=[\n        ToolCallRepairMiddleware()\n    ],\n)\n</code></pre>"},{"location":"adavance-guide/middleware/tool-call-repair/#convenient-usage-recommended","title":"Convenient Usage (Recommended)","text":"<p>Since instantiating <code>ToolCallRepairMiddleware</code> requires no configuration parameters, this library provides a pre-configured global instance <code>tool_call_repair</code>. It is recommended to use this directly to simplify your code:</p> <pre><code>from langchain_dev_utils.agents.middleware import tool_call_repair\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    tools=[run_python_code, get_current_time],\n    middleware=[tool_call_repair],\n)\n</code></pre> <p>Important Notes</p> <p>This middleware cannot guarantee 100% repair of all invalid tool calls. The actual repair effectiveness depends on the capabilities of the <code>json-repair</code> library. Furthermore, it only operates on the invalid calls within the <code>invalid_tool_calls</code> field.</p>"},{"location":"adavance-guide/openai-compatible/chat/","title":"Creation and Usage of Chat Models","text":""},{"location":"adavance-guide/openai-compatible/chat/#creating-chat-model-classes","title":"Creating Chat Model Classes","text":"<p>Use the <code>create_openai_compatible_model</code> function to create an integrated chat model class. This function accepts the following parameters:</p> Parameter Description <code>model_provider</code> Model provider name, e.g., <code>vllm</code>. Must start with a letter or number, can only contain letters, numbers, and underscores, with a maximum length of 20 characters.Type: <code>str</code>Required: Yes <code>base_url</code> Default API endpoint for the model provider.Type: <code>str</code>Required: No <code>compatibility_options</code> Compatibility options configuration.Type: <code>dict</code>Required: No <code>model_profiles</code> Profile configuration dictionary for each model of this provider.Type: <code>dict</code>Required: No <code>chat_model_cls_name</code> Chat model class name (must comply with Python class naming conventions). Default value is <code>Chat{model_provider}</code> (where <code>{model_provider}</code> is capitalized).Type: <code>str</code>Required: No <p>Among them, <code>compatibility_options</code> is a dictionary used to declare the provider's support for specific features of the OpenAI API, improving compatibility and stability.</p> <p>Currently supported configuration items:</p> Configuration Item Description <code>supported_tool_choice</code> List of supported <code>tool_choice</code> strategies.Type: <code>list[str]</code>Default: <code>[\"auto\"]</code> <code>supported_response_format</code> List of supported <code>response_format</code> formats (<code>json_schema</code>, <code>json_object</code>).Type: <code>list[str]</code>Default: <code>[]</code> <code>reasoning_keep_policy</code> Retention policy for the <code>reasoning_content</code> field in historical messages.Type: <code>str</code>Default: <code>\"never\"</code> <code>include_usage</code> Whether to include <code>usage</code> information in streaming responses.Type: <code>bool</code>Default: <code>True</code> <p>Supplement</p> <p>Since different models from the same provider may have varying support for parameters like <code>tool_choice</code> and <code>response_format</code>, these four compatibility options are instance attributes of the class. Therefore, when creating a chat model class, you can pass in values as global defaults (representing configurations supported by most models of that provider). If specific models require adjustments, you can override these parameters during instantiation.</p> <p>Tip</p> <p>This library constructs a provider-specific chat model class using the built-in <code>BaseChatOpenAICompatible</code> based on user-provided parameters. This class inherits from <code>langchain-openai</code>'s <code>BaseChatOpenAI</code> and is enhanced in the following aspects:</p> <ul> <li>Supports more reasoning content formats: In addition to the official OpenAI format, it also supports reasoning content returned via the <code>reasoning_content</code> parameter.</li> <li>Supports <code>video</code> type content_block: Fills the capability gap of <code>ChatOpenAI</code> regarding <code>video</code> type <code>content_block</code>.</li> <li>Automatically selects more suitable structured output methods: Based on the provider's actual support, automatically chooses between <code>function_calling</code> and <code>json_schema</code> for better solutions.</li> <li>Fine-grained adaptation of differences via <code>compatibility_options</code>: Configure support differences for parameters like <code>tool_choice</code> and <code>response_format</code> as needed.</li> </ul> <p>Use the following code to create a chat model class:</p> <pre><code>from langchain_dev_utils.chat_models.adapters import create_openai_compatible_model\n\nChatVLLM = create_openai_compatible_model(\n    model_provider=\"vllm\",\n    base_url=\"http://localhost:8000/v1\",\n    chat_model_cls_name=\"ChatVLLM\",\n)\n\nmodel = ChatVLLM(model=\"qwen3-4b\")\nprint(model.invoke(\"Hello\"))\n</code></pre> <p>When creating a chat model class, the <code>base_url</code> parameter can be omitted. If not provided, the library will read the corresponding environment variable by default, for example:</p> <pre><code>export VLLM_API_BASE=http://localhost:8000/v1\n</code></pre> <p>At this point, the code can omit <code>base_url</code>:</p> <pre><code>from langchain_dev_utils.chat_models.adapters import create_openai_compatible_model\n\nChatVLLM = create_openai_compatible_model(\n    model_provider=\"vllm\",\n    chat_model_cls_name=\"ChatVLLM\",\n)\n\nmodel = ChatVLLM(model=\"qwen3-4b\")\nprint(model.invoke(\"Hello\"))\n</code></pre> <p>Note: The above code successfully runs assuming the environment variable <code>VLLM_API_KEY</code> is configured. Although vLLM itself does not require an API Key, the chat model class initialization requires one. Therefore, please set this variable first, for example:</p> <pre><code>export VLLM_API_KEY=vllm_api_key\n</code></pre> <p>Note</p> <p>The naming rules for environment variables for created chat model classes (embedding model classes follow the same rules):</p> <ul> <li>API Base URL: <code>${PROVIDER_NAME}_API_BASE</code> (all uppercase, underscore separated).</li> <li>API Key: <code>${PROVIDER_NAME}_API_KEY</code> (all uppercase, underscore separated).</li> </ul>"},{"location":"adavance-guide/openai-compatible/chat/#using-the-chat-model-class","title":"Using the Chat Model Class","text":""},{"location":"adavance-guide/openai-compatible/chat/#standard-invocation","title":"Standard Invocation","text":"<p>Use the <code>invoke</code> method for standard invocation, returning the model's response.</p> <pre><code>from langchain_core.messages import HumanMessage\n\nmodel = ChatVLLM(model=\"qwen3-4b\")\nresponse = model.invoke([HumanMessage(\"Hello\")])\nprint(response)\n</code></pre> <p>Also supports asynchronous invocation via <code>ainvoke</code>:</p> <pre><code>from langchain_core.messages import HumanMessage\n\nmodel = ChatVLLM(model=\"qwen3-4b\")\nresponse = await model.ainvoke([HumanMessage(\"Hello\")])\nprint(response)\n</code></pre>"},{"location":"adavance-guide/openai-compatible/chat/#streaming-invocation","title":"Streaming Invocation","text":"<p>Use the <code>stream</code> method for streaming invocation, for streaming model responses.</p> <pre><code>from langchain_core.messages import HumanMessage\n\nmodel = ChatVLLM(model=\"qwen3-4b\")\nfor chunk in model.stream([HumanMessage(\"Hello\")]):\n    print(chunk)\n</code></pre> <p>And asynchronous streaming invocation via <code>astream</code>:</p> <pre><code>from langchain_core.messages import HumanMessage\n\nmodel = ChatVLLM(model=\"qwen3-4b\")\nasync for chunk in model.astream([HumanMessage(\"Hello\")]):\n    print(chunk)\n</code></pre> Streaming Output Options <p>You can use <code>stream_options={\"include_usage\": True}</code> to append token usage information (<code>prompt_tokens</code> and <code>completion_tokens</code>) at the end of streaming responses. This library enables this option by default; to disable it, you can pass the compatibility option <code>include_usage=False</code> when creating the model class or during instantiation.</p>"},{"location":"adavance-guide/openai-compatible/chat/#tool-calling","title":"Tool Calling","text":"<p>If the model supports tool calling, you can directly use <code>bind_tools</code> for tool calling:</p> <pre><code>from langchain_core.messages import HumanMessage\nfrom langchain_core.tools import tool\nimport datetime\n\n@tool\ndef get_current_time() -&gt; str:\n    \"\"\"Get the current timestamp\"\"\"\n    return str(datetime.datetime.now().timestamp())\n\nmodel = ChatVLLM(model=\"qwen3-4b\").bind_tools([get_current_time])\nresponse = model.invoke([HumanMessage(\"Get the current timestamp\")])\nprint(response)\n</code></pre> Parallel Tool Calls <p>If the model supports parallel tool calls, you can pass <code>parallel_tool_calls=True</code> in <code>bind_tools</code> to enable parallel tool calls (some model providers enable this by default, so explicit passing may not be necessary).</p> <p>For example:</p> <pre><code>from langchain_core.messages import HumanMessage\nfrom langchain_core.tools import tool\n\n\n@tool\ndef get_current_weather(location: str) -&gt; str:\n    \"\"\"Get the current weather\"\"\"\n    return f\"The weather in {location} is sunny\"\n\nmodel = ChatVLLM(model=\"qwen3-4b\").bind_tools(\n    [get_current_weather], parallel_tool_calls=True\n)\nresponse = model.invoke([HumanMessage(\"Get the weather in Los Angeles and London\")])\nprint(response)\n</code></pre> Forced Tool Calling <p>The <code>tool_choice</code> parameter controls whether the model calls tools and which tool to call in its response, improving accuracy, reliability, and controllability. Common values include:</p> <ul> <li><code>\"auto\"</code>: Model decides whether to call tools (default behavior);</li> <li><code>\"none\"</code>: Disable tool calling;</li> <li><code>\"required\"</code>: Force calling at least one tool;</li> <li>Specify a specific tool (in OpenAI-compatible APIs, specifically <code>{\"type\": \"function\", \"function\": {\"name\": \"xxx\"}}</code>).</li> </ul> <p>Different providers have varying support ranges for <code>tool_choice</code>. To address these differences, this library introduces the compatibility configuration item <code>supported_tool_choice</code>, with a default value of <code>[\"auto\"]</code>. In this case, the <code>tool_choice</code> passed in <code>bind_tools</code> can only be <code>auto</code>; other values will be filtered out.</p> <p>To support other <code>tool_choice</code> values, you must configure the supported items. The configuration value is a list of strings, with each string's optional values:</p> <ul> <li><code>\"auto\"</code>, <code>\"none\"</code>, <code>\"required\"</code>: Correspond to standard strategies;</li> <li><code>\"specific\"</code>: A unique identifier of this library, indicating support for specifying specific tools.</li> </ul> <p>For example, vLLM supports all strategies:</p> <pre><code>from langchain_dev_utils.chat_models.adapters import create_openai_compatible_model\n\nChatVLLM = create_openai_compatible_model(\n    model_provider=\"vllm\",\n    chat_model_cls_name=\"ChatVLLM\",\n    compatibility_options={\n        \"supported_tool_choice\": [\"auto\", \"required\", \"none\", \"specific\"]\n    },\n)\n\nmodel = ChatVLLM(model=\"qwen3-4b\").bind_tools(\n    [get_current_weather], tool_choice=\"required\"\n)\n</code></pre>"},{"location":"adavance-guide/openai-compatible/chat/#structured-output","title":"Structured Output","text":"<pre><code>from langchain_core.messages import HumanMessage\nfrom pydantic import BaseModel\n\nclass User(BaseModel):\n    name: str\n    age: int\n\nmodel = ChatVLLM(model=\"qwen3-4b\").with_structured_output(User)\nresponse = model.invoke([HumanMessage(\"Hello, my name is Zhang San, I am 25 years old\")])\nprint(response)\n</code></pre> Default Structured Output Method <p>There are currently three common structured output methods: <code>json_schema</code>, <code>function_calling</code>, <code>json_mode</code>. Among them, <code>json_schema</code> yields the best results, so this library's <code>with_structured_output</code> prioritizes using <code>json_schema</code> as the structured output method; when the provider does not support it, it automatically falls back to <code>function_calling</code>. Different model providers have varying levels of support for structured output. This library declares the supported structured output methods via the compatibility configuration item <code>supported_response_format</code>. The default value is <code>[]</code>, indicating neither <code>json_schema</code> nor <code>json_mode</code> is supported. In this case, <code>with_structured_output(method=...)</code> will consistently use <code>function_calling</code>; even if <code>json_schema</code> / <code>json_mode</code> is passed, it will be automatically converted to <code>function_calling</code>. If you want to use the corresponding structured output method, you need to explicitly pass the relevant parameters (especially for <code>json_schema</code>).</p> <p>For example, if a model deployed via vLLM supports the <code>json_schema</code> structured output method, you can declare it during registration:</p> <pre><code>from langchain_dev_utils.chat_models.adapters import create_openai_compatible_model\n\nChatVLLM = create_openai_compatible_model(\n    model_provider=\"vllm\",\n    chat_model_cls_name=\"ChatVLLM\",\n    compatibility_options={\"supported_response_format\": [\"json_schema\"]},\n)\n\nmodel = ChatVLLM(model=\"qwen3-4b\")\n</code></pre> <p>Note</p> <p>If <code>supported_response_format</code> includes <code>json_schema</code>, the <code>structured_output</code> field in <code>model.profile</code> will automatically be set to <code>True</code>. In this case, when using <code>create_agent</code> for structured output without specifying a specific structured output strategy, <code>json_schema</code> will be used as the default structured output strategy.</p> <p>For example:  <pre><code>from langchain_dev_utils.chat_models.adapters import create_openai_compatible_model\n\nChatVLLM = create_openai_compatible_model(\n    model_provider=\"vllm\",\n    base_url=\"http://localhost:8000/v1\",\n    compatibility_options={\"supported_response_format\": [\"json_schema\"]},\n)\n\nmodel = ChatVLLM(model=\"qwen3-4b\")\nprint(model.profile)\n</code></pre></p> <p>Output result:</p> <pre><code>{'structured_output': True}\n</code></pre>"},{"location":"adavance-guide/openai-compatible/chat/#passing-additional-parameters","title":"Passing Additional Parameters","text":"<p>Since this class inherits from <code>BaseChatOpenAI</code>, it supports passing model parameters of <code>BaseChatOpenAI</code>, such as <code>temperature</code>, <code>extra_body</code>, etc.</p> <p>For example, using <code>extra_body</code> to pass additional parameters (here, disabling thinking mode):</p> <pre><code>from langchain_core.messages import HumanMessage\n\nmodel = ChatVLLM(\n    model=\"qwen3-4b\",\n    extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},\n)\nresponse = model.invoke([HumanMessage(\"Hello\")])\nprint(response)\n</code></pre>"},{"location":"adavance-guide/openai-compatible/chat/#passing-multimodal-data","title":"Passing Multimodal Data","text":"<p>Supports passing multimodal data. You can use the OpenAI-compatible multimodal data format or directly use <code>content_block</code> from LangChain.</p> <p>Passing image data:</p> <p><pre><code>from langchain_core.messages import HumanMessage\nmessages = [\n    HumanMessage(\n        content_blocks=[\n            {\n                \"type\": \"image\",\n                \"url\": \"https://example.com/image.png\",\n            },\n            {\"type\": \"text\", \"text\": \"Describe this image\"},\n        ]\n    )\n]\n\nmodel = ChatVLLM(model=\"qwen3-vl-2b\")\nresponse = model.invoke(messages)\nprint(response)\n</code></pre> Passing video data:</p> <pre><code>from langchain_core.messages import HumanMessage\n\nmessages = [\n    HumanMessage(\n        content_blocks=[\n            {\n                \"type\": \"video\",\n                \"url\": \"https://example.com/video.mp4\",\n            },\n            {\"type\": \"text\", \"text\": \"Describe this video\"},\n        ]\n    )\n]\n\nmodel = ChatVLLM(model=\"qwen3-vl-2b\")\nresponse = model.invoke(messages)\nprint(response)\n</code></pre>"},{"location":"adavance-guide/openai-compatible/chat/#using-reasoning-models","title":"Using Reasoning Models","text":"<p>A major feature of the model classes created by this library is further adaptation to more reasoning models.</p> <p>For example:</p> <pre><code>from langchain_core.messages import HumanMessage\n\nmodel = ChatVLLM(model=\"qwen3-4b\")\nresponse = model.invoke(\"Why are parrot feathers so colorful?\")\nreasoning_steps = [b for b in response.content_blocks if b[\"type\"] == \"reasoning\"]\nprint(\" \".join(step[\"reasoning\"] for step in reasoning_steps))\n</code></pre> Support for Different Reasoning Modes <p>Different models have varying reasoning modes (especially important in Agent development): some require explicitly passing the <code>reasoning_content</code> field in the current call, while others do not. This library provides the <code>reasoning_keep_policy</code> compatibility configuration to adapt to these differences.</p> <p>This configuration item supports the following values:</p> <ul> <li> <p><code>never</code>: Do not retain any reasoning content in historical messages (default);</p> </li> <li> <p><code>current</code>: Only retain the <code>reasoning_content</code> field from the current conversation;</p> </li> <li> <p><code>all</code>: Retain the <code>reasoning_content</code> field from all conversations.</p> </li> </ul> <pre><code>graph LR\n    A[reasoning_content Retention Policy] --&gt; B{Value?};\n    B --&gt;|never| C[Contains no&lt;br&gt;reasoning_content];\n    B --&gt;|current| D[Only contains current conversation's&lt;br&gt;reasoning_content&lt;br&gt;Adapts to interleaved thinking mode];\n    B --&gt;|all| E[Contains all conversations'&lt;br&gt;reasoning_content];\n    C --&gt; F[Send to model];\n    D --&gt; F;\n    E --&gt; F;</code></pre> <p>For example, the user first asks \"What's the weather in New York?\", then follows up with \"What's the weather in London?\". We are currently in the second round of conversation and about to make the final model call.</p> <ul> <li>When the value is <code>never</code></li> </ul> <p>There will be no <code>reasoning_content</code> fields in the messages passed to the model. The messages the model receives are:</p> <pre><code>messages = [\n    {\"content\": \"Check the weather in New York?\", \"role\": \"user\"},\n    {\"content\": \"\", \"role\": \"assistant\", \"tool_calls\": [...]},\n    {\"content\": \"Cloudy 7~13\u00b0C\", \"role\": \"tool\", \"tool_call_id\": \"...\"},\n    {\"content\": \"The weather in New York today is cloudy, 7~13\u00b0C.\", \"role\": \"assistant\"},\n    {\"content\": \"Check the weather in London?\", \"role\": \"user\"},\n    {\"content\": \"\", \"role\": \"assistant\", \"tool_calls\": [...]},\n    {\"content\": \"Rainy, 14~20\u00b0C\", \"role\": \"tool\", \"tool_call_id\": \"...\"},\n]\n</code></pre> <ul> <li>When the value is <code>current</code></li> </ul> <p>Only retain the <code>reasoning_content</code> field from the current conversation. This strategy is suitable for Interleaved Thinking scenarios, where the model alternates between explicit reasoning and tool calls, requiring retention of reasoning content from the current round. The messages the model receives are: <pre><code>messages = [\n    {\"content\": \"Check the weather in New York?\", \"role\": \"user\"},\n    {\"content\": \"\", \"role\": \"assistant\", \"tool_calls\": [...]},\n    {\"content\": \"Cloudy 7~13\u00b0C\", \"role\": \"tool\", \"tool_call_id\": \"...\"},\n    {\"content\": \"The weather in New York today is cloudy, 7~13\u00b0C.\", \"role\": \"assistant\"},\n    {\"content\": \"Check the weather in London?\", \"role\": \"user\"},\n    {\n        \"content\": \"\",\n        \"reasoning_content\": \"Check London weather, need to directly call weather tool.\",  # Only retain current round's reasoning_content\n        \"role\": \"assistant\",\n        \"tool_calls\": [...],\n    },\n    {\"content\": \"Rainy, 14~20\u00b0C\", \"role\": \"tool\", \"tool_call_id\": \"...\"},\n]\n</code></pre></p> <ul> <li>When the value is <code>all</code></li> </ul> <p>Retain the <code>reasoning_content</code> field from all conversations. The messages the model receives are: <pre><code>messages = [\n    {\"content\": \"Check the weather in New York?\", \"role\": \"user\"},\n    {\n        \"content\": \"\",\n        \"reasoning_content\": \"Check New York weather, need to directly call weather tool.\",  # Retain reasoning_content\n        \"role\": \"assistant\",\n        \"tool_calls\": [...],\n    },\n    {\"content\": \"Cloudy 7~13\u00b0C\", \"role\": \"tool\", \"tool_call_id\": \"...\"},\n    {\n        \"content\": \"The weather in New York today is cloudy, 7~13\u00b0C.\",\n        \"reasoning_content\": \"Directly return New York weather result.\",  # Retain reasoning_content\n        \"role\": \"assistant\",\n    },\n    {\"content\": \"Check the weather in London?\", \"role\": \"user\"},\n    {\n        \"content\": \"\",\n        \"reasoning_content\": \"Check London weather, need to directly call weather tool.\",  # Retain reasoning_content\n        \"role\": \"assistant\",\n        \"tool_calls\": [...],\n    },\n    {\"content\": \"Rainy, 14~20\u00b0C\", \"role\": \"tool\", \"tool_call_id\": \"...\"},\n]\n</code></pre></p> <p>Note: If the current round does not involve tool calls, <code>current</code> and <code>never</code> have the same effect.</p> <p>It's worth noting that although this parameter is a compatibility configuration item, different models from the same provider, or even the same model in different scenarios, may require different <code>reasoning_content</code> retention policies. Therefore, it is recommended to explicitly specify it during instantiation, and it's not necessary to assign a value when creating the class.</p> <p>For example, for the GLM-4.7-Flash model, since it supports Interleaved Thinking mode, you generally need to set <code>reasoning_keep_policy</code> to <code>current</code> during instantiation to retain only the current round's <code>reasoning_content</code>. For example:</p> <p><pre><code>from langchain_core.messages import HumanMessage\n\nmodel = ChatVLLM(model=\"glm-4.7-flash\", reasoning_keep_policy=\"current\")\nagent = create_agent(\n    model=model,\n    tools=[get_current_weather],\n)\nresponse = agent.invoke({\"messages\": [HumanMessage(content=\"Check the weather in New York?\")]})\nprint(response)\n</code></pre> Additionally, the GLM-4.7-Flash model also supports another thinking mode called Preserved Thinking. This requires retaining all <code>reasoning_content</code> fields from historical messages, so you can set <code>reasoning_keep_policy</code> to <code>all</code>. For example:</p> <pre><code>from langchain_core.messages import HumanMessage\n\nmodel = ChatVLLM(\n    model=\"glm-4.7-flash\",\n    reasoning_keep_policy=\"all\",\n    extra_body={\"chat_template_kwargs\": {\"clear_thinking\": False}},\n)\n\nagent = create_agent(\n    model=model,\n    tools=[get_current_weather],\n)\nresponse = agent.invoke({\"messages\": [HumanMessage(content=\"Check the weather in New York?\")]})\nprint(response)\n</code></pre>"},{"location":"adavance-guide/openai-compatible/chat/#model-profiles","title":"Model Profiles","text":"<p>You can get the model's profile via <code>model.profile</code>. By default, it returns an empty dictionary.</p> <p>You can also explicitly pass a <code>profile</code> parameter during instantiation to specify the model profile.</p> <p>For example: <pre><code>from langchain_core.messages import HumanMessage\n\ncustom_profile = {\n    \"max_input_tokens\": 100_000,\n    \"tool_calling\": True,\n    \"structured_output\": True,\n    # ...\n}\nmodel = ChatVLLM(model=\"qwen3-4b\", profile=custom_profile)\nprint(model.profile)\n</code></pre> Or directly pass the <code>profile</code> parameter for all models of the provider during creation.</p> <p>For example: <pre><code>from langchain_dev_utils.chat_models.adapters import create_openai_compatible_model\n\nmodel_profiles = {\n    \"qwen3-4b\": {\n        \"max_input_tokens\": 131072,\n        \"max_output_tokens\": 8192,\n        \"image_inputs\": False,\n        \"audio_inputs\": False,\n        \"video_inputs\": False,\n        \"image_outputs\": False,\n        \"audio_outputs\": False,\n        \"video_outputs\": False,\n        \"reasoning_output\": True,\n        \"tool_calling\": True,\n    }\n    # More model profiles can be added here\n}\n\nChatVLLM = create_openai_compatible_model(\n    model_provider=\"vllm\",\n    base_url=\"http://localhost:8000/v1\",\n    model_profiles=model_profiles,\n)\n\nmodel = ChatVLLM(\n    model=\"qwen3-4b\",\n)\nprint(model.profile)\n</code></pre></p>"},{"location":"adavance-guide/openai-compatible/chat/#support-for-openais-latest-responses-api","title":"Support for OpenAI's Latest Responses API","text":"<p>This model class also supports OpenAI's latest <code>responses</code> API (parameter name <code>use_responses_api</code>). Currently, only a few providers support this style of interface; if your provider supports it, you can enable it via <code>use_responses_api=True</code>.</p> <p>For example, if vLLM supports the <code>responses</code> API, you can use it like this:</p> <pre><code>from langchain_core.messages import HumanMessage\n\nmodel = ChatVLLM(model=\"qwen3-4b\", use_responses_api=True)\nresponse = model.invoke([HumanMessage(content=\"Hello\")])\nprint(response)\n</code></pre> <p>Currently, the implementation of this feature relies entirely on <code>BaseChatOpenAI</code>'s implementation of the <code>responses</code> API, so there may be certain compatibility issues during use. Subsequent optimizations will be made based on actual circumstances.</p> <p>Note</p> <p>This library currently cannot guarantee 100% compatibility with all OpenAI-compatible interfaces (although compatibility configurations can improve compatibility). If the model provider has an official or community integration class, please prioritize using that integration class. If you encounter any compatibility issues, feel free to submit an issue on this library's GitHub repository.</p> <p>Note</p> <p>This function uses <code>pydantic.create_model</code> under the hood to create chat model classes, which incurs some performance overhead. Additionally, <code>create_openai_compatible_model</code> uses a global dictionary to record the <code>profiles</code> of each model provider. To avoid multi-threading concurrency issues, it is recommended to create integration classes during project startup and avoid dynamic creation afterward.</p> <p>Best Practice</p> <p>When connecting to an OpenAI-compatible API chat model provider, you can directly use <code>langchain-openai</code>'s <code>ChatOpenAI</code> and point <code>base_url</code> and <code>api_key</code> to your provider's service. This method is simple enough and suitable for relatively simple scenarios (especially when using ordinary chat models rather than reasoning models).</p> <p>However, it may have the following issues:</p> <ol> <li> <p>Cannot display the chain of thought (i.e., content returned by <code>reasoning_content</code>) of non-OpenAI official reasoning models.</p> </li> <li> <p>Does not support <code>video</code> type content_block.</p> </li> <li> <p>Lower coverage for default structured output strategies.</p> </li> </ol> <p>When you encounter the above differences, you can use the OpenAI-compatible integration classes provided by this library for adaptation.</p>"},{"location":"adavance-guide/openai-compatible/embedding/","title":"Creation and Usage of Embedding Models","text":""},{"location":"adavance-guide/openai-compatible/embedding/#creating-embedding-model-classes","title":"Creating Embedding Model Classes","text":"<p>Similar to chat model classes, you can use <code>create_openai_compatible_embedding</code> to create an integrated embedding model class. This function accepts the following parameters:</p> Parameter Description <code>embedding_provider</code> Embedding model provider name, e.g., <code>vllm</code>. Must start with a letter or number, can only contain letters, numbers, and underscores, with a maximum length of 20 characters.Type: <code>str</code>Required: Yes <code>base_url</code> Default API endpoint for the model provider.Type: <code>str</code>Required: No <code>embedding_model_cls_name</code> Embedding model class name (must comply with Python class naming conventions). Default value is <code>{Provider}Embeddings</code> (where <code>{Provider}</code> is the capitalized provider name).Type: <code>str</code>Required: No <p>Similarly, we use <code>create_openai_compatible_embedding</code> to integrate vLLM's embedding model.</p> <pre><code>from langchain_dev_utils.embeddings.adapters import create_openai_compatible_embedding\n\nVLLMEmbeddings = create_openai_compatible_embedding(\n    embedding_provider=\"vllm\",\n    base_url=\"http://localhost:8000/v1\",\n    embedding_model_cls_name=\"VLLMEmbeddings\",\n)\n\nembedding = VLLMEmbeddings(model=\"qwen3-embedding-4b\")\nprint(embedding.embed_query(\"Hello\"))\n</code></pre> <p><code>base_url</code> can also be omitted. If not provided, the library will read the environment variable <code>VLLM_API_BASE</code> by default:</p> <pre><code>export VLLM_API_BASE=\"http://localhost:8000/v1\"\n</code></pre> <p>At this point, the code can omit <code>base_url</code>:</p> <pre><code>from langchain_dev_utils.embeddings.adapters import create_openai_compatible_embedding\n\nVLLMEmbeddings = create_openai_compatible_embedding(\n    embedding_provider=\"vllm\",\n    embedding_model_cls_name=\"VLLMEmbeddings\",\n)\n\nembedding = VLLMEmbeddings(model=\"qwen3-embedding-4b\")\nprint(embedding.embed_query(\"Hello\"))\n</code></pre> <p>Note: The above code successfully runs assuming the environment variable <code>VLLM_API_KEY</code> is configured. Although vLLM itself does not require an API Key, the embedding model class initialization requires one. Therefore, please set this variable first, for example:</p> <pre><code>export VLLM_API_KEY=vllm_api_key\n</code></pre>"},{"location":"adavance-guide/openai-compatible/embedding/#using-the-embedding-model-class","title":"Using the Embedding Model Class","text":"<p>Here, we use the previously created <code>VLLMEmbeddings</code> class to initialize an embedding model instance.</p>"},{"location":"adavance-guide/openai-compatible/embedding/#vectorizing-queries","title":"Vectorizing Queries","text":"<pre><code>embedding = VLLMEmbeddings(model=\"qwen3-embedding-4b\")\nprint(embedding.embed_query(\"Hello\"))\n</code></pre> <p>Similarly, asynchronous invocation is also supported:</p> <pre><code>embedding = VLLMEmbeddings(model=\"qwen3-embedding-4b\")\nres = await embedding.aembed_query(\"Hello\")\nprint(res)\n</code></pre>"},{"location":"adavance-guide/openai-compatible/embedding/#vectorizing-a-list-of-strings","title":"Vectorizing a List of Strings","text":"<p><pre><code>documents = [\"Hello\", \"Hello, I am Zhang San\"]\nembedding = VLLMEmbeddings(model=\"qwen3-embedding-4b\")\nprint(embedding.embed_documents(documents))\n</code></pre> Similarly, asynchronous invocation is also supported:</p> <pre><code>documents = [\"Hello\", \"Hello, I am Zhang San\"]\nembedding = VLLMEmbeddings(model=\"qwen3-embedding-4b\")\nres = await embedding.aembed_documents(documents)\nprint(res)\n</code></pre> <p>Embedding Model Compatibility Notes</p> <p>OpenAI-compatible embedding APIs generally exhibit good compatibility, but the following differences should be noted:</p> <ol> <li> <p><code>check_embedding_ctx_length</code>: Set to <code>True</code> only when using the official OpenAI embedding service; for all other embedding models, set it to <code>False</code>.</p> </li> <li> <p><code>dimensions</code>: If the model supports custom vector dimensions (e.g., 1024, 4096), you can directly pass this parameter.</p> </li> <li> <p><code>chunk_size</code>: The maximum number of texts that can be processed in a single API call. For example, a <code>chunk_size</code> of 10 means a single request can vectorize up to 10 texts.</p> </li> <li> <p>Single-text token limit: Cannot be controlled via parameters; must be ensured during preprocessing and chunking stages.</p> </li> </ol> <p>Note</p> <p>Similarly, this function uses <code>pydantic.create_model</code> under the hood to create embedding model classes, which incurs some performance overhead. It is recommended to create integration classes during project startup and avoid dynamic creation afterward.</p> <p>Best Practice</p> <p>When connecting to an OpenAI-compatible API embedding model provider, you can directly use <code>langchain-openai</code>'s <code>OpenAIEmbeddings</code> and point <code>base_url</code> and <code>api_key</code> to your provider's service. Embedding model API compatibility is usually better: in most cases, you can directly use <code>OpenAIEmbeddings</code> with <code>check_embedding_ctx_length=False</code>.</p>"},{"location":"adavance-guide/openai-compatible/overview/","title":"Overview","text":"<p>Prerequisites</p> <p>To use this feature, you must install the standard version of the <code>langchain-dev-utils</code> library. Please refer to the installation section for details.</p> <p>Many model providers offer OpenAI Compatible API services, such as vLLM, OpenRouter, and Together AI. This library provides an OpenAI Compatible API integration solution covering both chat models and embedding models. It is especially suitable for scenarios where \"a provider offers an OpenAI Compatible API but lacks corresponding LangChain integration.\"</p> <p>This library provides two utility functions for creating chat model and embedding model integration classes:</p> Function Name Description <code>create_openai_compatible_model</code> Creates a chat model integration class <code>create_openai_compatible_embedding</code> Creates an embedding model integration class <p>Note</p> <p>The initial inspiration for the two utility functions provided by this library came from the JavaScript ecosystem's @ai-sdk/openai-compatible.</p> <p>This documentation will use integrating vLLM as an example to demonstrate how to use this feature.</p> vLLM Introduction <p>vLLM is a commonly used large model inference framework, ideal for high-performance inference services in local or self-hosted environments. It can deploy large models as OpenAI compatible APIs, facilitating the reuse of existing SDKs and calling methods. It supports deployment for both chat models and embedding models, as well as capabilities like multi-model serving, tool calling, and inference output, making it suitable for scenarios such as dialogue, tool calling, and multimodal tasks.</p> <p>The following examples are model deployment commands that will be used later in the content:</p> <p>Qwen3-4B:</p> <pre><code>vllm serve Qwen/Qwen3-4B \\\n--reasoning-parser qwen3 \\\n--enable-auto-tool-choice --tool-call-parser hermes \\\n--host 0.0.0.0 --port 8000 \\\n--served-model-name qwen3-4b\n</code></pre> <p>GLM-4.7-Flash:</p> <pre><code>vllm serve zai-org/GLM-4.7-Flash \\\n --tensor-parallel-size 4 \\\n --speculative-config.method mtp \\\n --speculative-config.num_speculative_tokens 1 \\\n --tool-call-parser glm47 \\\n --reasoning-parser glm45 \\\n --enable-auto-tool-choice \\\n --served-model-name glm-4.7-flash\n</code></pre> <p>Qwen3-VL-2B-Instruct:</p> <pre><code>vllm serve Qwen/Qwen3-VL-2B-Instruct \\\n--trust-remote-code \\\n--host 0.0.0.0 --port 8000 \\\n--served-model-name qwen3-vl-2b\n</code></pre> <p>Qwen3-Embedding-4B:</p> <p><pre><code>vllm serve Qwen/Qwen3-Embedding-4B \\\n--task embed \\\n--served-model-name qwen3-embedding-4b \\\n--host 0.0.0.0 --port 8000\n</code></pre> The service address is <code>http://localhost:8000/v1</code>.</p>"},{"location":"adavance-guide/openai-compatible/register/","title":"Integration with Model Management Functionality","text":"<p>This library seamlessly integrates this feature with model management functionality. When registering a chat model, simply set <code>chat_model</code> to <code>\"openai-compatible\"</code>; when registering an embedding model, set <code>embeddings_model</code> to <code>\"openai-compatible\"</code>.</p>"},{"location":"adavance-guide/openai-compatible/register/#chat-model-class-registration","title":"Chat Model Class Registration","text":"<p>The specific code is as follows:</p> <p>Method 1: Explicit parameter passing</p> <pre><code>from langchain_dev_utils.chat_models import register_model_provider\n\nregister_model_provider(\n    provider_name=\"vllm\",\n    chat_model=\"openai-compatible\",\n    base_url=\"http://localhost:8000/v1\"\n)\n</code></pre> <p>Method 2: Through environment variables (recommended for configuration management)</p> <pre><code>from langchain_dev_utils.chat_models import register_model_provider\n\nregister_model_provider(\n    provider_name=\"vllm\",\n    chat_model=\"openai-compatible\"\n    # Automatically reads VLLM_API_BASE\n)\n</code></pre> <p>Additionally, the <code>base_url</code>, <code>compatibility_options</code>, and <code>model_profiles</code> parameters from the <code>create_openai_compatible_model</code> function are also supported. You just need to pass the corresponding parameters in the <code>register_model_provider</code> function.</p> <p>For example:</p> <pre><code>from langchain_dev_utils.chat_models import register_model_provider\n\nregister_model_provider(\n    provider_name=\"vllm\",\n    chat_model=\"openai-compatible\",\n    base_url=\"http://localhost:8000/v1\",\n    compatibility_options={\n        \"supported_tool_choice\": [\"auto\", \"none\", \"required\", \"specific\"],\n        \"supported_response_format\": [\"json_schema\"]\n    },\n    model_profiles=model_profiles,\n)\n</code></pre>"},{"location":"adavance-guide/openai-compatible/register/#embedding-model-class-registration","title":"Embedding Model Class Registration","text":"<p>Similar to chat model class registration:</p> <p>Method 1: Explicit parameter passing</p> <pre><code>from langchain_dev_utils.embeddings import register_embeddings_provider\n\nregister_embeddings_provider(\n    provider_name=\"vllm\",\n    embeddings_model=\"openai-compatible\",\n    base_url=\"http://localhost:8000/v1\",\n)\n</code></pre> <p>Method 2: Environment variables (recommended)</p> <pre><code>export VLLM_API_BASE=http://localhost:8000/v1\n</code></pre> <pre><code>from langchain_dev_utils.embeddings import register_embeddings_provider\n\nregister_embeddings_provider(\n    provider_name=\"vllm\",\n    embeddings_model=\"openai-compatible\"\n)\n</code></pre>"},{"location":"api-reference/agent/","title":"Agent Module API Reference","text":""},{"location":"api-reference/agent/#create_agent","title":"create_agent","text":"<p>Creates an agent with the same functionality as the official <code>langchain</code> <code>create_agent</code>, but extends the model specification to a string.</p>"},{"location":"api-reference/agent/#function-signature","title":"Function Signature","text":"<pre><code>def create_agent(  # noqa: PLR0915\n    model: str,\n    tools: Sequence[BaseTool | Callable | dict[str, Any]] | None = None,\n    *,\n    system_prompt: str | SystemMessage | None = None,\n    response_format: ResponseFormat[ResponseT] | type[ResponseT] | None = None,\n    middleware: Sequence[AgentMiddleware[StateT_co, ContextT]] = (),\n    state_schema: type[AgentState[ResponseT]] | None = None,\n    context_schema: type[ContextT] | None = None,\n    checkpointer: Checkpointer | None = None,\n    store: BaseStore | None = None,\n    interrupt_before: list[str] | None = None,\n    interrupt_after: list[str] | None = None,\n    debug: bool = False,\n    name: str | None = None,\n    cache: BaseCache | None = None,\n) -&gt; CompiledStateGraph[\n    AgentState[ResponseT], ContextT, _InputAgentState, _OutputAgentState[ResponseT]\n]:\n</code></pre>"},{"location":"api-reference/agent/#parameters","title":"Parameters","text":"Parameter Type Required Default Description model str Yes - Model identifier string that can be loaded by <code>load_chat_model</code>. Can be specified in \"provider:model-name\" format tools Sequence[BaseTool | Callable | dict[str, Any]] | None No None List of tools available to the agent system_prompt str | SystemMessage | None No None Custom system prompt for the agent middleware Sequence[AgentMiddleware[AgentState[ResponseT], ContextT]] No () Agent middleware response_format ResponseFormat[ResponseT] | type[ResponseT] | None No None Response format for the agent state_schema type[AgentState[ResponseT]] | None No None State schema for the agent context_schema type[ContextT] | None No None Context schema for the agent checkpointer Checkpointer | None No None Checkpointer for state persistence store BaseStore | None No None Storage for data persistence interrupt_before list[str] | None No None Nodes to interrupt before execution interrupt_after list[str] | None No None Nodes to interrupt after execution debug bool No False Enable debug mode name str | None No None Agent name cache BaseCache | None No None Cache"},{"location":"api-reference/agent/#notes","title":"Notes","text":"<p>This function provides the same functionality as the official <code>langchain</code> <code>create_agent</code>, but extends the model selection. The main difference is that the <code>model</code> parameter must be a string that can be loaded by the <code>load_chat_model</code> function, allowing more flexible model selection using registered model providers.</p>"},{"location":"api-reference/agent/#example","title":"Example","text":"<pre><code>agent = create_agent(model=\"vllm:qwen3-4b\", tools=[get_current_time])\n</code></pre>"},{"location":"api-reference/agent/#wrap_agent_as_tool","title":"wrap_agent_as_tool","text":"<p>Wraps an agent as a tool.</p>"},{"location":"api-reference/agent/#function-signature_1","title":"Function Signature","text":"<pre><code>def wrap_agent_as_tool(\n    agent: CompiledStateGraph,\n    tool_name: Optional[str] = None,\n    tool_description: Optional[str] = None,\n    pre_input_hooks: Optional[\n        tuple[\n            Callable[[str, ToolRuntime], str | dict[str, Any]],\n            Callable[[str, ToolRuntime], Awaitable[str | dict[str, Any]]],\n        ]\n        | Callable[[str, ToolRuntime], str | dict[str, Any]]\n    ] = None,\n    post_output_hooks: Optional[\n        tuple[\n            Callable[[str, dict[str, Any], ToolRuntime], Any],\n            Callable[[str, dict[str, Any], ToolRuntime], Awaitable[Any]],\n        ]\n        | Callable[[str, dict[str, Any], ToolRuntime], Any]\n    ] = None,\n) -&gt; BaseTool:\n</code></pre>"},{"location":"api-reference/agent/#parameters_1","title":"Parameters","text":"Parameter Type Required Default Description agent CompiledStateGraph Yes - The agent graph to wrap tool_name Optional[str] No None Name of the resulting tool tool_description Optional[str] No None Description of the resulting tool pre_input_hooks - No None Hooks for processing input before passing to the agent post_output_hooks - No None Hooks for processing output after receiving from the agent"},{"location":"api-reference/agent/#example_1","title":"Example","text":"<pre><code>tool = wrap_agent_as_tool(agent)\n</code></pre>"},{"location":"api-reference/agent/#wrap_all_agents_as_tool","title":"wrap_all_agents_as_tool","text":"<p>Wraps all agents as a single tool.</p>"},{"location":"api-reference/agent/#function-signature_2","title":"Function Signature","text":"<pre><code>def wrap_all_agents_as_tool(\n    agents: list[CompiledStateGraph],\n    tool_name: Optional[str] = None,\n    tool_description: Optional[str] = None,\n    pre_input_hooks: Optional[\n        tuple[\n            Callable[[str, ToolRuntime], str | dict[str, Any]],\n            Callable[[str, ToolRuntime], Awaitable[str | dict[str, Any]]],\n        ]\n        | Callable[[str, ToolRuntime], str | dict[str, Any]]\n    ] = None,\n    post_output_hooks: Optional[\n        tuple[\n            Callable[[str, dict[str, Any], ToolRuntime], Any],\n            Callable[[str, dict[str, Any], ToolRuntime], Awaitable[Any]],\n        ]\n        | Callable[[str, dict[str, Any], ToolRuntime], Any]\n    ] = None,\n) -&gt; BaseTool:\n</code></pre>"},{"location":"api-reference/agent/#parameters_2","title":"Parameters","text":"Parameter Type Required Default Description agents list[CompiledStateGraph] Yes - List of agents (must contain at least 2 agents, and each agent must have a unique name) tool_name Optional[str] No None Name of the resulting tool tool_description Optional[str] No None Description of the resulting tool pre_input_hooks - No None Hooks for processing input before passing to the agents post_output_hooks - No None Hooks for processing output after receiving from the agents"},{"location":"api-reference/agent/#example_2","title":"Example","text":"<pre><code>tool = wrap_all_agents_as_tool([time_agent, weather_agent])\n</code></pre>"},{"location":"api-reference/agent/#summarizationmiddleware","title":"SummarizationMiddleware","text":"<p>Middleware for agent context summarization.</p>"},{"location":"api-reference/agent/#class-definition","title":"Class Definition","text":"<pre><code>class SummarizationMiddleware(_SummarizationMiddleware):\n    def __init__(\n        self,\n        model: str,\n        *,\n        trigger: ContextSize | list[ContextSize] | None = None,\n        keep: ContextSize = (\"messages\", _DEFAULT_MESSAGES_TO_KEEP),\n        token_counter: TokenCounter = count_tokens_approximately,\n        summary_prompt: str = DEFAULT_SUMMARY_PROMPT,\n        trim_tokens_to_summarize: int | None = _DEFAULT_TRIM_TOKEN_LIMIT,\n        **deprecated_kwargs: Any,\n    ) -&gt; None\n</code></pre>"},{"location":"api-reference/agent/#parameters_3","title":"Parameters","text":"Parameter Type Required Default Description model str Yes - Model identifier string that can be loaded by <code>load_chat_model</code>. Can be specified in \"provider:model-name\" format trigger ContextSize | list[ContextSize] | None No None Context size threshold that triggers summarization keep ContextSize No (\"messages\", _DEFAULT_MESSAGES_TO_KEEP) Context size to preserve after summarization token_counter TokenCounter No count_tokens_approximately Token counting function to use summary_prompt str No DEFAULT_SUMMARY_PROMPT System prompt used for summarization trim_tokens_to_summarize int | None No _DEFAULT_TRIM_TOKEN_LIMIT Number of tokens to trim from the context before summarizing"},{"location":"api-reference/agent/#example_3","title":"Example","text":"<pre><code>summarization_middleware = SummarizationMiddleware(model=\"vllm:qwen3-4b\")\n</code></pre>"},{"location":"api-reference/agent/#llmtoolselectormiddleware","title":"LLMToolSelectorMiddleware","text":"<p>Middleware for agent tool selection.</p>"},{"location":"api-reference/agent/#class-definition_1","title":"Class Definition","text":"<pre><code>class LLMToolSelectorMiddleware(_LLMToolSelectorMiddleware):\n    def __init__(\n        self,\n        *,\n        model: str,\n        system_prompt: Optional[str] = None,\n        max_tools: Optional[int] = None,\n        always_include: Optional[list[str]] = None,\n    ) -&gt; None\n</code></pre>"},{"location":"api-reference/agent/#parameters_4","title":"Parameters","text":"Parameter Type Required Default Description model str Yes - Model identifier string that can be loaded by <code>load_chat_model</code>. Can be specified in \"provider:model-name\" format system_prompt Optional[str] No None System prompt for the selection model max_tools Optional[int] No None Maximum number of tools to select always_include Optional[list[str]] No None List of tool names to always include in the selection"},{"location":"api-reference/agent/#example_4","title":"Example","text":"<pre><code>llm_tool_selector_middleware = LLMToolSelectorMiddleware(model=\"vllm:qwen3-4b\")\n</code></pre>"},{"location":"api-reference/agent/#planmiddleware","title":"PlanMiddleware","text":"<p>Middleware for agent plan management.</p>"},{"location":"api-reference/agent/#class-definition_2","title":"Class Definition","text":"<pre><code>class PlanMiddleware(AgentMiddleware):\n    state_schema = PlanState\n    def __init__(\n        self,\n        *,\n        system_prompt: Optional[str] = None,\n        custom_plan_tool_descriptions: Optional[PlanToolDescription] = None,\n        use_read_plan_tool: bool = True,\n    ) -&gt; None\n</code></pre>"},{"location":"api-reference/agent/#parameters_5","title":"Parameters","text":"Parameter Type Required Default Description system_prompt Optional[str] No None System prompt for the planning agent custom_plan_tool_descriptions Optional[PlanToolDescription] No None Custom descriptions for plan-related tools use_read_plan_tool bool No True Whether to enable the tool that allows the model to read the current plan"},{"location":"api-reference/agent/#example_5","title":"Example","text":"<pre><code>plan_middleware = PlanMiddleware()\n</code></pre>"},{"location":"api-reference/agent/#modelfallbackmiddleware","title":"ModelFallbackMiddleware","text":"<p>Middleware for agent model fallback.</p>"},{"location":"api-reference/agent/#class-definition_3","title":"Class Definition","text":"<pre><code>class ModelFallbackMiddleware(_ModelFallbackMiddleware):\n    def __init__(\n        self,\n        first_model: str,\n        *additional_models: str,\n    ) -&gt; None\n</code></pre>"},{"location":"api-reference/agent/#parameters_6","title":"Parameters","text":"Parameter Type Required Default Description first_model str Yes - Primary model identifier string (can be loaded by <code>load_chat_model</code>) additional_models str No - Fallback model identifier strings to use if the primary model fails"},{"location":"api-reference/agent/#example_6","title":"Example","text":"<pre><code>model_fallback_middleware = ModelFallbackMiddleware(\n    \"vllm:qwen3-4b\",\n    \"vllm:qwen3-8b\"\n)\n</code></pre>"},{"location":"api-reference/agent/#llmtoolemulator","title":"LLMToolEmulator","text":"<p>Middleware for simulating tool calls using large language models.</p>"},{"location":"api-reference/agent/#class-definition_4","title":"Class Definition","text":"<pre><code>class LLMToolEmulator(_LLMToolEmulator):\n    def __init__(\n        self,\n        *,\n        model: str,\n        tools: list[str | BaseTool] | None = None,\n    ) -&gt; None\n</code></pre>"},{"location":"api-reference/agent/#parameters_7","title":"Parameters","text":"Parameter Type Required Default Description model str Yes - Model identifier string that can be loaded by <code>load_chat_model</code>. Can be specified in \"provider:model-name\" format tools list[str | BaseTool] | None No None List of tools to be emulated by the LLM"},{"location":"api-reference/agent/#example_7","title":"Example","text":"<pre><code>llm_tool_emulator = LLMToolEmulator(model=\"vllm:qwen3-4b\", tools=[get_current_time])\n</code></pre>"},{"location":"api-reference/agent/#modelroutermiddleware","title":"ModelRouterMiddleware","text":"<p>Middleware for dynamically routing to appropriate models based on input content.</p>"},{"location":"api-reference/agent/#class-definition_5","title":"Class Definition","text":"<pre><code>class ModelRouterMiddleware(AgentMiddleware):\n    state_schema = ModelRouterState\n    def __init__(\n        self,\n        router_model: str | BaseChatModel,\n        model_list: list[ModelDict],\n        router_prompt: Optional[str] = None,\n    ) -&gt; None\n</code></pre>"},{"location":"api-reference/agent/#parameters_8","title":"Parameters","text":"Parameter Type Required Default Description router_model str | BaseChatModel Yes - Model used for routing. Accepts a string (loaded via <code>load_chat_model</code>) or a ChatModel instance model_list list[ModelDict] Yes - List of available models. Each entry must contain <code>model_name</code> and <code>model_description</code>, and optionally <code>tools</code>, <code>model_kwargs</code>, <code>model_instance</code>, and <code>model_system_prompt</code> router_prompt Optional[str] No None Custom prompt for the router model. Uses default prompt if not provided"},{"location":"api-reference/agent/#example_8","title":"Example","text":"<pre><code>model_router_middleware = ModelRouterMiddleware(\n    router_model=\"vllm:qwen3-4b\",\n    model_list=[\n        {\n            \"model_name\": \"vllm:qwen3-4b\",\n            \"model_description\": \"Suitable for general tasks such as conversation, text generation, etc.\"\n        },\n        {\n            \"model_name\": \"vllm:qwen3-8b\",\n            \"model_description\": \"Suitable for complex tasks such as code generation, data analysis, etc.\",\n        },\n    ]\n)\n</code></pre>"},{"location":"api-reference/agent/#handoffagentmiddleware","title":"HandoffAgentMiddleware","text":"<p>Middleware for implementing multi-agent handoffs.</p>"},{"location":"api-reference/agent/#class-definition_6","title":"Class Definition","text":"<pre><code>class HandoffAgentMiddleware(AgentMiddleware):\n    state_schema = MultiAgentState\n    def __init__(\n        self,\n        agents_config: dict[str, AgentConfig],\n        custom_handoffs_tool_descriptions: Optional[dict[str, str]] = None,\n        handoffs_tool_overrides: Optional[dict[str, BaseTool]] = None,\n    ) -&gt; None\n</code></pre>"},{"location":"api-reference/agent/#parameters_9","title":"Parameters","text":"Parameter Type Required Default Description agents_config dict[str, AgentConfig] Yes - Configuration dictionary for agents. Keys are agent names, values are agent configurations custom_handoffs_tool_descriptions Optional[dict[str, str]] No None Custom descriptions for handoff tools targeting other agents handoffs_tool_overrides Optional[dict[str, BaseTool]] No None Custom handoff tools targeting other agents"},{"location":"api-reference/agent/#example_9","title":"Example","text":"<pre><code>handoffs_agent_middleware = HandoffsAgentMiddleware({\n    \"time_agent\":{\n        \"model\":\"vllm:qwen3-4b\",\n        \"prompt\":\"You are a time agent responsible for answering time-related questions.\",\n        \"tools\":[get_current_time, transfer_to_default_agent],\n        \"handoffs\":[\"default_agent\"]\n    },\n    \"default_agent\":{\n        \"model\":\"vllm:qwen3-8b\",\n        \"prompt\":\"You are a complex task agent responsible for answering complex task-related questions.\",\n        \"default\":True,\n        \"handoffs\":[\"time_agent\"]\n    }\n})\n</code></pre>"},{"location":"api-reference/agent/#toolcallrepairmiddleware","title":"ToolCallRepairMiddleware","text":"<p>Middleware for repairing invalid tool calls.</p>"},{"location":"api-reference/agent/#class-definition_7","title":"Class Definition","text":"<pre><code>class ToolCallRepairMiddleware(AgentMiddleware):\n</code></pre>"},{"location":"api-reference/agent/#example_10","title":"Example","text":"<pre><code>tool_call_repair_middleware = ToolCallRepairMiddleware()\n</code></pre>"},{"location":"api-reference/agent/#formatpromptmiddleware","title":"FormatPromptMiddleware","text":"<p>Middleware for formatting prompts.</p>"},{"location":"api-reference/agent/#function-signature_3","title":"Function Signature","text":"<pre><code>class FormatPromptMiddleware(AgentMiddleware):\n    def __init__(\n        self,\n        *,\n        template_format: Literal[\"f-string\", \"jinja2\"] = \"f-string\",\n    ) -&gt; None:\n</code></pre>"},{"location":"api-reference/agent/#parameters_10","title":"Parameters","text":"Parameter Type Required Default Description template_format Literal[\"f-string\", \"jinja2\"] No \"f-string\" Template syntax. Valid values: <code>f-string</code> or <code>jinja2</code>"},{"location":"api-reference/agent/#example_11","title":"Example","text":"<pre><code>format_prompt_middleware = FormatPromptMiddleware(template_format=\"jinja2\")\n</code></pre>"},{"location":"api-reference/agent/#planstate","title":"PlanState","text":"<p>State Schema for Plan.</p>"},{"location":"api-reference/agent/#class-definition_8","title":"Class Definition","text":"<pre><code>class Plan(TypedDict):\n    content: str\n    status: Literal[\"pending\", \"in_progress\", \"done\"]\n\n\nclass PlanState(AgentState):\n    plan: NotRequired[list[Plan]]\n</code></pre>"},{"location":"api-reference/agent/#attributes","title":"Attributes","text":"Attribute Type Description plan NotRequired[list[Plan]] List of plan steps plan.content str Content of the plan step plan.status Literal[\"pending\", \"in_progress\", \"done\"] Status of the plan step. Valid values are <code>pending</code>, <code>in_progress</code>, <code>done</code>"},{"location":"api-reference/agent/#modeldict","title":"ModelDict","text":"<p>Type definition for the model list.</p>"},{"location":"api-reference/agent/#class-definition_9","title":"Class Definition","text":"<pre><code>class ModelDict(TypedDict):\n    model_name: str\n    model_description: str\n    tools: NotRequired[list[BaseTool | dict[str, Any]]]\n    model_kwargs: NotRequired[dict[str, Any]]\n    model_instance: NotRequired[BaseChatModel]\n    model_system_prompt: NotRequired[str]\n</code></pre>"},{"location":"api-reference/agent/#attributes_1","title":"Attributes","text":"Attribute Type Required Description model_name str Yes Name of the model model_description str Yes Description of the model's capabilities tools NotRequired[list[BaseTool | dict[str, Any]]] No Tools available to this model model_kwargs NotRequired[dict[str, Any]] No Additional keyword arguments to pass to the model model_instance NotRequired[BaseChatModel] No A specific model instance to use model_system_prompt NotRequired[str] No System prompt specific to this model"},{"location":"api-reference/agent/#selectmodel","title":"SelectModel","text":"<p>Tool class for selecting a model.</p>"},{"location":"api-reference/agent/#class-definition_10","title":"Class Definition","text":"<pre><code>class SelectModel(BaseModel):\n    \"\"\"Tool for model selection - Must call this tool to return the finally selected model\"\"\"\n\n    model_name: str = Field(\n        ...,\n        description=\"Selected model name (must be the full model name, for example, openai:gpt-4o)\",\n    )\n</code></pre>"},{"location":"api-reference/agent/#attributes_2","title":"Attributes","text":"Attribute Type Required Description model_name str Yes The name of the selected model (must be the full model name, e.g., openai:gpt-4o)"},{"location":"api-reference/agent/#multiagentstate","title":"MultiAgentState","text":"<p>State Schema for multi-agent handoffs.</p>"},{"location":"api-reference/agent/#class-definition_11","title":"Class Definition","text":"<pre><code>class MultiAgentState(AgentState):\n    active_agent: NotRequired[str]\n</code></pre>"},{"location":"api-reference/agent/#attributes_3","title":"Attributes","text":"Attribute Type Description active_agent NotRequired[str] The name of the currently active agent"},{"location":"api-reference/agent/#agentconfig","title":"AgentConfig","text":"<p>Type definition for agent configuration.</p>"},{"location":"api-reference/agent/#class-definition_12","title":"Class Definition","text":"<pre><code>class AgentConfig(TypedDict):\n    model: NotRequired[str | BaseChatModel]\n    prompt: str | SystemMessage\n    tools: NotRequired[list[BaseTool | dict[str, Any]]]\n    default: NotRequired[bool]\n    handoffs: list[str] | Literal[\"all\"]\n</code></pre>"},{"location":"api-reference/agent/#attributes_4","title":"Attributes","text":"Attribute Type Required Description model NotRequired[str | BaseChatModel] No Model name or model instance prompt str | SystemMessage Yes System prompt for the agent tools list[BaseTool | dict[str, Any]] Yes Tools available to the agent default NotRequired[bool] No Whether this agent is the default fallback handoffs list[str] | Literal[\"all\"] Yes List of agent names to hand off to, or \"all\" to allow handoffs to any agent"},{"location":"api-reference/chat_model/","title":"ChatModel Module API Reference Documentation","text":""},{"location":"api-reference/chat_model/#register_model_provider","title":"register_model_provider","text":"<p>Register a provider for chat models.</p>"},{"location":"api-reference/chat_model/#function-signature","title":"Function Signature","text":"<pre><code>def register_model_provider(\n    provider_name: str,\n    chat_model: ChatModelType,\n    base_url: Optional[str] = None,\n    model_profiles: Optional[dict[str, dict[str, Any]]] = None,\n    compatibility_options: Optional[CompatibilityOptions] = None,\n) -&gt; None:\n</code></pre>"},{"location":"api-reference/chat_model/#parameters","title":"Parameters","text":"Parameter Type Required Default Description provider_name str Yes - Custom provider name chat_model ChatModelType Yes - ChatModel class or supported provider string type base_url Optional[str] No None BaseURL of the provider model_profiles Optional[dict[str, dict[str, Any]]] No None Profiles of models supported by the provider, format: <code>{model_name: model_profile}</code> compatibility_options Optional[CompatibilityOptions] No None Compatibility options"},{"location":"api-reference/chat_model/#example","title":"Example","text":"<pre><code>register_model_provider(\"fakechat\", FakeChatModel)\nregister_model_provider(\"vllm\", \"openai-compatible\", base_url=\"http://localhost:8000/v1\")\n</code></pre>"},{"location":"api-reference/chat_model/#batch_register_model_provider","title":"batch_register_model_provider","text":"<p>Batch register model providers.</p>"},{"location":"api-reference/chat_model/#function-signature_1","title":"Function Signature","text":"<pre><code>def batch_register_model_provider(\n    providers: list[ChatModelProvider],\n) -&gt; None:\n</code></pre>"},{"location":"api-reference/chat_model/#parameters_1","title":"Parameters","text":"Parameter Type Required Default Description providers list[ChatModelProvider] Yes - List of provider configurations"},{"location":"api-reference/chat_model/#example_1","title":"Example","text":"<pre><code>batch_register_model_provider([\n    {\"provider_name\": \"fakechat\", \"chat_model\": FakeChatModel},\n    {\"provider_name\": \"vllm\", \"chat_model\": \"openai-compatible\", \"base_url\": \"http://localhost:8000/v1\"},\n])\n</code></pre>"},{"location":"api-reference/chat_model/#load_chat_model","title":"load_chat_model","text":"<p>Load a chat model from registered providers.</p>"},{"location":"api-reference/chat_model/#function-signature_2","title":"Function Signature","text":"<pre><code>def load_chat_model(\n    model: str,\n    *,\n    model_provider: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; BaseChatModel:\n</code></pre>"},{"location":"api-reference/chat_model/#parameters_2","title":"Parameters","text":"Parameter Type Required Default Description model str Yes - Model name, format: <code>model_name</code> or <code>provider_name:model_name</code> model_provider Optional[str] No None Model provider name **kwargs Any No - Additional model parameters"},{"location":"api-reference/chat_model/#example_2","title":"Example","text":"<pre><code>model = load_chat_model(\"vllm:qwen3-4b\")\n</code></pre>"},{"location":"api-reference/chat_model/#create_openai_compatible_model","title":"create_openai_compatible_model","text":"<p>Create an OpenAI compatible chat model class.</p>"},{"location":"api-reference/chat_model/#function-signature_3","title":"Function Signature","text":"<pre><code>def create_openai_compatible_model(\n    model_provider: str,\n    base_url: Optional[str] = None,\n    compatibility_options: Optional[CompatibilityOptions] = None,\n    model_profiles: Optional[dict[str, dict[str, Any]]] = None,\n    chat_model_cls_name: Optional[str] = None,\n) -&gt; type[BaseChatModel]:\n</code></pre>"},{"location":"api-reference/chat_model/#parameters_3","title":"Parameters","text":"Parameter Type Required Default Description model_provider str Yes - Model provider name base_url Optional[str] No None BaseURL of the model provider compatibility_options Optional[CompatibilityOptions] No None Compatibility options model_profiles Optional[dict[str, dict[str, Any]]] No None Profiles of models supported by the provider, format: <code>{model_name: model_profile}</code> chat_model_cls_name Optional[str] No None Custom chat model class name"},{"location":"api-reference/chat_model/#return-value","title":"Return Value","text":"Type Description type[BaseChatModel] Dynamically created OpenAI compatible chat model class"},{"location":"api-reference/chat_model/#example_3","title":"Example","text":"<pre><code>ChatVLLM = create_openai_compatible_model(\n    model_provider=\"vllm\",\n    base_url=\"http://localhost:8000/v1\",\n    chat_model_cls_name=\"ChatVLLM\",\n)\n</code></pre>"},{"location":"api-reference/chat_model/#chatmodeltype","title":"ChatModelType","text":"<p>Types supported by the <code>chat_model</code> parameter when registering model providers.</p>"},{"location":"api-reference/chat_model/#type-definition","title":"Type Definition","text":"<pre><code>ChatModelType = Union[type[BaseChatModel], Literal[\"openai-compatible\"]]\n</code></pre>"},{"location":"api-reference/chat_model/#toolchoicetype","title":"ToolChoiceType","text":"<p>Types supported by the <code>tool_choice</code> parameter.</p>"},{"location":"api-reference/chat_model/#type-definition_1","title":"Type Definition","text":"<pre><code>ToolChoiceType = list[Literal[\"auto\", \"none\", \"required\", \"specific\"]]\n</code></pre>"},{"location":"api-reference/chat_model/#responseformattype","title":"ResponseFormatType","text":"<p>Types supported by <code>response_format</code>.</p>"},{"location":"api-reference/chat_model/#type-definition_2","title":"Type Definition","text":"<pre><code>ResponseFormatType = list[Literal[\"json_schema\", \"json_mode\"]]\n</code></pre>"},{"location":"api-reference/chat_model/#reasoningkeeppolicy","title":"ReasoningKeepPolicy","text":"<p>Retention policy for the reasoning_content field in the messages list.</p>"},{"location":"api-reference/chat_model/#type-definition_3","title":"Type Definition","text":"<pre><code>ReasoningKeepPolicy = Literal[\"never\", \"current\", \"all\"]\n</code></pre>"},{"location":"api-reference/chat_model/#compatibilityoptions","title":"CompatibilityOptions","text":"<p>Compatibility options for model providers.</p>"},{"location":"api-reference/chat_model/#class-definition","title":"Class Definition","text":"<pre><code>class CompatibilityOptions(TypedDict):\n    supported_tool_choice: NotRequired[ToolChoiceType]\n    supported_response_format: NotRequired[ResponseFormatType]\n    reasoning_keep_policy: NotRequired[ReasoningKeepPolicy]\n    include_usage: NotRequired[bool]\n</code></pre>"},{"location":"api-reference/chat_model/#field-description","title":"Field Description","text":"Field Type Required Description supported_tool_choice NotRequired[ToolChoiceType] No List of supported <code>tool_choice</code> strategies supported_response_format NotRequired[ResponseFormatType] No List of supported <code>response_format</code> methods reasoning_keep_policy NotRequired[ReasoningKeepPolicy] No Retention policy for the <code>reasoning_content</code> field in historical messages (messages) passed to the model. Optional values are <code>never</code>, <code>current</code>, <code>all</code> include_usage NotRequired[bool] No Whether to include <code>usage</code> information in the last streaming response result"},{"location":"api-reference/chat_model/#chatmodelprovider","title":"ChatModelProvider","text":"<p>Chat model provider configuration type.</p>"},{"location":"api-reference/chat_model/#class-definition_1","title":"Class Definition","text":"<pre><code>class ChatModelProvider(TypedDict):\n    provider_name: str\n    chat_model: ChatModelType\n    base_url: NotRequired[str]\n    model_profiles: NotRequired[dict[str, dict[str, Any]]]\n    compatibility_options: NotRequired[CompatibilityOptions]\n</code></pre>"},{"location":"api-reference/chat_model/#field-description_1","title":"Field Description","text":"Field Type Required Description provider_name str Yes Provider name chat_model ChatModelType Yes Support passing chat model class or string (currently only supports <code>openai-compatible</code>) base_url NotRequired[str] No Base URL model_profiles NotRequired[dict[str, dict[str, Any]]] No Profiles of models supported by the provider, format: <code>{model_name: model_profile}</code> compatibility_options NotRequired[CompatibilityOptions] No Model provider compatibility options"},{"location":"api-reference/embeddings/","title":"Embeddings Module API Reference Documentation","text":""},{"location":"api-reference/embeddings/#register_embeddings_provider","title":"register_embeddings_provider","text":"<p>Register a provider for embedding models.</p>"},{"location":"api-reference/embeddings/#function-signature","title":"Function Signature","text":"<pre><code>def register_embeddings_provider(\n    provider_name: str,\n    embeddings_model: EmbeddingsType,\n    base_url: Optional[str] = None,\n) -&gt; None:\n</code></pre>"},{"location":"api-reference/embeddings/#parameters","title":"Parameters","text":"Parameter Type Required Default Description provider_name str Yes - Custom provider name embeddings_model EmbeddingsType Yes - Embedding model class or supported provider string type base_url Optional[str] No None BaseURL of the provider"},{"location":"api-reference/embeddings/#example","title":"Example","text":"<pre><code>register_embeddings_provider(\"fakeembeddings\", FakeEmbeddings)\nregister_embeddings_provider(\"vllm\", \"openai-compatible\", base_url=\"http://localhost:8000/v1\")\n</code></pre>"},{"location":"api-reference/embeddings/#batch_register_embeddings_provider","title":"batch_register_embeddings_provider","text":"<p>Batch register embedding model providers.</p>"},{"location":"api-reference/embeddings/#function-signature_1","title":"Function Signature","text":"<pre><code>def batch_register_embeddings_provider(\n    providers: list[EmbeddingProvider]\n) -&gt; None:\n</code></pre>"},{"location":"api-reference/embeddings/#parameters_1","title":"Parameters","text":"Parameter Type Required Default Description providers list[EmbeddingProvider] Yes - List of provider configurations"},{"location":"api-reference/embeddings/#example_1","title":"Example","text":"<pre><code>batch_register_embeddings_provider([\n    {\"provider_name\": \"fakeembeddings\", \"embeddings_model\": FakeEmbeddings},\n    {\"provider_name\": \"vllm\", \"embeddings_model\": \"openai-compatible\", \"base_url\": \"http://localhost:8000/v1\"},\n])\n</code></pre>"},{"location":"api-reference/embeddings/#load_embeddings","title":"load_embeddings","text":"<p>Load an embedding model from registered providers.</p>"},{"location":"api-reference/embeddings/#function-signature_2","title":"Function Signature","text":"<pre><code>def load_embeddings(\n    model: str,\n    *,\n    provider: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; Embeddings:\n</code></pre>"},{"location":"api-reference/embeddings/#parameters_2","title":"Parameters","text":"Parameter Type Required Default Description model str Yes - Model name, format: <code>model_name</code> or <code>provider_name:model_name</code> provider Optional[str] No None Model provider name **kwargs Any No - Additional model parameters"},{"location":"api-reference/embeddings/#example_2","title":"Example","text":"<pre><code>embeddings = load_embeddings(\"vllm:qwen3-embedding-4b\")\n</code></pre>"},{"location":"api-reference/embeddings/#create_openai_compatible_embedding","title":"create_openai_compatible_embedding","text":"<p>Create an OpenAI compatible embedding model class.</p>"},{"location":"api-reference/embeddings/#function-signature_3","title":"Function Signature","text":"<pre><code>def create_openai_compatible_embedding(\n    embedding_provider: str,\n    base_url: Optional[str] = None,\n    embedding_model_cls_name: Optional[str] = None,\n) -&gt; type[Embeddings]:\n</code></pre>"},{"location":"api-reference/embeddings/#parameters_3","title":"Parameters","text":"Parameter Type Required Default Description embedding_provider str Yes - Embedding model provider name base_url Optional[str] No None BaseURL of the model provider embedding_model_cls_name Optional[str] No None Custom embedding model class name"},{"location":"api-reference/embeddings/#return-value","title":"Return Value","text":"Type Description type[Embeddings] Dynamically created OpenAI compatible embedding model class"},{"location":"api-reference/embeddings/#example_3","title":"Example","text":""},{"location":"api-reference/embeddings/#vllmembeddings-create_openai_compatible_embedding-embedding_providervllm-base_urlhttplocalhost8000v1-embedding_model_cls_namevllmembeddings","title":"<pre><code>VLLMEmbeddings = create_openai_compatible_embedding(\n    embedding_provider=\"vllm\",\n    base_url=\"http://localhost:8000/v1\",\n    embedding_model_cls_name=\"VLLMEmbeddings\",\n)\n</code></pre>","text":""},{"location":"api-reference/embeddings/#embeddingstype","title":"EmbeddingsType","text":"<p>Types supported by the <code>embeddings_model</code> parameter when registering embedding providers.</p>"},{"location":"api-reference/embeddings/#type-definition","title":"Type Definition","text":"<pre><code>EmbeddingsType = Union[type[Embeddings], Literal[\"openai-compatible\"]]\n</code></pre>"},{"location":"api-reference/embeddings/#embeddingprovider","title":"EmbeddingProvider","text":"<p>Embedding model provider configuration type.</p>"},{"location":"api-reference/embeddings/#class-definition","title":"Class Definition","text":"<pre><code>class EmbeddingProvider(TypedDict):\n    provider_name: str\n    embeddings_model: EmbeddingsType\n    base_url: NotRequired[str]\n</code></pre>"},{"location":"api-reference/embeddings/#field-description","title":"Field Description","text":"Field Type Required Description provider_name str Yes Provider name embeddings_model EmbeddingsType Yes Embedding model class or string base_url NotRequired[str] No Base URL"},{"location":"api-reference/graph/","title":"Graph Module API Reference","text":""},{"location":"api-reference/graph/#create_sequential_graph","title":"create_sequential_graph","text":"<p>Combines multiple nodes into a state graph in a serial (sequential) manner.</p>"},{"location":"api-reference/graph/#function-signature","title":"Function Signature","text":"<pre><code>def create_sequential_graph(\n    nodes: list[Node],\n    state_schema: type[StateT],\n    graph_name: Optional[str] = None,\n    context_schema: type[ContextT] | None = None,\n    input_schema: type[InputT] | None = None,\n    output_schema: type[OutputT] | None = None,\n    checkpointer: Checkpointer | None = None,\n    store: BaseStore | None = None,\n    cache: BaseCache | None = None,\n) -&gt; CompiledStateGraph[StateT, ContextT, InputT, OutputT]:\n</code></pre>"},{"location":"api-reference/graph/#parameters","title":"Parameters","text":"Parameter Type Required Default Description nodes list[Node] Yes - The list of nodes to combine, which can be node functions or tuples consisting of a node name and a node function. state_schema type[StateT] Yes - The State Schema of the final generated graph. graph_name Optional[str] No None The name of the final generated graph. context_schema type[ContextT] | None No None The Context Schema of the final generated graph. input_schema type[InputT] | None No None The Input Schema of the final generated graph. output_schema type[OutputT] | None No None The Output Schema of the final generated graph. checkpointer Checkpointer | None No None The Checkpointer of the final generated graph. store BaseStore | None No None The Store of the final generated graph. cache BaseCache | None No None The Cache of the final generated graph."},{"location":"api-reference/graph/#example","title":"Example","text":"<pre><code>create_sequential_graph(\n    nodes=[node1, node2],\n    state_schema=State,\n    graph_name=\"sequential_pipeline\",\n    context_schema=Context,\n    input_schema=Input,\n    output_schema=Output,\n)\n</code></pre>"},{"location":"api-reference/graph/#create_parallel_graph","title":"create_parallel_graph","text":"<p>Combines multiple nodes into a state graph in a parallel manner.</p>"},{"location":"api-reference/graph/#function-signature_1","title":"Function Signature","text":"<pre><code>def create_parallel_graph(\n    nodes: list[Node],\n    state_schema: type[StateT],\n    graph_name: Optional[str] = None,\n    branches_fn: Optional[\n        Union[\n            Callable[..., list[Send]],\n            Callable[..., Awaitable[list[Send]]],\n        ]\n    ] = None,\n    context_schema: type[ContextT] | None = None,\n    input_schema: type[InputT] | None = None,\n    output_schema: type[OutputT] | None = None,\n    checkpointer: Checkpointer | None = None,\n    store: BaseStore | None = None,\n    cache: BaseCache | None = None,\n) -&gt; CompiledStateGraph[StateT, ContextT, InputT, OutputT]:\n</code></pre>"},{"location":"api-reference/graph/#parameters_1","title":"Parameters","text":"Parameter Type Required Default Description nodes list[Node] Yes - The list of nodes to combine, which can be node functions or tuples consisting of a node name and a node function. state_schema type[StateT] Yes - The State Schema of the final generated graph. graph_name Optional[str] No None The name of the final generated graph. branches_fn Optional[Union[Callable[..., list[Send]], Callable[..., Awaitable[list[Send]]]]] No None The parallel branch function that returns a list of Send objects to control parallel execution. context_schema type[ContextT] | None No None The Context Schema of the final generated graph. input_schema type[InputT] | None No None The Input Schema of the final generated graph. output_schema type[OutputT] | None No None The Output Schema of the final generated graph. checkpointer Checkpointer | None No None The Checkpointer of the final generated graph. store BaseStore | None No None The Store of the final generated graph. cache BaseCache | None No None The Cache of the final generated graph."},{"location":"api-reference/graph/#example_1","title":"Example","text":"<pre><code>create_parallel_graph(\n    nodes=[node1, node2],\n    state_schema=State,\n    graph_name=\"parallel_pipeline\",\n    branches_fn=lambda state: [Send(\"node1\", state), Send(\"node2\", state)],\n    context_schema=Context,\n    input_schema=Input,\n    output_schema=Output,\n)\n</code></pre>"},{"location":"api-reference/graph/#node-type","title":"Node Type","text":"<pre><code>Node = StateNode | tuple[str, StateNode]\n</code></pre>"},{"location":"api-reference/message_convert/","title":"Message Convert Module API Reference Documentation","text":""},{"location":"api-reference/message_convert/#convert_reasoning_content_for_ai_message","title":"convert_reasoning_content_for_ai_message","text":"<p>Merges the chain of thought into the final response.</p>"},{"location":"api-reference/message_convert/#function-signature","title":"Function Signature","text":"<pre><code>def convert_reasoning_content_for_ai_message(\n    model_response: AIMessage,\n    think_tag: Tuple[str, str] = (\"&lt;think&gt;\", \"&lt;/think&gt;\"),\n) -&gt; AIMessage\n</code></pre>"},{"location":"api-reference/message_convert/#parameters","title":"Parameters","text":"Parameter Type Required Default Description model_response AIMessage Yes - AI message containing reasoning content think_tag Tuple[str, str] No <code>(\"&lt;think&gt;\",\"&lt;/think&gt;\")</code> Start and end tags for reasoning content"},{"location":"api-reference/message_convert/#example","title":"Example","text":"<pre><code>response = convert_reasoning_content_for_ai_message(\n    response, think_tag=(\"&lt;start&gt;\", \"&lt;end&gt;\")\n)\n</code></pre>"},{"location":"api-reference/message_convert/#convert_reasoning_content_for_chunk_iterator","title":"convert_reasoning_content_for_chunk_iterator","text":"<p>Merges reasoning content for streaming message chunks.</p>"},{"location":"api-reference/message_convert/#function-signature_1","title":"Function Signature","text":"<pre><code>def convert_reasoning_content_for_chunk_iterator(\n    model_response: Iterator[AIMessageChunk | AIMessage],\n    think_tag: Tuple[str, str] = (\"think\", \"think\"),\n) -&gt; Iterator[AIMessageChunk | AIMessage]\n</code></pre>"},{"location":"api-reference/message_convert/#parameters_1","title":"Parameters","text":"Parameter Type Required Default Description model_response Iterator[AIMessageChunk | AIMessage] Yes - Iterator of message chunks think_tag Tuple[str, str] No <code>(\"&lt;think&gt;\",\"&lt;/think&gt;\")</code> Start and end tags for reasoning content"},{"location":"api-reference/message_convert/#example_1","title":"Example","text":"<pre><code>for chunk in convert_reasoning_content_for_chunk_iterator(\n    model.stream(\"Hello\"), think_tag=(\"&lt;think&gt;\", \"&lt;/think&gt;\")\n):\n    print(chunk.content, end=\"\", flush=True)\n</code></pre>"},{"location":"api-reference/message_convert/#aconvert_reasoning_content_for_chunk_iterator","title":"aconvert_reasoning_content_for_chunk_iterator","text":"<p>Asynchronous version of <code>convert_reasoning_content_for_chunk_iterator</code>.</p>"},{"location":"api-reference/message_convert/#function-signature_2","title":"Function Signature","text":"<pre><code>async def aconvert_reasoning_content_for_chunk_iterator(\n    model_response: AsyncIterator[AIMessageChunk | AIMessage],\n    think_tag: Tuple[str, str] = (\"think\", \"think\"),\n) -&gt; AsyncIterator[AIMessageChunk | AIMessage]\n</code></pre>"},{"location":"api-reference/message_convert/#parameters_2","title":"Parameters","text":"Parameter Type Required Default Description model_response AsyncIterator[AIMessageChunk | AIMessage] Yes - Async iterator of message chunks think_tag Tuple[str, str] No <code>(\"&lt;think&gt;\",\"&lt;/think&gt;\")</code> Start and end tags for reasoning content"},{"location":"api-reference/message_convert/#example_2","title":"Example","text":"<pre><code>async for chunk in aconvert_reasoning_content_for_chunk_iterator(\n    model.astream(\"Hello\"), think_tag=(\"&lt;think&gt;\", \"&lt;/think&gt;\")\n):\n    print(chunk.content, end=\"\", flush=True)\n</code></pre>"},{"location":"api-reference/message_convert/#merge_ai_message_chunk","title":"merge_ai_message_chunk","text":"<p>Merges streaming output chunks into a single AIMessage.</p>"},{"location":"api-reference/message_convert/#function-signature_3","title":"Function Signature","text":"<pre><code>def merge_ai_message_chunk(\n    chunks: Sequence[AIMessageChunk]\n) -&gt; AIMessage\n</code></pre>"},{"location":"api-reference/message_convert/#parameters_3","title":"Parameters","text":"Parameter Type Required Default Description chunks Sequence[AIMessageChunk] Yes - List of message chunks to merge"},{"location":"api-reference/message_convert/#example_3","title":"Example","text":"<pre><code>chunks = list(model.stream(\"Hello\"))\nmerged = merge_ai_message_chunk(chunks)\n</code></pre>"},{"location":"api-reference/message_convert/#format_sequence","title":"format_sequence","text":"<p>Formats a list of BaseMessage, Document, or strings into a single string.</p>"},{"location":"api-reference/message_convert/#function-signature_4","title":"Function Signature","text":"<pre><code>def format_sequence(\n    inputs: List[Union[BaseMessage, Document, str]],\n    separator: str = \"-\",\n    with_num: bool = False\n) -&gt; str\n</code></pre>"},{"location":"api-reference/message_convert/#parameters_4","title":"Parameters","text":"Parameter Type Required Default Description inputs List[Union[BaseMessage, Document, str]] Yes - List of items to format separator str No \"-\" Separator string with_num bool No False Whether to add number prefix"},{"location":"api-reference/message_convert/#example_4","title":"Example","text":"<pre><code>formatted = format_sequence(messages, separator=\"\\n\", with_num=True)\n</code></pre>"},{"location":"api-reference/tool_calling/","title":"Tool Calling Module API Reference Documentation","text":""},{"location":"api-reference/tool_calling/#has_tool_calling","title":"has_tool_calling","text":"<p>Checks if a message contains a tool call.</p>"},{"location":"api-reference/tool_calling/#function-signature","title":"Function Signature","text":"<pre><code>def has_tool_calling(\n    message: AIMessage\n) -&gt; bool\n</code></pre>"},{"location":"api-reference/tool_calling/#parameters","title":"Parameters","text":"Parameter Type Required Default Description message AIMessage Yes - The message to check"},{"location":"api-reference/tool_calling/#example","title":"Example","text":"<pre><code>if has_tool_calling(response):\n    # Handle tool call\n    pass\n</code></pre>"},{"location":"api-reference/tool_calling/#parse_tool_calling","title":"parse_tool_calling","text":"<p>Parses tool call arguments from a message.</p>"},{"location":"api-reference/tool_calling/#function-signature_1","title":"Function Signature","text":"<pre><code>def parse_tool_calling(\n    message: AIMessage, first_tool_call_only: bool = False\n) -&gt; Union[tuple[str, dict], list[tuple[str, dict]]]\n</code></pre>"},{"location":"api-reference/tool_calling/#parameters_1","title":"Parameters","text":"Parameter Type Required Default Description message AIMessage Yes - The message to parse first_tool_call_only bool No False Whether to return only the first tool call"},{"location":"api-reference/tool_calling/#example_1","title":"Example","text":"<pre><code># Get all tool calls\ntool_calls = parse_tool_calling(response)\n\n# Get only the first tool call\nname, args = parse_tool_calling(response, first_tool_call_only=True)\n</code></pre>"},{"location":"api-reference/tool_calling/#human_in_the_loop","title":"human_in_the_loop","text":"<p>A decorator to add \"human-in-the-loop\" manual review capability to synchronous tool functions.</p>"},{"location":"api-reference/tool_calling/#function-signature_2","title":"Function Signature","text":"<pre><code>def human_in_the_loop(\n    func: Optional[Callable] = None,\n    *,\n    handler: Optional[HumanInterruptHandler] = None\n) -&gt; Union[Callable[[Callable], BaseTool], BaseTool]\n</code></pre>"},{"location":"api-reference/tool_calling/#parameters_2","title":"Parameters","text":"Parameter Type Required Default Description func Optional[Callable] No None The synchronous function to be decorated (decorator syntactic sugar) handler Optional[HumanInterruptHandler] No None Custom interrupt handler function"},{"location":"api-reference/tool_calling/#example_2","title":"Example","text":"<pre><code>@human_in_the_loop\ndef get_current_time():\n    \"\"\"Get the current time\"\"\"\n    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n</code></pre>"},{"location":"api-reference/tool_calling/#human_in_the_loop_async","title":"human_in_the_loop_async","text":"<p>A decorator to add \"human-in-the-loop\" manual review capability to asynchronous tool functions.</p>"},{"location":"api-reference/tool_calling/#function-signature_3","title":"Function Signature","text":"<pre><code>def human_in_the_loop_async(\n    func: Optional[Callable] = None,\n    *,\n    handler: Optional[HumanInterruptHandler] = None\n) -&gt; Union[Callable[[Callable], BaseTool], BaseTool]\n</code></pre>"},{"location":"api-reference/tool_calling/#parameters_3","title":"Parameters","text":"Parameter Type Required Default Description func Optional[Callable] No None The asynchronous function to be decorated (decorator syntactic sugar) handler Optional[HumanInterruptHandler] No None Custom interrupt handler function"},{"location":"api-reference/tool_calling/#example_3","title":"Example","text":"<pre><code>@human_in_the_loop_async\nasync def get_current_time():\n    \"\"\"Get the current time\"\"\"\n    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n</code></pre>"},{"location":"api-reference/tool_calling/#interruptparams","title":"InterruptParams","text":"<p>The type of parameters passed to the interrupt handler function.</p>"},{"location":"api-reference/tool_calling/#class-definition","title":"Class Definition","text":"<pre><code>class InterruptParams(TypedDict):\n    tool_call_name: str\n    tool_call_args: Dict[str, Any]\n    tool: BaseTool\n</code></pre>"},{"location":"api-reference/tool_calling/#field-description","title":"Field Description","text":"Field Type Required Description tool_call_name str Yes The name of the tool call tool_call_args Dict[str, Any] Yes The arguments of the tool call tool BaseTool Yes The tool instance"},{"location":"api-reference/tool_calling/#humaninterrupthandler","title":"HumanInterruptHandler","text":"<p>Type alias for the interrupt handler function.</p>"},{"location":"api-reference/tool_calling/#type-definition","title":"Type Definition","text":"<pre><code>HumanInterruptHandler = Callable[[InterruptParams], Any]\n</code></pre>"},{"location":"getting-started-guide/chat/","title":"Chat Model Management","text":""},{"location":"getting-started-guide/chat/#overview","title":"Overview","text":"<p>LangChain's <code>init_chat_model</code> function only supports a limited number of model providers. This library provides a more flexible chat model management solution that supports custom model providers, particularly suitable for scenarios requiring integration with model services not natively supported (such as vLLM, etc.).</p>"},{"location":"getting-started-guide/chat/#registering-model-providers","title":"Registering Model Providers","text":"<p>Registering a chat model provider requires calling <code>register_model_provider</code>. The registration steps vary slightly depending on the situation.</p>"},{"location":"getting-started-guide/chat/#existing-langchain-chat-model-classes","title":"Existing LangChain Chat Model Classes","text":"<p>If the model provider already has a ready-made and suitable LangChain integration (see Chat Model Class Integrations), pass the corresponding integrated chat model class as the <code>chat_model</code> parameter.</p>"},{"location":"getting-started-guide/chat/#code-example","title":"Code Example","text":"<pre><code>from langchain_core.language_models.fake_chat_models import FakeChatModel\nfrom langchain_dev_utils.chat_models import register_model_provider\n\nregister_model_provider(\n    provider_name=\"fake_provider\",\n    chat_model=FakeChatModel,\n)\n\n# FakeChatModel is for testing only. In actual usage, you must pass a ChatModel class with real functionality.\n</code></pre> <p>Parameter Setting Instructions</p> <p><code>provider_name</code> represents the name of the model provider, used for subsequent referencing in <code>load_chat_model</code>. The name must start with a letter or number, can only contain letters, numbers, and underscores, with a maximum length of 20 characters.</p>"},{"location":"getting-started-guide/chat/#optional-parameter-description","title":"Optional Parameter Description","text":"<p>base_url</p> <p>This parameter typically does not need to be set (because chat model classes usually define a default API address internally). Pass <code>base_url</code> only when you need to override the chat model class's default address, and it only affects attributes with field names <code>api_base</code> or <code>base_url</code> (including aliases).</p> <p>model_profiles</p> <p>If your LangChain integrated chat model class fully supports the <code>profile</code> parameter (i.e., you can directly access the model's related properties via <code>model.profile</code>, such as <code>max_input_tokens</code>, <code>tool_calling</code>, etc.), you do not need to set <code>model_profiles</code> additionally.</p> <p>If accessing <code>model.profile</code> returns an empty dictionary <code>{}</code>, it indicates that the LangChain chat model class may not yet support the <code>profile</code> parameter. In this case, you can manually provide <code>model_profiles</code>.</p> <p><code>model_profiles</code> is a dictionary where each key is a model name and the value is the corresponding model's profile configuration:</p> <pre><code>{\n    \"model_name_1\": {\n        \"max_input_tokens\": 100_000,\n        \"tool_calling\": True,\n        \"structured_output\": True,\n        # ... other optional fields\n    },\n    \"model_name_2\": {\n        \"max_input_tokens\": 32768,\n        \"image_inputs\": True,\n        \"tool_calling\": False,\n        # ... other optional fields\n    },\n    # Can have any number of model configurations\n}\n</code></pre> <p>Tip</p> <p>It is recommended to use the <code>langchain-model-profiles</code> library to obtain profiles for the model providers you use.</p>"},{"location":"getting-started-guide/chat/#no-existing-langchain-chat-model-class-but-model-provider-supports-openai-compatible-api","title":"No Existing LangChain Chat Model Class, but Model Provider Supports OpenAI-Compatible API","text":"<p>In this case, the <code>chat_model</code> parameter must be set to <code>\"openai-compatible\"</code>.</p>"},{"location":"getting-started-guide/chat/#code-example_1","title":"Code Example","text":"<pre><code>register_model_provider(\n    provider_name=\"vllm\",\n    chat_model=\"openai-compatible\",\n    base_url=\"http://localhost:8000/v1\"\n)\n</code></pre> <p>Note: For more details about this part, please refer to OpenAI-Compatible API Integration.</p>"},{"location":"getting-started-guide/chat/#batch-registration","title":"Batch Registration","text":"<p>If you need to register multiple providers, you can use <code>batch_register_model_provider</code> to avoid repeated calls.</p>"},{"location":"getting-started-guide/chat/#code-example_2","title":"Code Example","text":"<pre><code>from langchain_dev_utils.chat_models import batch_register_model_provider\nfrom langchain_core.language_models.fake_chat_models import FakeChatModel\n\nbatch_register_model_provider(\n    providers=[\n        {\n            \"provider_name\": \"fake_provider\",\n            \"chat_model\": FakeChatModel,\n        },\n        {\n            \"provider_name\": \"vllm\",\n            \"chat_model\": \"openai-compatible\",\n            \"base_url\": \"http://localhost:8000/v1\",\n        },\n    ]\n)\n</code></pre> <p>Note</p> <p>Both registration functions are implemented based on a global dictionary. To avoid multi-threading issues, all registrations must be completed during application startup; dynamic registration during runtime is prohibited.</p> <p>Additionally, when setting <code>chat_model</code> to <code>openai-compatible</code> during registration, the library dynamically creates a new model class internally using <code>pydantic.create_model</code> (generating the corresponding chat model integration class based on <code>BaseChatOpenAICompatible</code>). This process involves Python metaclass operations and pydantic validation logic initialization, incurring some performance overhead. Therefore, avoid frequent registration during runtime.</p>"},{"location":"getting-started-guide/chat/#loading-chat-models","title":"Loading Chat Models","text":"<p>Use the <code>load_chat_model</code> function to load a chat model (initialize a chat model instance).</p> <p>This function receives the <code>model</code> parameter to specify the model name, the optional <code>model_provider</code> parameter to specify the model provider, and can also accept any number of keyword arguments for passing additional parameters to the chat model class.</p>"},{"location":"getting-started-guide/chat/#parameter-rules","title":"Parameter Rules","text":"<ul> <li>If <code>model_provider</code> is not passed, <code>model</code> must be in the format <code>provider_name:model_name</code>;</li> <li>If <code>model_provider</code> is passed, <code>model</code> must be only <code>model_name</code>.</li> </ul>"},{"location":"getting-started-guide/chat/#code-example_3","title":"Code Example","text":"<pre><code># Method 1: model includes provider information\nmodel = load_chat_model(\"vllm:qwen3-4b\")\n\n# Method 2: separately specify provider\nmodel = load_chat_model(\"qwen3-4b\", model_provider=\"vllm\")\n</code></pre>"},{"location":"getting-started-guide/chat/#model-methods-and-parameters","title":"Model Methods and Parameters","text":"<p>For supported model methods and parameters, refer to the usage documentation of the corresponding chat model class. If you are using the second case, all methods and parameters of the <code>BaseChatOpenAI</code> class are supported.</p>"},{"location":"getting-started-guide/chat/#compatibility-with-official-providers","title":"Compatibility with Official Providers","text":"<p><code>load_chat_model</code> looks up the global registration dictionary based on the <code>model_provider</code> parameter: if found, it instantiates using the corresponding model class from the dictionary; if not found, it initializes via <code>init_chat_model</code>. This means providers officially supported by LangChain (e.g., openai) can be called directly without registration.</p> <pre><code>model = load_chat_model(\"openai:gpt-4o-mini\")\n# or\nmodel = load_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n</code></pre> <p>Best Practice</p> <p>For using this module, you can choose based on the following three situations:</p> <ol> <li> <p>If all model providers you are integrating are supported by the official <code>init_chat_model</code>, use the official function directly for the best compatibility and stability.</p> </li> <li> <p>If some model providers you are integrating are not officially supported, use this module's functionality: first register the model provider using <code>register_model_provider</code>, then load the model using <code>load_chat_model</code>.</p> </li> <li> <p>If the model provider you are integrating does not have a suitable integration yet, but the provider offers an OpenAI-compatible API (such as vLLM), it is recommended to use this module's functionality: first register the model provider using <code>register_model_provider</code> (passing <code>openai-compatible</code> for <code>chat_model</code>), then load the model using <code>load_chat_model</code>.</p> </li> </ol>"},{"location":"getting-started-guide/embedding/","title":"Embedding Model Management","text":""},{"location":"getting-started-guide/embedding/#overview","title":"Overview","text":"<p>LangChain's <code>init_embeddings</code> function only supports a limited number of embedding model providers. This library provides a more flexible embedding model management solution, particularly suitable for scenarios requiring integration with embedding services not natively supported (such as vLLM, etc.).</p>"},{"location":"getting-started-guide/embedding/#registering-embedding-model-providers","title":"Registering Embedding Model Providers","text":"<p>Registering an embedding model provider requires calling <code>register_embeddings_provider</code>. The registration method varies slightly depending on the type of <code>embeddings_model</code>.</p>"},{"location":"getting-started-guide/embedding/#existing-langchain-embedding-model-classes","title":"Existing LangChain Embedding Model Classes","text":"<p>If the embedding model provider already has a ready-made and suitable LangChain integration (see Embedding Model Integration List), pass the corresponding embedding model class directly as the <code>embeddings_model</code> parameter.</p>"},{"location":"getting-started-guide/embedding/#code-example","title":"Code Example","text":"<pre><code>from langchain_core.embeddings.fake import FakeEmbeddings\nfrom langchain_dev_utils.embeddings import register_embeddings_provider\n\nregister_embeddings_provider(\n    provider_name=\"fake_provider\",\n    embeddings_model=FakeEmbeddings,\n)\n\n# FakeEmbeddings is for testing only. In actual usage, you must pass an Embeddings class with real functionality.\n</code></pre> <p>Parameter Setting Instructions</p> <p><code>provider_name</code> represents the name of the model provider, used for subsequent referencing in <code>load_embeddings</code>. <code>provider_name</code> must start with a letter or number, can only contain letters, numbers, and underscores, with a maximum length of 20 characters.</p>"},{"location":"getting-started-guide/embedding/#optional-parameter-description","title":"Optional Parameter Description","text":"<p>base_url</p> <p>This parameter typically does not need to be set (because embedding model classes usually define a default API address internally). Pass <code>base_url</code> only when you need to override the embedding model class's default address, and it only affects attributes with field names <code>api_base</code> or <code>base_url</code> (including aliases).</p>"},{"location":"getting-started-guide/embedding/#no-existing-langchain-embedding-model-class-but-provider-supports-openai-compatible-api","title":"No Existing LangChain Embedding Model Class, but Provider Supports OpenAI-Compatible API","text":"<p>Similar to chat model management, set <code>embeddings_model</code> to <code>\"openai-compatible\"</code>.</p>"},{"location":"getting-started-guide/embedding/#code-example_1","title":"Code Example","text":"<pre><code>register_embeddings_provider(\n    provider_name=\"vllm\",\n    embeddings_model=\"openai-compatible\",\n    base_url=\"http://localhost:8000/v1\"\n)\n</code></pre> <p>Note: For more details about this part, please refer to OpenAI-Compatible API Integration.</p>"},{"location":"getting-started-guide/embedding/#batch-registration","title":"Batch Registration","text":"<p>If you need to register multiple providers, you can use <code>batch_register_embeddings_provider</code>.</p>"},{"location":"getting-started-guide/embedding/#code-example_2","title":"Code Example","text":"<pre><code>from langchain_dev_utils.embeddings import batch_register_embeddings_provider\nfrom langchain_core.embeddings.fake import FakeEmbeddings\n\nbatch_register_embeddings_provider(\n    providers=[\n        {\n            \"provider_name\": \"fake_provider\",\n            \"embeddings_model\": FakeEmbeddings,\n        },\n        {\n            \"provider_name\": \"vllm\",\n            \"embeddings_model\": \"openai-compatible\",\n            \"base_url\": \"http://localhost:8000/v1\",\n        },\n    ]\n)\n</code></pre> <p>Note</p> <p>Both registration functions are implemented based on a global dictionary. All registrations must be completed during application startup; dynamic registration during runtime is prohibited to avoid multi-threading issues.</p> <p>Additionally, when setting <code>embeddings_model</code> to <code>openai-compatible</code> during registration, the library dynamically creates a new model class internally using <code>pydantic.create_model</code> (generating the corresponding embedding model integration class based on <code>BaseEmbeddingOpenAICompatible</code>). This process involves Python metaclass operations and pydantic validation logic initialization, incurring some performance overhead. Therefore, avoid frequent registration during runtime.</p>"},{"location":"getting-started-guide/embedding/#loading-embedding-models","title":"Loading Embedding Models","text":"<p>Use <code>load_embeddings</code> to initialize an embedding model instance.</p> <p>This function receives the <code>model</code> parameter to specify the model name, the optional <code>provider</code> parameter to specify the model provider, and can also accept any number of keyword arguments for passing additional parameters to the embedding model class.</p>"},{"location":"getting-started-guide/embedding/#parameter-rules","title":"Parameter Rules","text":"<ul> <li>If <code>provider</code> is not passed, <code>model</code> must be in the format <code>provider_name:embeddings_name</code>;</li> <li>If <code>provider</code> is passed, <code>model</code> must be only <code>embeddings_name</code>.</li> </ul>"},{"location":"getting-started-guide/embedding/#code-example_3","title":"Code Example","text":"<pre><code># Method 1: model includes provider information\nembedding = load_embeddings(\"vllm:qwen3-embedding-4b\")\n\n# Method 2: separately specify provider\nembedding = load_embeddings(\"qwen3-embedding-4b\", provider=\"vllm\")\n</code></pre>"},{"location":"getting-started-guide/embedding/#model-methods-and-parameters","title":"Model Methods and Parameters","text":"<p>For supported model methods and parameters, refer to the usage documentation of the corresponding embedding model class. If you are using the second case, all methods and parameters of the <code>OpenAIEmbeddings</code> class are supported.</p>"},{"location":"getting-started-guide/embedding/#compatibility-with-official-providers","title":"Compatibility with Official Providers","text":"<p><code>load_embeddings</code> looks up the global registration dictionary based on the <code>provider</code> parameter: if found, it instantiates using the corresponding model class from the dictionary; if not found, it initializes via <code>init_embeddings</code>. This means providers officially supported by LangChain (such as OpenAI) can be called directly without registration.</p> <pre><code>model = load_embeddings(\"openai:text-embedding-3-large\")\n# or\nmodel = load_embeddings(\"text-embedding-3-large\", provider=\"openai\")\n</code></pre> <p>Best Practice</p> <p>For using this module, you can choose based on the following three situations:</p> <ol> <li> <p>If all embedding model providers you are integrating are supported by the official <code>init_embeddings</code>, use the official function directly for the best compatibility.</p> </li> <li> <p>If some embedding model providers you are integrating are not officially supported, utilize this module's registration and loading mechanism: first register the model provider using <code>register_embeddings_provider</code>, then load the model using <code>load_embeddings</code>.</p> </li> <li> <p>If the embedding model provider you are integrating does not have a suitable integration yet, but the provider offers an OpenAI-compatible API (such as vLLM), it is recommended to use this module's functionality: first register the model provider using <code>register_embeddings_provider</code> (passing <code>openai-compatible</code> for <code>embeddings_model</code>), then load the model using <code>load_embeddings</code>.</p> </li> </ol>"},{"location":"getting-started-guide/format/","title":"Sequence Formatting","text":""},{"location":"getting-started-guide/format/#overview","title":"Overview","text":"<p>Used to format a list consisting of Documents, Messages, or strings into a single text string. The specific function is <code>format_sequence</code>.</p>"},{"location":"getting-started-guide/format/#usage-examples","title":"Usage Examples","text":""},{"location":"getting-started-guide/format/#message","title":"Message","text":""},{"location":"getting-started-guide/format/#use-cases","title":"Use Cases","text":"<ul> <li>Convert conversation history (system/human/ai/tool) into readable text, making it easy to inject into the next prompt.</li> <li>Debug printing: render a message list into a readable format for logs.</li> </ul>"},{"location":"getting-started-guide/format/#code-example","title":"Code Example","text":"<pre><code>from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, ToolMessage\n\nfrom langchain_dev_utils.message_convert import format_sequence\n\nformated1 = format_sequence(\n    [\n        SystemMessage(content=\"You are a weather query assistant\"),\n        HumanMessage(content=\"Check the weather in London and San Francisco\"),\n        AIMessage(\n            content=\"I will use the get_weather tool to check the weather for these two cities\",\n            tool_calls=[\n                {\"name\": \"get_weather\", \"args\": {\"location\": \"London\"}, \"id\": \"123\"},\n                {\"name\": \"get_weather\", \"args\": {\"location\": \"San Francisco\"}, \"id\": \"456\"},\n            ],\n        ),\n        ToolMessage(\n            content=\"The weather in London is 25 degrees Celsius\",\n            tool_call_id=\"123\",\n        ),\n        ToolMessage(\n            content=\"The weather in San Francisco is 22 degrees Celsius\",\n            tool_call_id=\"456\",\n        ),\n        AIMessage(\n            content=\"Based on the tool call results, the weather in London is 25 degrees Celsius, and the weather in San Francisco is 22 degrees Celsius\",\n        ),\n    ]\n    # The separator parameter defaults to \"-\" and with_num defaults to False; using defaults here\n)\nprint(formated1)\n</code></pre>"},{"location":"getting-started-guide/format/#output-result","title":"Output Result","text":"<pre><code>-System: You are a weather query assistant\n-Human: Check the weather in London and San Francisco\n-AI: I will use the get_weather tool to check the weather for these two cities\n&lt;tool_call&gt;get_weather&lt;/tool_call&gt;\n&lt;tool_call&gt;get_weather&lt;/tool_call&gt;\n-Tool: The weather in London is 25 degrees Celsius\n-Tool: The weather in San Francisco is 22 degrees Celsius\n-AI: Based on the tool call results, the weather in London is 25 degrees Celsius, and the weather in San Francisco is 22 degrees Celsius\n</code></pre>"},{"location":"getting-started-guide/format/#document","title":"Document","text":""},{"location":"getting-started-guide/format/#use-cases_1","title":"Use Cases","text":"<ul> <li>RAG: format the retrieved <code>Document</code> list into a <code>context</code> text block and paste it directly into your prompt.</li> </ul>"},{"location":"getting-started-guide/format/#code-example_1","title":"Code Example","text":"<pre><code>from langchain_core.documents import Document\n\nfrom langchain_dev_utils.message_convert import format_sequence\n\nformat2 = format_sequence(\n    [\n        Document(page_content=\"[Source: Product Manual] Refund policy: refunds are allowed within 7 days.\"),\n        Document(page_content=\"[Source: FAQ] Refunds usually take 1-3 business days to arrive.\"),\n        Document(page_content=\"[Source: Support Guidelines] In disputes, apologize first and ask for the order ID.\"),\n    ],\n    separator=\"&gt;\",  # Set the separator to \"&gt;\" so each line starts with this symbol\n    with_num=True,  # Enable numbering so documents are labeled 1, 2, 3...\n)\nprint(format2)\n</code></pre>"},{"location":"getting-started-guide/format/#output-result_1","title":"Output Result","text":"<pre><code>&gt;1. [Source: Product Manual] Refund policy: refunds are allowed within 7 days.\n&gt;2. [Source: FAQ] Refunds usually take 1-3 business days to arrive.\n&gt;3. [Source: Support Guidelines] In disputes, apologize first and ask for the order ID.\n</code></pre>"},{"location":"getting-started-guide/format/#string","title":"String","text":""},{"location":"getting-started-guide/format/#use-cases_2","title":"Use Cases","text":"<ul> <li>Format a set of bullet points (requirements, checklist items, todos, etc.) into multi-line text for prompt composition.</li> </ul>"},{"location":"getting-started-guide/format/#code-example_2","title":"Code Example","text":"<pre><code>from langchain_dev_utils.message_convert import format_sequence\n\nformat3 = format_sequence(\n    [\n        \"Answer the user's question only; do not add extra content.\",\n        \"If uncertain, state your assumptions clearly.\",\n        \"Use a Markdown list in the output.\",\n    ],\n    separator=\"&gt;\",\n    with_num=True,\n)\nprint(format3)\n</code></pre>"},{"location":"getting-started-guide/format/#output-result_2","title":"Output Result","text":"<pre><code>&gt;1. Answer the user's question only; do not add extra content.\n&gt;2. If uncertain, state your assumptions clearly.\n&gt;3. Use a Markdown list in the output.\n</code></pre>"},{"location":"getting-started-guide/human-in-the-loop/","title":"Adding Human-in-the-Loop Review for Tool Calling","text":""},{"location":"getting-started-guide/human-in-the-loop/#overview","title":"Overview","text":"<p>This library provides decorator functions for adding human-in-the-loop review support to tool calls, enabling human review during tool execution.</p> Decorator Applicable Scenario <code>human_in_the_loop</code> For synchronous tool functions <code>human_in_the_loop_async</code> For asynchronous tool functions"},{"location":"getting-started-guide/human-in-the-loop/#usage-examples","title":"Usage Examples","text":""},{"location":"getting-started-guide/human-in-the-loop/#using-the-default-handler","title":"Using the Default Handler","text":"<pre><code>from langchain_dev_utils.tool_calling import human_in_the_loop\nimport datetime\n\n\n@human_in_the_loop\ndef get_current_time() -&gt; str:\n    \"\"\"Get the current timestamp\"\"\"\n    return str(datetime.datetime.now().timestamp())\n</code></pre>"},{"location":"getting-started-guide/human-in-the-loop/#asynchronous-tool-example","title":"Asynchronous Tool Example","text":"<pre><code>from langchain_dev_utils.tool_calling import human_in_the_loop_async\nimport asyncio\nimport datetime\n\n\n@human_in_the_loop_async\nasync def async_get_current_time() -&gt; str:\n    \"\"\"Asynchronously get the current timestamp\"\"\"\n    await asyncio.sleep(1)\n    return str(datetime.datetime.now().timestamp())\n</code></pre>"},{"location":"getting-started-guide/human-in-the-loop/#implementation-of-the-default-handler","title":"Implementation of the Default Handler","text":"<p>The default handler is implemented as follows:</p> <pre><code>def _get_human_in_the_loop_request(params: InterruptParams) -&gt; dict[str, Any]:\n    return {\n        \"action_request\": {\n            \"action\": params[\"tool_call_name\"],\n            \"args\": params[\"tool_call_args\"],\n        },\n        \"config\": {\n            \"allow_accept\": True,\n            \"allow_edit\": True,\n            \"allow_respond\": True,\n        },\n        \"description\": f\"Please review tool call: {params['tool_call_name']}\",\n    }\n\n\ndef default_handler(params: InterruptParams) -&gt; Any:\n    request = _get_human_in_the_loop_request(params)\n    response = interrupt(request)\n\n    if response[\"type\"] == \"accept\":\n        return params[\"tool\"].invoke(params[\"tool_call_args\"])\n    elif response[\"type\"] == \"edit\":\n        updated_args = response[\"args\"]\n        return params[\"tool\"].invoke(updated_args)\n    elif response[\"type\"] == \"response\":\n        return response[\"args\"]\n    else:\n        raise ValueError(f\"Unsupported interrupt response type: {response['type']}\")\n</code></pre>"},{"location":"getting-started-guide/human-in-the-loop/#interrupt-request-format","title":"Interrupt Request Format","text":"<p>During an interrupt, a request in the following JSON Schema format is sent:</p> Field Description <code>action_request.action</code> Tool call name.Type: <code>str</code> <code>action_request.args</code> Tool call arguments.Type: <code>dict</code> <code>config.allow_accept</code> Whether to allow the accept action.Type: <code>bool</code> <code>config.allow_edit</code> Whether to allow editing arguments.Type: <code>bool</code> <code>config.allow_respond</code> Whether to allow direct response.Type: <code>bool</code> <code>description</code> Action description.Type: <code>str</code>"},{"location":"getting-started-guide/human-in-the-loop/#interrupt-response-format","title":"Interrupt Response Format","text":"<p>The response must return data in the following JSON Schema format:</p> Field Description <code>type</code> Response type, with possible values <code>accept</code>, <code>edit</code>, <code>response</code>.Type: <code>str</code>Required: Yes <code>args</code> When <code>type</code> is <code>edit</code> or <code>response</code>, contains the updated arguments or response content.Type: <code>dict</code>Required: No"},{"location":"getting-started-guide/human-in-the-loop/#custom-handler-example","title":"Custom Handler Example","text":"<p>You can fully control the interrupt behavior, such as only allowing \"accept/reject\", or customizing the prompt:</p> <pre><code>from typing import Any\nfrom langchain_dev_utils.tool_calling import human_in_the_loop_async, InterruptParams\nfrom langgraph.types import interrupt\n\n\nasync def custom_handler(params: InterruptParams) -&gt; Any:\n    response = interrupt(\n        f\"I am about to call tool {params['tool_call_name']} with arguments {params['tool_call_args']}. Please confirm whether to proceed.\"\n    )\n    if response[\"type\"] == \"accept\":\n        return await params[\"tool\"].ainvoke(params[\"tool_call_args\"])\n    elif response[\"type\"] == \"reject\":\n        return \"User rejected calling this tool\"\n    else:\n        raise ValueError(f\"Unsupported response type: {response['type']}\")\n\n\n@human_in_the_loop_async(handler=custom_handler)\nasync def get_weather(city: str) -&gt; str:\n    \"\"\"Get weather information\"\"\"\n    return f\"The weather in {city} is sunny.\"\n</code></pre> <p>Best Practice</p> <p>To implement custom human-in-the-loop logic with this decorator, you need to pass a <code>handler</code> parameter. This <code>handler</code> parameter is a function that must internally use LangGraph's <code>interrupt</code> function to perform the interrupt operation. Therefore, if you only need to add custom human-in-the-loop logic for a single tool, it is recommended to use LangGraph's <code>interrupt</code> function directly. When multiple tools require the same custom human-in-the-loop logic, using this decorator can effectively avoid code duplication.</p>"},{"location":"getting-started-guide/installation/","title":"Installation","text":"<p><code>langchain-dev-utils</code> supports installation via multiple package managers, including <code>pip</code>, <code>poetry</code>, and <code>uv</code>.</p> <p>To install the base version of <code>langchain-dev-utils</code>:</p> pippoetryuv <pre><code>pip install -U langchain-dev-utils\n</code></pre> <pre><code>poetry add langchain-dev-utils\n</code></pre> <pre><code>uv add langchain-dev-utils\n</code></pre> <p>To install the full-featured version of <code>langchain-dev-utils</code>:</p> pippoetryuv <pre><code>pip install -U \"langchain-dev-utils[standard]\"\n</code></pre> <pre><code>poetry add \"langchain-dev-utils[standard]\"\n</code></pre> <pre><code>uv add langchain-dev-utils[standard]\n</code></pre>"},{"location":"getting-started-guide/installation/#verifying-the-installation","title":"Verifying the Installation","text":"<p>After installation, verify that the package is correctly installed:</p> <pre><code>import langchain_dev_utils\nprint(langchain_dev_utils.__version__)\n</code></pre>"},{"location":"getting-started-guide/installation/#dependencies","title":"Dependencies","text":"<p>The package automatically installs the following dependencies:</p> <ul> <li><code>langchain</code></li> <li><code>langgraph</code> (installed alongside <code>langchain</code>)</li> </ul> <p>If you install the <code>standard</code> version, the following additional dependencies will also be installed:</p> <ul> <li><code>langchain-openai</code> (for model management)</li> <li><code>json-repair</code> (for fixing tool-call errors in middleware)</li> <li><code>jinja2</code> (for formatting system prompt templates in middleware)</li> </ul>"},{"location":"getting-started-guide/message/","title":"Message Processing","text":""},{"location":"getting-started-guide/message/#overview","title":"Overview","text":"<p>Main features include:</p> <ul> <li>Merge reasoning content into final responses</li> <li>Merge streaming output Chunks</li> </ul>"},{"location":"getting-started-guide/message/#merge-reasoning-content-into-final-response","title":"Merge Reasoning Content into Final Response","text":"<p>Used to merge reasoning content (<code>reasoning_content</code>) into the final response (<code>content</code>).</p>"},{"location":"getting-started-guide/message/#function-description","title":"Function Description","text":"Function Description <code>convert_reasoning_content_for_ai_message</code> Merge reasoning content in AIMessage into the content field (for model's invoke and ainvoke) <code>convert_reasoning_content_for_chunk_iterator</code> Merge reasoning content in streaming responses into the content field (for model's stream) <code>aconvert_reasoning_content_for_chunk_iterator</code> Async version of <code>convert_reasoning_content_for_chunk_iterator</code>, for async streaming processing (for model's astream)"},{"location":"getting-started-guide/message/#code-example","title":"Code Example","text":"<pre><code>from langchain_dev_utils.message_convert import (\n    convert_reasoning_content_for_ai_message,\n    convert_reasoning_content_for_chunk_iterator,\n)\n\nresponse = model.invoke(\"\u4f60\u597d\")\nconverted_response = convert_reasoning_content_for_ai_message(\n    response, think_tag=(\"&lt;start&gt;\", \"&lt;end&gt;\")\n)\nprint(converted_response.content)\n\nfor chunk in convert_reasoning_content_for_chunk_iterator(\n    model.stream(\"\u4f60\u597d\"), think_tag=(\"&lt;start&gt;\", \"&lt;end&gt;\")\n):\n    print(chunk.content, end=\"\", flush=True)\n</code></pre>"},{"location":"getting-started-guide/message/#merge-streaming-output-chunks","title":"Merge Streaming Output Chunks","text":"<p>Provides utility functions to merge multiple AIMessageChunks generated due to streaming output into a single AIMessage.</p>"},{"location":"getting-started-guide/message/#core-functions","title":"Core Functions","text":"Function Description <code>merge_ai_message_chunk</code> Merge AI message chunks"},{"location":"getting-started-guide/message/#code-example_1","title":"Code Example","text":"<pre><code>from langchain_dev_utils.message_convert import merge_ai_message_chunk\n\nchunks = []\nfor chunk in model.stream(\"\u4f60\u597d\"):\n    chunks.append(chunk)\n\nmerged_message = merge_ai_message_chunk(chunks)\nprint(merged_message)\n</code></pre>"},{"location":"getting-started-guide/tool/","title":"Tool Call Processing","text":""},{"location":"getting-started-guide/tool/#overview","title":"Overview","text":"<p>Provides utilities for detecting and parsing tool call arguments.</p>"},{"location":"getting-started-guide/tool/#detect-tool-calls","title":"Detect Tool Calls","text":"<p>Detects whether a message contains a tool call. The core function is <code>has_tool_calling</code>.</p>"},{"location":"getting-started-guide/tool/#code-example","title":"Code Example","text":"<pre><code>import datetime\nfrom langchain_core.tools import tool\nfrom langchain_dev_utils.tool_calling import has_tool_calling\n\n@tool\ndef get_current_time() -&gt; str:\n    \"\"\"Get the current timestamp\"\"\"\n    return str(datetime.datetime.now().timestamp())\n\nresponse = model.bind_tools([get_current_time]).invoke(\"What time is it now?\")\nprint(has_tool_calling(response))\n</code></pre>"},{"location":"getting-started-guide/tool/#parse-tool-call-arguments","title":"Parse Tool Call Arguments","text":"<p>Provides a utility function to parse tool call arguments, extracting parameter information from a message. The core function is <code>parse_tool_calling</code>.</p>"},{"location":"getting-started-guide/tool/#code-example_1","title":"Code Example","text":"<pre><code>import datetime\nfrom langchain_core.tools import tool\nfrom langchain_dev_utils.tool_calling import has_tool_calling, parse_tool_calling\n\n@tool\ndef get_current_time() -&gt; str:\n    \"\"\"Get the current timestamp\"\"\"\n    return str(datetime.datetime.now().timestamp())\n\nresponse = model.bind_tools([get_current_time]).invoke(\"What time is it now?\")\n\nif has_tool_calling(response):\n    name, args = parse_tool_calling(\n        response, first_tool_call_only=True\n    )\n    print(name, args)\n</code></pre>"},{"location":"resource/coding/","title":"Programmatically Access the Documentation","text":"<p>This documentation is designed for deep integration into AI workflows. You can seamlessly connect it to your development environment via AI assistants, IDE plugins, or the Model Context Protocol (MCP) for efficient, automated content retrieval and invocation.</p>"},{"location":"resource/coding/#quick-markdown-copy","title":"Quick Markdown Copy","text":"<p>A convenient Copy button is located in the upper-right corner of every documentation page:</p> <p> </p> <p>Steps:</p> <ol> <li> <p>Click the copy button in the upper-right corner.</p> </li> <li> <p>The complete Markdown content of the current page is automatically copied to your clipboard.</p> </li> <li> <p>Paste it directly into your code editor (e.g., VS Code) or an AI assistant dialog (e.g., ChatGPT, Claude) and start using it right away.</p> </li> </ol>"},{"location":"resource/coding/#integration-via-mcp","title":"Integration via MCP","text":"<p>This documentation is synchronized to the Context7 platform and can be invoked directly through Model Context Protocol (MCP) tools.</p> <p>How to configure: Refer to the Context7 MCP tool configuration guide to learn how to connect and configure MCP tools in your code editor or AI assistant.</p> <p>Search tip</p> <p>Because the Context7 library indexes three historical versions of this project, explicitly prompt the AI to \"please consult the most recently updated version of the documentation\" to ensure it retrieves the latest content.</p>"},{"location":"resource/coding/#best-practice-recommendation","title":"Best-practice recommendation","text":"<p>Given that Context7's content sync cycle is roughly two weeks, search results may lag behind. To guarantee accuracy and real-time freshness, always prefer the \"Copy\" button in the upper-right corner to obtain the latest Markdown content instead of relying on Context7's cached data.</p>"},{"location":"resource/example-project/","title":"Langchain-dev-utils Example Project","text":"<p>This repository provides an example project <code>langchain-dev-utils-example</code> designed to help developers quickly understand how to use the utility functions provided by <code>langchain-dev-utils</code> to efficiently build two typical agent systems:</p> <ul> <li>Single Agent: Suitable for executing simple tasks and tasks related to long-term memory storage.</li> <li>Supervisor-Multi-Agent Architecture: Coordinates multiple specialized agents through a central supervisor, suitable for complex scenarios requiring task decomposition, planning, and iterative optimization.</li> </ul> <p> </p>"},{"location":"resource/example-project/#quick-start","title":"Quick Start","text":"<ol> <li>Clone this repository: <pre><code>git clone https://github.com/TBice123123/langchain-dev-utils-example.git  \ncd langchain-dev-utils-example\n</code></pre></li> <li>Install dependencies using uv: <pre><code>uv sync\n</code></pre></li> <li>Create a .env file <pre><code>cp .env.example .env\n</code></pre></li> <li> <p>Edit the <code>.env</code> file and fill in your API keys (requires <code>ZhipuAI</code> and <code>Tavily</code> API keys).</p> </li> <li> <p>Start the project <pre><code>langgraph dev\n</code></pre></p> </li> </ol>"},{"location":"resource/example-project/#features-used","title":"Features Used","text":"<p>Single Agent:</p> <p>Features from this library used:</p> <ul> <li>Chat model management (including OpenAI compatible API integration): <code>register_model_provider</code>, <code>load_chat_model</code></li> <li>Embedding model management: <code>register_embeddings_provider</code>, <code>load_embeddings</code></li> <li>Format sequence: <code>format_sequence</code></li> <li>Middleware: <code>format_prompt</code></li> </ul> <p>Supervisor-Multi-Agent Architecture:</p> <p>Features from this library used:</p> <ul> <li>Chat model management (including OpenAI compatible API integration): <code>register_model_provider</code>, <code>load_chat_model</code></li> <li>Multi-agent construction: <code>wrap_agent_as_tool</code></li> </ul>"},{"location":"resource/example-project/#how-to-customize","title":"How to Customize","text":"<p>You can customize this project according to your actual needs.</p>"},{"location":"resource/example-project/#1-replace-chat-model-provider","title":"1. Replace Chat Model Provider","text":"<p>This project uses ZhipuAI's GLM series as the core model by default, as follows:</p> <ul> <li><code>GLM-5</code>: Used as the main agent for both <code>simple-agent</code> and <code>supervisor-agent</code></li> <li><code>GLM-4.7-Flash</code>: Used as the subagent for <code>supervisor-agent</code></li> <li><code>GLM-4.6V</code>: Used as the vision subagent for <code>supervisor-agent</code></li> </ul> <p>To customize the model provider, please modify <code>src/utils/providers/chat_models/register.py</code> and register your model provider using the <code>register_model_provider</code> function in the <code>register_all_model_providers</code> function.</p> <p>It is also recommended to modify <code>src/utils/providers/chat_models/load.py</code> and add corresponding loading logic in the <code>load_chat_model</code> function.</p> <p>Chat Model Management Best Practice</p> <p>The <code>load_chat_model</code> function uses keyword arguments to receive additional parameters for different chat model classes (LangChain official functions also use this approach). This approach improves universality but weakens IDE type hints and increases the risk of parameter misuse. Therefore, if a specific provider is already determined, you can extend the parameter signature for its integrated chat model class (or embedding model class) to restore type hints. Refer to <code>src/utils/providers/chat_models/load.py</code> for targeted modifications.</p>"},{"location":"resource/example-project/#2-register-embedding-model-provider","title":"2. Register Embedding Model Provider","text":"<p>The registration method for embedding model providers is similar to chat models. Please modify <code>src/utils/providers/embeddings/register.py</code> and register your embedding model provider using the <code>register_embeddings_provider</code> function in the <code>register_all_embeddings_providers</code> function.</p> <p>To customize the loading logic, you can modify <code>src/utils/providers/embeddings/load.py</code> and add corresponding loading logic in the <code>load_embeddings</code> function.</p>"},{"location":"resource/example-project/#3-customize-tools","title":"3. Customize Tools","text":"<p>Single Agent (simple-agent) Tool implementations are located in <code>src/agents/simple_agent/tools.py</code>, with built-in: - <code>save_user_memory</code> \u2014 Persist user memory - <code>get_user_memory</code> \u2014 Read user memory  </p> <p>To extend, simply add new tool implementations directly in this file.</p> <p>Supervisor-Multi-Agent (supervisor-agent) Tool implementations are located in <code>src/agents/supervisor/subagent/tools.py</code>. These are tool implementations for sub-agents. To add custom tools for sub-agents, simply add new tool implementations directly in this file.</p> <p>Note: The <code>supervisor</code> only has two tools for \"calling sub-agents\" by default. If you need to add custom tools for the <code>supervisor</code>, it is recommended to create a new <code>tools.py</code> under <code>src/agents/supervisor/</code>, write the implementations, and then import and pass them to the <code>create_agent</code> function in <code>src/agents/supervisor/agent.py</code>.</p>"},{"location":"zh/","title":"\ud83e\udd9c\ufe0f\ud83e\uddf0 langchain-dev-utils","text":"<p> \ud83d\ude80 \u4e13\u4e3a LangChain \u548c LangGraph \u5f00\u53d1\u8005\u6253\u9020\u7684\u9ad8\u6548\u5de5\u5177\u5e93 </p> <p> </p>"},{"location":"zh/#langchain-dev-utils_1","title":"\u4e3a\u4ec0\u4e48\u9009\u62e9 langchain-dev-utils\uff1f","text":"<p>\u538c\u5026\u4e86\u5728 LangChain \u5f00\u53d1\u4e2d\u7f16\u5199\u91cd\u590d\u4ee3\u7801\uff1f<code>langchain-dev-utils</code> \u6b63\u662f\u60a8\u9700\u8981\u7684\u89e3\u51b3\u65b9\u6848\uff01\u8fd9\u4e2a\u8f7b\u91cf\u4f46\u529f\u80fd\u5f3a\u5927\u7684\u5de5\u5177\u5e93\u4e13\u4e3a\u63d0\u5347 LangChain \u548c LangGraph \u5f00\u53d1\u4f53\u9a8c\u800c\u8bbe\u8ba1\uff0c\u5e2e\u52a9\u60a8\uff1a</p> <ul> <li>\u63d0\u5347\u5f00\u53d1\u6548\u7387 - \u51cf\u5c11\u6837\u677f\u4ee3\u7801\uff0c\u8ba9\u60a8\u4e13\u6ce8\u4e8e\u6838\u5fc3\u529f\u80fd</li> <li>\u7b80\u5316\u590d\u6742\u6d41\u7a0b - \u8f7b\u677e\u7ba1\u7406\u591a\u6a21\u578b\u3001\u591a\u5de5\u5177\u548c\u591a\u667a\u80fd\u4f53\u5e94\u7528</li> <li>\u589e\u5f3a\u4ee3\u7801\u8d28\u91cf - \u63d0\u9ad8\u4e00\u81f4\u6027\u548c\u53ef\u8bfb\u6027\uff0c\u51cf\u5c11\u7ef4\u62a4\u6210\u672c</li> <li>\u52a0\u901f\u539f\u578b\u5f00\u53d1 - \u5feb\u901f\u5b9e\u73b0\u60f3\u6cd5\uff0c\u66f4\u5feb\u8fed\u4ee3\u9a8c\u8bc1</li> </ul>"},{"location":"zh/#_1","title":"\u6838\u5fc3\u529f\u80fd","text":"<ul> <li> <p> \u7edf\u4e00\u7684\u6a21\u578b\u7ba1\u7406</p> <p>\u901a\u8fc7\u5b57\u7b26\u4e32\u6307\u5b9a\u6a21\u578b\u63d0\u4f9b\u5546\uff0c\u8f7b\u677e\u5207\u6362\u548c\u7ec4\u5408\u4e0d\u540c\u6a21\u578b\u3002</p> </li> <li> <p> \u5185\u7f6eOpenAI-Compatible\u96c6\u6210\u7c7b</p> <p>\u5185\u7f6e OpenAI-Compatible API\u96c6\u6210\u7c7b\uff0c\u53ef\u901a\u8fc7\u663e\u5f0f\u914d\u7f6e\uff0c\u63d0\u5347\u6a21\u578b\u517c\u5bb9\u6027\u3002</p> </li> <li> <p> \u7075\u6d3b\u7684\u6d88\u606f\u5904\u7406</p> <p>\u652f\u6301\u601d\u7ef4\u94fe\u62fc\u63a5\u3001\u6d41\u5f0f\u5904\u7406\u548c\u6d88\u606f\u683c\u5f0f\u5316</p> </li> <li> <p> \u5f3a\u5927\u7684\u5de5\u5177\u8c03\u7528</p> <p>\u5185\u7f6e\u5de5\u5177\u8c03\u7528\u68c0\u6d4b\u3001\u53c2\u6570\u89e3\u6790\u548c\u4eba\u5de5\u5ba1\u6838\u529f\u80fd</p> </li> <li> <p> \u9ad8\u6548\u7684 Agent \u5f00\u53d1</p> <p>\u7b80\u5316\u667a\u80fd\u4f53\u521b\u5efa\u6d41\u7a0b\uff0c\u6269\u5145\u66f4\u591a\u7684\u5e38\u7528\u4e2d\u95f4\u4ef6</p> </li> <li> <p> \u65b9\u4fbf\u7684\u72b6\u6001\u56fe\u6784\u5efa</p> <p>\u63d0\u4f9b\u9884\u6784\u5efa\u7684\u4e24\u4e2a\u51fd\u6570\uff0c\u65b9\u4fbf\u7528\u4e8e\u6784\u5efa\u987a\u5e8f\u6267\u884c\u548c\u5e76\u884c\u6267\u884c\u7684\u72b6\u6001\u56fe\u3002</p> </li> </ul>"},{"location":"zh/#_2","title":"\u5feb\u901f\u5f00\u59cb","text":"<p>1. \u5b89\u88c5 <code>langchain-dev-utils</code></p> <pre><code>pip install -U \"langchain-dev-utils[standard]\"\n</code></pre> <p>2. \u5f00\u59cb\u4f7f\u7528</p> <pre><code>from langchain.tools import tool\nfrom langchain_core.messages import HumanMessage\nfrom langchain_dev_utils.chat_models import register_model_provider, load_chat_model\nfrom langchain_dev_utils.agents import create_agent\n\n# \u6ce8\u518c\u6a21\u578b\u63d0\u4f9b\u5546\nregister_model_provider(\"vllm\", \"openai-compatible\", base_url=\"http://localhost:8000/v1\")\n\n@tool\ndef get_current_weather(location: str) -&gt; str:\n    \"\"\"\u83b7\u53d6\u6307\u5b9a\u5730\u70b9\u7684\u5f53\u524d\u5929\u6c14\"\"\"\n    return f\"25\u5ea6\uff0c{location}\"\n\n# \u4f7f\u7528\u5b57\u7b26\u4e32\u52a8\u6001\u52a0\u8f7d\u6a21\u578b\nmodel = load_chat_model(\"vllm:qwen3-4b\")\nresponse = model.invoke(\"\u4f60\u597d\")\nprint(response)\n\n# \u521b\u5efa\u667a\u80fd\u4f53\nagent = create_agent(\"vllm:qwen3-4b\", tools=[get_current_weather])\nresponse = agent.invoke({\"messages\": [HumanMessage(content=\"\u4eca\u5929\u7ebd\u7ea6\u7684\u5929\u6c14\u5982\u4f55\uff1f\")]})\nprint(response)\n</code></pre>"},{"location":"zh/#github","title":"GitHub \u4ed3\u5e93","text":"<p>\u8bbf\u95ee GitHub \u4ed3\u5e93 \u67e5\u770b\u6e90\u4ee3\u7801\u548c\u95ee\u9898\u3002</p>"},{"location":"zh/adavance-guide/graph/","title":"\u9884\u7f6eStateGraph\u6784\u5efa\u51fd\u6570","text":""},{"location":"zh/adavance-guide/graph/#_1","title":"\u6982\u8ff0","text":"<p>LangGraph \u662f LangChain \u5b98\u65b9\u63a8\u51fa\u7684\u7f16\u6392\u6846\u67b6\uff0c\u7528\u4e8e\u642d\u5efa\u590d\u6742\u5de5\u4f5c\u6d41\u3002\u4f46\u5728\u5b9e\u9645\u4e1a\u52a1\u4e2d\uff0c\u76f4\u63a5\u4f7f\u7528 LangGraph \u5f80\u5f80\u9700\u8981\u7f16\u5199\u8f83\u591a\u6837\u677f\u4ee3\u7801\uff08\u8282\u70b9\u547d\u540d\u3001\u8fb9\u8fde\u63a5\u3001\u56fe\u7f16\u8bd1\u7b49\uff09\u3002</p> <p>\u4e3a\u964d\u4f4e\u4e0a\u624b\u6210\u672c\uff0c\u672c\u5e93\u63d0\u4f9b\u4e24\u4e2a\u9884\u7f6e\u51fd\u6570\uff0c\u7528\u4e8e\u5feb\u901f\u6784\u5efa\u987a\u5e8f\u6267\u884c\u6216\u5e76\u884c\u6267\u884c\u7684\u72b6\u6001\u56fe\u3002\u5f00\u53d1\u8005\u53ea\u9700\u4e13\u6ce8\u4e8e\u5b9e\u73b0\u4e1a\u52a1\u8282\u70b9\uff0c\u5176\u4f59\u7f16\u6392\u5de5\u4f5c\u7531\u51fd\u6570\u81ea\u52a8\u5b8c\u6210\u3002</p> <p>\u4e24\u4e2a\u51fd\u6570\u5982\u4e0b\uff1a</p> \u51fd\u6570\u540d \u529f\u80fd\u63cf\u8ff0 \u9002\u7528\u573a\u666f create_sequential_graph \u6309\u987a\u5e8f\u7ec4\u5408\u591a\u4e2a\u8282\u70b9\uff0c\u5f62\u6210\u987a\u5e8f\u6267\u884c\u72b6\u6001\u56fe \u4efb\u52a1\u9700\u6309\u6b65\u9aa4\u6267\u884c\u4e14\u4f9d\u8d56\u524d\u4e00\u6b65\u8f93\u51fa create_parallel_graph \u5e76\u884c\u7ec4\u5408\u591a\u4e2a\u8282\u70b9\uff0c\u5f62\u6210\u5e76\u884c\u6267\u884c\u72b6\u6001\u56fe \u591a\u4e2a\u4efb\u52a1\u76f8\u4e92\u72ec\u7acb\uff0c\u53ef\u540c\u65f6\u6267\u884c\u4ee5\u63d0\u9ad8\u6548\u7387"},{"location":"zh/adavance-guide/graph/#_2","title":"\u987a\u5e8f\u5de5\u4f5c\u6d41","text":"<p>\u987a\u5e8f\u5de5\u4f5c\u6d41\u9002\u7528\u4e8e\u201c\u5fc5\u987b\u6309\u6b65\u9aa4\u6267\u884c\uff0c\u4e14\u540e\u4e00\u6b65\u4f9d\u8d56\u524d\u4e00\u6b65\u8f93\u51fa\u201d\u7684\u573a\u666f\u3002\u5728 LangGraph \u4e2d\uff0c\u6bcf\u4e00\u6b65\u901a\u5e38\u5bf9\u5e94\u4e00\u4e2a\u72b6\u6001\u56fe\u8282\u70b9\u3002</p> <p>\u4f7f\u7528 <code>create_sequential_graph</code> \u53ef\u5c06\u591a\u4e2a\u8282\u70b9\u6309\u56fa\u5b9a\u987a\u5e8f\u7ec4\u5408\u6210\u72b6\u6001\u56fe\u3002</p>"},{"location":"zh/adavance-guide/graph/#_3","title":"\u5178\u578b\u573a\u666f","text":"<p>\u4ee5\u7528\u6237\u8d2d\u4e70\u5546\u54c1\u4e3a\u4f8b\uff0c\u5178\u578b\u6d41\u7a0b\u5982\u4e0b\uff1a</p> <pre><code>graph LR\n    Start([\u7528\u6237\u4e0b\u5355\u8bf7\u6c42])\n    Inv[\u5e93\u5b58\u786e\u8ba4]\n    Ord[\u521b\u5efa\u8ba2\u5355]\n    Pay[\u5b8c\u6210\u652f\u4ed8]\n    Del[\u786e\u8ba4\u53d1\u8d27]\n    End([\u8ba2\u5355\u5b8c\u6210])\n\n    Start --&gt; Inv --&gt; Ord --&gt; Pay --&gt; Del --&gt; End</code></pre> <p>\u8be5\u6d41\u7a0b\u73af\u73af\u76f8\u6263\uff0c\u987a\u5e8f\u4e0d\u53ef\u98a0\u5012\u3002</p> <p>\u5176\u4e2d\u8fd9\u56db\u4e2a\u73af\u8282\uff08\u5e93\u5b58\u786e\u8ba4\u3001\u521b\u5efa\u8ba2\u5355\u3001\u5b8c\u6210\u652f\u4ed8\u3001\u786e\u8ba4\u53d1\u8d27\uff09\u53ef\u62bd\u8c61\u4e3a\u72ec\u7acb\u8282\u70b9\uff0c\u5e76\u7531\u4e13\u5c5e\u667a\u80fd\u4f53\u8d1f\u8d23\u6267\u884c\u3002 \u4f7f\u7528 <code>create_sequential_graph</code> \u5c06\u56db\u4e2a\u8282\u70b9\u6309\u987a\u5e8f\u8fde\u63a5\uff0c\u5373\u53ef\u5f62\u6210\u9ad8\u5ea6\u81ea\u52a8\u5316\u3001\u804c\u8d23\u6e05\u6670\u7684\u5546\u54c1\u8d2d\u4e70\u5de5\u4f5c\u6d41\u3002</p> <p>\u4e0b\u9762\u793a\u4f8b\u5c55\u793a\u5982\u4f55\u7528 <code>create_sequential_graph</code> \u6784\u5efa\u5546\u54c1\u8d2d\u4e70\u7684\u987a\u5e8f\u5de5\u4f5c\u6d41\u3002</p> <p>\u5148\u521b\u5efa\u5bf9\u8bdd\u6a21\u578b\u5bf9\u8c61\u3002\u8fd9\u91cc\u4ee5\u63a5\u5165\u672c\u5730 vLLM \u90e8\u7f72\u7684 <code>qwen3-4b</code> \u4e3a\u4f8b\uff0c\u5176\u63a5\u53e3\u4e0e OpenAI \u517c\u5bb9\uff0c\u56e0\u6b64\u53ef\u76f4\u63a5\u7528 <code>create_openai_compatible_model</code> \u6784\u5efa\u6a21\u578b\u7c7b\u3002</p> <p><pre><code>from langchain_dev_utils.chat_models.adapters import create_openai_compatible_model\n\nChatVLLM = create_openai_compatible_model(\n    model_provider=\"vllm\",\n    base_url=\"http://localhost:8000/v1\",\n    chat_model_cls_name=\"ChatVLLM\",\n)\n</code></pre> \u518d\u5b9e\u4f8b\u5316\u4e00\u4e2a <code>ChatVLLM</code> \u5bf9\u8c61\uff0c\u4f9b\u540e\u7eed\u667a\u80fd\u4f53\u8c03\u7528\u3002</p> <p><pre><code>model = ChatVLLM(model=\"qwen3-4b\")\n</code></pre> \u968f\u540e\u521b\u5efa\u76f8\u5173\u5de5\u5177\uff0c\u4f8b\u5982\u67e5\u8be2\u5e93\u5b58\u3001\u521b\u5efa\u8ba2\u5355\u3001\u8fdb\u884c\u652f\u4ed8\u7b49\u3002</p> \u5de5\u5177\u7684\u5b9e\u73b0\u53c2\u8003 <pre><code>from langchain_core.tools import tool\n\n@tool\ndef check_inventory(product_name: str) -&gt; dict:\n    \"\"\"\u67e5\u8be2\u5e93\u5b58\"\"\"\n    return {\"product_name\": product_name, \"in_stock\": True, \"available\": 42}\n\n@tool\ndef create_order(product_name: str, quantity: int) -&gt; str:\n    \"\"\"\u521b\u5efa\u8ba2\u5355\"\"\"\n    return f\"\u5df2\u521b\u5efa\u8ba2\u5355 ORD-10001\uff0c\u5546\u54c1\uff1a{product_name}\uff0c\u6570\u91cf\uff1a{quantity}\u3002\"\n\n@tool\ndef pay_order(order_id: str) -&gt; str:\n    \"\"\"\u652f\u4ed8\u8ba2\u5355\"\"\"\n    return f\"\u8ba2\u5355 {order_id} \u652f\u4ed8\u6210\u529f\u3002\"\n\n@tool\ndef confirm_delivery(order_id: str, address: str) -&gt; str:\n    \"\"\"\u786e\u8ba4\u53d1\u8d27\"\"\"\n    return f\"\u8ba2\u5355 {order_id} \u5df2\u5b89\u6392\u53d1\u8d27\uff0c\u6536\u8d27\u5730\u5740\uff1a{address}\u3002\"\n</code></pre> <p>\u7136\u540e\u521b\u5efa\u5bf9\u5e94\u7684\u56db\u4e2a\u5b50\u667a\u80fd\u4f53\u4ee5\u53ca\u5bf9\u5e94\u7684\u8c03\u7528\u6b64\u667a\u80fd\u4f53\u7684\u8282\u70b9\u51fd\u6570\u3002</p> <pre><code>from langchain.agents import create_agent\n\ninventory_agent = create_agent(\n    model=model,\n    tools=[check_inventory],\n    system_prompt=\"\u4f60\u662f\u5e93\u5b58\u52a9\u624b\uff0c\u8d1f\u8d23\u786e\u8ba4\u5546\u54c1\u662f\u5426\u6709\u8d27\u3002\u6700\u7ec8\u8bf7\u8f93\u51fa\u5e93\u5b58\u67e5\u8be2\u7ed3\u679c\u3002\",\n    name=\"inventory_agent\",\n)\n\norder_agent = create_agent(\n    model=model,\n    tools=[create_order],\n    system_prompt=\"\u4f60\u662f\u4e0b\u5355\u52a9\u624b\uff0c\u8d1f\u8d23\u521b\u5efa\u8ba2\u5355\u3002\",\n    name=\"order_agent\"\n)\n\npayment_agent = create_agent(\n    model=model,\n    tools=[pay_order],\n    system_prompt=\"\u4f60\u662f\u652f\u4ed8\u52a9\u624b\uff0c\u8d1f\u8d23\u5b8c\u6210\u652f\u4ed8\u3002\",\n    name=\"payment_agent\"\n)\n\ndelivery_agent = create_agent(\n    model=model,\n    tools=[confirm_delivery],\n    system_prompt=(\n        \"\u4f60\u662f\u53d1\u8d27\u52a9\u624b\uff0c\u8d1f\u8d23\u786e\u8ba4\u53d1\u8d27\u4fe1\u606f\u540e\u5b89\u6392\u53d1\u8d27\u3002\"\n    ),\n    name=\"delivery_agent\",\n    state_schema=AgentState\n)\n\ndef inventory(state: AgentState):\n    response = inventory_agent.invoke({\"messages\": state[\"messages\"]})\n    return {\"messages\": [AIMessage(content=response[\"messages\"][-1].content)]}\n\ndef order(state: AgentState):\n    response = order_agent.invoke({\"messages\": state[\"messages\"]})\n    return {\"messages\": [AIMessage(content=response[\"messages\"][-1].content)]}\n\ndef payment(state: AgentState):\n    response = payment_agent.invoke({\"messages\": state[\"messages\"]})\n    return {\"messages\": [AIMessage(content=response[\"messages\"][-1].content)]}\n\ndef delivery(state: AgentState):\n    response = delivery_agent.invoke({\"messages\": state[\"messages\"]})\n    return {\"messages\": [AIMessage(content=response[\"messages\"][-1].content)]}\n</code></pre> <p>\u6ce8\u610f</p> <p>\u867d\u7136 LangGraph \u53ef\u76f4\u63a5\u5c06\u667a\u80fd\u4f53\uff08\u5b50\u56fe\uff09\u4f5c\u4e3a\u8282\u70b9\u52a0\u5165\u56fe\u4e2d\uff0c\u4f46\u8fd9\u6837\u4f1a\u5bfc\u81f4\u5f53\u524d\u667a\u80fd\u4f53\u7684\u4e0a\u4e0b\u6587\u4e2d\u5305\u542b\u5148\u524d\u667a\u80fd\u4f53\u7684\u5168\u90e8\u8fd0\u884c\u4e0a\u4e0b\u6587\uff0c\u8fdd\u80cc\u4e0a\u4e0b\u6587\u5de5\u7a0b\u7ba1\u7406\u7684\u6700\u4f73\u5b9e\u8df5\u3002\u56e0\u6b64\u63a8\u8350\u5c06\u667a\u80fd\u4f53\u5c01\u88c5\u5728\u8282\u70b9\u5185\u90e8\uff0c\u4ec5\u8f93\u51fa\u6700\u7ec8\u7ed3\u679c\u3002</p> <p>\u6700\u540e\u4f7f\u7528 <code>create_sequential_graph</code> \u5c06\u8fd9\u56db\u4e2a\u8282\u70b9\u6309\u987a\u5e8f\u8fde\u63a5\u6210\u72b6\u6001\u56fe\u3002</p> <p><pre><code>from langchain_dev_utils.graph import create_sequential_graph\n\ngraph = create_sequential_graph(\n    nodes=[\n        inventory,\n        order,\n        payment,\n        delivery,\n    ],\n    state_schema=AgentState\n)\n</code></pre> \u8fd0\u884c\u793a\u4f8b\uff1a</p> <pre><code>response = graph.invoke(\n    {\n        \"messages\": [\n            HumanMessage(\"\u6211\u8981\u4e70\u4e00\u526f\u65e0\u7ebf\u8033\u673a\uff0c\u6570\u91cf2\uff0c\u8bf7\u4e0b\u5355\uff0c\u6536\u8d27\u5730\u5740\u662fX\u5e02X\u533aX\u8defX\u53f7\")\n        ]\n    }\n)\nprint(response)\n</code></pre>"},{"location":"zh/adavance-guide/graph/#_4","title":"\u5e76\u884c\u5de5\u4f5c\u6d41","text":"<p>\u5e76\u884c\u5de5\u4f5c\u6d41\u9002\u7528\u4e8e\u201c\u591a\u4e2a\u4efb\u52a1\u76f8\u4e92\u72ec\u7acb\u3001\u53ef\u540c\u65f6\u6267\u884c\u201d\u7684\u573a\u666f\uff0c\u901a\u8fc7\u5e76\u53d1\u6267\u884c\u63d0\u5347\u6574\u4f53\u541e\u5410\u6216\u964d\u4f4e\u7aef\u5230\u7aef\u8017\u65f6\u3002</p> <p>\u4f7f\u7528 <code>create_parallel_graph</code> \u53ef\u5c06\u591a\u4e2a\u8282\u70b9\u4ee5\u5e76\u884c\u65b9\u5f0f\u7ec4\u5408\u6210\u72b6\u6001\u56fe\u3002</p>"},{"location":"zh/adavance-guide/graph/#_5","title":"\u5178\u578b\u573a\u666f","text":"<p>\u5728\u5546\u54c1\u8d2d\u4e70\u573a\u666f\u4e2d\uff0c\u7528\u6237\u53ef\u80fd\u540c\u65f6\u9700\u8981\u591a\u79cd\u67e5\u8be2\uff0c\u4f8b\u5982\u5546\u54c1\u8be6\u60c5\u3001\u5e93\u5b58\u3001\u4f18\u60e0\u4e0e\u8fd0\u8d39\u4f30\u7b97\uff0c\u53ef\u5e76\u884c\u6267\u884c\u3002</p> <p>\u6d41\u7a0b\u5982\u4e0b\uff1a</p> <pre><code>graph LR\n    Start([\u7528\u6237\u8bf7\u6c42])\n\n    subgraph Parallel [\u5e76\u884c\u6267\u884c]\n        direction TB\n        Prod[\u5546\u54c1\u8be6\u60c5\u67e5\u8be2]\n        Inv[\u5e93\u5b58\u67e5\u8be2]\n        Prom[\u4f18\u60e0\u8ba1\u7b97]\n        Ship[\u8fd0\u8d39\u4f30\u7b97]\n    end\n\n    End([\u805a\u5408\u7ed3\u679c])\n\n    Start --&gt; Prod\n    Start --&gt; Inv\n    Start --&gt; Prom\n    Start --&gt; Ship\n\n    Prod --&gt; End\n    Inv --&gt; End\n    Prom --&gt; End\n    Ship --&gt; End</code></pre> <p>\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u521b\u5efa\u4e00\u4e2a\u5e76\u884c\u5de5\u4f5c\u6d41\uff0c\u7528\u4e8e\u5b9e\u73b0\u4e0a\u8ff0\u6d41\u7a0b\u3002</p> <p>\u5148\u521b\u5efa\u51e0\u4e2a\u5de5\u5177\u3002</p> \u5de5\u5177\u7684\u5b9e\u73b0\u53c2\u8003 <pre><code>@tool\ndef get_product_detail(product_name: str) -&gt; dict:\n    \"\"\"\u67e5\u8be2\u5546\u54c1\u8be6\u60c5\"\"\"\n    return {\n        \"product_name\": product_name,\n        \"sku\": \"SKU-10001\",\n        \"price\": 299,\n        \"highlights\": [\"\u4e3b\u52a8\u964d\u566a\", \"\u84dd\u72595.3\", \"30\u5c0f\u65f6\u7eed\u822a\"],\n    }\n\n@tool\ndef check_inventory(product_name: str) -&gt; dict:\n    \"\"\"\u67e5\u8be2\u5e93\u5b58\"\"\"\n    return {\"product_name\": product_name, \"in_stock\": True, \"available\": 42}\n\n@tool\ndef calculate_promotions(product_name: str, quantity: int) -&gt; dict:\n    \"\"\"\u8ba1\u7b97\u4f18\u60e0\"\"\"\n    return {\n        \"product_name\": product_name,\n        \"quantity\": quantity,\n        \"discounts\": [\"\u6ee1300\u51cf30\", \"\u4f1a\u545895\u6298\"],\n        \"estimated_discount\": 45,\n    }\n\n@tool\ndef estimate_shipping(address: str) -&gt; dict:\n    \"\"\"\u4f30\u7b97\u8fd0\u8d39\u548c\u65f6\u6548\"\"\"\n    return {\n        \"address\": address,\n        \"fee\": 12,\n        \"eta_days\": 2,\n    }\n</code></pre> <p>\u4ee5\u53ca\u5bf9\u5e94\u7684\u5b50\u667a\u80fd\u4f53\uff1a</p> <pre><code>product_agent = create_agent(\n    model,\n    tools=[get_product_detail],\n    system_prompt=\"\u4f60\u662f\u5546\u54c1\u52a9\u7406\uff0c\u8d1f\u8d23\u89e3\u6790\u7528\u6237\u9700\u6c42\u5e76\u67e5\u8be2\u5546\u54c1\u8be6\u60c5\u3002\",\n    name=\"product_agent\",\n    state_schema=AgentState,\n)\n\ninventory_agent = create_agent(\n    model,\n    tools=[check_inventory],\n    system_prompt=\"\u4f60\u662f\u5e93\u5b58\u52a9\u7406\uff0c\u8d1f\u8d23\u6839\u636eSKU\u67e5\u8be2\u5e93\u5b58\u3002\",\n    name=\"inventory_agent\",\n    state_schema=AgentState,\n)\n\npromotion_agent = create_agent(\n    model,\n    tools=[calculate_promotions],\n    system_prompt=\"\u4f60\u662f\u4f18\u60e0\u52a9\u7406\uff0c\u8d1f\u8d23\u8ba1\u7b97\u5f53\u524d\u53ef\u7528\u4f18\u60e0\u548c\u9884\u8ba1\u6298\u6263\u3002\",\n    name=\"promotion_agent\",\n    state_schema=AgentState,\n)\n\nshipping_agent = create_agent(\n    model,\n    tools=[estimate_shipping],\n    system_prompt=\"\u4f60\u662f\u914d\u9001\u52a9\u7406\uff0c\u8d1f\u8d23\u4f30\u7b97\u8fd0\u8d39\u548c\u65f6\u6548\u3002\",\n    name=\"shipping_agent\",\n    state_schema=AgentState,\n)\n\ndef product(state: AgentState):\n    response = product_agent.invoke({\"messages\": state[\"messages\"]})\n    return {\"messages\": [AIMessage(content=response[\"messages\"][-1].content)]}\n\ndef inventory(state: AgentState):\n    response = inventory_agent.invoke({\"messages\": state[\"messages\"]})\n    return {\"messages\": [AIMessage(content=response[\"messages\"][-1].content)]}\n\ndef promotion(state: AgentState):\n    response = promotion_agent.invoke({\"messages\": state[\"messages\"]})\n    return {\"messages\": [AIMessage(content=response[\"messages\"][-1].content)]}\n\ndef shipping(state: AgentState):\n    response = shipping_agent.invoke({\"messages\": state[\"messages\"]})\n    return {\"messages\": [AIMessage(content=response[\"messages\"][-1].content)]}\n</code></pre> <p>\u7528 <code>create_parallel_graph</code> \u5b8c\u6210\u5e76\u884c\u72b6\u6001\u56fe\u7684\u7f16\u6392\u3002</p> <p><pre><code>from langchain_dev_utils.graph import create_parallel_graph\n\ngraph = create_parallel_graph(\n    nodes=[\n       product,\n       inventory,\n       promotion,\n       shipping,\n    ],\n    state_schema=AgentState,\n    graph_name=\"parallel_graph\",\n)\n</code></pre> \u8fd0\u884c\u793a\u4f8b\uff1a</p> <pre><code>response = graph.invoke(\n    {\"messages\": [HumanMessage(\"\u6211\u60f3\u4e70\u4e00\u526f\u65e0\u7ebf\u8033\u673a\uff0c\u6570\u91cf2\uff0c\u6536\u8d27\u5730\u5740X\u5e02X\u533aX\u8defX\u53f7\")]}\n)\nprint(response)\n</code></pre>"},{"location":"zh/adavance-guide/graph/#_6","title":"\u6309\u9700\u5e76\u884c","text":"<p>\u6709\u4e9b\u60c5\u51b5\u4e0b\uff0c\u5e76\u4e0d\u5e0c\u671b\u6240\u6709\u8282\u70b9\u90fd\u53c2\u4e0e\u5e76\u884c\u6267\u884c\uff0c\u800c\u662f\u5e0c\u671b\u201c\u6309\u6761\u4ef6\u9009\u62e9\u5176\u4e2d\u90e8\u5206\u8282\u70b9\u5e76\u884c\u8fd0\u884c\u201d\u3002\u6b64\u65f6\u53ef\u901a\u8fc7 <code>branches_fn</code> \u6307\u5b9a\u5206\u652f\u51fd\u6570\u3002</p> <p>\u5206\u652f\u51fd\u6570\u9700\u8981\u8fd4\u56de <code>Send</code> \u5217\u8868\uff0c\u6bcf\u4e2a <code>Send</code> \u5305\u542b\u76ee\u6807\u8282\u70b9\u540d\u79f0\u4e0e\u8be5\u8282\u70b9\u7684\u8f93\u5165\u3002</p>"},{"location":"zh/adavance-guide/graph/#router","title":"Router \u591a\u667a\u80fd\u4f53\u67b6\u6784","text":"<p>\u6309\u9700\u5e76\u884c\u529f\u80fd\u53ef\u7528\u4e8e\u5b9e\u73b0 Router \u591a\u667a\u80fd\u4f53\u67b6\u6784\u7684\u6838\u5fc3\u90e8\u5206\u3002</p> <p>\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\uff0c<code>Router</code>\uff08\u8def\u7531\uff09\u67b6\u6784\u901a\u8fc7\u5c06\u590d\u6742\u4efb\u52a1\u62c6\u89e3\u5e76\u5206\u53d1\u7ed9\u4e13\u95e8\u7684\u5b50\u667a\u80fd\u4f53\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u5e76\u884c\u5904\u7406\u3002\u8be5\u67b6\u6784\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u6b65\u9aa4\uff1a</p> <ol> <li> <p>\u610f\u56fe\u8bc6\u522b\uff1a\u7531\u8def\u7531\u6a21\u578b\u5206\u6790\u7528\u6237\u8bf7\u6c42\uff0c\u62c6\u89e3\u4efb\u52a1\u5e76\u786e\u5b9a\u8c03\u7528\u7684\u667a\u80fd\u4f53\u3002</p> </li> <li> <p>\u5e76\u884c\u6267\u884c\uff1a\u591a\u4e2a\u4e1a\u52a1\u667a\u80fd\u4f53\u540c\u65f6\u5904\u7406\u5b50\u4efb\u52a1\u3002</p> </li> <li> <p>\u7ed3\u679c\u6c47\u603b\uff1a\u5c06\u5404\u5b50\u667a\u80fd\u4f53\u7684\u56de\u590d\u6574\u5408\u6210\u6700\u7ec8\u7b54\u6848\u3002</p> </li> </ol> <p>\u5728\u8ba2\u5355\u67e5\u8be2\u573a\u666f\u4e2d\uff0c\u7528\u6237\u53ef\u80fd\u540c\u65f6\u5173\u5fc3\u8ba2\u5355\u72b6\u6001\u3001\u5546\u54c1\u4fe1\u606f\u6216\u9000\u6b3e\u653f\u7b56\u3002\u6b64\u65f6\uff0c\u7cfb\u7edf\u53ef\u4ee5\u5e76\u884c\u8c03\u7528\u8ba2\u5355\u3001\u5546\u54c1\u548c\u9000\u6b3e\u667a\u80fd\u4f53\uff0c\u6700\u540e\u7edf\u4e00\u56de\u590d\u3002</p> <pre><code>graph LR\n    Start([\u7528\u6237\u8bf7\u6c42]) --&gt; Router([\u610f\u56fe\u5206\u7c7b\u5668])\n    Router --&gt;|\u8ba2\u5355\u67e5\u8be2| OrderAgent\n    Router --&gt;|\u5546\u54c1\u67e5\u8be2| ProductAgent\n    Router --&gt;|\u9000\u6b3e\u67e5\u8be2| RefundAgent\n    OrderAgent --&gt; Synthesize\n    ProductAgent --&gt; Synthesize\n    RefundAgent --&gt; Synthesize\n    Synthesize --&gt; End([\u5408\u6210\u56de\u590d])</code></pre> <p>1. \u73af\u5883\u51c6\u5907\u4e0e\u5de5\u5177\u5b9a\u4e49</p> <p>\u9996\u5148\uff0c\u5b9a\u4e49\u5404\u4e1a\u52a1\u667a\u80fd\u4f53\u9700\u8981\u7528\u5230\u7684\u5de5\u5177\u3002</p> \u70b9\u51fb\u5c55\u5f00\u5de5\u5177\u5b9e\u73b0\u4ee3\u7801 <pre><code>from langchain_core.tools import tool\n\n@tool\ndef list_orders() -&gt; dict:\n    \"\"\"\u67e5\u8be2\u7528\u6237\u8ba2\u5355\u5217\u8868\"\"\"\n    return {\n        \"orders\": [\n            {\n                \"order_id\": \"ORD-20250101-0001\",\n                \"status\": \"\u5df2\u53d1\u8d27\",\n                \"items\": [{\"product_name\": \"\u65e0\u7ebf\u8033\u673a\", \"qty\": 1}],\n                \"created_at\": \"2025-01-01 10:02:11\",\n            },\n            {\n                \"order_id\": \"ORD-20241215-0234\",\n                \"status\": \"\u5df2\u5b8c\u6210\",\n                \"items\": [{\"product_name\": \"\u673a\u68b0\u952e\u76d8\", \"qty\": 1}],\n                \"created_at\": \"2024-12-15 21:18:03\",\n            },\n        ],\n    }\n\n@tool\ndef get_order_detail(order_id: str) -&gt; dict:\n    \"\"\"\u67e5\u8be2\u8ba2\u5355\u8be6\u60c5\"\"\"\n    return {\n        \"status\": \"\u5df2\u53d1\u8d27\",\n        \"receiver\": {\"name\": \"\u5f20\u4e09\", \"phone\": \"138****0000\"},\n        \"items\": [\n            {\n                \"product_id\": \"P-10001\",\n                \"product_name\": \"\u65e0\u7ebf\u8033\u673a\",\n                \"qty\": 1,\n                \"price\": 299,\n            }\n        ],\n    }\n\n@tool\ndef get_shipping_trace(tracking_no: str) -&gt; dict:\n    \"\"\"\u67e5\u8be2\u7269\u6d41\u8f68\u8ff9\"\"\"\n    return {\n        \"events\": [\n            {\"time\": \"2025-01-02 09:10\", \"status\": \"\u5feb\u4ef6\u5df2\u63fd\u6536\"},\n            {\"time\": \"2025-01-02 18:45\", \"status\": \"\u5feb\u4ef6\u8fd0\u8f93\u4e2d\"},\n            {\"time\": \"2025-01-03 11:20\", \"status\": \"\u5feb\u4ef6\u5df2\u5230\u8fbe\u6d3e\u9001\u7ad9\"},\n        ],\n    }\n\n@tool\ndef search_products(query: str) -&gt; dict:\n    \"\"\"\u641c\u7d22\u4ea7\u54c1\"\"\"\n    return {\n        \"results\": [\n            {\n                \"product_id\": \"P-10001\",\n                \"name\": \"\u65e0\u7ebf\u8033\u673a Pro\",\n                \"price\": 299,\n                \"highlights\": [\"\u964d\u566a\", \"\u84dd\u72595.3\", \"\u7eed\u822a30\u5c0f\u65f6\"],\n            },\n            {\n                \"product_id\": \"P-10002\",\n                \"name\": \"\u65e0\u7ebf\u8033\u673a Lite\",\n                \"price\": 199,\n                \"highlights\": [\"\u8f7b\u91cf\", \"\u4f4e\u5ef6\u8fdf\", \"\u7eed\u822a24\u5c0f\u65f6\"],\n            },\n        ],\n    }\n\n@tool\ndef get_product_detail(product_id: str) -&gt; dict:\n    \"\"\"\u67e5\u8be2\u4ea7\u54c1\u8be6\u60c5\"\"\"\n    return {\n        \"product_id\": product_id,\n        \"name\": \"\u65e0\u7ebf\u8033\u673a Pro\",\n        \"price\": 299,\n        \"specs\": {\"color\": [\"\u9ed1\", \"\u767d\"], \"warranty_months\": 12},\n        \"description\": \"\u4e3b\u6253\u964d\u566a\u4e0e\u957f\u7eed\u822a\u7684\u771f\u65e0\u7ebf\u8033\u673a\u3002\",\n    }\n\n@tool\ndef check_inventory(product_name: str) -&gt; dict:\n    \"\"\"\u67e5\u8be2\u5e93\u5b58\"\"\"\n    return {\"product_name\": product_name, \"in_stock\": True, \"available\": 42}\n\n@tool\ndef create_refund(order_id: str, reason: str) -&gt; dict:\n    \"\"\"\u53d1\u8d77\u9000\u6b3e\"\"\"\n    return {\n        \"refund_id\": \"RFD-20250103-0009\",\n        \"status\": \"\u5df2\u63d0\u4ea4\",\n        \"reason\": reason,\n        \"estimated_days\": 3,\n    }\n\n@tool\ndef get_refund_status(refund_id: str) -&gt; dict:\n    \"\"\"\u67e5\u8be2\u9000\u6b3e\u72b6\u6001\"\"\"\n    return {\n        \"refund_id\": refund_id,\n        \"status\": \"\u5904\u7406\u4e2d\",\n        \"progress\": [\n            {\"time\": \"2025-01-03 12:05\", \"status\": \"\u5df2\u63d0\u4ea4\"},\n            {\"time\": \"2025-01-03 12:20\", \"status\": \"\u5ba2\u670d\u5ba1\u6838\u4e2d\"},\n        ],\n        \"estimated_days\": 2,\n    }\n\n@tool\ndef refund_policy() -&gt; dict:\n    \"\"\"\u67e5\u770b\u9000\u6b3e\u653f\u7b56\"\"\"\n    return {\n        \"window_days\": 7,\n        \"requirements\": [\"\u5546\u54c1\u5b8c\u597d\", \"\u914d\u4ef6\u9f50\u5168\", \"\u63d0\u4f9b\u8ba2\u5355\u53f7\"],\n        \"notes\": [\"\u90e8\u5206\u6d3b\u52a8\u5546\u54c1\u4e0d\u652f\u6301\u65e0\u7406\u7531\u9000\u6b3e\", \"\u5230\u8d26\u65f6\u95f4\u89c6\u652f\u4ed8\u6e20\u9053\u800c\u5b9a\"],\n    }\n</code></pre> <p>2. \u5b9a\u4e49\u72b6\u6001 Schema</p> <p><code>RouterState</code> \u4ee3\u8868\u6700\u7ec8\u72b6\u6001\u56fe\u7684\u72b6\u6001Schema\uff0c\u800c<code>AgentInput</code> \u4e0e <code>AgentOutput</code> \u5206\u522b\u5b9a\u4e49\u5b50\u667a\u80fd\u4f53\u8282\u70b9\u7684\u8f93\u5165\u4e0e\u8f93\u51fa\u72b6\u6001\u3002</p> <pre><code>import operator\nfrom typing import Annotated, Literal\n\nfrom typing_extensions import TypedDict\n\n\nclass AgentInput(TypedDict):\n    \"\"\"\u5b50\u667a\u80fd\u4f53\u7684\u8f93\u5165\u7ed3\u6784\"\"\"\n    query: str\n\n\nclass AgentOutput(TypedDict):\n    \"\"\"\u5b50\u667a\u80fd\u4f53\u7684\u8f93\u51fa\u7ed3\u6784\"\"\"\n    source: str\n    result: str\n\n\nclass Classification(TypedDict):\n    \"\"\"\u8def\u7531\u5206\u7c7b\u7ed3\u679c\"\"\"\n    source: Literal[\"order\", \"refund\", \"product\"]\n    query: str\n\n\nclass RouterState(TypedDict):\n    \"\"\"\u5168\u5c40\u72b6\u6001 Schema\"\"\"\n    query: str\n    classifications: list[Classification]\n    results: Annotated[list[AgentOutput], operator.add] \n    final_answer: str\n</code></pre> <p>3. \u521b\u5efa\u5b50\u667a\u80fd\u4f53</p> <p>\u5229\u7528 LangChain \u7684 <code>create_agent</code> \u5feb\u901f\u6784\u5efa\u4e09\u4e2a\u4e1a\u52a1\u667a\u80fd\u4f53\uff0c\u5e76\u4e3a\u5b83\u4eec\u7ed1\u5b9a\u5bf9\u5e94\u7684\u5de5\u5177\u548c\u63d0\u793a\u8bcd\u3002</p> <pre><code>from langchain.agents import create_agent\nfrom langchain_core.messages import HumanMessage\n\nORDER_AGENT_PROMPT = (\n    \"\u4f60\u662f\u8ba2\u5355\u7ba1\u7406\u52a9\u624b\u3002\\n\"\n    \"\u4f60\u53ef\u4ee5\u4f7f\u7528\u5de5\u5177\u6765\u67e5\u8be2\u8ba2\u5355\u5217\u8868\u3001\u8ba2\u5355\u8be6\u60c5\u3001\u7269\u6d41\u8f68\u8ff9\u3002\\n\"\n    \"\u4f18\u5148\u4f7f\u7528\u5de5\u5177\u83b7\u53d6\u4fe1\u606f\uff0c\u518d\u57fa\u4e8e\u5de5\u5177\u7ed3\u679c\u7ed9\u51fa\u7ed3\u8bba\u3002\\n\"\n    \"\u8f93\u51fa\u8981\u6c42\uff1a\u7528\u4e2d\u6587\u56de\u7b54\uff0c\u7ed3\u6784\u6e05\u6670\uff0c\u5fc5\u8981\u65f6\u7528\u6761\u76ee\u5217\u51fa\u8ba2\u5355\u4fe1\u606f\u3002\\n\"\n)\n\norder_agent = create_agent(\n    model,\n    system_prompt=ORDER_AGENT_PROMPT,\n    tools=[list_orders, get_order_detail, get_shipping_trace],\n    name=\"order_agent\",\n)\n\nPRODUCT_AGENT_PROMPT = (\n    \"\u4f60\u662f\u4ea7\u54c1\u7ba1\u7406\u52a9\u624b\u3002\\n\"\n    \"\u4f60\u53ef\u4ee5\u4f7f\u7528\u5de5\u5177\u6765\u641c\u7d22\u4ea7\u54c1\u3001\u67e5\u770b\u4ea7\u54c1\u8be6\u60c5\u3001\u67e5\u8be2\u5e93\u5b58\u3002\\n\"\n    \"\u4f18\u5148\u4f7f\u7528\u5de5\u5177\u83b7\u53d6\u4fe1\u606f\uff0c\u518d\u57fa\u4e8e\u5de5\u5177\u7ed3\u679c\u7ed9\u51fa\u5efa\u8bae\u3002\\n\"\n    \"\u5f53\u7528\u6237\u7684\u9700\u6c42\u4e0d\u660e\u786e\u65f6\uff0c\u5148\u63d0\u51fa\u4e00\u4e2a\u6f84\u6e05\u95ee\u9898\uff08\u4f8b\u5982\u54c1\u7c7b/\u9884\u7b97/\u7528\u9014\uff09\u3002\\n\"\n    \"\u8f93\u51fa\u8981\u6c42\uff1a\u7528\u4e2d\u6587\u56de\u7b54\uff0c\u7ed9\u51fa\u53ef\u6267\u884c\u7684\u4e0b\u4e00\u6b65\u5efa\u8bae\u3002\\n\"\n)\n\nproduct_agent = create_agent(\n    model,\n    system_prompt=PRODUCT_AGENT_PROMPT,\n    tools=[search_products, get_product_detail, check_inventory],\n    name=\"product_agent\",\n)\n\nREFUND_AGENT_PROMPT = (\n    \"\u4f60\u662f\u9000\u6b3e\u7ba1\u7406\u52a9\u624b\u3002\\n\"\n    \"\u4f60\u53ef\u4ee5\u4f7f\u7528\u5de5\u5177\u6765\u53d1\u8d77\u9000\u6b3e\u3001\u67e5\u8be2\u9000\u6b3e\u72b6\u6001\u3001\u67e5\u770b\u9000\u6b3e\u653f\u7b56\u3002\\n\"\n    \"\u4f18\u5148\u4f7f\u7528\u5de5\u5177\u83b7\u53d6\u4fe1\u606f\uff1b\u82e5\u7528\u6237\u7f3a\u5c11\u5173\u952e\u5b57\u6bb5\uff08\u4f8b\u5982\u8ba2\u5355\u53f7\uff09\uff0c\u5148\u8ffd\u95ee\u3002\\n\"\n    \"\u8f93\u51fa\u8981\u6c42\uff1a\u7528\u4e2d\u6587\u56de\u7b54\uff0c\u660e\u786e\u544a\u77e5\u9000\u6b3e\u8fdb\u5ea6/\u6240\u9700\u6750\u6599/\u9884\u8ba1\u65f6\u95f4\u3002\\n\"\n)\n\nrefund_agent = create_agent(\n    model,\n    system_prompt=REFUND_AGENT_PROMPT,\n    tools=[create_refund, get_refund_status, refund_policy],\n    name=\"refund_agent\",\n)\n</code></pre> <p>4. \u5c01\u88c5\u667a\u80fd\u4f53\u8c03\u7528\u903b\u8f91</p> <p>\u5c06\u667a\u80fd\u4f53\u7684\u8c03\u7528\u903b\u8f91\u5c01\u88c5\u4e3a\u8282\u70b9\u51fd\u6570\u3002\u6bcf\u4e2a\u51fd\u6570\u8d1f\u8d23\u8c03\u7528\u7279\u5b9a\u7684\u667a\u80fd\u4f53\uff0c\u5e76\u5c06\u7ed3\u679c\u683c\u5f0f\u5316\u5199\u5165 <code>results</code> \u5b57\u6bb5\u3002</p> <pre><code>def order(state: AgentInput):\n    response = order_agent.invoke({\"messages\": [HumanMessage(content=state[\"query\"])]})\n    return {\n        \"results\": [{\"source\": \"order\", \"result\": response[\"messages\"][-1].content}]\n    }\n\ndef product(state: AgentInput): \n    response = product_agent.invoke(\n        {\"messages\": [HumanMessage(content=state[\"query\"])]}\n    )\n    return {\n        \"results\": [{\"source\": \"product\", \"result\": response[\"messages\"][-1].content}]\n    }\n\ndef refund(state: AgentInput):\n    response = refund_agent.invoke({\"messages\": [HumanMessage(content=state[\"query\"])]})\n    return {\n        \"results\": [{\"source\": \"refund\", \"result\": response[\"messages\"][-1].content}]\n    }\n</code></pre> <p>5. \u5b9e\u73b0\u8def\u7531\u4e0e\u5206\u652f\u903b\u8f91</p> <p>\u8fd9\u91cc\u5c31\u9700\u8981\u4f7f\u7528\u6309\u9700\u5e76\u884c\u7684\u529f\u80fd\u3002\u6211\u4eec\u5b9a\u4e49\u4e24\u4e2a\u8282\u70b9\uff1a 1.  <code>classify_query</code>\uff1a\u4f7f\u7528\u5927\u6a21\u578b\u8fdb\u884c\u610f\u56fe\u8bc6\u522b\uff0c\u8f93\u51fa\u9700\u8981\u8c03\u7528\u7684\u667a\u80fd\u4f53\u5217\u8868\u4ee5\u53ca\u4efb\u52a1\u5185\u5bb9\u3002 2.  <code>route_to_agents</code>\uff1a\u6839\u636e\u5206\u7c7b\u7ed3\u679c\uff0c\u751f\u6210 <code>Send</code> \u5bf9\u8c61\u5217\u8868\uff0c\u51b3\u5b9a\u5e76\u884c\u6267\u884c\u54ea\u4e9b\u8282\u70b9\u3002</p> <pre><code>from typing import cast\nfrom langchain_core.messages import SystemMessage\nfrom pydantic import BaseModel, Field\nfrom langgraph.constants import Send\n\nclass ClassificationResult(BaseModel):\n    classifications: list[Classification] = Field(\n        description=\"\u8981\u8c03\u7528\u7684\u667a\u80fd\u4f53\u5217\u8868\u53ca\u5176\u5bf9\u5e94\u7684\u5b50\u95ee\u9898\"\n    )\n\nROUTER_SYSTEM_PROMPT = (\n    \"\u4f60\u662f\u4e00\u4e2aRouter\u6a21\u578b\uff0c\u53ea\u8d1f\u8d23\u628a\u7528\u6237\u95ee\u9898\u62c6\u5206\u5e76\u5206\u53d1\u5230\u5408\u9002\u7684\u4e1a\u52a1\u5b50\u667a\u80fd\u4f53\u3002\\n\"\n    \"\u53ef\u9009\u7684\u4e1a\u52a1\u57df\u53ea\u6709\uff1aorder\uff08\u8ba2\u5355\uff09\u3001product\uff08\u4ea7\u54c1\uff09\u3001refund\uff08\u9000\u6b3e\uff09\u3002\\n\"\n    \"\u4f60\u5fc5\u987b\u8f93\u51fa\u4e00\u4e2a classifications \u5217\u8868\uff08\u7528\u4e8e\u5e76\u884c\u8c03\u7528\u591a\u4e2a\u5b50\u667a\u80fd\u4f53\uff09\u3002\\n\"\n    \"\u89c4\u5219\uff1a\\n\"\n    \"1) source \u5fc5\u987b\u662f\u4e0a\u8ff0\u4e09\u4e2a\u4e4b\u4e00\uff1b\\n\"\n    \"2) query \u5fc5\u987b\u662f\u53d1\u7ed9\u8be5\u5b50\u667a\u80fd\u4f53\u7684\u3001\u53ef\u76f4\u63a5\u6267\u884c\u7684\u4efb\u52a1\u63cf\u8ff0\uff1b\\n\"\n    \"3) \u5982\u679c\u7528\u6237\u4e00\u53e5\u8bdd\u4e2d\u540c\u65f6\u6d89\u53ca\u591a\u4e2a\u4e1a\u52a1\u57df\uff08\u4f8b\u5982\u2018\u67e5\u8ba2\u5355\u2019+\u2018\u770b\u4ea7\u54c1\u2019+\u2018\u95ee\u9000\u6b3e\u2019\uff09\uff0c\u5fc5\u987b\u62c6\u6210\u591a\u4e2a classification\uff0c\u4ee5\u4fbf\u5e76\u884c\u6267\u884c\uff1b\\n\"\n    \"4) \u5982\u679c\u65e0\u6cd5\u5224\u65ad\uff0c\u4f18\u5148\u9009\u62e9 product\uff0c\u5e76\u628a\u95ee\u9898\u539f\u6837\u4ea4\u7ed9\u5b83\u3002\\n\"\n    \"\u793a\u4f8bA\uff1a\u7528\u6237\uff1a\u2018\u67e5\u4e00\u4e0bORD-1\u7269\u6d41\uff0c\u5e76\u770b\u770b\u8fd9\u6b3e\u8033\u673a\u6709\u6ca1\u6709\u8d27\u2019 -&gt; \u8fd4\u56de2\u6761\uff1aorder(\u67e5\u8be2\u7269\u6d41)+product(\u67e5\u8be2\u5e93\u5b58)\u3002\\n\"\n    \"\u793a\u4f8bB\uff1a\u7528\u6237\uff1a\u2018\u6211\u60f3\u9000ORD-1\uff0c\u9000\u6b3e\u591a\u4e45\u5230\u8d26\u2019 -&gt; \u8fd4\u56de1\u6761\uff1arefund(\u53d1\u8d77/\u67e5\u8be2\u9000\u6b3e)\u3002\\n\"\n    \"\u793a\u4f8bC\uff1a\u7528\u6237\uff1a\u2018\u6211\u60f3\u77e5\u9053\u8fd9\u6b3e\u8033\u673a\u7684\u89c4\u683c\u2019 -&gt; \u8fd4\u56de1\u6761\uff1aproduct(\u67e5\u8be2\u8be6\u60c5)\u3002\\n\"\n)\n\ndef classify_query(state: RouterState):\n    structured_llm = model.with_structured_output(ClassificationResult)\n\n    classify_result = cast(\n        ClassificationResult,\n        structured_llm.invoke(\n            [\n                SystemMessage(ROUTER_SYSTEM_PROMPT),\n                HumanMessage(state[\"query\"]),\n            ]\n        ),\n    )\n\n    return {\"classifications\": classify_result.classifications}\n\ndef route_to_agents(state: RouterState) -&gt; list[Send]:\n    \"\"\"\u6839\u636e\u5206\u7c7b\u7ed3\u679c\u751f\u6210\u5e76\u884c\u6267\u884c\u7684\u6307\u4ee4\"\"\"\n    return [Send(c[\"source\"], {\"query\": c[\"query\"]}) for c in state[\"classifications\"]]\n</code></pre> <p>6. \u7f16\u6392\u5e76\u884c\u56fe\u4e0e\u6c47\u603b</p> <p>\u4f7f\u7528 <code>create_parallel_graph</code> \u521b\u5efa\u5e76\u884c\u5b50\u56fe\u3002\u8fd9\u91cc\u4f20\u5165 <code>branches_fn</code> \u53c2\u6570\uff0c\u5b9e\u73b0\u4e86\u6839\u636e\u6761\u4ef6\u7684\u52a8\u6001\u5e76\u884c\u6267\u884c\u3002</p> <pre><code>from langchain_dev_utils.graph import create_parallel_graph\n\nrouter_graph = create_parallel_graph(\n    nodes=[\n        order,\n        product,\n        refund,\n    ],\n    state_schema=RouterState,\n    branches_fn=route_to_agents, # \u6838\u5fc3\u903b\u8f91\uff1a\u7531\u51fd\u6570\u51b3\u5b9a\u8fd0\u884c\u54ea\u4e9b\u5206\u652f\n)\n</code></pre> <p>\u63a5\u4e0b\u6765\u7f16\u5199\u6c47\u603b\u8282\u70b9 <code>synthesize_results</code>\uff0c\u7528\u4e8e\u5c06\u5e76\u884c\u6267\u884c\u7684\u7ed3\u679c\u6574\u5408\u4e3a\u901a\u987a\u7684\u56de\u7b54\u3002</p> <pre><code>SYNTHESIS_SYSTEM_PROMPT = (\n    \"\u7efc\u5408\u8fd9\u4e9b\u7ed3\u679c\u4ee5\u56de\u7b54\u539f\u59cb\u95ee\u9898\uff1a{query}\\n\"\n    \"- \u5408\u5e76\u6765\u81ea\u591a\u4e2a\u6765\u6e90\u7684\u4fe1\u606f\uff0c\u907f\u514d\u5197\u4f59\\n\"\n    \"- \u7a81\u51fa\u6700\u76f8\u5173\u4e14\u53ef\u64cd\u4f5c\u7684\u4fe1\u606f\\n\"\n    \"- \u6ce8\u660e\u6765\u6e90\u4e4b\u95f4\u7684\u4efb\u4f55\u5dee\u5f02\\n\"\n    \"- \u4fdd\u6301\u56de\u7b54\u7b80\u6d01\u4e14\u6761\u7406\u6e05\u6670\\n\"\n)\n\ndef synthesize_results(state: RouterState) -&gt; dict:\n    if not state[\"results\"]:\n        return {\"final_answer\": \"No results found from any knowledge source.\"}\n\n    # \u683c\u5f0f\u5316\u5404\u5b50\u667a\u80fd\u4f53\u7684\u8f93\u51fa\n    formatted = [\n        f\"**From {r['source'].title()}:**\\n{r['result']}\" for r in state[\"results\"]\n    ]\n\n    synthesis_response = model.invoke(\n        [\n            {\n                \"role\": \"system\",\n                \"content\": SYNTHESIS_SYSTEM_PROMPT.format(query=state[\"query\"]),\n            },\n            {\"role\": \"user\", \"content\": \"\\n\\n\".join(formatted)},\n        ]\n    )\n\n    return {\"final_answer\": synthesis_response.content}\n</code></pre> <p>7. \u6784\u5efa\u6700\u7ec8\u7684StateGraph</p> <p>\u6700\u540e\uff0c\u4f7f\u7528 <code>create_sequential_graph</code> \u5c06\u201c\u610f\u56fe\u5206\u7c7b -&gt; \u6309\u9700\u5e76\u884c -&gt; \u7ed3\u679c\u6c47\u603b\u201d\u4e32\u8054\u8d77\u6765\uff0c\u5f62\u6210\u5b8c\u6574\u7684\u5e94\u7528\u6d41\u3002</p> <pre><code>from langchain_dev_utils.graph import create_sequential_graph\n\ngraph = create_sequential_graph(\n    nodes=[\n        classify_query,\n        router_graph,\n        synthesize_results,\n    ],\n    state_schema=RouterState,\n)\n</code></pre> <p>8. \u8fd0\u884c\u793a\u4f8b</p> <pre><code># \u793a\u4f8b 1\uff1a\u5355\u4e00\u610f\u56fe\uff08\u4ea7\u54c1\u67e5\u8be2\uff09\nresponse = graph.invoke({\"query\": \"\u4f60\u597d\uff0c\u6211\u8981\u67e5\u8be2\u4e00\u4e0b\u4e4b\u524d\u8d2d\u4e70\u7684\u4ea7\u54c1\"})\nprint(response[\"final_answer\"])\n\n# \u793a\u4f8b 2\uff1a\u6df7\u5408\u610f\u56fe\uff08\u4ea7\u54c1\u67e5\u8be2 + \u9000\u6b3e\u653f\u7b56\uff09\uff0c\u5c06\u89e6\u53d1\u5e76\u884c\u6267\u884c\nresponse = graph.invoke({\"query\": \"\u63a8\u8350\u4e00\u6b3e\u9002\u5408\u901a\u52e4\u7684\u65e0\u7ebf\u8033\u673a\u5e76\u770b\u770b\u5e93\u5b58\uff1b\u540c\u65f6\uff0c\u544a\u8bc9\u6211\u4f60\u4eec\u5546\u54c1\u7684\u9000\u6b3e\u653f\u7b56\uff1f\"})\nprint(response[\"final_answer\"])\n</code></pre>"},{"location":"zh/adavance-guide/multi-agent/","title":"\u5b50\u667a\u80fd\u4f53\u5de5\u5177\uff08Agent as Tool\uff09","text":""},{"location":"zh/adavance-guide/multi-agent/#_1","title":"\u6982\u8ff0","text":"<p>\u5728\u6784\u5efa\u590d\u6742\u7684 AI \u5e94\u7528\u65f6\uff0c\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u662f\u4e00\u79cd\u5f3a\u5927\u7684\u67b6\u6784\u6a21\u5f0f\u3002\u901a\u8fc7\u5c06\u4e0d\u540c\u804c\u8d23\u5206\u914d\u7ed9\u4e13\u95e8\u7684\u667a\u80fd\u4f53\uff0c\u53ef\u4ee5\u5b9e\u73b0\u4efb\u52a1\u7684\u4e13\u4e1a\u5316\u5206\u5de5\u548c\u9ad8\u6548\u534f\u4f5c\u3002</p> <p>\u5b9e\u73b0\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6709\u591a\u79cd\u65b9\u5f0f\uff0c\u5176\u4e2d\u5de5\u5177\u8c03\u7528\u662f\u4e00\u79cd\u5e38\u7528\u4e14\u7075\u6d3b\u7684\u5b9e\u73b0\u65b9\u5f0f\u3002\u901a\u8fc7\u5c06\u5b50\u667a\u80fd\u4f53\uff08subagents\uff09\u5c01\u88c5\u4e3a\u5de5\u5177\uff0c\u4e3b\u667a\u80fd\u4f53\u53ef\u4ee5\u6839\u636e\u4efb\u52a1\u9700\u6c42\u52a8\u6001\u59d4\u6d3e\u7ed9\u4e13\u95e8\u7684\u5b50\u667a\u80fd\u4f53\u5904\u7406\u3002</p> <p>\u672c\u5e93\u63d0\u4f9b\u4e86\u4e24\u4e2a\u9884\u6784\u5efa\u51fd\u6570\u6765\u7b80\u5316\u8fd9\u79cd\u5b9e\u73b0\u65b9\u5f0f\uff1a</p> \u51fd\u6570\u540d \u529f\u80fd\u63cf\u8ff0 <code>wrap_agent_as_tool</code> \u5c06\u5355\u4e2a\u667a\u80fd\u4f53\u5b9e\u4f8b\u5c01\u88c5\u4e3a\u4e00\u4e2a\u72ec\u7acb\u5de5\u5177 <code>wrap_all_agents_as_tool</code> \u5c06\u591a\u4e2a\u667a\u80fd\u4f53\u5b9e\u4f8b\u5c01\u88c5\u4e3a\u4e00\u4e2a\u7edf\u4e00\u5de5\u5177\uff0c\u901a\u8fc7\u53c2\u6570\u6307\u5b9a\u8c03\u7528\u54ea\u4e2a\u5b50\u667a\u80fd\u4f53"},{"location":"zh/adavance-guide/multi-agent/#_2","title":"\u5c01\u88c5\u5355\u4e2a\u667a\u80fd\u4f53\u4e3a\u5de5\u5177","text":"<p>\u5c01\u88c5\u5355\u4e2a\u667a\u80fd\u4f53\u53ea\u9700\u4e09\u6b65\uff1a</p> <ol> <li>\u5bfc\u5165 <code>wrap_agent_as_tool</code></li> <li>\u628a\u667a\u80fd\u4f53\u5b9e\u4f8b\u4f5c\u4e3a\u53c2\u6570\u4f20\u5165</li> <li>\u83b7\u5f97\u53ef\u76f4\u63a5\u88ab\u5176\u4ed6\u667a\u80fd\u4f53\u8c03\u7528\u7684\u5de5\u5177\u5bf9\u8c61</li> </ol>"},{"location":"zh/adavance-guide/multi-agent/#_3","title":"\u4f7f\u7528\u793a\u4f8b","text":"<p>\u4e0b\u9762\u6211\u4eec\u4ee5 <code>supervisor</code> \u667a\u80fd\u4f53\u4e3a\u4f8b\uff0c\u4ecb\u7ecd\u5982\u4f55\u901a\u8fc7 <code>wrap_agent_as_tool</code> \u5c06\u5b50\u667a\u80fd\u4f53\u5c01\u88c5\u4e3a\u5de5\u5177\u3002</p> <p>\u9996\u5148\u5b9e\u73b0\u4e24\u4e2a\u5b50\u667a\u80fd\u4f53\uff0c\u4e00\u4e2a\u7528\u4e8e\u53d1\u9001\u90ae\u4ef6\uff0c\u4e00\u4e2a\u7528\u4e8e\u65e5\u7a0b\u67e5\u8be2\u548c\u5b89\u6392\u3002</p> <p>\u9996\u5148\u662f\u90ae\u4ef6\u667a\u80fd\u4f53\u7684\u5b9e\u73b0</p> <pre><code>from langchain_core.tools import tool\nfrom langchain_dev_utils.chat_models import register_model_provider\nfrom langchain_dev_utils.agents import create_agent, wrap_agent_as_tool \n\nregister_model_provider(\n    \"vllm\",\n    \"openai-compatible\",\n    base_url=\"http://localhost:8000/v1\",\n)\n\n\n@tool\ndef send_email(\n    to: list[str],  # \u7535\u5b50\u90ae\u4ef6\u5730\u5740\n    subject: str,\n    body: str,\n    cc: list[str] = [],\n) -&gt; str:\n    \"\"\"\u901a\u8fc7\u7535\u5b50\u90ae\u4ef6API\u53d1\u9001\u90ae\u4ef6\u3002\u8981\u6c42\u6b63\u786e\u683c\u5f0f\u7684\u5730\u5740\u3002\"\"\"\n    # \u5b58\u6839\uff1a\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u8fd9\u91cc\u4f1a\u8c03\u7528SendGrid\u3001Gmail API\u7b49\n    return f\"\u90ae\u4ef6\u5df2\u53d1\u9001\u81f3 {', '.join(to)} - \u4e3b\u9898: {subject}\"\n\n\nEMAIL_AGENT_PROMPT = (\n    \"\u4f60\u662f\u4e00\u4e2a\u7535\u5b50\u90ae\u4ef6\u52a9\u624b\u3002\"\n    \"\u6839\u636e\u81ea\u7136\u8bed\u8a00\u8bf7\u6c42\u64b0\u5199\u4e13\u4e1a\u90ae\u4ef6\u3002\"\n    \"\u63d0\u53d6\u6536\u4ef6\u4eba\u4fe1\u606f\u5e76\u5236\u4f5c\u6070\u5f53\u7684\u4e3b\u9898\u884c\u548c\u6b63\u6587\u5185\u5bb9\u3002\"\n    \"\u4f7f\u7528 send_email \u6765\u53d1\u9001\u90ae\u4ef6\u3002\"\n    \"\u59cb\u7ec8\u5728\u6700\u7ec8\u56de\u590d\u4e2d\u786e\u8ba4\u5df2\u53d1\u9001\u7684\u5185\u5bb9\u3002\"\n)\n\nemail_agent = create_agent(\n    \"vllm:qwen3-4b\",\n    tools=[send_email],\n    system_prompt=EMAIL_AGENT_PROMPT,\n    name=\"email_agent\",\n)\n</code></pre> <p>\u4ee5\u53ca\u65e5\u5e38\u65e5\u7a0b\u667a\u80fd\u4f53\u7684\u5b9e\u73b0</p> <pre><code>@tool\ndef create_calendar_event(\n    title: str,\n    start_time: str,  # ISO\u683c\u5f0f: \"2024-01-15T14:00:00\"\n    end_time: str,  # ISO\u683c\u5f0f: \"2024-01-15T15:00:00\"\n    attendees: list[str],  # \u7535\u5b50\u90ae\u4ef6\u5730\u5740\n    location: str = \"\",\n) -&gt; str:\n    \"\"\"\u521b\u5efa\u65e5\u5386\u4e8b\u4ef6\u3002\u8981\u6c42\u7cbe\u786e\u7684ISO\u65e5\u671f\u65f6\u95f4\u683c\u5f0f\u3002\"\"\"\n    # \u5b58\u6839\uff1a\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u8fd9\u91cc\u4f1a\u8c03\u7528Google Calendar API\u3001Outlook API\u7b49\n    return f\"\u4e8b\u4ef6\u5df2\u521b\u5efa\uff1a{title} \u4ece {start_time} \u5230 {end_time}\uff0c\u5171\u6709 {len(attendees)} \u4f4d\u53c2\u4e0e\u8005\"\n\n\n@tool\ndef get_available_time_slots(\n    attendees: list[str],\n    date: str,  # ISO\u683c\u5f0f: \"2024-01-15\"\n    duration_minutes: int,\n) -&gt; list[str]:\n    \"\"\"\u5728\u7279\u5b9a\u65e5\u671f\u67e5\u8be2\u53c2\u4e0e\u8005\u7684\u65e5\u5386\u53ef\u7528\u65f6\u95f4\u3002\"\"\"\n    # \u5b58\u6839\uff1a\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u8fd9\u91cc\u4f1a\u67e5\u8be2\u65e5\u5386API\n    return [\"09:00\", \"14:00\", \"16:00\"]\n\n\nCALENDAR_AGENT_PROMPT = (\n    \"\u4f60\u662f\u4e00\u4e2a\u65e5\u5386\u65e5\u7a0b\u5b89\u6392\u52a9\u624b\u3002\"\n    \"\u5c06\u81ea\u7136\u8bed\u8a00\u7684\u65e5\u7a0b\u5b89\u6392\u8bf7\u6c42\uff08\u4f8b\u5982'\u4e0b\u5468\u4e8c\u4e0b\u53482\u70b9'\uff09\u89e3\u6790\u4e3a\u6b63\u786e\u7684ISO\u65e5\u671f\u65f6\u95f4\u683c\u5f0f\u3002\"\n    \"\u9700\u8981\u65f6\u4f7f\u7528 get_available_time_slots \u6765\u68c0\u67e5\u53ef\u7528\u65f6\u95f4\u3002\"\n    \"\u4f7f\u7528 create_calendar_event \u6765\u5b89\u6392\u4e8b\u4ef6\u3002\"\n    \"\u59cb\u7ec8\u5728\u6700\u7ec8\u56de\u590d\u4e2d\u786e\u8ba4\u5df2\u5b89\u6392\u7684\u5185\u5bb9\u3002\"\n)\n\ncalendar_agent = create_agent(\n    \"vllm:qwen3-4b\",\n    tools=[create_calendar_event, get_available_time_slots],\n    system_prompt=CALENDAR_AGENT_PROMPT,\n    name=\"calendar_agent\",\n)\n</code></pre> <p>\u63a5\u4e0b\u6765\uff0c\u4f7f\u7528 <code>wrap_agent_as_tool</code> \u5c06\u8fd9\u4e24\u4e2a\u5b50\u667a\u80fd\u4f53\u5c01\u88c5\u4e3a\u5de5\u5177\u3002</p> <pre><code>schedule_event = wrap_agent_as_tool(\n    calendar_agent,\n    tool_name=\"schedule_event\",\n    tool_description=(\n        \"\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u5b89\u6392\u65e5\u5386\u4e8b\u4ef6\u3002\"\n        \"\u5728\u7528\u6237\u60f3\u8981\u521b\u5efa\u3001\u4fee\u6539\u6216\u68c0\u67e5\u65e5\u5386\u7ea6\u4f1a\u65f6\u4f7f\u7528\u6b64\u529f\u80fd\u3002\"\n        \"\u80fd\u591f\u5904\u7406\u65e5\u671f/\u65f6\u95f4\u89e3\u6790\u3001\u67e5\u8be2\u53ef\u7528\u65f6\u95f4\u548c\u521b\u5efa\u4e8b\u4ef6\u3002\"\n        \"\u8f93\u5165\uff1a\u81ea\u7136\u8bed\u8a00\u65e5\u5386\u5b89\u6392\u8bf7\u6c42\uff08\u4f8b\u5982'\u4e0e\u8bbe\u8ba1\u56e2\u961f\u4e0b\u4e2a\u661f\u671f\u4e8c\u4e0b\u53482\u70b9\u7684\u4f1a\u8bae'\uff09\"\n    ),\n)\nmanage_email = wrap_agent_as_tool(\n    email_agent,\n    tool_name=\"manage_email\",\n    tool_description=(\n        \"\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u53d1\u9001\u7535\u5b50\u90ae\u4ef6\u3002\"\n        \"\u5728\u7528\u6237\u60f3\u8981\u53d1\u9001\u901a\u77e5\u3001\u63d0\u9192\u6216\u4efb\u4f55\u7535\u5b50\u90ae\u4ef6\u901a\u4fe1\u65f6\u4f7f\u7528\u6b64\u529f\u80fd\u3002\"\n        \"\u80fd\u591f\u63d0\u53d6\u6536\u4ef6\u4eba\u4fe1\u606f\u3001\u4e3b\u9898\u751f\u6210\u548c\u7535\u5b50\u90ae\u4ef6\u64b0\u5199\u3002\"\n        \"\u8f93\u5165\uff1a\u81ea\u7136\u8bed\u8a00\u7535\u5b50\u90ae\u4ef6\u8bf7\u6c42\uff08\u4f8b\u5982'\u5411\u4ed6\u4eec\u53d1\u9001\u4f1a\u8bae\u63d0\u9192'\uff09\"\n    ),\n)\n</code></pre> <p>\u63d0\u793a</p> <p><code>wrap_agent_as_tool</code> \u7684 <code>tool_name</code> \u4e0e <code>tool_description</code> \u5747\u4e3a\u53ef\u9009\u53c2\u6570\uff1b\u82e5\u7701\u7565\uff0c\u5de5\u5177\u540d\u9ed8\u8ba4\u4e3a <code>transfer_to_{agent_name}</code>\uff0c\u63cf\u8ff0\u9ed8\u8ba4\u4e3a <code>This tool transforms input to {agent_name}</code>\u3002 \u4e3a\u8ba9\u4e3b\u667a\u80fd\u4f53\u66f4\u7cbe\u51c6\u5730\u8bc6\u522b\u4e0e\u8c03\u7528\u5b50\u667a\u80fd\u4f53\uff0c\u5efa\u8bae\u663e\u5f0f\u6307\u5b9a\u8fd9\u4e24\u9879\uff08\u5c24\u5176\u662f\u63cf\u8ff0\uff09\uff0c\u4ee5\u6e05\u6670\u4f20\u8fbe\u5b50\u667a\u80fd\u4f53\u7684\u804c\u8d23\u4e0e\u80fd\u529b\u3002</p> <p>\u6700\u7ec8\u521b\u5efa\u4e00\u4e2a <code>supervisor_agent</code>\uff0c\u5b83\u53ef\u4ee5\u8c03\u7528\u8fd9\u4e24\u4e2a\u5de5\u5177\u3002</p> <pre><code>SUPERVISOR_PROMPT = (\n    \"\u4f60\u662f\u4e00\u4e2a\u6709\u7528\u7684\u4e2a\u4eba\u52a9\u624b\u3002\"\n    \"\u4f60\u53ef\u4ee5\u5b89\u6392\u65e5\u5386\u4e8b\u4ef6\u5e76\u53d1\u9001\u7535\u5b50\u90ae\u4ef6\u3002\"\n    \"\u5c06\u7528\u6237\u8bf7\u6c42\u5206\u89e3\u4e3a\u9002\u5f53\u7684\u5de5\u5177\u8c03\u7528\uff0c\u5e76\u534f\u8c03\u7ed3\u679c\u3002\"\n    \"\u5f53\u8bf7\u6c42\u6d89\u53ca\u591a\u4e2a\u64cd\u4f5c\u65f6\uff0c\u8bf7\u4f7f\u7528\u591a\u4e2a\u5de5\u5177\u6309\u987a\u5e8f\u64cd\u4f5c\u3002\"\n)\n\n\nsupervisor_agent = create_agent(\n    \"vllm:qwen3-4b\",\n    tools=[schedule_event, manage_email],\n    system_prompt=SUPERVISOR_PROMPT,\n)\n\nprint(\n    supervisor_agent.invoke({\"messages\": [HumanMessage(content=\"\u67e5\u8be2\u660e\u5929\u7684\u7a7a\u95f2\u65f6\u95f4\")]})\n)\nprint(\n    supervisor_agent.invoke(\n        {\"messages\": [HumanMessage(content=\"\u7ed9test@123.com\u53d1\u9001\u90ae\u4ef6\u4f1a\u8bae\u63d0\u9192\")]}\n    )\n)\n</code></pre> <p>\u63d0\u793a</p> <p>\u4e0a\u8ff0\u793a\u4f8b\u4e2d\uff0c\u6211\u4eec\u662f\u4ece <code>langchain_dev_utils.agents</code> \u4e2d\u5bfc\u5165\u4e86 <code>create_agent</code> \u51fd\u6570\uff0c\u800c\u4e0d\u662f <code>langchain.agents</code>\u3002\u8fd9\u662f\u56e0\u4e3a\u672c\u5e93\u4e5f\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e0e\u5b98\u65b9 <code>create_agent</code> \u51fd\u6570\u529f\u80fd\u5b8c\u5168\u76f8\u540c\u7684\u51fd\u6570\uff0c\u53ea\u662f\u6269\u5145\u4e86\u901a\u8fc7\u5b57\u7b26\u4e32\u6307\u5b9a\u6a21\u578b\u7684\u529f\u80fd\u3002\u8fd9\u4f7f\u5f97\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 <code>register_model_provider</code> \u6ce8\u518c\u7684\u6a21\u578b\uff0c\u800c\u65e0\u9700\u521d\u59cb\u5316\u6a21\u578b\u5b9e\u4f8b\u540e\u4f20\u5165\u3002</p>"},{"location":"zh/adavance-guide/multi-agent/#_4","title":"\u5c01\u88c5\u591a\u4e2a\u667a\u80fd\u4f53\u4e3a\u5355\u4e00\u5de5\u5177","text":"<p>\u5c06\u591a\u4e2a\u667a\u80fd\u4f53\u5c01\u88c5\u4e3a\u5355\u4e00\u5de5\u5177\u53ea\u9700\u4e09\u6b65\uff1a</p> <ol> <li>\u5bfc\u5165 <code>wrap_all_agents_as_tool</code></li> <li>\u628a\u591a\u4e2a\u667a\u80fd\u4f53\u5b9e\u4f8b\u4f5c\u4e3a\u5217\u8868\u4e00\u6b21\u6027\u4f20\u5165</li> <li>\u83b7\u5f97\u53ef\u76f4\u63a5\u88ab\u5176\u4ed6\u667a\u80fd\u4f53\u8c03\u7528\u7684\u7edf\u4e00\u5de5\u5177\u5bf9\u8c61</li> </ol>"},{"location":"zh/adavance-guide/multi-agent/#_5","title":"\u4f7f\u7528\u793a\u4f8b","text":"<p>\u5bf9\u4e8e\u4e0a\u4e00\u4e2a\u793a\u4f8b\u7684 <code>calendar_agent</code> \u548c <code>email_agent</code>\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06\u5b83\u4eec\u5c01\u88c5\u4e3a\u4e00\u4e2a\u5de5\u5177 <code>call_subagent</code>\uff1a</p> <pre><code>call_subagent_tool = wrap_all_agents_as_tool(\n    [calendar_agent, email_agent],\n    tool_name=\"call_subagent\",\n    tool_description=(\n        \"\u8c03\u7528\u5b50\u667a\u80fd\u4f53\u6267\u884c\u4efb\u52a1\u3002\"\n        \"\u53ef\u4ee5\u4f7f\u7528\u7684\u667a\u80fd\u4f53\u6709\uff1a\"\n        \"- calendar_agent\uff1a\u7528\u4e8e\u5b89\u6392\u65e5\u5386\u4e8b\u4ef6\"\n        \"- email_agent\uff1a\u7528\u4e8e\u53d1\u9001\u7535\u5b50\u90ae\u4ef6\"\n    ),\n)\n\nMAIN_AGENT_PROMPT = (\n    \"\u4f60\u662f\u4e00\u4e2a\u6709\u7528\u7684\u4e2a\u4eba\u52a9\u624b\u3002\"\n    \"\u4f60\u53ef\u4ee5\u4f7f\u7528**call_subagent**\u5de5\u5177\u8c03\u7528\u5b50\u667a\u80fd\u4f53\u6267\u884c\u4efb\u52a1\u3002\"\n    \"\u5c06\u7528\u6237\u8bf7\u6c42\u5206\u89e3\u4e3a\u9002\u5f53\u7684\u5de5\u5177\u8c03\u7528\uff0c\u5e76\u534f\u8c03\u7ed3\u679c\u3002\"\n    \"\u5f53\u8bf7\u6c42\u6d89\u53ca\u591a\u4e2a\u64cd\u4f5c\u65f6\uff0c\u8bf7\u4f7f\u7528\u591a\u4e2a\u5de5\u5177\u6309\u987a\u5e8f\u64cd\u4f5c\u3002\"\n)\n\nmain_agent = create_agent(\n    \"vllm:qwen3-4b\",\n    tools=[call_subagent_tool],\n    system_prompt=MAIN_AGENT_PROMPT,\n)\n</code></pre> <p>\u63d0\u793a</p> <p><code>wrap_all_agents_as_tool</code> \u7684 <code>tool_name</code> \u4e0e <code>tool_description</code> \u5747\u4e3a\u53ef\u9009\u53c2\u6570\uff1b\u82e5\u7701\u7565\uff0c\u5de5\u5177\u540d\u9ed8\u8ba4\u4e3a <code>task</code>\uff0c\u63cf\u8ff0\u9ed8\u8ba4\u4e3a <code>Launch an ephemeral subagent for a task.\\nAvailable agents:\\n {available_agents}</code>\u3002 \u4e3a\u786e\u4fdd\u4e3b\u667a\u80fd\u4f53\u51c6\u786e\u8bc6\u522b\u5e76\u8c03\u7528\u5b50\u667a\u80fd\u4f53\uff0c\u5efa\u8bae\u663e\u5f0f\u586b\u5199\u8fd9\u4e24\u9879\uff0c\u5c24\u5176\u662f\u63cf\u8ff0\uff0c\u4ee5\u4fbf\u6e05\u6670\u4f20\u8fbe\u5404\u5b50\u667a\u80fd\u4f53\u7684\u804c\u8d23\u4e0e\u80fd\u529b\u3002</p> <p>\u63d0\u793a</p> <p>\u9664\u4e86\u4f7f\u7528\u672c\u5e93\u63d0\u4f9b\u7684 <code>wrap_all_agents_as_tool</code> \u5c06\u591a\u4e2a\u667a\u80fd\u4f53\u5c01\u88c5\u4e3a\u5355\u4e00\u5de5\u5177\u5916\uff0c\u4f60\u8fd8\u53ef\u4ee5\u4f7f\u7528 <code>deepagents</code> \u5e93\u63d0\u4f9b\u7684 <code>SubAgentMiddleware</code> \u4e2d\u95f4\u4ef6\u5b9e\u73b0\u7c7b\u4f3c\u7684\u6548\u679c\u3002</p>"},{"location":"zh/adavance-guide/multi-agent/#_6","title":"\u94a9\u5b50\u51fd\u6570\u673a\u5236","text":"<p>\u672c\u5e93\u5185\u7f6e\u4e86\u7075\u6d3b\u7684\u94a9\u5b50\uff08hook\uff09\u673a\u5236\uff0c\u5141\u8bb8\u5728\u5b50\u667a\u80fd\u4f53\u8fd0\u884c\u524d\u540e\u63d2\u5165\u81ea\u5b9a\u4e49\u903b\u8f91\u3002\u8be5\u673a\u5236\u540c\u65f6\u9002\u7528\u4e8e <code>wrap_agent_as_tool</code> \u4e0e <code>wrap_all_agents_as_tool</code>\uff0c\u4e0b\u6587\u4ee5 <code>wrap_agent_as_tool</code> \u4e3a\u4f8b\u8fdb\u884c\u8bf4\u660e\u3002</p> <p>\u5bf9\u4e8e\u94a9\u5b50\u51fd\u6570\u7684\u8fd0\u884c\u6d41\u7a0b\uff0c\u5982\u4e0b\u56fe\u6240\u793a\uff1a</p> <pre><code>graph LR\n    Start([\u5f00\u59cb]) --&gt; InHook{process_input \u5b58\u5728?}\n\n    %% --- \u8f93\u5165\u94a9\u5b50 ---\n    InHook -- \u662f --&gt; DoInHook[\u6267\u884c process_input]\n    InHook -- \u5426 --&gt; Raw[\u4f7f\u7528\u539f\u59cb request]\n\n    DoInHook --&gt; TypeCheck{\u8fd4\u56de\u503c\u7c7b\u578b\u5224\u65ad}\n    Raw --&gt; TypeCheck\n\n    TypeCheck -- \u5b57\u7b26\u4e32 --&gt; BuildDict[\u6784\u9020\u5305\u542b messages \u7684\u5b57\u5178]\n    TypeCheck -- \u5b57\u5178 --&gt; UseDict[\u76f4\u63a5\u4f7f\u7528\u8be5\u5b57\u5178]\n\n    BuildDict --&gt; Core\n    UseDict --&gt; Core\n\n    %% --- \u4e2d\u95f4\u6267\u884c ---\n    Core([\u6267\u884c agent.invoke])\n\n    %% --- \u8f93\u51fa\u94a9\u5b50 ---\n    Core --&gt; OutHook{process_output \u5b58\u5728?}\n\n    OutHook -- \u662f --&gt; DoOutHook[\u6267\u884c process_output]\n    OutHook -- \u5426 --&gt; ExtractContent[\u63d0\u53d6 response \u54cd\u5e94\u4e2d\u7684&lt;br/&gt; messages \u5217\u8868\u6700\u540e\u4e00\u9879\u7684 \u6587\u672c\u5185\u5bb9]\n\n    DoOutHook --&gt; End([\u8fd4\u56de\u7ed3\u679c])\n    ExtractContent --&gt; End</code></pre>"},{"location":"zh/adavance-guide/multi-agent/#1-pre_input_hooks","title":"1. pre_input_hooks","text":"<p>\u5728\u667a\u80fd\u4f53\u8fd0\u884c\u524d\u5bf9\u8f93\u5165\u8fdb\u884c\u9884\u5904\u7406\u3002\u53ef\u7528\u4e8e\u8f93\u5165\u589e\u5f3a\u3001\u4e0a\u4e0b\u6587\u6ce8\u5165\u3001\u683c\u5f0f\u6821\u9a8c\u3001\u6743\u9650\u68c0\u67e5\u7b49\u3002</p>"},{"location":"zh/adavance-guide/multi-agent/#_7","title":"\u652f\u6301\u7684\u4f20\u5165\u7c7b\u578b","text":"\u7c7b\u578b \u8bf4\u660e \u5355\u4e2a\u540c\u6b65\u51fd\u6570 \u540c\u65f6\u7528\u4e8e\u540c\u6b65\uff08<code>invoke</code>\uff09\u548c\u5f02\u6b65\uff08<code>ainvoke</code>\uff09\u8c03\u7528\u8def\u5f84\uff08\u5f02\u6b65\u8def\u5f84\u4e2d\u4e0d\u4f1a <code>await</code>\uff0c\u76f4\u63a5\u8c03\u7528\uff09 \u4e8c\u5143\u7ec4 <code>(sync_func, async_func)</code> \u7b2c\u4e00\u4e2a\u51fd\u6570\u7528\u4e8e\u540c\u6b65\u8c03\u7528\u8def\u5f84\uff1b\u7b2c\u4e8c\u4e2a\u51fd\u6570\uff08\u5fc5\u987b\u662f <code>async def</code>\uff09\u7528\u4e8e\u5f02\u6b65\u8c03\u7528\u8def\u5f84\uff0c\u5e76\u4f1a\u88ab <code>await</code>"},{"location":"zh/adavance-guide/multi-agent/#_8","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def pre_input_hook(request: str, runtime: ToolRuntime) -&gt; str | dict[str, Any]:\n    \"\"\"\n    \u53c2\u6570:\n        request: \u539f\u59cb\u5de5\u5177\u8c03\u7528\u8f93\u5165\n        runtime: langchain \u7684 ToolRuntime\n\n    \u8fd4\u56de:\n        \u5904\u7406\u540e\u7684\u8f93\u5165\uff0c\u4f5c\u4e3a agent \u7684\u5b9e\u9645\u8f93\u5165\uff08\u9700\u8981\u662f str \u6216 dict\uff09\n    \"\"\"\n</code></pre> <p>\u6ce8\u610f\uff1a</p> <ul> <li> <p>\u94a9\u5b50\u51fd\u6570\u7684\u8fd4\u56de\u503c\u5fc5\u987b\u662f str \u6216 dict\uff0c\u5426\u5219\u4f1a\u5f15\u53d1 ValueError\u3002</p> </li> <li> <p>\u82e5\u8fd4\u56de dict\uff0c\u5219\u4f1a\u88ab\u76f4\u63a5\u4f5c\u4e3a agent \u7684\u5b9e\u9645\u8f93\u5165\u3002</p> </li> <li> <p>\u82e5\u8fd4\u56de str\uff0c\u5219\u4f1a\u88ab\u5305\u88c5\u4e3a <code>HumanMessage(content=...)</code>\uff0c\u6700\u7ec8\u4ee5 <code>{\"messages\": [HumanMessage(content=...)]}</code> \u4f5c\u4e3a agent \u7684\u5b9e\u9645\u8f93\u5165\u3002</p> </li> <li> <p>\u82e5\u672a\u63d0\u4f9b <code>pre_input_hooks</code>\uff0c\u5219\u76f4\u63a5\u5c06\u539f\u59cb\u8f93\u5165\u4ee5 <code>{\"messages\": [HumanMessage(content=request)]}</code> \u4f5c\u4e3a agent \u7684\u5b9e\u9645\u8f93\u5165\u3002</p> </li> </ul>"},{"location":"zh/adavance-guide/multi-agent/#_9","title":"\u4f7f\u7528\u793a\u4f8b","text":"<p>\u4f8b\u5982\uff0c\u5c06\u989d\u5916\u7684\u4f1a\u8bdd\u4e0a\u4e0b\u6587\u4f20\u9012\u7ed9SubAgent\uff0c\u4ee5\u63d0\u4f9b\u66f4\u7cbe\u51c6\u7684\u4efb\u52a1\u4e0a\u4e0b\u6587\u3002</p> <pre><code>from langchain.tools import ToolRuntime\nfrom langchain_dev_utils.agents import wrap_agent_as_tool\n\n\ndef process_input(request: str, runtime: ToolRuntime) -&gt; str:\n    original_user_message = next(\n        message for message in runtime.state[\"messages\"] if message.type == \"human\"\n    )\n    prompt = (\n        \"\u4f60\u6b63\u5728\u534f\u52a9\u5904\u7406\u4ee5\u4e0b\u7528\u6237\u8be2\u95ee\uff1a\\n\\n\"\n        f\"{original_user_message.text}\\n\\n\"\n        \"\u4f60\u88ab\u5206\u914d\u4e86\u4ee5\u4e0b\u5b50\u4efb\u52a1\uff1a\\n\\n\"\n        f\"{request}\"\n    )\n    return prompt\n\n\n# \u4f7f\u7528\ncall_agent_tool = wrap_agent_as_tool(agent, pre_input_hooks=process_input)\n</code></pre>"},{"location":"zh/adavance-guide/multi-agent/#2-post_output_hooks","title":"2. post_output_hooks","text":"<p>\u5728\u667a\u80fd\u4f53\u8fd0\u884c\u5b8c\u6210\u540e\uff0c\u5bf9\u5176\u8fd4\u56de\u7684\u5b8c\u6574\u6d88\u606f\u5217\u8868\u8fdb\u884c\u540e\u5904\u7406\uff0c\u4ee5\u751f\u6210\u5de5\u5177\u7684\u6700\u7ec8\u8fd4\u56de\u503c\u3002\u53ef\u7528\u4e8e\u7ed3\u679c\u63d0\u53d6\u3001\u7ed3\u6784\u5316\u8f6c\u6362\u7b49\u3002</p>"},{"location":"zh/adavance-guide/multi-agent/#_10","title":"\u652f\u6301\u7684\u4f20\u5165\u7c7b\u578b","text":"\u7c7b\u578b \u8bf4\u660e \u5355\u4e2a\u51fd\u6570 \u540c\u65f6\u7528\u4e8e\u540c\u6b65\u548c\u5f02\u6b65\u8def\u5f84\uff08\u5f02\u6b65\u8def\u5f84\u4e2d\u4e0d <code>await</code>\uff09 \u4e8c\u5143\u7ec4 <code>(sync_func, async_func)</code> \u7b2c\u4e00\u4e2a\u7528\u4e8e\u540c\u6b65\u8def\u5f84\uff1b\u7b2c\u4e8c\u4e2a\uff08<code>async def</code>\uff09\u7528\u4e8e\u5f02\u6b65\u8def\u5f84\uff0c\u5e76\u4f1a\u88ab <code>await</code>"},{"location":"zh/adavance-guide/multi-agent/#_11","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def post_output_hook(request: str, response: dict[str, Any], runtime: ToolRuntime) -&gt; Union[str, Command]:\n    \"\"\"\n    \u53c2\u6570:\n        request: \u672a\u7ecf\u5904\u7406\u7684\u539f\u59cb\u8f93\u5165\n        response: agent \u8fd4\u56de\u7684\u5b8c\u6574\u54cd\u5e94\n        runtime: langchain \u7684 ToolRuntime\n\n    \u8fd4\u56de:\n        \u80fd\u591f\u88ab\u5e8f\u5217\u5316\u4e3a\u5b57\u7b26\u4e32\u7684\u503c\uff0c\u6216\u8005\u662f Command \u5bf9\u8c61\n    \"\"\"\n</code></pre> <p>\u6ce8\u610f\uff1a</p> <ul> <li> <p>\u94a9\u5b50\u51fd\u6570\u7684\u8fd4\u56de\u503c\u5fc5\u987b\u662f\u53ef\u4ee5\u88ab\u5e8f\u5217\u5316\u4e3a\u5b57\u7b26\u4e32\u7684\u503c\u6216\u8005 <code>Command</code> \u5bf9\u8c61\u3002</p> </li> <li> <p>\u94a9\u5b50\u51fd\u6570\u7684\u4e24\u4e2a\u5165\u53c2\uff0c<code>request</code> \u662f\u672a\u7ecf\u5904\u7406\u7684\u539f\u59cb\u8f93\u5165\uff0c<code>response</code> \u662f agent \u8fd4\u56de\u7684\u5b8c\u6574\u54cd\u5e94\uff08\u5373 <code>agent.invoke(input)</code> \u7684\u8fd4\u56de\u503c\uff09\u3002</p> </li> <li> <p>\u82e5\u672a\u63d0\u4f9b <code>post_output_hooks</code>\uff0c\u5219\u4f1a\u5c06 agent \u7684\u6700\u7ec8\u54cd\u5e94\u76f4\u63a5\u4f5c\u4e3a\u5de5\u5177\u7684\u8fd4\u56de\u503c\uff08\u5373 <code>response[\"messages\"][-1].text</code>\uff09\u3002</p> </li> </ul>"},{"location":"zh/adavance-guide/multi-agent/#_12","title":"\u4f7f\u7528\u793a\u4f8b","text":"<p>\u4f8b\u5982\uff0c\u589e\u52a0\u989d\u5916\u7684\u8fd4\u56de\u5185\u5bb9\u7ed9\u4e3b\u667a\u80fd\u4f53\u3002</p> <pre><code>import json\n\ndef process_output(request: str, response: dict[str, Any], runtime: ToolRuntime) -&gt; str:\n    return json.dumps(\n        {\n            \"status\": \"success\",\n            \"event_id\": \"evt_123\",\n            \"summary\": response[\"messages\"][-1].text,\n        }\n    )\n\n\n# \u4f7f\u7528\ncall_agent_tool = wrap_agent_as_tool(agent, post_output_hooks=process_output)\n</code></pre> <p>\u63d0\u793a</p> <p>\u5bf9\u4e8e<code>wrap_all_agents_as_tool</code> \u51fd\u6570\uff0c\u82e5\u9700\u4e3a\u4e0d\u540c\u5b50\u667a\u80fd\u4f53\u5b9a\u5236 <code>pre_input_hooks</code> \u6216 <code>post_output_hooks</code>\uff0c\u53ef\u5728\u94a9\u5b50\u5185\u8c03\u7528 <code>get_subagent_name(runtime)</code> \u83b7\u53d6\u5f53\u524d\u667a\u80fd\u4f53\u540d\u79f0\uff0c\u518d\u6309\u540d\u79f0\u5206\u522b\u5904\u7406\u3002 \u4f8b\u5982\uff0c\u5047\u8bbe\u5f53\u524d\u4ec5\u9700\u8981\u5bf9\u4e8e<code>weather_agent</code> \u5b50\u667a\u80fd\u4f53\u5b9a\u5236 <code>pre_input_hooks</code>(\u4f8b\u5982\u6dfb\u52a0\u5f53\u524d\u57ce\u5e02\u548c\u65f6\u95f4)\uff0c\u5219\u53ef\u4ee5\u8fd9\u6837\u5b9e\u73b0\uff1a</p> <pre><code>from langchain_dev_utils.agents.wrap import get_subagent_name\nfrom datetime import datetime\n\ndef process_input(request: str, runtime: ToolRuntime):\n    subagent_name = get_subagent_name(runtime)\n    if subagent_name == \"weather_agent\":\n        city = runtime.state.get(\"city\", \"\u672a\u77e5\u57ce\u5e02\")\n        time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        return f\"\u5f53\u524d\u57ce\u5e02\u662f\uff1a{city}\uff0c\u65f6\u95f4\u4e3a\uff1a{time}\u3002\u8bf7\u6839\u636e\u4e0a\u8ff0\u5185\u5bb9\uff0c\u5b8c\u6210\u4efb\u52a1\" + request \n    return request\n</code></pre>"},{"location":"zh/adavance-guide/middleware/format/","title":"\u683c\u5f0f\u5316\u7cfb\u7edf\u63d0\u793a\u8bcd","text":"<p>\u7cfb\u7edf\u63d0\u793a\u8bcd\u4e2d\u7684\u5360\u4f4d\u7b26\u53d8\u91cf\u53ef\u4ee5\u901a\u8fc7\u4e2d\u95f4\u4ef6\u8fdb\u884c\u52a8\u6001\u89e3\u6790\u3002\u672c\u5e93\u63d0\u4f9b\u4e86\u4e24\u79cd\u5b9e\u73b0\u65b9\u5f0f\uff1a</p> <ol> <li>\u5168\u5c40\u5b9e\u4f8b <code>format_prompt</code>\uff1a\u9884\u7f6e\u4e86 f-string \u98ce\u683c\uff08\u5982 <code>{name}</code>\uff09\u7684\u683c\u5f0f\u5316\u903b\u8f91\uff0c\u9002\u7528\u4e8e\u7edd\u5927\u591a\u6570\u573a\u666f\uff0c\u63a8\u8350\u76f4\u63a5\u4f7f\u7528\u3002</li> <li>\u4e2d\u95f4\u4ef6\u7c7b <code>FormatPromptMiddleware</code>\uff1a\u5f53\u9700\u8981\u4f7f\u7528 jinja2 \u98ce\u683c\uff08\u5982 <code>{{ name }}</code>\uff09\u65f6\uff0c\u9700\u624b\u52a8\u5b9e\u4f8b\u5316\u6b64\u7c7b\u3002</li> </ol> <p>\u4f7f\u7528\u987b\u77e5</p> <p>\u82e5\u9700\u4f7f\u7528 jinja2 \u98ce\u683c\u6a21\u677f\uff0c\u8bf7\u786e\u4fdd\u5df2\u5b89\u88c5 <code>langchain-dev-utils[standard]</code>\uff0c\u8be6\u89c1\u5b89\u88c5\u6307\u5357\u3002</p>"},{"location":"zh/adavance-guide/middleware/format/#_2","title":"\u53d8\u91cf\u89e3\u6790\u987a\u5e8f","text":"<p>\u5360\u4f4d\u7b26\u4e2d\u7684\u53d8\u91cf\u503c\u9075\u5faa\u4ee5\u4e0b\u4f18\u5148\u7ea7\u4ece\u9ad8\u5230\u4f4e\u7684\u89e3\u6790\u987a\u5e8f\uff1a</p> <ol> <li>\u4f18\u5148\u4ece <code>state</code> \u4e2d\u67e5\u627e\uff1a\u4f1a\u5148\u4ece <code>state</code> \u5b57\u5178\u4e2d\u67e5\u627e\u4e0e\u5360\u4f4d\u7b26\u540c\u540d\u7684\u5b57\u6bb5\u3002</li> <li>\u5176\u6b21\u4ece <code>context</code> \u4e2d\u67e5\u627e\uff1a\u5982\u679c\u5728 <code>state</code> \u4e2d\u672a\u627e\u5230\u8be5\u5b57\u6bb5\uff0c\u5219\u4f1a\u7ee7\u7eed\u5728 <code>context</code> \u5bf9\u8c61\u4e2d\u67e5\u627e\u3002</li> </ol> <p>\u8fd9\u610f\u5473\u7740 <code>state</code> \u4e2d\u7684\u503c\u62e5\u6709\u66f4\u9ad8\u7684\u4f18\u5148\u7ea7\uff0c\u53ef\u4ee5\u8986\u76d6 <code>context</code> \u4e2d\u540c\u540d\u7684\u503c\u3002</p>"},{"location":"zh/adavance-guide/middleware/format/#f-string-format_prompt","title":"\u4f7f\u7528 f-string \u98ce\u683c (<code>format_prompt</code>)","text":"<p><code>format_prompt</code> \u662f\u6700\u5e38\u7528\u7684\u5168\u5c40\u5b9e\u4f8b\uff0c\u91c7\u7528 Python \u539f\u751f\u7684 f-string \u8bed\u6cd5\u3002\u4ee5\u4e0b\u6240\u6709\u793a\u4f8b\u5747\u57fa\u4e8e\u6b64\u5b9e\u4f8b\u3002</p>"},{"location":"zh/adavance-guide/middleware/format/#state","title":"\u4ec5\u4ece <code>state</code> \u4e2d\u83b7\u53d6\u53d8\u91cf","text":"<p>\u8fd9\u662f\u6700\u57fa\u7840\u7684\u7528\u6cd5\uff0c\u6240\u6709\u5360\u4f4d\u7b26\u53d8\u91cf\u5747\u7531 <code>state</code> \u63d0\u4f9b\u3002</p> <pre><code>from langchain_dev_utils.agents.middleware import format_prompt\nfrom langchain.agents import AgentState\n\nclass AssistantState(AgentState):\n    name: str\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    system_prompt=\"\u4f60\u662f\u4e00\u4e2a\u667a\u80fd\u52a9\u624b\uff0c\u4f60\u7684\u540d\u5b57\u53eb\u505a{name}\u3002\",\n    middleware=[format_prompt],\n    state_schema=AssistantState,\n)\n\n# \u8c03\u7528\u65f6\uff0c\u5fc5\u987b\u4e3a state \u63d0\u4f9b 'name' \u7684\u503c\nresponse = agent.invoke(\n    {\"messages\": [HumanMessage(content=\"\u4f60\u597d\u554a\")], \"name\": \"assistant\"}\n)\nprint(response)\n</code></pre>"},{"location":"zh/adavance-guide/middleware/format/#state-context","title":"\u540c\u65f6\u4ece <code>state</code> \u548c <code>context</code> \u4e2d\u83b7\u53d6\u53d8\u91cf","text":"<p>\u4ee5\u4e0b\u793a\u4f8b\u5c55\u793a\u4e86\u5982\u4f55\u6df7\u5408\u4f7f\u7528 <code>state</code> \u548c <code>context</code> \u7684\u6570\u636e\uff1a</p> <pre><code>from dataclasses import dataclass\n\n@dataclass\nclass Context:\n    user: str\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    # {name} \u5c06\u4ece state \u83b7\u53d6\uff0c{user} \u5c06\u4ece context \u83b7\u53d6\n    system_prompt=\"\u4f60\u662f\u4e00\u4e2a\u667a\u80fd\u52a9\u624b\uff0c\u4f60\u7684\u540d\u5b57\u53eb\u505a{name}\u3002\u4f60\u7684\u4f7f\u7528\u8005\u53eb\u505a{user}\u3002\",\n    middleware=[format_prompt],\n    state_schema=AssistantState,\n    context_schema=Context,\n)\n\n# \u8c03\u7528\u65f6\uff0c\u4e3a state \u63d0\u4f9b 'name'\uff0c\u4e3a context \u63d0\u4f9b 'user'\nresponse = agent.invoke(\n    {\n        \"messages\": [HumanMessage(content=\"\u6211\u8981\u53bbNew York\u73a9\u51e0\u5929\uff0c\u5e2e\u6211\u89c4\u5212\u884c\u7a0b\")],\n        \"name\": \"assistant\",\n    },\n    context=Context(user=\"\u5f20\u4e09\"),\n)\nprint(response)\n</code></pre>"},{"location":"zh/adavance-guide/middleware/format/#_3","title":"\u53d8\u91cf\u8986\u76d6\u793a\u4f8b","text":"<p>\u5f53 <code>state</code> \u548c <code>context</code> \u4e2d\u5b58\u5728\u540c\u540d\u53d8\u91cf\u65f6\uff0c<code>state</code> \u7684\u503c\u4f1a\u4f18\u5148\u751f\u6548\u3002</p> <pre><code>from dataclasses import dataclass\n\n@dataclass\nclass Context:\n    # context \u4e2d\u5b9a\u4e49\u4e86 'name'\n    name: str\n    user: str\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    system_prompt=\"\u4f60\u662f\u4e00\u4e2a\u667a\u80fd\u52a9\u624b\uff0c\u4f60\u7684\u540d\u5b57\u53eb\u505a{name}\u3002\u4f60\u7684\u4f7f\u7528\u8005\u53eb\u505a{user}\u3002\",\n    middleware=[format_prompt],\n    state_schema=AssistantState, # state \u4e2d\u4e5f\u5b9a\u4e49\u4e86 'name'\n    context_schema=Context,\n)\n\n# \u8c03\u7528\u65f6\uff0cstate \u548c context \u90fd\u63d0\u4f9b\u4e86 'name' \u7684\u503c\nresponse = agent.invoke(\n    {\n        \"messages\": [HumanMessage(content=\"\u4f60\u53eb\u4ec0\u4e48\u540d\u5b57\uff1f\")],\n        \"name\": \"assistant-1\",\n    },\n    context=Context(name=\"assistant-2\", user=\"\u5f20\u4e09\"),\n)\n\n# \u6700\u7ec8\u7684\u7cfb\u7edf\u63d0\u793a\u8bcd\u4f1a\u662f \"\u4f60\u662f\u4e00\u4e2a\u667a\u80fd\u52a9\u624b\uff0c\u4f60\u7684\u540d\u5b57\u53eb\u505aassistant-1\u3002\u4f60\u7684\u4f7f\u7528\u8005\u53eb\u505a\u5f20\u4e09\u3002\"\n# \u56e0\u4e3a state \u7684\u4f18\u5148\u7ea7\u66f4\u9ad8\nprint(response)\n</code></pre>"},{"location":"zh/adavance-guide/middleware/format/#jinja2-formatpromptmiddleware","title":"\u4f7f\u7528 Jinja2 \u98ce\u683c (<code>FormatPromptMiddleware</code>)","text":"<p>\u5982\u679c\u4f60\u7684\u7cfb\u7edf\u63d0\u793a\u8bcd\u9700\u8981\u66f4\u590d\u6742\u7684\u903b\u8f91\uff08\u5982\u5faa\u73af\u3001\u6761\u4ef6\u5224\u65ad\uff09\uff0c\u6216\u8005\u4e60\u60ef\u4f7f\u7528 Jinja2 \u8bed\u6cd5\uff0c\u8bf7\u4f7f\u7528 <code>FormatPromptMiddleware</code> \u5e76\u6307\u5b9a <code>template_format=\"jinja2\"</code>\u3002</p>"},{"location":"zh/adavance-guide/middleware/format/#_4","title":"\u57fa\u7840\u793a\u4f8b","text":"<p>\u4e0b\u9762\u7684\u793a\u4f8b\u5c55\u793a\u4e86\u5982\u4f55\u4f7f\u7528 Jinja2 \u8bed\u6cd5\u6839\u636e\u6761\u4ef6\u52a8\u6001\u751f\u6210\u63d0\u793a\u8bcd\u3002</p> <pre><code>from langchain_dev_utils.agents.middleware import FormatPromptMiddleware\nfrom dataclasses import dataclass\nfrom typing import Optional\n\n@dataclass\nclass Context:\n    user_role: Optional[str] = None  # \u7528\u6237\u89d2\u8272\uff0c\u4f8b\u5982 \"VIP\", \"Admin\"\n\n# \u624b\u52a8\u5b9e\u4f8b\u5316\u4e2d\u95f4\u4ef6\uff0c\u6307\u5b9a\u683c\u5f0f\u4e3a jinja2\njinja2_formatter = FormatPromptMiddleware(template_format=\"jinja2\")\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    # \u4f7f\u7528 {{ }} \u8bed\u6cd5\n    system_prompt=(\n        \"\u4f60\u662f\u4e00\u4e2a\u667a\u80fd\u52a9\u624b\u3002\\n\"\n        \"{% if user_role == 'VIP' %}\"\n        \"\u8bf7\u52a1\u5fc5\u63d0\u4f9b\u5c0a\u8d35\u3001\u5468\u5230\u7684\u670d\u52a1\u3002\\n\"\n        \"{% elif user_role == 'Admin' %}\"\n        \"\u8bf7\u5c55\u793a\u7cfb\u7edf\u7ba1\u7406\u5458\u7684\u6743\u9650\u548c\u4e25\u8c28\u6027\u3002\\n\"\n        \"{% else %}\"\n        \"\u8bf7\u63d0\u4f9b\u6807\u51c6\u7684\u7528\u6237\u670d\u52a1\u3002\\n\"\n        \"{% endif %}\"\n    ),\n    middleware=[jinja2_formatter],\n    context_schema=Context,\n)\n\n# \u793a\u4f8b 1\uff1a\u666e\u901a\u7528\u6237\nresponse = agent.invoke(\n    {\"messages\": [HumanMessage(content=\"\u4f60\u597d\")]},\n    context=Context(user_role=\"Guest\"),\n)\n\n# \u793a\u4f8b 2\uff1aVIP \u7528\u6237\n# \u7cfb\u7edf\u63d0\u793a\u8bcd\u5c06\u5305\u542b \"\u8bf7\u52a1\u5fc5\u63d0\u4f9b\u5c0a\u8d35\u3001\u5468\u5230\u7684\u670d\u52a1\"\nresponse = agent.invoke(\n    {\"messages\": [HumanMessage(content=\"\u4f60\u597d\")]},\n    context=Context(user_role=\"VIP\"),\n)\n</code></pre> <p>Jinja2 \u6a21\u677f\u6ce8\u610f\u4e8b\u9879</p> <p>\u542f\u7528 Jinja2 \u6a21\u677f\u540e\uff0c<code>system_prompt</code> \u4f1a\u88ab\u7f16\u8bd1\u4e3a Template \u5bf9\u8c61\u3002\u52a1\u5fc5\u5c06\u63d0\u793a\u8bcd\u4e3b\u5e72\u786c\u7f16\u7801\u5728\u4ee3\u7801\u4e2d\uff0c\u4ec5\u901a\u8fc7 <code>state</code> \u6216 <code>context</code> \u4f20\u5165\u52a8\u6001\u6570\u636e\uff1b\u4e0d\u8981\u5c06\u7528\u6237\u8f93\u5165\u76f4\u63a5\u4f5c\u4e3a <code>system_prompt</code> \u53c2\u6570\u4f20\u5165 <code>create_agent</code>\u3002\uff08\u5b9e\u9645\u4e0a\uff0c\u65e0\u8bba\u662f\u5426\u542f\u7528\u672c\u529f\u80fd\uff0c\u5927\u90e8\u5206\u60c5\u51b5\u4e0b\uff0c<code>system_prompt</code> \u5747\u5e94\u7531\u5f00\u53d1\u8005\u5168\u6743\u63a7\u5236\uff0c\u800c\u4e0d\u5e94\u8be5\u76f4\u63a5\u91c7\u7528\u5916\u90e8\u7528\u6237\u8f93\u5165\u4f5c\u4e3a\u4e3b\u5e72\u7cfb\u7edf\u63d0\u793a\u8bcd\uff09</p>"},{"location":"zh/adavance-guide/middleware/handoffs/","title":"\u667a\u80fd\u4f53\u4ea4\u63a5","text":"<p><code>HandoffAgentMiddleware</code> \u662f\u4e00\u4e2a\u7528\u4e8e\u5728\u591a\u4e2a\u5b50 Agent \u4e4b\u95f4\u7075\u6d3b\u5207\u6362\u7684\u4e2d\u95f4\u4ef6\uff0c\u5b8c\u6574\u5b9e\u73b0\u4e86 LangChain \u5b98\u65b9\u7684 <code>handoffs</code> \u591a\u667a\u80fd\u4f53\u534f\u4f5c\u65b9\u6848\u3002</p>"},{"location":"zh/adavance-guide/middleware/handoffs/#_2","title":"\u53c2\u6570\u8bf4\u660e","text":"\u53c2\u6570 \u8bf4\u660e <code>agents_config</code> \u667a\u80fd\u4f53\u914d\u7f6e\u5b57\u5178\uff0c\u952e\u4e3a\u667a\u80fd\u4f53\u540d\u79f0\uff0c\u503c\u4e3a\u667a\u80fd\u4f53\u914d\u7f6e\u5b57\u5178\u3002\u7c7b\u578b: <code>dict[str, AgentConfig]</code>\u5fc5\u586b: \u662f <code>custom_handoffs_tool_descriptions</code> \u81ea\u5b9a\u4e49\u4ea4\u63a5\u5de5\u5177\u7684\u63cf\u8ff0\uff0c\u952e\u4e3a\u667a\u80fd\u4f53\u540d\u79f0\uff0c\u503c\u4e3a\u5bf9\u5e94\u7684\u4ea4\u63a5\u5de5\u5177\u63cf\u8ff0\u3002\u7c7b\u578b: <code>dict[str, str]</code>\u5fc5\u586b: \u5426 <code>handoffs_tool_overrides</code> \u81ea\u5b9a\u4e49\u4ea4\u63a5\u5de5\u5177\u7684\u5b9e\u73b0\uff0c\u952e\u4e3a\u667a\u80fd\u4f53\u540d\u79f0\uff0c\u503c\u4e3a\u5bf9\u5e94\u7684\u4ea4\u63a5\u5de5\u5177\u5b9e\u73b0\u3002\u7c7b\u578b: <code>dict[str, BaseTool]</code>\u5fc5\u586b: \u5426"},{"location":"zh/adavance-guide/middleware/handoffs/#agents_config","title":"<code>agents_config</code> \u914d\u7f6e\u8bf4\u660e","text":"<p>\u6bcf\u4e2a\u667a\u80fd\u4f53\u914d\u7f6e\u4e3a\u4e00\u4e2a\u5b57\u5178\uff0c\u5305\u542b\u4ee5\u4e0b\u5b57\u6bb5\uff1a</p> \u5b57\u6bb5 \u8bf4\u660e <code>model</code> \u6307\u5b9a\u8be5\u667a\u80fd\u4f53\u4f7f\u7528\u7684\u6a21\u578b\uff1b\u82e5\u4e0d\u4f20\uff0c\u5219\u6cbf\u7528 <code>create_agent</code> \u7684 <code>model</code> \u53c2\u6570\u5bf9\u5e94\u7684\u6a21\u578b\u3002\u652f\u6301\u5b57\u7b26\u4e32\uff08\u987b\u4e3a <code>provider:model-name</code> \u683c\u5f0f\uff0c\u5982 <code>vllm:qwen3-4b</code>\uff09\u6216 <code>BaseChatModel</code> \u5b9e\u4f8b\u3002\u7c7b\u578b: <code>str</code> | <code>BaseChatModel</code>\u5fc5\u586b: \u5426 <code>prompt</code> \u667a\u80fd\u4f53\u7684\u7cfb\u7edf\u63d0\u793a\u8bcd\u3002\u7c7b\u578b: <code>str</code> | <code>SystemMessage</code>\u5fc5\u586b: \u662f <code>tools</code> \u667a\u80fd\u4f53\u53ef\u8c03\u7528\u7684\u5de5\u5177\u5217\u8868\uff1b\u82e5\u4e0d\u4f20\uff0c\u8be5\u667a\u80fd\u4f53\u4ec5\u62e5\u6709\u76f8\u5173\u7684\u4ea4\u63a5\u5de5\u5177\u3002\u7c7b\u578b: <code>list[BaseTool]</code>\u5fc5\u586b: \u5426 <code>default</code> \u662f\u5426\u8bbe\u4e3a\u9ed8\u8ba4\u667a\u80fd\u4f53\uff1b\u7f3a\u7701\u4e3a <code>False</code>\u3002\u5168\u90e8\u914d\u7f6e\u4e2d\u5fc5\u987b\u4e14\u53ea\u80fd\u6709\u4e00\u4e2a\u667a\u80fd\u4f53\u8bbe\u4e3a <code>True</code>\u3002\u7c7b\u578b: <code>bool</code>\u5fc5\u586b: \u5426 <code>handoffs</code> \u8be5\u667a\u80fd\u4f53\u53ef\u4ea4\u63a5\u7ed9\u7684\u5176\u5b83\u667a\u80fd\u4f53\u540d\u79f0\u5217\u8868\u3002\u82e5\u8bbe\u4e3a <code>\"all\"</code>\uff0c\u5219\u8868\u793a\u8be5\u667a\u80fd\u4f53\u53ef\u4ea4\u63a5\u7ed9\u6240\u6709\u5176\u5b83\u667a\u80fd\u4f53\u3002\u7c7b\u578b: <code>list[str]</code> | <code>str</code>\u5fc5\u586b: \u662f <p>\u6ce8\u610f</p> <p>\u4f7f\u7528\u672c\u4e2d\u95f4\u4ef6\u540e\uff0c<code>create_agent</code> \u7684 <code>tools</code> \u4e0e <code>system_prompt</code> \u53c2\u6570\u4f1a\u88ab\u5ffd\u7565\uff0c\u6545\u65e0\u9700\u586b\u5199\u3002</p> <p>\u5bf9\u4e8e\u8fd9\u79cd\u8303\u5f0f\u7684\u591a\u667a\u80fd\u4f53\u5b9e\u73b0\uff0c\u5f80\u5f80\u9700\u8981\u4e00\u4e2a\u7528\u4e8e\u4ea4\u63a5\uff08handoffs\uff09\u7684\u5de5\u5177\u3002\u672c\u4e2d\u95f4\u4ef6\u5229\u7528\u6bcf\u4e2a\u667a\u80fd\u4f53\u7684 <code>handoffs</code> \u914d\u7f6e\uff0c\u81ea\u52a8\u4e3a\u6bcf\u4e2a\u667a\u80fd\u4f53\u521b\u5efa\u5bf9\u5e94\u7684\u4ea4\u63a5\u5de5\u5177\u3002\u5982\u679c\u8981\u81ea\u5b9a\u4e49\u4ea4\u63a5\u5de5\u5177\u7684\u63cf\u8ff0\uff0c\u5219\u53ef\u4ee5\u901a\u8fc7 <code>custom_handoffs_tool_descriptions</code> \u53c2\u6570\u5b9e\u73b0\u3002</p>"},{"location":"zh/adavance-guide/middleware/handoffs/#_3","title":"\u57fa\u7840\u7528\u6cd5","text":"<p>\u672c\u793a\u4f8b\u4e2d\uff0c\u5c06\u4f7f\u7528\u56db\u4e2a\u667a\u80fd\u4f53\uff1a<code>time_agent</code>\u3001<code>weather_agent</code>\u3001<code>code_agent</code> \u548c <code>default_agent</code>\u3002</p> <p>\u63a5\u4e0b\u6765\u8981\u521b\u5efa\u5bf9\u5e94\u667a\u80fd\u4f53\u7684\u914d\u7f6e\u5b57\u5178 <code>agent_config</code>\u3002</p> <pre><code>from langchain_dev_utils.agents.middleware.handoffs import AgentConfig\n\nagent_config: dict[str, AgentConfig] = {\n    \"time_agent\": {\n        \"model\": \"vllm:qwen3-8b\",\n        \"prompt\": \"\u4f60\u662f\u4e00\u4e2a\u65f6\u95f4\u52a9\u624b\",\n        \"tools\": [get_current_time],\n        \"handoffs\": [\"default_agent\"],  # \u8be5\u667a\u80fd\u4f53\u53ea\u80fd\u4ea4\u63a5\u5230default_agent\n    },\n    \"weather_agent\": {\n        \"prompt\": \"\u4f60\u662f\u4e00\u4e2a\u5929\u6c14\u52a9\u624b\",\n        \"tools\": [get_current_weather, get_current_city],\n        \"handoffs\": [\"default_agent\"],\n    },\n    \"code_agent\": {\n        \"model\": load_chat_model(\"vllm:qwen3-coder-flash\"),\n        \"prompt\": \"\u4f60\u662f\u4e00\u4e2a\u4ee3\u7801\u52a9\u624b\",\n        \"tools\": [\n            run_code,\n        ],\n        \"handoffs\": [\"default_agent\"],\n    },\n    \"default_agent\": {\n        \"prompt\": \"\u4f60\u662f\u4e00\u4e2a\u52a9\u624b\",\n        \"default\": True, # \u8bbe\u4e3a\u9ed8\u8ba4\u667a\u80fd\u4f53\n        \"handoffs\": \"all\",  # \u8be5\u667a\u80fd\u4f53\u53ef\u4ee5\u4ea4\u63a5\u5230\u6240\u6709\u5176\u5b83\u667a\u80fd\u4f53\n    },\n}\n</code></pre> <p>\u6700\u7ec8\u5c06\u8fd9\u4e2a\u914d\u7f6e\u4f20\u9012\u7ed9 <code>HandoffAgentMiddleware</code> \u5373\u53ef\u3002</p> <pre><code>from langchain_dev_utils.agents.middleware import HandoffAgentMiddleware\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    middleware=[HandoffAgentMiddleware(agents_config=agent_config)],\n)\n\nresponse = agent.invoke({\"messages\": [HumanMessage(content=\"\u5f53\u524d\u65f6\u95f4\u662f\u591a\u5c11\uff1f\")]})\nprint(response)\n</code></pre>"},{"location":"zh/adavance-guide/middleware/handoffs/#_4","title":"\u81ea\u5b9a\u4e49\u4ea4\u63a5\u5de5\u5177\u63cf\u8ff0","text":"<p>\u5982\u679c\u60f3\u8981\u81ea\u5b9a\u4e49\u4ea4\u63a5\u5de5\u5177\u7684\u63cf\u8ff0\uff0c\u53ef\u4ee5\u4f20\u9012\u7b2c\u4e8c\u4e2a\u53c2\u6570 <code>custom_handoffs_tool_descriptions</code>\u3002</p> <pre><code>agent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    middleware=[\n        HandoffAgentMiddleware(\n            agents_config=agent_config,\n            custom_handoffs_tool_descriptions={\n                \"time_agent\": \"\u6b64\u5de5\u5177\u7528\u4e8e\u4ea4\u63a5\u5230\u65f6\u95f4\u52a9\u624b\u53bb\u89e3\u51b3\u65f6\u95f4\u67e5\u8be2\u95ee\u9898\",\n                \"weather_agent\": \"\u6b64\u5de5\u5177\u7528\u4e8e\u4ea4\u63a5\u5230\u5929\u6c14\u52a9\u624b\u53bb\u89e3\u51b3\u5929\u6c14\u67e5\u8be2\u95ee\u9898\",\n                \"code_agent\": \"\u6b64\u5de5\u5177\u7528\u4e8e\u4ea4\u63a5\u5230\u4ee3\u7801\u52a9\u624b\u53bb\u89e3\u51b3\u4ee3\u7801\u95ee\u9898\",\n                \"default_agent\": \"\u6b64\u5de5\u5177\u7528\u4e8e\u4ea4\u63a5\u5230\u9ed8\u8ba4\u7684\u52a9\u624b\",\n            },\n        )\n    ],\n)\n</code></pre>"},{"location":"zh/adavance-guide/middleware/handoffs/#_5","title":"\u81ea\u5b9a\u4e49\u4ea4\u63a5\u5de5\u5177\u5b9e\u73b0","text":"<p>\u5982\u679c\u4f60\u60f3\u5b8c\u5168\u81ea\u5b9a\u4e49\u5b9e\u73b0\u4ea4\u63a5\u5de5\u5177\u7684\u903b\u8f91\uff0c\u5219\u53ef\u4ee5\u4f20\u9012\u7b2c\u4e09\u4e2a\u53c2\u6570 <code>handoffs_tool_overrides</code>\u3002\u4e0e\u7b2c\u4e8c\u4e2a\u53c2\u6570\u7c7b\u4f3c\uff0c\u5b83\u4e5f\u662f\u4e00\u4e2a\u5b57\u5178\uff0c\u952e\u4e3a\u667a\u80fd\u4f53\u540d\u79f0\uff0c\u503c\u4e3a\u5bf9\u5e94\u7684\u4ea4\u63a5\u5de5\u5177\u5b9e\u73b0\u3002</p> <p>\u81ea\u5b9a\u4e49\u4ea4\u63a5\u5de5\u5177\u5fc5\u987b\u8fd4\u56de\u4e00\u4e2a <code>Command</code> \u5bf9\u8c61\uff0c\u5176 <code>update</code> \u5c5e\u6027\u9700\u5305\u542b <code>messages</code> \u952e\uff08\u8fd4\u56de\u5de5\u5177\u54cd\u5e94\uff09\u548c <code>active_agent</code> \u952e\uff08\u503c\u4e3a\u8981\u4ea4\u63a5\u7684\u667a\u80fd\u4f53\u540d\u79f0\uff0c\u7528\u4e8e\u5207\u6362\u5f53\u524d\u667a\u80fd\u4f53\uff09\u3002</p> <p>\u4f8b\u5982\uff1a</p> <pre><code>@tool\ndef transfer_to_code_agent(runtime: ToolRuntime) -&gt; Command:\n    \"\"\"\u6b64\u5de5\u5177\u5e2e\u52a9\u4f60\u4ea4\u63a5\u5230\u4ee3\u7801\u52a9\u624b\"\"\"\n    #\u8fd9\u91cc\u4f60\u53ef\u4ee5\u6dfb\u52a0\u81ea\u5b9a\u4e49\u903b\u8f91\n    return Command(\n        update={\n            \"messages\": [\n                ToolMessage(\n                    content=\"transfer to code agent\",\n                    tool_call_id=runtime.tool_call_id,\n                )\n            ],\n            \"active_agent\": \"code_agent\",\n            #\u8fd9\u91cc\u4f60\u53ef\u4ee5\u6dfb\u52a0\u5176\u5b83\u7684\u8981\u66f4\u65b0\u7684\u952e\n        }\n    )\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    middleware=[\n        HandoffAgentMiddleware(\n            agents_config=agent_config,\n            handoffs_tool_overrides={\n                \"code_agent\": transfer_to_code_agent,\n            },\n        )\n    ],\n)\n</code></pre> <p><code>handoffs_tool_overrides</code> \u7528\u4e8e\u9ad8\u5ea6\u5b9a\u5236\u5316\u4ea4\u63a5\u5de5\u5177\u7684\u5b9e\u73b0\uff0c\u5982\u679c\u4ec5\u4ec5\u662f\u60f3\u8981\u81ea\u5b9a\u4e49\u4ea4\u63a5\u5de5\u5177\u7684\u63cf\u8ff0\uff0c\u5219\u5e94\u8be5\u4f7f\u7528 <code>custom_handoffs_tool_descriptions</code>\u3002</p>"},{"location":"zh/adavance-guide/middleware/official/","title":"LangChain \u5185\u7f6e\u4e2d\u95f4\u4ef6\u6269\u5c55","text":"<p>\u672c\u5e93\u5bf9\u4ee5\u4e0b\u5b98\u65b9\u4e2d\u95f4\u4ef6\u8fdb\u884c\u4e86\u589e\u5f3a\uff0c\u652f\u6301\u901a\u8fc7\u5b57\u7b26\u4e32\u76f4\u63a5\u6307\u5b9a\u6a21\u578b\u2014\u2014\u524d\u63d0\u662f\u6a21\u578b\u5fc5\u987b\u5df2\u901a\u8fc7 <code>register_model_provider</code> \u6ce8\u518c\uff1a</p> <ul> <li><code>SummarizationMiddleware</code></li> <li><code>LLMToolSelectorMiddleware</code></li> <li><code>ModelFallbackMiddleware</code></li> <li><code>LLMToolEmulator</code></li> </ul> <p>\u5bfc\u5165\u672c\u5e93\u4e2d\u7684\u4e2d\u95f4\u4ef6\u5373\u53ef\u4f7f\u7528\uff0c\u7528\u6cd5\u4e0e\u5b98\u65b9\u4e00\u81f4\uff1a</p> <pre><code>from langchain_core.messages import AIMessage\nfrom langchain_dev_utils.agents.middleware import SummarizationMiddleware\nfrom langchain_dev_utils.chat_models import register_model_provider\n\n# \u5fc5\u987b\u5148\u901a\u8fc7 register_model_provider \u6ce8\u518c\u6a21\u578b\nregister_model_provider(\n    provider_name=\"vllm\",\n    chat_model=\"openai-compatible\",\n    base_url=\"http://localhost:8000/v1\",\n)\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    middleware=[\n        SummarizationMiddleware(\n            model=\"vllm:qwen3-4b\",\n            trigger=(\"tokens\", 50),\n            keep=(\"messages\", 1),\n        )\n    ],\n    system_prompt=\"\u4f60\u662f\u4e00\u4e2a\u667a\u80fd\u7684AI\u52a9\u624b\uff0c\u53ef\u4ee5\u89e3\u51b3\u7528\u6237\u7684\u95ee\u9898\",\n)\nresponse = agent.invoke({\"messages\": messages})\nprint(response)\n</code></pre>"},{"location":"zh/adavance-guide/middleware/overview/","title":"\u6982\u8ff0","text":"<p>\u4e2d\u95f4\u4ef6\u662f\u4e13\u4e3a LangChain \u9884\u6784\u5efa Agent \u8bbe\u8ba1\u7684\u53ef\u63d2\u62d4\u7ec4\u4ef6\uff0c\u65e8\u5728\u5b9e\u73b0\u5bf9 Agent \u5185\u90e8\u884c\u4e3a\u7684\u7cbe\u7ec6\u5316\u63a7\u5236\u3002\u9664 LangChain \u6846\u67b6\u5185\u7f6e\u7684\u4e2d\u95f4\u4ef6\u5916\uff0c\u672c\u5e93\u7ed3\u5408\u5b9e\u9645\u5e94\u7528\u573a\u666f\uff0c\u8fdb\u4e00\u6b65\u8865\u5145\u4e86\u66f4\u4e3a\u4e30\u5bcc\u7684\u4e2d\u95f4\u4ef6\u652f\u6301\u3002</p> <p>\u672c\u5e93\u63d0\u4f9b\u7684\u4e2d\u95f4\u4ef6\u5305\u62ec\uff1a</p> <ul> <li><code>PlanMiddleware</code>\uff1a\u4efb\u52a1\u89c4\u5212\uff0c\u5c06\u590d\u6742\u4efb\u52a1\u62c6\u89e3\u4e3a\u6709\u5e8f\u5b50\u4efb\u52a1</li> <li><code>ModelRouterMiddleware</code>\uff1a\u6839\u636e\u8f93\u5165\u5185\u5bb9\u52a8\u6001\u8def\u7531\u5230\u6700\u9002\u914d\u7684\u6a21\u578b</li> <li><code>HandoffAgentMiddleware</code>\uff1a\u5728\u591a\u4e2a\u5b50 Agent \u4e4b\u95f4\u7075\u6d3b\u4ea4\u63a5\u4efb\u52a1</li> <li><code>ToolCallRepairMiddleware</code>\uff1a\u81ea\u52a8\u4fee\u590d\u5927\u6a21\u578b\u65e0\u6548\u5de5\u5177\u8c03\u7528</li> <li><code>FormatPromptMiddleware</code>\uff1a\u52a8\u6001\u683c\u5f0f\u5316\u7cfb\u7edf\u63d0\u793a\u8bcd\u4e2d\u7684\u5360\u4f4d\u7b26</li> </ul> <p>\u6b64\u5916\uff0c\u672c\u5e93\u8fd8\u6269\u5145\u4e86\u5b98\u65b9\u4e2d\u95f4\u4ef6\u7684\u529f\u80fd\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u914d\u7f6e\u7684\u53ef\u7528\u6027\uff0c\u652f\u6301\u901a\u8fc7\u5b57\u7b26\u4e32\u53c2\u6570\u6307\u5b9a\u6a21\u578b\uff1a</p> <ul> <li>SummarizationMiddleware</li> <li>LLMToolSelectorMiddleware</li> <li>ModelFallbackMiddleware</li> <li>LLMToolEmulator</li> </ul>"},{"location":"zh/adavance-guide/middleware/plan/","title":"\u4efb\u52a1\u89c4\u5212","text":"<p><code>PlanMiddleware</code> \u662f\u4e00\u4e2a\u7528\u4e8e\u5728\u6267\u884c\u590d\u6742\u4efb\u52a1\u524d\u8fdb\u884c\u7ed3\u6784\u5316\u5206\u89e3\u4e0e\u8fc7\u7a0b\u7ba1\u7406\u7684\u4e2d\u95f4\u4ef6\u3002</p> <p>\u8865\u5145\u8bf4\u660e</p> <p>\u4efb\u52a1\u89c4\u5212\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u4e0a\u4e0b\u6587\u5de5\u7a0b\u7ba1\u7406\u7b56\u7565\u3002\u5728\u6267\u884c\u4efb\u52a1\u4e4b\u524d\uff0c\u5927\u6a21\u578b\u9996\u5148\u5c06\u6574\u4f53\u4efb\u52a1\u62c6\u89e3\u4e3a\u591a\u4e2a\u6709\u5e8f\u7684\u5b50\u4efb\u52a1\uff0c\u5f62\u6210\u4efb\u52a1\u89c4\u5212\u5217\u8868\uff08\u5728\u672c\u5e93\u4e2d\u79f0\u4e3a plan\uff09\u3002\u968f\u540e\u6309\u987a\u5e8f\u6267\u884c\u5404\u5b50\u4efb\u52a1\uff0c\u5e76\u5728\u6bcf\u5b8c\u6210\u4e00\u4e2a\u6b65\u9aa4\u540e\u52a8\u6001\u66f4\u65b0\u4efb\u52a1\u72b6\u6001\uff0c\u76f4\u81f3\u6240\u6709\u5b50\u4efb\u52a1\u6267\u884c\u5b8c\u6bd5\u3002</p>"},{"location":"zh/adavance-guide/middleware/plan/#_2","title":"\u53c2\u6570\u8bf4\u660e","text":"\u53c2\u6570 \u8bf4\u660e <code>system_prompt</code> \u7cfb\u7edf\u63d0\u793a\u8bcd\uff0c\u82e5\u4e3a <code>None</code> \u5219\u4f7f\u7528\u9ed8\u8ba4\u63d0\u793a\u8bcd\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u5426 <code>custom_plan_tool_descriptions</code> \u81ea\u5b9a\u4e49\u8ba1\u5212\u76f8\u5173\u5de5\u5177\u7684\u63cf\u8ff0\u3002\u7c7b\u578b: <code>dict</code>\u5fc5\u586b: \u5426 <code>use_read_plan_tool</code> \u662f\u5426\u542f\u7528\u8bfb\u8ba1\u5212\u5de5\u5177\u3002\u7c7b\u578b: <code>bool</code>\u5fc5\u586b: \u5426\u9ed8\u8ba4\u503c: <code>True</code> <p><code>custom_plan_tool_descriptions</code> \u5b57\u5178\u7684\u952e\u53ef\u53d6\u4ee5\u4e0b\u4e09\u4e2a\u503c\uff1a</p> \u952e \u8bf4\u660e <code>write_plan</code> \u5199\u8ba1\u5212\u5de5\u5177\u7684\u63cf\u8ff0 <code>finish_sub_plan</code> \u5b8c\u6210\u5b50\u8ba1\u5212\u5de5\u5177\u7684\u63cf\u8ff0 <code>read_plan</code> \u8bfb\u8ba1\u5212\u5de5\u5177\u7684\u63cf\u8ff0"},{"location":"zh/adavance-guide/middleware/plan/#_3","title":"\u4f7f\u7528\u793a\u4f8b","text":"<pre><code>from langchain_dev_utils.agents.middleware import PlanMiddleware\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    middleware=[\n        PlanMiddleware(\n            custom_plan_tool_descriptions={\n                \"write_plan\": \"\u7528\u4e8e\u5199\u8ba1\u5212\uff0c\u5c06\u4efb\u52a1\u62c6\u89e3\u4e3a\u591a\u4e2a\u6709\u5e8f\u7684\u5b50\u4efb\u52a1\u3002\",\n                \"finish_sub_plan\": \"\u7528\u4e8e\u5b8c\u6210\u5b50\u4efb\u52a1\uff0c\u66f4\u65b0\u5b50\u4efb\u52a1\u72b6\u6001\u4e3a\u5df2\u5b8c\u6210\u3002\",\n                \"read_plan\": \"\u7528\u4e8e\u67e5\u8be2\u5f53\u524d\u7684\u4efb\u52a1\u89c4\u5212\u5217\u8868\u3002\"\n            },\n            use_read_plan_tool=True,  # \u5982\u679c\u4e0d\u4f7f\u7528\u8bfb\u8ba1\u5212\u5de5\u5177\uff0c\u53ef\u4ee5\u8bbe\u7f6e\u6b64\u53c2\u6570\u4e3a False\n        )\n    ],\n)\n\nresponse = agent.invoke(\n    {\"messages\": [HumanMessage(content=\"\u6211\u8981\u53bbNew York\u73a9\u51e0\u5929\uff0c\u5e2e\u6211\u89c4\u5212\u884c\u7a0b\")]}\n)\nprint(response)\n</code></pre>"},{"location":"zh/adavance-guide/middleware/plan/#_4","title":"\u5de5\u5177\u8bf4\u660e","text":"<p><code>PlanMiddleware</code> \u8981\u6c42\u5fc5\u987b\u4f7f\u7528 <code>write_plan</code> \u548c <code>finish_sub_plan</code> \u4e24\u4e2a\u5de5\u5177\uff0c\u800c <code>read_plan</code> \u5de5\u5177\u9ed8\u8ba4\u542f\u7528\uff1b\u82e5\u4e0d\u9700\u8981\u4f7f\u7528\uff0c\u53ef\u5c06 <code>use_read_plan_tool</code> \u53c2\u6570\u8bbe\u4e3a <code>False</code>\u3002</p>"},{"location":"zh/adavance-guide/middleware/plan/#to-do-list","title":"\u4e0e\u5b98\u65b9 To-do list \u4e2d\u95f4\u4ef6\u7684\u5bf9\u6bd4","text":"<p>\u672c\u4e2d\u95f4\u4ef6\u4e0e LangChain \u5b98\u65b9\u63d0\u4f9b\u7684 To-do list \u4e2d\u95f4\u4ef6 \u529f\u80fd\u5b9a\u4f4d\u76f8\u4f3c\uff0c\u4f46\u5728\u5de5\u5177\u8bbe\u8ba1\u4e0a\u5b58\u5728\u5dee\u5f02\uff1a</p> \u7279\u6027 \u5b98\u65b9 To-do list \u4e2d\u95f4\u4ef6 \u672c\u5e93 PlanMiddleware \u5de5\u5177\u6570\u91cf 1 \u4e2a\uff08<code>write_todo</code>\uff09 3 \u4e2a\uff08<code>write_plan</code>\u3001<code>finish_sub_plan</code>\u3001<code>read_plan</code>\uff09 \u529f\u80fd\u5b9a\u4f4d \u9762\u5411\u5f85\u529e\u6e05\u5355\uff08todo list\uff09 \u4e13\u95e8\u7528\u4e8e\u89c4\u5212\u5217\u8868\uff08plan list\uff09 \u64cd\u4f5c\u65b9\u5f0f \u6dfb\u52a0\u548c\u4fee\u6539\u901a\u8fc7\u4e00\u4e2a\u5de5\u5177\u5b8c\u6210 \u5199\u5165\u3001\u4fee\u6539\u3001\u67e5\u8be2\u5206\u522b\u7531\u4e0d\u540c\u5de5\u5177\u5b8c\u6210 <p>\u65e0\u8bba\u662f <code>todo</code> \u8fd8\u662f <code>plan</code>\uff0c\u5176\u672c\u8d28\u90fd\u662f\u540c\u4e00\u4e2a\u6982\u5ff5\u3002\u672c\u4e2d\u95f4\u4ef6\u533a\u522b\u4e8e\u5b98\u65b9\u7684\u5173\u952e\u70b9\u5728\u4e8e\u63d0\u4f9b\u4e86\u4e09\u4e2a\u4e13\u7528\u5de5\u5177\uff1a</p> <ul> <li><code>write_plan</code>\uff1a\u7528\u4e8e\u5199\u5165\u8ba1\u5212\u6216\u66f4\u65b0\u8ba1\u5212\u5185\u5bb9</li> <li><code>finish_sub_plan</code>\uff1a\u7528\u4e8e\u5728\u5b8c\u6210\u67d0\u4e2a\u5b50\u4efb\u52a1\u540e\u66f4\u65b0\u5176\u72b6\u6001</li> <li><code>read_plan</code>\uff1a\u7528\u4e8e\u67e5\u8be2\u8ba1\u5212\u5185\u5bb9</li> </ul>"},{"location":"zh/adavance-guide/middleware/router/","title":"\u6a21\u578b\u8def\u7531","text":"<p><code>ModelRouterMiddleware</code> \u662f\u4e00\u4e2a\u7528\u4e8e\u6839\u636e\u8f93\u5165\u5185\u5bb9\u52a8\u6001\u8def\u7531\u5230\u6700\u9002\u914d\u6a21\u578b\u7684\u4e2d\u95f4\u4ef6\u3002\u5b83\u901a\u8fc7\u4e00\u4e2a\"\u8def\u7531\u6a21\u578b\"\u5206\u6790\u7528\u6237\u8bf7\u6c42\uff0c\u4ece\u9884\u5b9a\u4e49\u7684\u6a21\u578b\u5217\u8868\u4e2d\u9009\u62e9\u6700\u9002\u5408\u5f53\u524d\u4efb\u52a1\u7684\u6a21\u578b\u8fdb\u884c\u5904\u7406\u3002</p>"},{"location":"zh/adavance-guide/middleware/router/#_2","title":"\u53c2\u6570\u8bf4\u660e","text":"\u53c2\u6570 \u8bf4\u660e <code>router_model</code> \u7528\u4e8e\u6267\u884c\u8def\u7531\u51b3\u7b56\u7684\u6a21\u578b\u3002\u7c7b\u578b: <code>str</code> | <code>BaseChatModel</code>\u5fc5\u586b: \u662f <code>model_list</code> \u6a21\u578b\u914d\u7f6e\u5217\u8868\u3002\u7c7b\u578b: <code>list[ModelDict]</code>\u5fc5\u586b: \u662f <code>router_prompt</code> \u81ea\u5b9a\u4e49\u8def\u7531\u6a21\u578b\u7684\u63d0\u793a\u8bcd\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u5426"},{"location":"zh/adavance-guide/middleware/router/#model_list","title":"<code>model_list</code> \u914d\u7f6e\u8bf4\u660e","text":"<p>\u6bcf\u4e2a\u6a21\u578b\u914d\u7f6e\u4e3a\u4e00\u4e2a\u5b57\u5178\uff0c\u5305\u542b\u4ee5\u4e0b\u5b57\u6bb5\uff1a</p> \u5b57\u6bb5 \u8bf4\u660e <code>model_name</code> \u6a21\u578b\u7684\u552f\u4e00\u6807\u8bc6\uff0c\u4f7f\u7528 <code>provider:model-name</code> \u683c\u5f0f\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u662f <code>model_description</code> \u6a21\u578b\u80fd\u529b\u6216\u9002\u7528\u573a\u666f\u7684\u7b80\u8981\u63cf\u8ff0\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u662f <code>tools</code> \u8be5\u6a21\u578b\u53ef\u8c03\u7528\u7684\u5de5\u5177\u767d\u540d\u5355\uff0c\u5982\u679c\u4e0d\u4f20\uff0c\u9ed8\u8ba4\u8be5\u6a21\u578b\u62e5\u6709\u6240\u6709\u5de5\u5177\u7684\u4f7f\u7528\u6743\u9650\u3002\u7c7b\u578b: <code>list[BaseTool]</code>\u5fc5\u586b: \u5426 <code>model_kwargs</code> \u6a21\u578b\u52a0\u8f7d\u65f6\u7684\u989d\u5916\u53c2\u6570\u3002\u7c7b\u578b: <code>dict</code>\u5fc5\u586b: \u5426 <code>model_system_prompt</code> \u6a21\u578b\u7684\u7cfb\u7edf\u7ea7\u63d0\u793a\u8bcd\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u5426 <code>model_instance</code> \u5df2\u5b9e\u4f8b\u5316\u7684\u6a21\u578b\u5bf9\u8c61\u3002\u7c7b\u578b: <code>BaseChatModel</code>\u5fc5\u586b: \u5426 <p>model_instance \u5b57\u6bb5\u8bf4\u660e</p> <ul> <li>\u82e5\u63d0\u4f9b\uff1a\u76f4\u63a5\u4f7f\u7528\u8be5\u5b9e\u4f8b\uff0c<code>model_name</code> \u4ec5\u4f5c\u6807\u8bc6\uff0c<code>model_kwargs</code> \u88ab\u5ffd\u7565\uff1b\u9002\u7528\u4e8e\u4e0d\u4f7f\u7528\u672c\u5e93\u7684\u5bf9\u8bdd\u6a21\u578b\u7ba1\u7406\u529f\u80fd\u7684\u60c5\u51b5\u3002</li> <li>\u82e5\u672a\u63d0\u4f9b\uff1a\u6839\u636e <code>model_name</code> \u548c <code>model_kwargs</code> \u4f7f\u7528 <code>load_chat_model</code> \u52a0\u8f7d\u6a21\u578b\u3002</li> <li>\u547d\u540d\u683c\u5f0f\uff1a\u65e0\u8bba\u54ea\u79cd\u60c5\u51b5\uff0c<code>model_name</code> \u7684\u547d\u540d\u90fd\u63a8\u8350\u91c7\u7528 <code>provider:model-name</code> \u683c\u5f0f\u3002</li> </ul>"},{"location":"zh/adavance-guide/middleware/router/#_3","title":"\u4f7f\u7528\u793a\u4f8b","text":"<p>\u6b65\u9aa4\u4e00\uff1a\u5b9a\u4e49\u6a21\u578b\u5217\u8868</p> <pre><code>from langchain_dev_utils.agents.middleware.model_router import ModelDict\n\nmodel_list: list[ModelDict] = [\n    {\n        \"model_name\": \"vllm:qwen3-8b\",\n        \"model_description\": \"\u9002\u5408\u666e\u901a\u4efb\u52a1\uff0c\u5982\u5bf9\u8bdd\u3001\u6587\u672c\u751f\u6210\u7b49\",\n        \"model_kwargs\": {\n            \"temperature\": 0.7,\n            \"extra_body\": {\"chat_template_kwargs\": {\"enable_thinking\": False}},\n        },\n        \"model_system_prompt\": \"\u4f60\u662f\u4e00\u4e2a\u52a9\u624b\uff0c\u64c5\u957f\u5904\u7406\u666e\u901a\u4efb\u52a1\uff0c\u5982\u5bf9\u8bdd\u3001\u6587\u672c\u751f\u6210\u7b49\u3002\",\n    },\n    {\n        \"model_name\": \"vllm:qwen3-vl-2b\",\n        \"model_description\": \"\u9002\u5408\u89c6\u89c9\u4efb\u52a1\",\n        \"tools\": [],  # \u5982\u679c\u8be5\u6a21\u578b\u4e0d\u9700\u8981\u4efb\u4f55\u5de5\u5177\uff0c\u8bf7\u5c06\u6b64\u5b57\u6bb5\u8bbe\u7f6e\u4e3a\u7a7a\u5217\u8868 []\n    },\n    {\n        \"model_name\": \"vllm:qwen3-coder-flash\",\n        \"model_description\": \"\u9002\u5408\u4ee3\u7801\u751f\u6210\u4efb\u52a1\",\n        \"tools\": [run_python_code],  # \u4ec5\u5141\u8bb8\u4f7f\u7528 run_python_code \u5de5\u5177\n    },\n    {\n        \"model_name\": \"openai:gpt-4o\",\n        \"model_description\": \"\u9002\u5408\u7efc\u5408\u7c7b\u9ad8\u96be\u5ea6\u4efb\u52a1\",\n        \"model_system_prompt\": \"\u4f60\u662f\u4e00\u4e2a\u52a9\u624b\uff0c\u64c5\u957f\u5904\u7406\u7efc\u5408\u7c7b\u7684\u9ad8\u96be\u5ea6\u4efb\u52a1\",\n        \"model_instance\": ChatOpenAI(\n            model=\"gpt-4o\"\n        ),  # \u76f4\u63a5\u4f20\u5165\u5b9e\u4f8b\uff0c\u6b64\u65f6 model_name \u4ec5\u4f5c\u6807\u8bc6\uff0cmodel_kwargs \u88ab\u5ffd\u7565\n    },\n]\n</code></pre> <p>\u6b65\u9aa4\u4e8c\uff1a\u521b\u5efa Agent \u5e76\u542f\u7528\u4e2d\u95f4\u4ef6</p> <pre><code>from langchain_dev_utils.agents.middleware import ModelRouterMiddleware\nfrom langchain_core.messages import HumanMessage\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",  # \u6b64\u6a21\u578b\u4ec5\u4f5c\u5360\u4f4d\uff0c\u5b9e\u9645\u7531\u4e2d\u95f4\u4ef6\u52a8\u6001\u66ff\u6362\n    tools=[get_current_time],\n    middleware=[\n        ModelRouterMiddleware(\n            router_model=\"vllm:qwen3-4b\",\n            model_list=model_list,\n        )\n    ],\n)\n\n# \u8def\u7531\u4e2d\u95f4\u4ef6\u4f1a\u6839\u636e\u8f93\u5165\u5185\u5bb9\u81ea\u52a8\u9009\u62e9\u6700\u5408\u9002\u7684\u6a21\u578b\nresponse = agent.invoke({\"messages\": [HumanMessage(content=\"\u5e2e\u6211\u5199\u4e00\u4e2a\u5192\u6ce1\u6392\u5e8f\u4ee3\u7801\")]})\nprint(response)\n</code></pre> <p>tools\u53c2\u6570</p> <p>\u4f7f\u7528\u672c\u4e2d\u95f4\u4ef6\u540e\uff0c<code>create_agent</code> \u7684 <code>tools</code> \u53c2\u6570\u88ab\u89c6\u4e3a\u201c\u5168\u5c40\u8865\u5145\u5de5\u5177\u201d\u3002\u53ea\u6709\u5f53<code>model_list</code>\u4e2d\u7684\u6a21\u578b\u7684<code>tools</code> \u5b57\u6bb5\u672a\u5b9a\u4e49\u65f6\uff0c\u8fd9\u4e9b\u5168\u5c40\u5de5\u5177\u624d\u4f1a\u88ab\u8ffd\u52a0\u5230\u8be5\u6a21\u578b\u7684\u53ef\u7528\u5de5\u5177\u5217\u8868\u4e2d\uff1b\u4e14\u8fd9\u4e9b\u5168\u5c40\u5de5\u5177\u4e0d\u80fd\u5305\u542b\u5728<code>model_list</code>\u4e2d\u7684\u6a21\u578b\u7684<code>tools</code> \u5b57\u6bb5\u4e2d\u3002</p> <p>\u901a\u8fc7 <code>ModelRouterMiddleware</code>\uff0c\u4f60\u53ef\u4ee5\u8f7b\u677e\u6784\u5efa\u4e00\u4e2a\u591a\u6a21\u578b\u3001\u591a\u80fd\u529b\u7684 Agent\uff0c\u6839\u636e\u4efb\u52a1\u7c7b\u578b\u81ea\u52a8\u9009\u62e9\u6700\u4f18\u6a21\u578b\uff0c\u63d0\u5347\u54cd\u5e94\u8d28\u91cf\u4e0e\u6548\u7387\u3002</p> <p>\u5e76\u884c\u6267\u884c</p> <p>\u91c7\u7528\u4e2d\u95f4\u4ef6\u5b9e\u73b0\u6a21\u578b\u8def\u7531\uff0c\u6bcf\u6b21\u4ec5\u4f1a\u5206\u914d\u4e00\u4e2a\u4efb\u52a1\u8fdb\u884c\u6267\u884c\uff0c\u5982\u679c\u4f60\u60f3\u8981\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u591a\u4e2a\u5b50\u4efb\u52a1\u7531\u591a\u4e2a\u6a21\u578b\u8fdb\u884c\u5e76\u884c\u6267\u884c\uff0c\u8bf7\u53c2\u8003\u9884\u7f6eStateGraph\u6784\u5efa\u51fd\u6570\u3002</p>"},{"location":"zh/adavance-guide/middleware/tool-call-repair/","title":"\u5de5\u5177\u8c03\u7528\u4fee\u590d","text":"<p><code>ToolCallRepairMiddleware</code> \u662f\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u4fee\u590d\u5927\u6a21\u578b\u65e0\u6548\u5de5\u5177\u8c03\u7528\u7684\u4e2d\u95f4\u4ef6\u3002</p> <p>\u5927\u6a21\u578b\u5728\u8f93\u51fa\u7b26\u5408 JSON Schema \u7684\u5de5\u5177\u8c03\u7528\u53c2\u6570\u65f6\uff0c\u6709\u65f6\u4f1a\u56e0\u6a21\u578b\u80fd\u529b\u9650\u5236\u751f\u6210\u683c\u5f0f\u9519\u8bef\u7684 JSON \u5185\u5bb9\uff08\u9519\u8bef\u901a\u5e38\u51fa\u73b0\u5728 <code>arguments</code> \u5b57\u6bb5\uff09\u3002\u8fd9\u7c7b\u89e3\u6790\u5931\u8d25\u7684\u8c03\u7528\u4f1a\u88ab LangChain \u6807\u8bb0\u5e76\u5b58\u5165 <code>invalid_tool_calls</code> \u5b57\u6bb5\u4e2d\u3002<code>ToolCallRepairMiddleware</code> \u4f1a\u81ea\u52a8\u68c0\u6d4b\u8be5\u5b57\u6bb5\uff0c\u5e76\u8c03\u7528 <code>json-repair</code> \u5e93\u5c1d\u8bd5\u4fee\u590d\u683c\u5f0f\uff0c\u4f7f\u5de5\u5177\u8c03\u7528\u80fd\u591f\u6b63\u5e38\u6267\u884c\u3002</p> <p>\u4f7f\u7528\u987b\u77e5</p> <p>\u4f7f\u7528\u672c\u4e2d\u95f4\u4ef6\u524d\uff0c\u8bf7\u786e\u4fdd\u5df2\u5b89\u88c5 <code>langchain-dev-utils[standard]</code>\uff0c\u8be6\u89c1\u5b89\u88c5\u6307\u5357\u3002</p>"},{"location":"zh/adavance-guide/middleware/tool-call-repair/#_2","title":"\u53c2\u6570\u8bf4\u660e","text":"<p>\u8be5\u4e2d\u95f4\u4ef6\u8bbe\u8ba1\u4e3a\u96f6\u914d\u7f6e\u5f00\u7bb1\u5373\u7528\uff0c\u5b9e\u4f8b\u5316\u65f6\u65e0\u9700\u4f20\u5165\u4efb\u4f55\u53c2\u6570\u3002</p>"},{"location":"zh/adavance-guide/middleware/tool-call-repair/#_3","title":"\u4f7f\u7528\u793a\u4f8b","text":""},{"location":"zh/adavance-guide/middleware/tool-call-repair/#_4","title":"\u6807\u51c6\u7528\u6cd5","text":"<pre><code>from langchain_dev_utils.agents.middleware import ToolCallRepairMiddleware\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    tools=[run_python_code, get_current_time],\n    middleware=[\n        ToolCallRepairMiddleware()\n    ],\n)\n</code></pre>"},{"location":"zh/adavance-guide/middleware/tool-call-repair/#_5","title":"\u4fbf\u6377\u7528\u6cd5\uff08\u63a8\u8350\uff09","text":"<p>\u7531\u4e8e <code>ToolCallRepairMiddleware</code> \u5b9e\u4f8b\u5316\u65f6\u65e0\u9700\u914d\u7f6e\u53c2\u6570\uff0c\u672c\u5e93\u9884\u7f6e\u4e86\u4e00\u4e2a\u5168\u5c40\u5b9e\u4f8b <code>tool_call_repair</code>\uff0c\u63a8\u8350\u76f4\u63a5\u4f7f\u7528\u4ee5\u7b80\u5316\u4ee3\u7801\uff1a</p> <pre><code>from langchain_dev_utils.agents.middleware import tool_call_repair\n\nagent = create_agent(\n    model=\"vllm:qwen3-4b\",\n    tools=[run_python_code, get_current_time],\n    middleware=[tool_call_repair],\n)\n</code></pre> <p>\u6ce8\u610f\u4e8b\u9879</p> <p>\u672c\u4e2d\u95f4\u4ef6\u65e0\u6cd5\u4fdd\u8bc1 100% \u4fee\u590d\u6240\u6709\u65e0\u6548\u5de5\u5177\u8c03\u7528\uff0c\u5b9e\u9645\u4fee\u590d\u6548\u679c\u53d6\u51b3\u4e8e <code>json-repair</code> \u5e93\u7684\u80fd\u529b\uff1b\u6b64\u5916\uff0c\u5b83\u4ec5\u4f5c\u7528\u4e8e <code>invalid_tool_calls</code> \u5b57\u6bb5\u4e2d\u7684\u65e0\u6548\u8c03\u7528\u5185\u5bb9\u3002</p>"},{"location":"zh/adavance-guide/openai-compatible/chat/","title":"\u5bf9\u8bdd\u6a21\u578b\u7684\u521b\u5efa\u4e0e\u4f7f\u7528","text":""},{"location":"zh/adavance-guide/openai-compatible/chat/#_2","title":"\u521b\u5efa\u5bf9\u8bdd\u6a21\u578b\u7c7b","text":"<p>\u4f7f\u7528 <code>create_openai_compatible_model</code> \u51fd\u6570\u53ef\u4ee5\u521b\u5efa\u5bf9\u8bdd\u6a21\u578b\u96c6\u6210\u7c7b\u3002\u8be5\u51fd\u6570\u63a5\u53d7\u4ee5\u4e0b\u53c2\u6570\uff1a</p> \u53c2\u6570 \u8bf4\u660e <code>model_provider</code> \u6a21\u578b\u63d0\u4f9b\u5546\u540d\u79f0\uff0c\u4f8b\u5982 <code>vllm</code>\u3002\u5fc5\u987b\u4ee5\u5b57\u6bcd\u6216\u6570\u5b57\u5f00\u5934\uff0c\u53ea\u80fd\u5305\u542b\u5b57\u6bcd\u3001\u6570\u5b57\u548c\u4e0b\u5212\u7ebf\uff0c\u957f\u5ea6\u4e0d\u8d85\u8fc7 20 \u4e2a\u5b57\u7b26\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u662f <code>base_url</code> \u6a21\u578b\u63d0\u4f9b\u5546\u9ed8\u8ba4 API \u5730\u5740\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u5426 <code>compatibility_options</code> \u517c\u5bb9\u6027\u9009\u9879\u914d\u7f6e\u3002\u7c7b\u578b: <code>dict</code>\u5fc5\u586b: \u5426 <code>model_profiles</code> \u8be5\u63d0\u4f9b\u5546\u5404\u6a21\u578b\u7684 profile \u914d\u7f6e\u5b57\u5178\u3002\u7c7b\u578b: <code>dict</code>\u5fc5\u586b: \u5426 <code>chat_model_cls_name</code> \u5bf9\u8bdd\u6a21\u578b\u7c7b\u540d\uff08\u9700\u7b26\u5408 Python \u7c7b\u540d\u89c4\u8303\uff09\u3002\u9ed8\u8ba4\u503c\u4e3a <code>Chat{model_provider}</code>\uff08\u5176\u4e2d <code>{model_provider}</code> \u9996\u5b57\u6bcd\u5927\u5199\uff09\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u5426 <p>\u5176\u4e2d\uff0c<code>compatibility_options</code> \u662f\u4e00\u4e2a\u5b57\u5178\uff0c\u7528\u4e8e\u58f0\u660e\u8be5\u63d0\u4f9b\u5546\u5bf9 OpenAI API \u7684\u90e8\u5206\u7279\u6027\u7684\u652f\u6301\u60c5\u51b5\uff0c\u4ee5\u63d0\u9ad8\u517c\u5bb9\u6027\u548c\u7a33\u5b9a\u6027\u3002</p> <p>\u76ee\u524d\u652f\u6301\u4ee5\u4e0b\u914d\u7f6e\u9879\uff1a</p> \u914d\u7f6e\u9879 \u8bf4\u660e <code>supported_tool_choice</code> \u652f\u6301\u7684 <code>tool_choice</code> \u7b56\u7565\u5217\u8868\u3002\u7c7b\u578b: <code>list[str]</code>\u9ed8\u8ba4\u503c: <code>[\"auto\"]</code> <code>supported_response_format</code> \u652f\u6301\u7684 <code>response_format</code> \u683c\u5f0f\u5217\u8868\uff08<code>json_schema</code>\u3001<code>json_object</code>\uff09\u3002\u7c7b\u578b: <code>list[str]</code>\u9ed8\u8ba4\u503c: <code>[]</code> <code>reasoning_keep_policy</code> \u5386\u53f2\u6d88\u606f\u4e2d <code>reasoning_content</code> \u5b57\u6bb5\u7684\u4fdd\u7559\u7b56\u7565\u3002\u7c7b\u578b: <code>str</code>\u9ed8\u8ba4\u503c: <code>\"never\"</code> <code>include_usage</code> \u662f\u5426\u5728\u6d41\u5f0f\u8fd4\u56de\u7ed3\u679c\u4e2d\u5305\u542b <code>usage</code> \u4fe1\u606f\u3002\u7c7b\u578b: <code>bool</code>\u9ed8\u8ba4\u503c: <code>True</code> <p>\u8865\u5145</p> <p>\u7531\u4e8e\u540c\u4e00\u6a21\u578b\u63d0\u4f9b\u5546\u7684\u4e0d\u540c\u6a21\u578b\u5bf9 <code>tool_choice</code>\u3001<code>response_format</code> \u7b49\u53c2\u6570\u7684\u652f\u6301\u60c5\u51b5\u5b58\u5728\u5dee\u5f02\uff0c\u8fd9\u56db\u4e2a\u517c\u5bb9\u6027\u9009\u9879\u4e3a\u7c7b\u7684\u5b9e\u4f8b\u5c5e\u6027\u3002\u56e0\u6b64\uff0c\u521b\u5efa\u5bf9\u8bdd\u6a21\u578b\u7c7b\u65f6\u53ef\u4ee5\u4f20\u5165\u503c\u4f5c\u4e3a\u5168\u5c40\u9ed8\u8ba4\u503c\uff08\u4ee3\u8868\u8be5\u63d0\u4f9b\u5546\u5927\u90e8\u5206\u6a21\u578b\u652f\u6301\u7684\u914d\u7f6e\uff09\uff0c\u540e\u7eed\u5982\u9700\u9488\u5bf9\u7279\u5b9a\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u53ef\u5728\u5b9e\u4f8b\u5316\u65f6\u8986\u76d6\u540c\u540d\u53c2\u6570\u3002</p> <p>\u63d0\u793a</p> <p>\u672c\u5e93\u4f1a\u57fa\u4e8e\u7528\u6237\u4f20\u5165\u7684\u53c2\u6570\uff0c\u4f7f\u7528\u5185\u7f6e\u7684 <code>BaseChatOpenAICompatible</code> \u6784\u5efa\u9762\u5411\u7279\u5b9a\u63d0\u4f9b\u5546\u7684\u5bf9\u8bdd\u6a21\u578b\u7c7b\u3002\u8be5\u7c7b\u7ee7\u627f\u81ea <code>langchain-openai</code> \u7684 <code>BaseChatOpenAI</code>\uff0c\u5e76\u5728\u4ee5\u4e0b\u65b9\u9762\u8fdb\u884c\u4e86\u589e\u5f3a\uff1a</p> <ul> <li>\u652f\u6301\u66f4\u591a\u683c\u5f0f\u7684\u63a8\u7406\u5185\u5bb9\uff1a\u9664 OpenAI \u5b98\u65b9\u683c\u5f0f\u5916\uff0c\u4e5f\u652f\u6301\u4ee5 <code>reasoning_content</code> \u53c2\u6570\u8fd4\u56de\u7684\u63a8\u7406\u5185\u5bb9\u683c\u5f0f\u3002</li> <li>\u652f\u6301 <code>video</code> \u7c7b\u578b\u7684 content_block\uff1a\u8865\u9f50 <code>ChatOpenAI</code> \u5728\u89c6\u9891\u7c7b\u578b\u7684 <code>content_block</code> \u4e0a\u7684\u80fd\u529b\u7f3a\u53e3\u3002</li> <li>\u81ea\u52a8\u9009\u62e9\u66f4\u5408\u9002\u7684\u7ed3\u6784\u5316\u8f93\u51fa\u65b9\u5f0f\uff1a\u6839\u636e\u63d0\u4f9b\u5546\u5b9e\u9645\u652f\u6301\u60c5\u51b5\uff0c\u5728 <code>function_calling</code> \u4e0e <code>json_schema</code> \u4e4b\u95f4\u81ea\u52a8\u9009\u62e9\u66f4\u4f18\u65b9\u6848\u3002</li> <li>\u901a\u8fc7 <code>compatibility_options</code> \u7cbe\u7ec6\u9002\u914d\u5dee\u5f02\uff1a\u6309\u9700\u914d\u7f6e\u5bf9 <code>tool_choice</code>\u3001<code>response_format</code> \u7b49\u53c2\u6570\u7684\u652f\u6301\u5dee\u5f02\u3002</li> </ul> <p>\u4f7f\u7528\u5982\u4e0b\u4ee3\u7801\u521b\u5efa\u5bf9\u8bdd\u6a21\u578b\u7c7b\uff1a</p> <pre><code>from langchain_dev_utils.chat_models.adapters import create_openai_compatible_model\n\nChatVLLM = create_openai_compatible_model(\n    model_provider=\"vllm\",\n    base_url=\"http://localhost:8000/v1\",\n    chat_model_cls_name=\"ChatVLLM\",\n)\n\nmodel = ChatVLLM(model=\"qwen3-4b\")\nprint(model.invoke(\"\u4f60\u597d\"))\n</code></pre> <p>\u5728\u521b\u5efa\u5bf9\u8bdd\u6a21\u578b\u7c7b\u65f6\uff0c<code>base_url</code> \u53c2\u6570\u53ef\u4ee5\u7701\u7565\u3002\u672a\u4f20\u5165\u65f6\uff0c\u672c\u5e93\u4f1a\u9ed8\u8ba4\u8bfb\u53d6\u5bf9\u5e94\u73af\u5883\u53d8\u91cf\uff0c\u4f8b\u5982\uff1a</p> <pre><code>export VLLM_API_BASE=http://localhost:8000/v1\n</code></pre> <p>\u6b64\u65f6\u4ee3\u7801\u53ef\u4ee5\u7701\u7565 <code>base_url</code>\uff1a</p> <pre><code>from langchain_dev_utils.chat_models.adapters import create_openai_compatible_model\n\nChatVLLM = create_openai_compatible_model(\n    model_provider=\"vllm\",\n    chat_model_cls_name=\"ChatVLLM\",\n)\n\nmodel = ChatVLLM(model=\"qwen3-4b\")\nprint(model.invoke(\"\u4f60\u597d\"))\n</code></pre> <p>\u6ce8\u610f\uff1a\u4e0a\u8ff0\u4ee3\u7801\u6210\u529f\u8fd0\u884c\u7684\u524d\u63d0\u662f\u5df2\u914d\u7f6e\u73af\u5883\u53d8\u91cf <code>VLLM_API_KEY</code>\u3002\u867d\u7136 vLLM \u672c\u8eab\u4e0d\u8981\u6c42 API Key\uff0c\u4f46\u5bf9\u8bdd\u6a21\u578b\u7c7b\u521d\u59cb\u5316\u65f6\u9700\u8981\u4f20\u5165\uff0c\u56e0\u6b64\u8bf7\u5148\u8bbe\u7f6e\u8be5\u53d8\u91cf\uff0c\u4f8b\u5982\uff1a</p> <pre><code>export VLLM_API_KEY=vllm_api_key\n</code></pre> <p>\u63d0\u793a</p> <p>\u521b\u5efa\u7684\u5bf9\u8bdd\u6a21\u578b\u7c7b\uff08\u5d4c\u5165\u6a21\u578b\u7c7b\u4e5f\u9075\u5faa\u6b64\u547d\u540d\u89c4\u5219\uff09\u73af\u5883\u53d8\u91cf\u7684\u547d\u540d\u89c4\u5219\uff1a</p> <ul> <li> <p>API \u5730\u5740\uff1a<code>${PROVIDER_NAME}_API_BASE</code>\uff08\u5168\u5927\u5199\uff0c\u4e0b\u5212\u7ebf\u5206\u9694\uff09\u3002</p> </li> <li> <p>API Key\uff1a<code>${PROVIDER_NAME}_API_KEY</code>\uff08\u5168\u5927\u5199\uff0c\u4e0b\u5212\u7ebf\u5206\u9694\uff09\u3002</p> </li> </ul>"},{"location":"zh/adavance-guide/openai-compatible/chat/#_3","title":"\u4f7f\u7528\u5bf9\u8bdd\u6a21\u578b\u7c7b","text":""},{"location":"zh/adavance-guide/openai-compatible/chat/#_4","title":"\u666e\u901a\u8c03\u7528","text":"<p>\u901a\u8fc7 <code>invoke</code> \u65b9\u6cd5\u53ef\u4ee5\u8fdb\u884c\u666e\u901a\u8c03\u7528\uff0c\u8fd4\u56de\u6a21\u578b\u54cd\u5e94\u3002</p> <pre><code>from langchain_core.messages import HumanMessage\n\nmodel = ChatVLLM(model=\"qwen3-4b\")\nresponse = model.invoke([HumanMessage(\"Hello\")])\nprint(response)\n</code></pre> <p>\u540c\u65f6\u4e5f\u652f\u6301 <code>ainvoke</code> \u8fdb\u884c\u5f02\u6b65\u8c03\u7528\uff1a</p> <pre><code>from langchain_core.messages import HumanMessage\n\nmodel = ChatVLLM(model=\"qwen3-4b\")\nresponse = await model.ainvoke([HumanMessage(\"Hello\")])\nprint(response)\n</code></pre>"},{"location":"zh/adavance-guide/openai-compatible/chat/#_5","title":"\u6d41\u5f0f\u8c03\u7528","text":"<p>\u901a\u8fc7 <code>stream</code> \u65b9\u6cd5\u53ef\u4ee5\u8fdb\u884c\u6d41\u5f0f\u8c03\u7528\uff0c\u7528\u4e8e\u6d41\u5f0f\u8fd4\u56de\u6a21\u578b\u54cd\u5e94\u3002</p> <pre><code>from langchain_core.messages import HumanMessage\n\nmodel = ChatVLLM(model=\"qwen3-4b\")\nfor chunk in model.stream([HumanMessage(\"Hello\")]):\n    print(chunk)\n</code></pre> <p>\u4ee5\u53ca\u901a\u8fc7 <code>astream</code> \u8fdb\u884c\u5f02\u6b65\u6d41\u5f0f\u8c03\u7528\uff1a</p> <pre><code>from langchain_core.messages import HumanMessage\n\nmodel = ChatVLLM(model=\"qwen3-4b\")\nasync for chunk in model.astream([HumanMessage(\"Hello\")]):\n    print(chunk)\n</code></pre> \u6d41\u5f0f\u8f93\u51fa\u9009\u9879 <p>\u53ef\u4ee5\u901a\u8fc7 <code>stream_options={\"include_usage\": True}</code> \u5728\u6d41\u5f0f\u54cd\u5e94\u672b\u5c3e\u9644\u52a0 token \u4f7f\u7528\u60c5\u51b5\uff08<code>prompt_tokens</code> \u548c <code>completion_tokens</code>\uff09\u3002 \u672c\u5e93\u9ed8\u8ba4\u5f00\u542f\u8be5\u9009\u9879\uff1b\u82e5\u9700\u5173\u95ed\uff0c\u53ef\u5728\u521b\u5efa\u6a21\u578b\u7c7b\u6216\u5b9e\u4f8b\u5316\u65f6\u4f20\u5165\u517c\u5bb9\u6027\u9009\u9879 <code>include_usage=False</code>\u3002</p>"},{"location":"zh/adavance-guide/openai-compatible/chat/#_6","title":"\u5de5\u5177\u8c03\u7528","text":"<p>\u5982\u679c\u6a21\u578b\u672c\u8eab\u652f\u6301\u5de5\u5177\u8c03\u7528\uff0c\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 <code>bind_tools</code> \u8fdb\u884c\u5de5\u5177\u8c03\u7528\uff1a</p> <pre><code>from langchain_core.messages import HumanMessage\nfrom langchain_core.tools import tool\nimport datetime\n\n@tool\ndef get_current_time() -&gt; str:\n    \"\"\"\u83b7\u53d6\u5f53\u524d\u65f6\u95f4\u6233\"\"\"\n    return str(datetime.datetime.now().timestamp())\n\nmodel = ChatVLLM(model=\"qwen3-4b\").bind_tools([get_current_time])\nresponse = model.invoke([HumanMessage(\"\u83b7\u53d6\u5f53\u524d\u65f6\u95f4\u6233\")])\nprint(response)\n</code></pre> \u5e76\u884c\u5de5\u5177\u8c03\u7528 <p>\u5982\u679c\u6a21\u578b\u652f\u6301\u5e76\u884c\u5de5\u5177\u8c03\u7528\uff0c\u53ef\u4ee5\u5728 <code>bind_tools</code> \u4e2d\u4f20\u9012 <code>parallel_tool_calls=True</code> \u5f00\u542f\u5e76\u884c\u5de5\u5177\u8c03\u7528\uff08\u90e8\u5206\u6a21\u578b\u63d0\u4f9b\u5546\u9ed8\u8ba4\u5f00\u542f\uff0c\u5219\u65e0\u9700\u663e\u5f0f\u4f20\u53c2\uff09\u3002</p> <p>\u4f8b\u5982\uff1a</p> <pre><code>from langchain_core.messages import HumanMessage\nfrom langchain_core.tools import tool\n\n\n@tool\ndef get_current_weather(location: str) -&gt; str:\n    \"\"\"\u83b7\u53d6\u5f53\u524d\u5929\u6c14\"\"\"\n    return f\"\u5f53\u524d{location}\u7684\u5929\u6c14\u662f\u6674\u6717\"\n\nmodel = ChatVLLM(model=\"qwen3-4b\").bind_tools(\n    [get_current_weather], parallel_tool_calls=True\n)\nresponse = model.invoke([HumanMessage(\"\u83b7\u53d6\u6d1b\u6749\u77f6\u548c\u4f26\u6566\u7684\u5929\u6c14\")])\nprint(response)\n</code></pre> \u5f3a\u5236\u5de5\u5177\u8c03\u7528 <p>\u901a\u8fc7 <code>tool_choice</code> \u53c2\u6570\uff0c\u53ef\u4ee5\u63a7\u5236\u6a21\u578b\u5728\u54cd\u5e94\u65f6\u662f\u5426\u8c03\u7528\u5de5\u5177\u4ee5\u53ca\u8c03\u7528\u54ea\u4e2a\u5de5\u5177\uff0c\u4ee5\u63d0\u5347\u51c6\u786e\u6027\u3001\u53ef\u9760\u6027\u548c\u53ef\u63a7\u6027\u3002\u5e38\u89c1\u53d6\u503c\u6709\uff1a</p> <ul> <li><code>\"auto\"</code>\uff1a\u6a21\u578b\u81ea\u4e3b\u51b3\u5b9a\u662f\u5426\u8c03\u7528\u5de5\u5177\uff08\u9ed8\u8ba4\u884c\u4e3a\uff09\uff1b</li> <li><code>\"none\"</code>\uff1a\u7981\u6b62\u8c03\u7528\u5de5\u5177\uff1b</li> <li><code>\"required\"</code>\uff1a\u5f3a\u5236\u8c03\u7528\u81f3\u5c11\u4e00\u4e2a\u5de5\u5177\uff1b</li> <li>\u6307\u5b9a\u5177\u4f53\u5de5\u5177\uff08\u5728 OpenAI \u517c\u5bb9 API \u4e2d\uff0c\u5177\u4f53\u4e3a <code>{\"type\": \"function\", \"function\": {\"name\": \"xxx\"}}</code>\uff09\u3002</li> </ul> <p>\u4e0d\u540c\u63d0\u4f9b\u5546\u5bf9<code>tool_choice</code>\u7684\u652f\u6301\u8303\u56f4\u4e0d\u540c\u3002\u4e3a\u89e3\u51b3\u5dee\u5f02\uff0c\u672c\u5e93\u5f15\u5165\u517c\u5bb9\u6027\u914d\u7f6e\u9879 <code>supported_tool_choice</code>\uff0c\u9ed8\u8ba4\u503c\u4e3a <code>[\"auto\"]</code>\uff0c\u6b64\u65f6<code>bind_tools</code> \u4e2d\u4f20\u5165\u7684 <code>tool_choice</code> \u53ea\u80fd\u4e3a <code>auto</code>\uff0c\u5176\u4ed6\u53d6\u503c\u4f1a\u88ab\u8fc7\u6ee4\u3002</p> <p>\u82e5\u9700\u652f\u6301\u4f20\u9012\u5176\u4ed6 <code>tool_choice</code> \u53d6\u503c\uff0c\u5fc5\u987b\u914d\u7f6e\u652f\u6301\u9879\u3002\u914d\u7f6e\u503c\u4e3a\u5b57\u7b26\u4e32\u5217\u8868\uff0c\u6bcf\u4e2a\u5b57\u7b26\u4e32\u7684\u53ef\u9009\u503c\uff1a</p> <ul> <li><code>\"auto\"</code>, <code>\"none\"</code>, <code>\"required\"</code>\uff1a\u5bf9\u5e94\u6807\u51c6\u7b56\u7565\uff1b</li> <li><code>\"specific\"</code>\uff1a\u672c\u5e93\u7279\u6709\u6807\u8bc6\uff0c\u8868\u793a\u652f\u6301\u6307\u5b9a\u5177\u4f53\u5de5\u5177\u3002</li> </ul> <p>\u4f8b\u5982 vLLM \u652f\u6301\u5168\u90e8\u7b56\u7565\uff1a</p> <pre><code>from langchain_dev_utils.chat_models.adapters import create_openai_compatible_model\n\nChatVLLM = create_openai_compatible_model(\n    model_provider=\"vllm\",\n    chat_model_cls_name=\"ChatVLLM\",\n    compatibility_options={\n        \"supported_tool_choice\": [\"auto\", \"required\", \"none\", \"specific\"]\n    },\n)\n\nmodel = ChatVLLM(model=\"qwen3-4b\").bind_tools(\n    [get_current_weather], tool_choice=\"required\"\n)\n</code></pre>"},{"location":"zh/adavance-guide/openai-compatible/chat/#_7","title":"\u7ed3\u6784\u5316\u8f93\u51fa","text":"<pre><code>from langchain_core.messages import HumanMessage\nfrom pydantic import BaseModel\n\nclass User(BaseModel):\n    name: str\n    age: int\n\nmodel = ChatVLLM(model=\"qwen3-4b\").with_structured_output(User)\nresponse = model.invoke([HumanMessage(\"\u4f60\u597d\uff0c\u6211\u53eb\u5f20\u4e09\uff0c\u4eca\u5e7425\u5c81\")])\nprint(response)\n</code></pre> \u9ed8\u8ba4\u7684\u7ed3\u6784\u5316\u8f93\u51fa\u65b9\u6cd5 <p>\u76ee\u524d\u5e38\u89c1\u7684\u7ed3\u6784\u5316\u8f93\u51fa\u65b9\u6cd5\u6709\u4e09\u79cd\uff1a<code>json_schema</code>\u3001<code>function_calling</code>\u3001<code>json_mode</code>\u3002\u5176\u4e2d\uff0c\u6548\u679c\u6700\u597d\u7684\u662f<code>json_schema</code>\uff0c\u6545\u672c\u5e93\u7684 <code>with_structured_output</code> \u4f1a\u4f18\u5148\u4f7f\u7528<code>json_schema</code>\u4f5c\u4e3a\u7ed3\u6784\u5316\u8f93\u51fa\u65b9\u6cd5\uff1b\u5f53\u63d0\u4f9b\u5546\u4e0d\u652f\u6301\u65f6\uff0c\u624d\u4f1a\u81ea\u52a8\u964d\u7ea7\u4e3a <code>function_calling</code>\u3002\u4e0d\u540c\u7684\u6a21\u578b\u63d0\u4f9b\u5546\u5bf9\u4e8e\u7ed3\u6784\u5316\u8f93\u51fa\u7684\u652f\u6301\u7a0b\u5ea6\u6709\u6240\u4e0d\u540c\u3002\u672c\u5e93\u901a\u8fc7\u517c\u5bb9\u6027\u914d\u7f6e\u9879 <code>supported_response_format</code> \u58f0\u660e\u63d0\u4f9b\u5546\u652f\u6301\u7684\u7ed3\u6784\u5316\u8f93\u51fa\u65b9\u6cd5\u3002\u9ed8\u8ba4\u503c\u4e3a <code>[]</code>\uff0c\u8868\u793a\u65e2\u4e0d\u652f\u6301 <code>json_schema</code> \u4e5f\u4e0d\u652f\u6301 <code>json_mode</code>\u3002\u6b64\u65f6 <code>with_structured_output(method=...)</code> \u4f1a\u56fa\u5b9a\u4f7f\u7528 <code>function_calling</code>\uff1b\u5373\u4f7f\u4f20\u5165 <code>json_schema</code> / <code>json_mode</code> \u4e5f\u4f1a\u81ea\u52a8\u8f6c\u5316\u4e3a <code>function_calling</code>\uff0c\u5982\u679c\u60f3\u8981\u4f7f\u7528\u5bf9\u5e94\u7684\u7ed3\u6784\u5316\u8f93\u51fa\u65b9\u6cd5\uff0c\u9700\u8981\u663e\u5f0f\u4f20\u5165\u54cd\u5e94\u7684\u53c2\u6570\uff08\u5c24\u5176\u662f<code>json_schema</code>)\u3002</p> <p>\u4f8b\u5982\uff0cvLLM \u90e8\u7f72\u7684\u6a21\u578b\u652f\u6301 <code>json_schema</code> \u7ed3\u6784\u5316\u8f93\u51fa\u65b9\u6cd5\uff0c\u5219\u53ef\u4ee5\u5728\u6ce8\u518c\u65f6\u8fdb\u884c\u58f0\u660e\uff1a</p> <pre><code>from langchain_dev_utils.chat_models.adapters import create_openai_compatible_model\n\nChatVLLM = create_openai_compatible_model(\n    model_provider=\"vllm\",\n    chat_model_cls_name=\"ChatVLLM\",\n    compatibility_options={\"supported_response_format\": [\"json_schema\"]},\n)\n\nmodel = ChatVLLM(model=\"qwen3-4b\")\n</code></pre> <p>\u6ce8\u610f</p> <p>\u82e5 <code>supported_response_format</code> \u5305\u542b <code>json_schema</code>\uff0c\u5219 <code>model.profile</code> \u4e2d\u7684 <code>structured_output</code> \u5b57\u6bb5\u5c06\u81ea\u52a8\u7f6e\u4e3a <code>True</code>\uff0c\u6b64\u65f6\u4f7f\u7528 <code>create_agent</code> \u8fdb\u884c\u7ed3\u6784\u5316\u8f93\u51fa\u65f6\u5982\u679c\u672a\u6307\u5b9a\u5177\u4f53\u7684\u7ed3\u6784\u5316\u8f93\u51fa\u7b56\u7565\uff0c\u9ed8\u8ba4\u4f1a\u91c7\u7528 <code>json_schema</code> \u4f5c\u4e3a\u7ed3\u6784\u5316\u8f93\u51fa\u7b56\u7565\u3002</p> <p>\u4f8b\u5982:  <pre><code>from langchain_dev_utils.chat_models.adapters import create_openai_compatible_model\n\nChatVLLM = create_openai_compatible_model(\n    model_provider=\"vllm\",\n    base_url=\"http://localhost:8000/v1\",\n    compatibility_options={\"supported_response_format\": [\"json_schema\"]},\n)\n\nmodel = ChatVLLM(model=\"qwen3-4b\")\nprint(model.profile)\n</code></pre></p> <p>\u8f93\u51fa\u7ed3\u679c\u4e3a</p> <pre><code>{'structured_output': True}\n</code></pre>"},{"location":"zh/adavance-guide/openai-compatible/chat/#_8","title":"\u4f20\u9012\u989d\u5916\u53c2\u6570","text":"<p>\u7531\u4e8e\u8be5\u7c7b\u7ee7\u627f\u81ea <code>BaseChatOpenAI</code>\uff0c\u56e0\u6b64\u652f\u6301\u4f20\u9012 <code>BaseChatOpenAI</code> \u7684\u6a21\u578b\u53c2\u6570\uff0c\u4f8b\u5982 <code>temperature</code>\u3001<code>extra_body</code> \u7b49\u3002</p> <p>\u4f8b\u5982\uff0c\u4f7f\u7528 <code>extra_body</code> \u4f20\u9012\u989d\u5916\u53c2\u6570\uff08\u6b64\u5904\u4e3a\u5173\u95ed\u601d\u8003\u6a21\u5f0f\uff09\uff1a</p> <pre><code>from langchain_core.messages import HumanMessage\n\nmodel = ChatVLLM(\n    model=\"qwen3-4b\",\n    extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},\n)\nresponse = model.invoke([HumanMessage(\"Hello\")])\nprint(response)\n</code></pre>"},{"location":"zh/adavance-guide/openai-compatible/chat/#_9","title":"\u4f20\u9012\u591a\u6a21\u6001\u6570\u636e","text":"<p>\u652f\u6301\u4f20\u9012\u591a\u6a21\u6001\u6570\u636e\u3002\u4f60\u53ef\u4ee5\u4f7f\u7528 OpenAI \u517c\u5bb9\u7684\u591a\u6a21\u6001\u6570\u636e\u683c\u5f0f\uff0c\u6216\u76f4\u63a5\u4f7f\u7528 LangChain \u4e2d\u7684 <code>content_block</code>\u3002</p> <p>\u4f20\u9012\u56fe\u7247\u7c7b\u6570\u636e\uff1a</p> <p><pre><code>from langchain_core.messages import HumanMessage\nmessages = [\n    HumanMessage(\n        content_blocks=[\n            {\n                \"type\": \"image\",\n                \"url\": \"https://example.com/image.png\",\n            },\n            {\"type\": \"text\", \"text\": \"\u63cf\u8ff0\u8fd9\u5f20\u56fe\u7247\"},\n        ]\n    )\n]\n\nmodel = ChatVLLM(model=\"qwen3-vl-2b\")\nresponse = model.invoke(messages)\nprint(response)\n</code></pre> \u4f20\u9012\u89c6\u9891\u7c7b\u6570\u636e\uff1a</p> <pre><code>from langchain_core.messages import HumanMessage\n\nmessages = [\n    HumanMessage(\n        content_blocks=[\n            {\n                \"type\": \"video\",\n                \"url\": \"https://example.com/video.mp4\",\n            },\n            {\"type\": \"text\", \"text\": \"\u63cf\u8ff0\u8fd9\u6bb5\u89c6\u9891\"},\n        ]\n    )\n]\n\nmodel = ChatVLLM(model=\"qwen3-vl-2b\")\nresponse = model.invoke(messages)\nprint(response)\n</code></pre>"},{"location":"zh/adavance-guide/openai-compatible/chat/#_10","title":"\u4f7f\u7528\u63a8\u7406\u6a21\u578b","text":"<p>\u672c\u5e93\u521b\u5efa\u7684\u6a21\u578b\u7c7b\u7684\u4e00\u5927\u7279\u70b9\u5c31\u662f\u8fdb\u4e00\u6b65\u9002\u914d\u4e86\u66f4\u591a\u7684\u63a8\u7406\u6a21\u578b\u3002</p> <p>\u4f8b\u5982\uff1a</p> <pre><code>from langchain_core.messages import HumanMessage\n\nmodel = ChatVLLM(model=\"qwen3-4b\")\nresponse = model.invoke(\"\u4e3a\u4ec0\u4e48\u9e66\u9e49\u7684\u7fbd\u6bdb\u5982\u6b64\u9c9c\u8273\uff1f\")\nreasoning_steps = [b for b in response.content_blocks if b[\"type\"] == \"reasoning\"]\nprint(\" \".join(step[\"reasoning\"] for step in reasoning_steps))\n</code></pre> \u4e0d\u540c\u63a8\u7406\u6a21\u5f0f\u7684\u652f\u6301 <p>\u4e0d\u540c\u6a21\u578b\u7684\u63a8\u7406\u6a21\u5f0f\u4e0d\u5c3d\u76f8\u540c\uff08\u8fd9\u70b9\u5728Agent\u5f00\u53d1\u4e2d\u5c24\u4e3a\u91cd\u8981\uff09\uff1a\u6709\u4e9b\u9700\u8981\u5728\u672c\u6b21\u8c03\u7528\u4e2d\u663e\u5f0f\u4f20\u9012 <code>reasoning_content</code> \u5b57\u6bb5\uff0c\u6709\u4e9b\u5219\u65e0\u9700\u3002\u672c\u5e93\u63d0\u4f9b <code>reasoning_keep_policy</code> \u517c\u5bb9\u6027\u914d\u7f6e\u4ee5\u9002\u914d\u8fd9\u4e9b\u5dee\u5f02\u3002</p> <p>\u8be5\u914d\u7f6e\u9879\u652f\u6301\u4ee5\u4e0b\u53d6\u503c\uff1a</p> <ul> <li> <p><code>never</code>\uff1a\u5728\u5386\u53f2\u6d88\u606f\u4e2d\u4e0d\u4fdd\u7559\u4efb\u4f55\u63a8\u7406\u5185\u5bb9\uff08\u9ed8\u8ba4)\uff1b</p> </li> <li> <p><code>current</code>\uff1a\u4ec5\u4fdd\u7559\u5f53\u524d\u5bf9\u8bdd\u4e2d\u7684 <code>reasoning_content</code> \u5b57\u6bb5\uff1b</p> </li> <li> <p><code>all</code>\uff1a\u4fdd\u7559\u6240\u6709\u5bf9\u8bdd\u4e2d\u7684 <code>reasoning_content</code> \u5b57\u6bb5\u3002</p> </li> </ul> <pre><code>graph LR\n    A[reasoning_content \u4fdd\u7559\u7b56\u7565] --&gt; B{\u53d6\u503c?};\n    B --&gt;|never| C[\u4e0d\u5305\u542b\u4efb\u4f55&lt;br&gt;reasoning_content];\n    B --&gt;|current| D[\u4ec5\u5305\u542b\u5f53\u524d\u5bf9\u8bdd\u7684&lt;br&gt;reasoning_content&lt;br&gt;\u9002\u914d\u4ea4\u9519\u5f0f\u601d\u8003\u6a21\u5f0f];\n    B --&gt;|all| E[\u5305\u542b\u6240\u6709\u5bf9\u8bdd\u7684&lt;br&gt;reasoning_content];\n    C --&gt; F[\u53d1\u9001\u7ed9\u6a21\u578b];\n    D --&gt; F;\n    E --&gt; F;</code></pre> <p>\u4f8b\u5982\uff0c\u7528\u6237\u5148\u63d0\u95ee\"\u7ebd\u7ea6\u5929\u6c14\u5982\u4f55\uff1f\"\uff0c\u968f\u540e\u8ffd\u95ee\"\u4f26\u6566\u5929\u6c14\u5982\u4f55\uff1f\"\uff0c\u5f53\u524d\u6b63\u8981\u8fdb\u884c\u7b2c\u4e8c\u8f6e\u5bf9\u8bdd\uff0c\u4e14\u5373\u5c06\u8fdb\u884c\u6700\u540e\u4e00\u6b21\u6a21\u578b\u8c03\u7528\u3002</p> <ul> <li>\u53d6\u503c\u4e3a<code>never</code>\u65f6</li> </ul> <p>\u6700\u7ec8\u4f20\u9012\u7ed9\u6a21\u578b\u7684 messages \u4e2d\u4e0d\u4f1a\u6709\u4efb\u4f55 <code>reasoning_content</code> \u5b57\u6bb5\uff0c\u6a21\u578b\u6536\u5230\u7684 messages \u4e3a\uff1a</p> <pre><code>messages = [\n    {\"content\": \"\u67e5\u7ebd\u7ea6\u5929\u6c14\u5982\u4f55\uff1f\", \"role\": \"user\"},\n    {\"content\": \"\", \"role\": \"assistant\", \"tool_calls\": [...]},\n    {\"content\": \"\u591a\u4e91 7~13\u00b0C\", \"role\": \"tool\", \"tool_call_id\": \"...\"},\n    {\"content\": \"\u7ebd\u7ea6\u4eca\u5929\u5929\u6c14\u4e3a\u591a\u4e91\uff0c7~13\u00b0C\u3002\", \"role\": \"assistant\"},\n    {\"content\": \"\u67e5\u4f26\u6566\u5929\u6c14\u5982\u4f55\uff1f\", \"role\": \"user\"},\n    {\"content\": \"\", \"role\": \"assistant\", \"tool_calls\": [...]},\n    {\"content\": \"\u96e8\u5929\uff0c14~20\u00b0C\", \"role\": \"tool\", \"tool_call_id\": \"...\"},\n]\n</code></pre> <ul> <li>\u53d6\u503c\u4e3a<code>current</code>\u65f6</li> </ul> <p>\u4ec5\u4fdd\u7559\u5f53\u524d\u5bf9\u8bdd\u4e2d\u7684 <code>reasoning_content</code> \u5b57\u6bb5\u3002\u8be5\u7b56\u7565\u9002\u7528\u4e8e\u4ea4\u9519\u5f0f\u601d\u8003\uff08Interleaved Thinking\uff09\u573a\u666f\uff0c\u5373\u6a21\u578b\u5728\u663e\u5f0f\u63a8\u7406\u4e0e\u5de5\u5177\u8c03\u7528\u4e4b\u95f4\u4ea4\u66ff\u8fdb\u884c\uff0c\u6b64\u65f6\u9700\u8981\u5c06\u5f53\u524d\u8f6e\u6b21\u7684\u63a8\u7406\u5185\u5bb9\u8fdb\u884c\u4fdd\u7559\u3002\u6a21\u578b\u6536\u5230\u7684 messages \u4e3a\uff1a <pre><code>messages = [\n    {\"content\": \"\u67e5\u7ebd\u7ea6\u5929\u6c14\u5982\u4f55\uff1f\", \"role\": \"user\"},\n    {\"content\": \"\", \"role\": \"assistant\", \"tool_calls\": [...]},\n    {\"content\": \"\u591a\u4e91 7~13\u00b0C\", \"role\": \"tool\", \"tool_call_id\": \"...\"},\n    {\"content\": \"\u7ebd\u7ea6\u4eca\u5929\u5929\u6c14\u4e3a\u591a\u4e91\uff0c7~13\u00b0C\u3002\", \"role\": \"assistant\"},\n    {\"content\": \"\u67e5\u4f26\u6566\u5929\u6c14\u5982\u4f55\uff1f\", \"role\": \"user\"},\n    {\n        \"content\": \"\",\n        \"reasoning_content\": \"\u67e5\u4f26\u6566\u5929\u6c14\uff0c\u9700\u8981\u76f4\u63a5\u8c03\u7528\u5929\u6c14\u5de5\u5177\u3002\",  # \u4ec5\u4fdd\u7559\u672c\u8f6e\u5bf9\u8bdd\u7684 reasoning_content\n        \"role\": \"assistant\",\n        \"tool_calls\": [...],\n    },\n    {\"content\": \"\u96e8\u5929\uff0c14~20\u00b0C\", \"role\": \"tool\", \"tool_call_id\": \"...\"},\n]\n</code></pre></p> <ul> <li>\u53d6\u503c\u4e3a<code>all</code>\u65f6</li> </ul> <p>\u4fdd\u7559\u6240\u6709\u5bf9\u8bdd\u4e2d\u7684 <code>reasoning_content</code> \u5b57\u6bb5\u3002\u6a21\u578b\u6536\u5230\u7684 messages \u4e3a\uff1a <pre><code>messages = [\n    {\"content\": \"\u67e5\u7ebd\u7ea6\u5929\u6c14\u5982\u4f55\uff1f\", \"role\": \"user\"},\n    {\n        \"content\": \"\",\n        \"reasoning_content\": \"\u67e5\u7ebd\u7ea6\u5929\u6c14\uff0c\u9700\u8981\u76f4\u63a5\u8c03\u7528\u5929\u6c14\u5de5\u5177\u3002\",  # \u4fdd\u7559 reasoning_content\n        \"role\": \"assistant\",\n        \"tool_calls\": [...],\n    },\n    {\"content\": \"\u591a\u4e91 7~13\u00b0C\", \"role\": \"tool\", \"tool_call_id\": \"...\"},\n    {\n        \"content\": \"\u7ebd\u7ea6\u4eca\u5929\u5929\u6c14\u4e3a\u591a\u4e91\uff0c7~13\u00b0C\u3002\",\n        \"reasoning_content\": \"\u76f4\u63a5\u8fd4\u56de\u7ebd\u7ea6\u5929\u6c14\u7ed3\u679c\u3002\",  # \u4fdd\u7559 reasoning_content\n        \"role\": \"assistant\",\n    },\n    {\"content\": \"\u67e5\u4f26\u6566\u5929\u6c14\u5982\u4f55\uff1f\", \"role\": \"user\"},\n    {\n        \"content\": \"\",\n        \"reasoning_content\": \"\u67e5\u4f26\u6566\u5929\u6c14\uff0c\u9700\u8981\u76f4\u63a5\u8c03\u7528\u5929\u6c14\u5de5\u5177\u3002\",  # \u4fdd\u7559 reasoning_content\n        \"role\": \"assistant\",\n        \"tool_calls\": [...],\n    },\n    {\"content\": \"\u96e8\u5929\uff0c14~20\u00b0C\", \"role\": \"tool\", \"tool_call_id\": \"...\"},\n]\n</code></pre></p> <p>\u6ce8\u610f\uff1a\u82e5\u672c\u8f6e\u5bf9\u8bdd\u4e0d\u6d89\u53ca\u5de5\u5177\u8c03\u7528\uff0c\u5219<code>current</code>\u4e0e<code>never</code>\u6548\u679c\u76f8\u540c\u3002</p> <p>\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u867d\u7136\u8be5\u53c2\u6570\u5c5e\u4e8e\u517c\u5bb9\u6027\u914d\u7f6e\u9879\uff0c\u4f46\u540c\u4e00\u63d0\u4f9b\u5546\u7684\u4e0d\u540c\u6a21\u578b\u3001\u751a\u81f3\u540c\u4e00\u6a21\u578b\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u5bf9 <code>reasoning_content</code> \u7684\u4fdd\u7559\u7b56\u7565\u8981\u6c42\u4e5f\u53ef\u80fd\u4e0d\u540c\uff0c\u56e0\u6b64\u5efa\u8bae\u5728\u5b9e\u4f8b\u5316\u65f6\u663e\u5f0f\u6307\u5b9a\uff0c\u521b\u5efa\u7c7b\u65f6\u65e0\u9700\u8d4b\u503c\u3002</p> <p>\u4f8b\u5982\uff0c\u4ee5 GLM-4.7-Flash \u6a21\u578b\u4e3a\u4f8b\uff0c\u7531\u4e8e\u5176\u652f\u6301\u4ea4\u9519\u5f0f\u601d\u8003\uff08Interleaved Thinking\uff09\u6a21\u5f0f\uff0c\u4e00\u822c\u9700\u8981\u5728\u5b9e\u4f8b\u5316\u65f6\u5c06 <code>reasoning_keep_policy</code> \u8bbe\u7f6e\u4e3a <code>current</code>\uff0c\u4ee5\u4fbf\u4ec5\u4fdd\u7559\u5f53\u524d\u8f6e\u6b21\u7684 <code>reasoning_content</code>\u3002\u4f8b\u5982\uff1a</p> <p><pre><code>from langchain_core.messages import HumanMessage\n\nmodel = ChatVLLM(model=\"glm-4.7-flash\", reasoning_keep_policy=\"current\")\nagent = create_agent(\n    model=model,\n    tools=[get_current_weather],\n)\nresponse = agent.invoke({\"messages\": [HumanMessage(content=\"\u67e5\u7ebd\u7ea6\u5929\u6c14\u5982\u4f55\uff1f\")]})\nprint(response)\n</code></pre> \u540c\u65f6\uff0cGLM-4.7-Flash \u6a21\u578b\u4e5f\u652f\u6301\u53e6\u4e00\u79cd\u601d\u8003\u6a21\u5f0f\uff0c\u88ab\u79f0\u4e4b\u4e3aPreserved Thinking\u3002\u6b64\u65f6\u9700\u8981\u4fdd\u7559\u5386\u53f2\u6d88\u606f\u4e2d\u7684\u6240\u6709 <code>reasoning_content</code> \u5b57\u6bb5\uff0c\u53ef\u4ee5\u5c06 <code>reasoning_keep_policy</code> \u8bbe\u7f6e\u4e3a <code>all</code>\u3002\u4f8b\u5982\uff1a</p> <pre><code>from langchain_core.messages import HumanMessage\n\nmodel = ChatVLLM(\n    model=\"glm-4.7-flash\",\n    reasoning_keep_policy=\"all\",\n    extra_body={\"chat_template_kwargs\": {\"clear_thinking\": False}},\n)\n\nagent = create_agent(\n    model=model,\n    tools=[get_current_weather],\n)\nresponse = agent.invoke({\"messages\": [HumanMessage(content=\"\u67e5\u7ebd\u7ea6\u5929\u6c14\u5982\u4f55\uff1f\")]})\nprint(response)\n</code></pre>"},{"location":"zh/adavance-guide/openai-compatible/chat/#model-profiles","title":"Model profiles","text":"<p>\u53ef\u4ee5\u901a\u8fc7 <code>model.profile</code> \u83b7\u53d6\u6a21\u578b\u7684 profile\u3002\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u8fd4\u56de\u7a7a\u5b57\u5178\u3002</p> <p>\u4f60\u4e5f\u53ef\u4ee5\u5728\u5b9e\u4f8b\u5316\u65f6\u663e\u5f0f\u4f20\u5165 <code>profile</code> \u53c2\u6570\u6307\u5b9a\u6a21\u578b profile\u3002</p> <p>\u4f8b\u5982\uff1a <pre><code>from langchain_core.messages import HumanMessage\n\ncustom_profile = {\n    \"max_input_tokens\": 100_000,\n    \"tool_calling\": True,\n    \"structured_output\": True,\n    # ...\n}\nmodel = ChatVLLM(model=\"qwen3-4b\", profile=custom_profile)\nprint(model.profile)\n</code></pre> \u6216\u8005\u76f4\u63a5\u5728\u521b\u5efa\u65f6\u4f20\u5165\u6a21\u578b\u63d0\u4f9b\u5546\u7684\u6240\u6709\u6a21\u578b\u7684<code>profile</code>\u53c2\u6570\u3002</p> <p>\u4f8b\u5982\uff1a <pre><code>from langchain_dev_utils.chat_models.adapters import create_openai_compatible_model\n\nmodel_profiles = {\n    \"qwen3-4b\": {\n        \"max_input_tokens\": 131072,\n        \"max_output_tokens\": 8192,\n        \"image_inputs\": False,\n        \"audio_inputs\": False,\n        \"video_inputs\": False,\n        \"image_outputs\": False,\n        \"audio_outputs\": False,\n        \"video_outputs\": False,\n        \"reasoning_output\": True,\n        \"tool_calling\": True,\n    }\n    # \u6b64\u5904\u8fd8\u53ef\u4ee5\u5199\u66f4\u591a\u7684model profile\n}\n\nChatVLLM = create_openai_compatible_model(\n    model_provider=\"vllm\",\n    base_url=\"http://localhost:8000/v1\",\n    model_profiles=model_profiles,\n)\n\nmodel = ChatVLLM(\n    model=\"qwen3-4b\",\n)\nprint(model.profile)\n</code></pre></p>"},{"location":"zh/adavance-guide/openai-compatible/chat/#openai-responses-api","title":"\u652f\u6301 OpenAI \u6700\u65b0\u7684 Responses API","text":"<p>\u8be5\u6a21\u578b\u7c7b\u4e5f\u652f\u6301 OpenAI \u6700\u65b0\u7684 <code>responses</code> API\uff08\u53c2\u6570\u540d\u4e3a <code>use_responses_api</code>\uff09\u3002\u76ee\u524d\u4ec5\u5c11\u91cf\u63d0\u4f9b\u5546\u652f\u6301\u8be5\u98ce\u683c\u63a5\u53e3\uff1b\u82e5\u4f60\u7684\u63d0\u4f9b\u5546\u652f\u6301\uff0c\u53ef\u901a\u8fc7 <code>use_responses_api=True</code> \u5f00\u542f\u3002</p> <p>\u4f8b\u5982 vLLM \u652f\u6301 <code>responses</code> API\uff0c\u5219\u53ef\u4ee5\u8fd9\u6837\u4f7f\u7528\uff1a</p> <pre><code>from langchain_core.messages import HumanMessage\n\nmodel = ChatVLLM(model=\"qwen3-4b\", use_responses_api=True)\nresponse = model.invoke([HumanMessage(content=\"\u4f60\u597d\")])\nprint(response)\n</code></pre> <p>\u76ee\u524d\u8be5\u529f\u80fd\u7684\u5b9e\u73b0\u5b8c\u5168\u4f9d\u6258\u4e8e<code>BaseChatOpenAI</code>\u5bf9\u4e8e<code>responses</code> API\u7684\u5b9e\u73b0\uff0c\u6545\u5728\u4f7f\u7528\u4e2d\u53ef\u80fd\u5b58\u5728\u4e00\u5b9a\u7684\u517c\u5bb9\u6027\u95ee\u9898\uff0c\u540e\u7eed\u5c06\u4f1a\u6839\u636e\u5b9e\u9645\u60c5\u51b5\u8fdb\u884c\u4f18\u5316\u3002</p> <p>\u6ce8\u610f</p> <p>\u672c\u5e93\u76ee\u524d\u65e0\u6cd5\u4fdd\u8bc1 100% \u9002\u914d\u6240\u6709 OpenAI \u517c\u5bb9\u63a5\u53e3\uff08\u5c3d\u7ba1\u53ef\u4ee5\u4f7f\u7528\u517c\u5bb9\u6027\u914d\u7f6e\u6765\u63d0\u5347\u517c\u5bb9\u6027\uff09\u3002\u82e5\u6a21\u578b\u63d0\u4f9b\u5546\u5df2\u6709\u5b98\u65b9\u6216\u793e\u533a\u96c6\u6210\u7c7b\uff0c\u8bf7\u4f18\u5148\u91c7\u7528\u8be5\u96c6\u6210\u7c7b\u3002\u5982\u9047\u5230\u4efb\u4f55\u517c\u5bb9\u6027\u95ee\u9898\uff0c\u6b22\u8fce\u5728\u672c\u5e93 GitHub \u4ed3\u5e93\u63d0\u4ea4 issue\u3002</p> <p>\u6ce8\u610f</p> <p>\u8be5\u51fd\u6570\u5e95\u5c42\u4f7f\u7528 <code>pydantic.create_model</code> \u521b\u5efa\u5bf9\u8bdd\u6a21\u578b\u7c7b\uff0c\u4f1a\u5e26\u6765\u4e00\u5b9a\u7684\u6027\u80fd\u5f00\u9500\u3002\u6b64\u5916\uff0c<code>create_openai_compatible_model</code> \u4f7f\u7528\u5168\u5c40\u5b57\u5178\u8bb0\u5f55\u5404\u6a21\u578b\u63d0\u4f9b\u5546\u7684 <code>profiles</code>\uff0c\u4e3a\u4e86\u907f\u514d\u591a\u7ebf\u7a0b\u5e76\u53d1\u95ee\u9898\uff0c\u5efa\u8bae\u5728\u9879\u76ee\u542f\u52a8\u9636\u6bb5\u521b\u5efa\u597d\u96c6\u6210\u7c7b\uff0c\u540e\u7eed\u907f\u514d\u52a8\u6001\u521b\u5efa\u3002</p> <p>\u6700\u4f73\u5b9e\u8df5</p> <p>\u63a5\u5165 OpenAI \u517c\u5bb9 API \u7684\u5bf9\u8bdd\u6a21\u578b\u63d0\u4f9b\u5546\u65f6\uff0c\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 <code>langchain-openai</code> \u7684 <code>ChatOpenAI</code>\uff0c\u5e76\u901a\u8fc7 <code>base_url</code> \u4e0e <code>api_key</code> \u6307\u5411\u4f60\u7684\u63d0\u4f9b\u5546\u670d\u52a1\u3002\u8be5\u65b9\u5f0f\u8db3\u591f\u7b80\u5355\uff0c\u9002\u7528\u4e8e\u6bd4\u8f83\u7b80\u5355\u7684\u573a\u666f\uff08\u5c24\u5176\u662f\u4f7f\u7528\u7684\u662f\u666e\u901a\u7684\u5bf9\u8bdd\u6a21\u578b\u800c\u4e0d\u662f\u63a8\u7406\u6a21\u578b\uff09\u3002</p> <p>\u4f46\u662f\u4f1a\u5b58\u5728\u4ee5\u4e0b\u95ee\u9898\uff1a</p> <ol> <li> <p>\u65e0\u6cd5\u663e\u793a\u975e OpenAI \u5b98\u65b9\u63a8\u7406\u6a21\u578b\u7684\u601d\u7ef4\u94fe\uff08\u5373<code>reasoning_content</code>\u8fd4\u56de\u7684\u5185\u5bb9\uff09</p> </li> <li> <p>\u4e0d\u652f\u6301 <code>video</code> \u7c7b\u578b\u7684 content_block</p> </li> <li> <p>\u7ed3\u6784\u5316\u8f93\u51fa\u7684\u9ed8\u8ba4\u7b56\u7565\u8986\u76d6\u7387\u8f83\u4f4e</p> </li> </ol> <p>\u5f53\u4f60\u9047\u5230\u4e0a\u8ff0\u5dee\u5f02\u65f6\uff0c\u53ef\u4ee5\u4f7f\u7528\u672c\u5e93\u63d0\u4f9b\u7684 OpenAI \u517c\u5bb9\u96c6\u6210\u7c7b\u8fdb\u884c\u9002\u914d\u3002</p>"},{"location":"zh/adavance-guide/openai-compatible/embedding/","title":"\u5d4c\u5165\u6a21\u578b\u7684\u521b\u5efa\u4e0e\u4f7f\u7528","text":""},{"location":"zh/adavance-guide/openai-compatible/embedding/#_2","title":"\u521b\u5efa\u5d4c\u5165\u6a21\u578b\u7c7b","text":"<p>\u4e0e\u5bf9\u8bdd\u6a21\u578b\u7c7b\u7c7b\u4f3c\uff0c\u53ef\u4ee5\u4f7f\u7528 <code>create_openai_compatible_embedding</code> \u521b\u5efa\u5d4c\u5165\u6a21\u578b\u96c6\u6210\u7c7b\u3002\u8be5\u51fd\u6570\u63a5\u53d7\u4ee5\u4e0b\u53c2\u6570\uff1a</p> \u53c2\u6570 \u8bf4\u660e <code>embedding_provider</code> \u5d4c\u5165\u6a21\u578b\u63d0\u4f9b\u5546\u540d\u79f0\uff0c\u4f8b\u5982 <code>vllm</code>\u3002\u5fc5\u987b\u4ee5\u5b57\u6bcd\u6216\u6570\u5b57\u5f00\u5934\uff0c\u53ea\u80fd\u5305\u542b\u5b57\u6bcd\u3001\u6570\u5b57\u548c\u4e0b\u5212\u7ebf\uff0c\u957f\u5ea6\u4e0d\u8d85\u8fc7 20 \u4e2a\u5b57\u7b26\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u662f <code>base_url</code> \u6a21\u578b\u63d0\u4f9b\u5546\u9ed8\u8ba4 API \u5730\u5740\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u5426 <code>embedding_model_cls_name</code> \u5d4c\u5165\u6a21\u578b\u7c7b\u540d\uff08\u9700\u7b26\u5408 Python \u7c7b\u540d\u89c4\u8303\uff09\u3002\u9ed8\u8ba4\u503c\u4e3a <code>{Provider}Embeddings</code>\uff08\u5176\u4e2d <code>{Provider}</code> \u4e3a\u9996\u5b57\u6bcd\u5927\u5199\u7684\u63d0\u4f9b\u5546\u540d\u79f0\uff09\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u5426 <p>\u540c\u6837\uff0c\u6211\u4eec\u4f7f\u7528 <code>create_openai_compatible_embedding</code> \u6765\u96c6\u6210 vLLM \u7684\u5d4c\u5165\u6a21\u578b\u3002</p> <pre><code>from langchain_dev_utils.embeddings.adapters import create_openai_compatible_embedding\n\nVLLMEmbeddings = create_openai_compatible_embedding(\n    embedding_provider=\"vllm\",\n    base_url=\"http://localhost:8000/v1\",\n    embedding_model_cls_name=\"VLLMEmbeddings\",\n)\n\nembedding = VLLMEmbeddings(model=\"qwen3-embedding-4b\")\nprint(embedding.embed_query(\"\u4f60\u597d\"))\n</code></pre> <p><code>base_url</code> \u4e5f\u53ef\u4ee5\u7701\u7565\u3002\u672a\u4f20\u5165\u65f6\uff0c\u672c\u5e93\u4f1a\u9ed8\u8ba4\u8bfb\u53d6\u73af\u5883\u53d8\u91cf <code>VLLM_API_BASE</code>\uff1a</p> <pre><code>export VLLM_API_BASE=\"http://localhost:8000/v1\"\n</code></pre> <p>\u6b64\u65f6\u4ee3\u7801\u53ef\u4ee5\u7701\u7565 <code>base_url</code>\uff1a</p> <pre><code>from langchain_dev_utils.embeddings.adapters import create_openai_compatible_embedding\n\nVLLMEmbeddings = create_openai_compatible_embedding(\n    embedding_provider=\"vllm\",\n    embedding_model_cls_name=\"VLLMEmbeddings\",\n)\n\nembedding = VLLMEmbeddings(model=\"qwen3-embedding-4b\")\nprint(embedding.embed_query(\"\u4f60\u597d\"))\n</code></pre> <p>\u6ce8\u610f\uff1a\u4e0a\u8ff0\u4ee3\u7801\u6210\u529f\u8fd0\u884c\u7684\u524d\u63d0\u662f\u5df2\u914d\u7f6e\u73af\u5883\u53d8\u91cf <code>VLLM_API_KEY</code>\u3002\u867d\u7136 vLLM \u672c\u8eab\u4e0d\u8981\u6c42 API Key\uff0c\u4f46\u5d4c\u5165\u6a21\u578b\u7c7b\u521d\u59cb\u5316\u65f6\u9700\u8981\u4f20\u5165\uff0c\u56e0\u6b64\u8bf7\u5148\u8bbe\u7f6e\u8be5\u53d8\u91cf\uff0c\u4f8b\u5982\uff1a</p> <pre><code>export VLLM_API_KEY=vllm_api_key\n</code></pre>"},{"location":"zh/adavance-guide/openai-compatible/embedding/#_3","title":"\u4f7f\u7528\u5d4c\u5165\u6a21\u578b\u7c7b","text":"<p>\u8fd9\u91cc\u4f7f\u7528\u524d\u9762\u521b\u5efa\u597d\u7684 <code>VLLMEmbeddings</code> \u7c7b\u6765\u521d\u59cb\u5316\u5d4c\u5165\u6a21\u578b\u5b9e\u4f8b\u3002</p>"},{"location":"zh/adavance-guide/openai-compatible/embedding/#_4","title":"\u5411\u91cf\u5316\u67e5\u8be2","text":"<pre><code>embedding = VLLMEmbeddings(model=\"qwen3-embedding-4b\")\nprint(embedding.embed_query(\"\u4f60\u597d\"))\n</code></pre> <p>\u540c\u6837\uff0c\u4e5f\u652f\u6301\u5f02\u6b65\u8c03\u7528\uff1a</p> <pre><code>embedding = VLLMEmbeddings(model=\"qwen3-embedding-4b\")\nres = await embedding.aembed_query(\"\u4f60\u597d\")\nprint(res)\n</code></pre>"},{"location":"zh/adavance-guide/openai-compatible/embedding/#_5","title":"\u5411\u91cf\u5316\u5b57\u7b26\u4e32\u5217\u8868","text":"<p><pre><code>documents = [\"\u4f60\u597d\", \"\u4f60\u597d\uff0c\u6211\u662f\u5f20\u4e09\"]\nembedding = VLLMEmbeddings(model=\"qwen3-embedding-4b\")\nprint(embedding.embed_documents(documents))\n</code></pre> \u540c\u6837\uff0c\u4e5f\u652f\u6301\u5f02\u6b65\u8c03\u7528\uff1a</p> <pre><code>documents = [\"\u4f60\u597d\", \"\u4f60\u597d\uff0c\u6211\u662f\u5f20\u4e09\"]\nembedding = VLLMEmbeddings(model=\"qwen3-embedding-4b\")\nres = await embedding.aembed_documents(documents)\nprint(res)\n</code></pre> <p>\u5d4c\u5165\u6a21\u578b\u517c\u5bb9\u6027\u8bf4\u660e</p> <p>\u517c\u5bb9 OpenAI \u7684\u5d4c\u5165 API \u901a\u5e38\u8868\u73b0\u51fa\u8f83\u597d\u7684\u517c\u5bb9\u6027\uff0c\u4f46\u4ecd\u9700\u6ce8\u610f\u4ee5\u4e0b\u5dee\u5f02\u70b9\uff1a</p> <ol> <li> <p><code>check_embedding_ctx_length</code>\uff1a\u4ec5\u5728\u4f7f\u7528\u5b98\u65b9 OpenAI \u5d4c\u5165\u670d\u52a1\u65f6\u8bbe\u4e3a <code>True</code>\uff1b\u5176\u4f59\u5d4c\u5165\u6a21\u578b\u4e00\u5f8b\u8bbe\u4e3a <code>False</code>\u3002</p> </li> <li> <p><code>dimensions</code>\uff1a\u82e5\u6a21\u578b\u652f\u6301\u81ea\u5b9a\u4e49\u5411\u91cf\u7ef4\u5ea6\uff08\u5982 1024\u30014096\uff09\uff0c\u53ef\u76f4\u63a5\u4f20\u5165\u8be5\u53c2\u6570\u3002</p> </li> <li> <p><code>chunk_size</code>\uff1a\u4e3a\u5355\u6b21API\u8c03\u7528\u4e2d\u80fd\u5904\u7406\u7684\u6587\u672c\u6570\u91cf\u4e0a\u9650\u3002\u4f8b\u5982<code>chunk_size</code>\u5927\u5c0f\u4e3a10\uff0c\u610f\u5473\u7740\u4e00\u6b21\u8bf7\u6c42\u6700\u591a\u53ef\u4f20\u516510\u4e2a\u6587\u672c\u8fdb\u884c\u5411\u91cf\u5316\u3002</p> </li> <li> <p>\u5355\u6587\u672c token \u4e0a\u9650\uff1a\u65e0\u6cd5\u901a\u8fc7\u53c2\u6570\u63a7\u5236\uff0c\u9700\u5728\u9884\u5904\u7406\u5206\u5757\u9636\u6bb5\u81ea\u884c\u4fdd\u8bc1\u3002</p> </li> </ol> <p>\u6ce8\u610f</p> <p>\u540c\u6837\uff0c\u8be5\u51fd\u6570\u5e95\u5c42\u4f7f\u7528 <code>pydantic.create_model</code> \u521b\u5efa\u5d4c\u5165\u6a21\u578b\u7c7b\uff0c\u4f1a\u5e26\u6765\u4e00\u5b9a\u7684\u6027\u80fd\u5f00\u9500\u3002\u5efa\u8bae\u5728\u9879\u76ee\u542f\u52a8\u9636\u6bb5\u521b\u5efa\u597d\u96c6\u6210\u7c7b\uff0c\u540e\u7eed\u907f\u514d\u52a8\u6001\u521b\u5efa\u3002</p> <p>\u6700\u4f73\u5b9e\u8df5</p> <p>\u63a5\u5165 OpenAI \u517c\u5bb9 API \u7684\u5d4c\u5165\u6a21\u578b\u63d0\u4f9b\u5546\u65f6\uff0c\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 <code>langchain-openai</code> \u7684 <code>OpenAIEmbeddings</code>\uff0c\u5e76\u901a\u8fc7 <code>base_url</code> \u4e0e <code>api_key</code> \u6307\u5411\u4f60\u7684\u63d0\u4f9b\u5546\u670d\u52a1\uff0c\u5d4c\u5165\u6a21\u578bAPI\u7684\u517c\u5bb9\u6027\u901a\u5e38\u66f4\u597d\uff1a\u591a\u6570\u60c5\u51b5\u4e0b\u76f4\u63a5\u4f7f\u7528 <code>OpenAIEmbeddings</code> \u5e76\u5c06 <code>check_embedding_ctx_length=False</code> \u5373\u53ef\u3002</p>"},{"location":"zh/adavance-guide/openai-compatible/overview/","title":"\u6982\u8ff0","text":"<p>\u524d\u63d0\u6761\u4ef6</p> <p>\u4f7f\u7528\u6b64\u529f\u80fd\u65f6\uff0c\u5fc5\u987b\u5b89\u88c5 standard \u7248\u672c\u7684 <code>langchain-dev-utils</code> \u5e93\u3002\u5177\u4f53\u53ef\u4ee5\u53c2\u8003\u5b89\u88c5\u90e8\u5206\u7684\u4ecb\u7ecd\u3002</p> <p>\u8bb8\u591a\u6a21\u578b\u63d0\u4f9b\u5546\u90fd\u63d0\u4f9b OpenAI \u517c\u5bb9 API \u670d\u52a1\uff0c\u4f8b\u5982 vLLM\u3001OpenRouter \u548c Together AI \u7b49\u3002\u672c\u5e93\u63d0\u4f9b\u4e00\u5957 OpenAI \u517c\u5bb9 API \u96c6\u6210\u65b9\u6848\uff0c\u8986\u76d6\u5bf9\u8bdd\u6a21\u578b\u4e0e\u5d4c\u5165\u6a21\u578b\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u300c\u63d0\u4f9b\u5546\u5df2\u63d0\u4f9b OpenAI \u517c\u5bb9 API\uff0c\u4f46\u5c1a\u65e0\u5bf9\u5e94 LangChain \u96c6\u6210\u300d\u7684\u573a\u666f\u3002</p> <p>\u672c\u5e93\u63d0\u4f9b\u4e86\u4e24\u4e2a\u5de5\u5177\u51fd\u6570\uff0c\u7528\u4e8e\u521b\u5efa\u5bf9\u8bdd\u6a21\u578b\u96c6\u6210\u7c7b\u4e0e\u5d4c\u5165\u6a21\u578b\u96c6\u6210\u7c7b\uff1a</p> \u51fd\u6570\u540d \u8bf4\u660e <code>create_openai_compatible_model</code> \u521b\u5efa\u5bf9\u8bdd\u6a21\u578b\u96c6\u6210\u7c7b <code>create_openai_compatible_embedding</code> \u521b\u5efa\u5d4c\u5165\u6a21\u578b\u96c6\u6210\u7c7b <p>\u8bf4\u660e</p> <p>\u672c\u5e93\u63d0\u4f9b\u7684\u4e24\u4e2a\u5de5\u5177\u51fd\u6570\u7684\u6700\u521d\u7075\u611f\u501f\u9274\u81ea JavaScript \u751f\u6001\u7684 @ai-sdk/openai-compatible\u3002</p> <p>\u672c\u6587\u6863\u5c06\u4ee5\u63a5\u5165 vLLM \u4e3a\u4f8b\uff0c\u5c55\u793a\u5982\u4f55\u4f7f\u7528\u672c\u529f\u80fd\u3002</p> vLLM \u4ecb\u7ecd <p>vLLM \u662f\u5e38\u7528\u7684\u5927\u6a21\u578b\u63a8\u7406\u6846\u67b6\uff0c\u9002\u5408\u672c\u5730\u6216\u81ea\u5efa\u73af\u5883\u4e0b\u7684\u9ad8\u6027\u80fd\u63a8\u7406\u670d\u52a1\u3002\u5b83\u53ef\u4ee5\u5c06\u5927\u6a21\u578b\u90e8\u7f72\u4e3a OpenAI \u517c\u5bb9\u7684 API\uff0c\u4fbf\u4e8e\u590d\u7528\u73b0\u6709\u7684 SDK \u4e0e\u8c03\u7528\u65b9\u5f0f\uff1b\u540c\u65f6\u652f\u6301\u5bf9\u8bdd\u6a21\u578b\u4e0e\u5d4c\u5165\u6a21\u578b\u7684\u90e8\u7f72\uff0c\u4ee5\u53ca\u591a\u6a21\u578b\u670d\u52a1\u3001\u5de5\u5177\u8c03\u7528\u4e0e\u63a8\u7406\u8f93\u51fa\u7b49\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u5bf9\u8bdd\u3001\u5de5\u5177\u8c03\u7528\u4e0e\u591a\u6a21\u6001\u7b49\u573a\u666f\u3002</p> <p>\u4ee5\u4e0b\u793a\u4f8b\u5747\u4e3a\u540e\u7eed\u5185\u5bb9\u4e2d\u4f1a\u7528\u5230\u7684\u6a21\u578b\u90e8\u7f72\u547d\u4ee4\uff1a</p> <p>Qwen3-4B\uff1a</p> <pre><code>vllm serve Qwen/Qwen3-4B \\\n--reasoning-parser qwen3 \\\n--enable-auto-tool-choice --tool-call-parser hermes \\\n--host 0.0.0.0 --port 8000 \\\n--served-model-name qwen3-4b\n</code></pre> <p>GLM-4.7-Flash\uff1a</p> <pre><code>vllm serve zai-org/GLM-4.7-Flash \\\n --tensor-parallel-size 4 \\\n --speculative-config.method mtp \\\n --speculative-config.num_speculative_tokens 1 \\\n --tool-call-parser glm47 \\\n --reasoning-parser glm45 \\\n --enable-auto-tool-choice \\\n --served-model-name glm-4.7-flash\n</code></pre> <p>Qwen3-VL-2B-Instruct\uff1a</p> <pre><code>vllm serve Qwen/Qwen3-VL-2B-Instruct \\\n--trust-remote-code \\\n--host 0.0.0.0 --port 8000 \\\n--served-model-name qwen3-vl-2b\n</code></pre> <p>Qwen3-Embedding-4B\uff1a</p> <p><pre><code>vllm serve Qwen/Qwen3-Embedding-4B \\\n--task embed \\\n--served-model-name qwen3-embedding-4b \\\n--host 0.0.0.0 --port 8000\n</code></pre> \u670d\u52a1\u5730\u5740\u4e3a <code>http://localhost:8000/v1</code>\u3002</p>"},{"location":"zh/adavance-guide/openai-compatible/register/","title":"\u4e0e\u6a21\u578b\u7ba1\u7406\u529f\u80fd\u96c6\u6210","text":"<p>\u672c\u5e93\u5df2\u5c06\u6b64\u529f\u80fd\u65e0\u7f1d\u63a5\u5165\u6a21\u578b\u7ba1\u7406\u529f\u80fd\u3002\u6ce8\u518c\u5bf9\u8bdd\u6a21\u578b\u65f6\uff0c\u53ea\u9700\u5c06 <code>chat_model</code> \u8bbe\u4e3a <code>\"openai-compatible\"</code>\uff1b\u6ce8\u518c\u5d4c\u5165\u6a21\u578b\u65f6\uff0c\u5c06 <code>embeddings_model</code> \u8bbe\u4e3a <code>\"openai-compatible\"</code> \u5373\u53ef\u3002</p>"},{"location":"zh/adavance-guide/openai-compatible/register/#_2","title":"\u5bf9\u8bdd\u6a21\u578b\u7c7b\u6ce8\u518c","text":"<p>\u5177\u4f53\u4ee3\u7801\u5982\u4e0b\uff1a</p> <p>\u65b9\u5f0f\u4e00\uff1a\u663e\u5f0f\u4f20\u53c2</p> <pre><code>from langchain_dev_utils.chat_models import register_model_provider\n\nregister_model_provider(\n    provider_name=\"vllm\",\n    chat_model=\"openai-compatible\",\n    base_url=\"http://localhost:8000/v1\"\n)\n</code></pre> <p>\u65b9\u5f0f\u4e8c\uff1a\u901a\u8fc7\u73af\u5883\u53d8\u91cf\uff08\u63a8\u8350\u7528\u4e8e\u914d\u7f6e\u7ba1\u7406\uff09</p> <pre><code>from langchain_dev_utils.chat_models import register_model_provider\n\nregister_model_provider(\n    provider_name=\"vllm\",\n    chat_model=\"openai-compatible\"\n    # \u81ea\u52a8\u8bfb\u53d6 VLLM_API_BASE\n)\n</code></pre> <p>\u540c\u65f6\uff0c<code>create_openai_compatible_model</code>\u51fd\u6570\u4e2d\u7684<code>base_url</code>\u3001<code>compatibility_options</code>\u3001<code>model_profiles</code>\u53c2\u6570\u4e5f\u652f\u6301\u4f20\u5165\u3002\u53ea\u9700\u8981\u5728<code>register_model_provider</code>\u51fd\u6570\u4e2d\u4f20\u5165\u5bf9\u5e94\u7684\u53c2\u6570\u5373\u53ef\u3002</p> <p>\u4f8b\u5982\uff1a</p> <pre><code>from langchain_dev_utils.chat_models import register_model_provider\n\nregister_model_provider(\n    provider_name=\"vllm\",\n    chat_model=\"openai-compatible\",\n    base_url=\"http://localhost:8000/v1\",\n    compatibility_options={\n        \"supported_tool_choice\": [\"auto\", \"none\", \"required\", \"specific\"],\n        \"supported_response_format\": [\"json_schema\"]\n    },\n    model_profiles=model_profiles,\n)\n</code></pre>"},{"location":"zh/adavance-guide/openai-compatible/register/#_3","title":"\u5d4c\u5165\u6a21\u578b\u7c7b\u6ce8\u518c","text":"<p>\u4e0e\u5bf9\u8bdd\u6a21\u578b\u7c7b\u6ce8\u518c\u7c7b\u4f3c\uff1a</p> <p>\u65b9\u5f0f\u4e00\uff1a\u663e\u5f0f\u4f20\u53c2</p> <pre><code>from langchain_dev_utils.embeddings import register_embeddings_provider\n\nregister_embeddings_provider(\n    provider_name=\"vllm\",\n    embeddings_model=\"openai-compatible\",\n    base_url=\"http://localhost:8000/v1\",\n)\n</code></pre> <p>\u65b9\u5f0f\u4e8c\uff1a\u73af\u5883\u53d8\u91cf\uff08\u63a8\u8350\uff09</p> <pre><code>export VLLM_API_BASE=http://localhost:8000/v1\n</code></pre> <pre><code>from langchain_dev_utils.embeddings import register_embeddings_provider\n\nregister_embeddings_provider(\n    provider_name=\"vllm\",\n    embeddings_model=\"openai-compatible\"\n)\n</code></pre>"},{"location":"zh/api-reference/agent/","title":"Agent \u6a21\u5757 API \u53c2\u8003\u6587\u6863","text":""},{"location":"zh/api-reference/agent/#create_agent","title":"create_agent","text":"<p>\u521b\u5efa\u4e00\u4e2a\u667a\u80fd\u4f53\uff0c\u63d0\u4f9b\u4e0e langchain \u5b98\u65b9 <code>create_agent</code> \u5b8c\u5168\u76f8\u540c\u7684\u529f\u80fd\uff0c\u4f46\u62d3\u5c55\u4e86\u5b57\u7b26\u4e32\u6307\u5b9a\u6a21\u578b\u3002</p>"},{"location":"zh/api-reference/agent/#_1","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def create_agent(  # noqa: PLR0915\n    model: str,\n    tools: Sequence[BaseTool | Callable | dict[str, Any]] | None = None,\n    *,\n    system_prompt: str | SystemMessage | None = None,\n    response_format: ResponseFormat[ResponseT] | type[ResponseT] | None = None,\n    middleware: Sequence[AgentMiddleware[StateT_co, ContextT]] = (),\n    state_schema: type[AgentState[ResponseT]] | None = None,\n    context_schema: type[ContextT] | None = None,\n    checkpointer: Checkpointer | None = None,\n    store: BaseStore | None = None,\n    interrupt_before: list[str] | None = None,\n    interrupt_after: list[str] | None = None,\n    debug: bool = False,\n    name: str | None = None,\n    cache: BaseCache | None = None,\n) -&gt; CompiledStateGraph[\n    AgentState[ResponseT], ContextT, _InputAgentState, _OutputAgentState[ResponseT]\n]:\n</code></pre>"},{"location":"zh/api-reference/agent/#_2","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 model str \u662f - \u53ef\u7531 <code>load_chat_model</code> \u52a0\u8f7d\u7684\u6a21\u578b\u6807\u8bc6\u7b26\u5b57\u7b26\u4e32\u3002\u53ef\u6307\u5b9a\u4e3a \"provider:model-name\" \u683c\u5f0f tools Sequence[BaseTool | Callable | dict[str, Any]] | None \u5426 None \u667a\u80fd\u4f53\u53ef\u7528\u7684\u5de5\u5177\u5217\u8868 system_prompt str | SystemMessage | None \u5426 None \u667a\u80fd\u4f53\u7684\u81ea\u5b9a\u4e49\u7cfb\u7edf\u63d0\u793a\u8bcd middleware Sequence[AgentMiddleware[AgentState[ResponseT], ContextT]] \u5426 () \u667a\u80fd\u4f53\u7684\u4e2d\u95f4\u4ef6 response_format ResponseFormat[ResponseT] | type[ResponseT] | None \u5426 None \u667a\u80fd\u4f53\u7684\u54cd\u5e94\u683c\u5f0f state_schema type[AgentState[ResponseT]] | None \u5426 None \u667a\u80fd\u4f53\u7684\u72b6\u6001\u6a21\u5f0f context_schema type[ContextT] | None \u5426 None \u667a\u80fd\u4f53\u7684\u4e0a\u4e0b\u6587\u6a21\u5f0f checkpointer Checkpointer | None \u5426 None \u72b6\u6001\u6301\u4e45\u5316\u7684\u68c0\u67e5\u70b9 store BaseStore | None \u5426 None \u6570\u636e\u6301\u4e45\u5316\u7684\u5b58\u50a8 interrupt_before list[str] | None \u5426 None \u6267\u884c\u524d\u8981\u4e2d\u65ad\u7684\u8282\u70b9 interrupt_after list[str] | None \u5426 None \u6267\u884c\u540e\u8981\u4e2d\u65ad\u7684\u8282\u70b9 debug bool \u5426 False \u542f\u7528\u8c03\u8bd5\u6a21\u5f0f name str | None \u5426 None \u667a\u80fd\u4f53\u540d\u79f0 cache BaseCache | None \u5426 None \u7f13\u5b58"},{"location":"zh/api-reference/agent/#_3","title":"\u6ce8\u610f\u4e8b\u9879","text":"<p>\u6b64\u51fd\u6570\u63d0\u4f9b\u4e0e <code>langchain</code> \u5b98\u65b9 <code>create_agent</code> \u5b8c\u5168\u76f8\u540c\u7684\u529f\u80fd\uff0c\u4f46\u62d3\u5c55\u4e86\u6a21\u578b\u9009\u62e9\u3002\u4e3b\u8981\u533a\u522b\u5728\u4e8e <code>model</code> \u53c2\u6570\u5fc5\u987b\u662f\u53ef\u7531 <code>load_chat_model</code> \u51fd\u6570\u52a0\u8f7d\u7684\u5b57\u7b26\u4e32\uff0c\u5141\u8bb8\u4f7f\u7528\u6ce8\u518c\u7684\u6a21\u578b\u63d0\u4f9b\u8005\u8fdb\u884c\u66f4\u7075\u6d3b\u7684\u6a21\u578b\u9009\u62e9\u3002</p>"},{"location":"zh/api-reference/agent/#_4","title":"\u793a\u4f8b","text":"<pre><code>agent = create_agent(model=\"vllm:qwen3-4b\", tools=[get_current_time])\n</code></pre>"},{"location":"zh/api-reference/agent/#wrap_agent_as_tool","title":"wrap_agent_as_tool","text":"<p>\u5c06\u667a\u80fd\u4f53\u5305\u88c5\u4e3a\u5de5\u5177\u3002</p>"},{"location":"zh/api-reference/agent/#_5","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def wrap_agent_as_tool(\n    agent: CompiledStateGraph,\n    tool_name: Optional[str] = None,\n    tool_description: Optional[str] = None,\n    pre_input_hooks: Optional[\n        tuple[\n            Callable[[str, ToolRuntime], str | dict[str, Any]],\n            Callable[[str, ToolRuntime], Awaitable[str | dict[str, Any]]],\n        ]\n        | Callable[[str, ToolRuntime], str | dict[str, Any]]\n    ] = None,\n    post_output_hooks: Optional[\n        tuple[\n            Callable[[str, dict[str, Any], ToolRuntime], Any],\n            Callable[[str, dict[str, Any], ToolRuntime], Awaitable[Any]],\n        ]\n        | Callable[[str, dict[str, Any], ToolRuntime], Any]\n    ] = None,\n) -&gt; BaseTool:\n</code></pre>"},{"location":"zh/api-reference/agent/#_6","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 agent CompiledStateGraph \u662f - \u667a\u80fd\u4f53 tool_name Optional[str] \u5426 None \u5de5\u5177\u540d\u79f0 tool_description Optional[str] \u5426 None \u5de5\u5177\u63cf\u8ff0 pre_input_hooks - \u5426 None Agent \u8f93\u5165\u9884\u5904\u7406\u51fd\u6570 post_output_hooks - \u5426 None Agent \u8f93\u51fa\u540e\u5904\u7406\u51fd\u6570"},{"location":"zh/api-reference/agent/#_7","title":"\u793a\u4f8b","text":"<pre><code>tool = wrap_agent_as_tool(agent)\n</code></pre>"},{"location":"zh/api-reference/agent/#wrap_all_agents_as_tool","title":"wrap_all_agents_as_tool","text":"<p>\u5c06\u6240\u6709\u667a\u80fd\u4f53\u5305\u88c5\u4e3a\u5355\u4e2a\u5de5\u5177\u3002</p>"},{"location":"zh/api-reference/agent/#_8","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def wrap_all_agents_as_tool(\n    agents: list[CompiledStateGraph],\n    tool_name: Optional[str] = None,\n    tool_description: Optional[str] = None,\n    pre_input_hooks: Optional[\n        tuple[\n            Callable[[str, ToolRuntime], str | dict[str, Any]],\n            Callable[[str, ToolRuntime], Awaitable[str | dict[str, Any]]],\n        ]\n        | Callable[[str, ToolRuntime], str | dict[str, Any]]\n    ] = None,\n    post_output_hooks: Optional[\n        tuple[\n            Callable[[str, dict[str, Any], ToolRuntime], Any],\n            Callable[[str, dict[str, Any], ToolRuntime], Awaitable[Any]],\n        ]\n        | Callable[[str, dict[str, Any], ToolRuntime], Any]\n    ] = None,\n) -&gt; BaseTool:\n</code></pre>"},{"location":"zh/api-reference/agent/#_9","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 agents list[CompiledStateGraph] \u662f - \u667a\u80fd\u4f53\u5217\u8868(\u81f3\u5c11\u5305\u542b2\u4e2a\uff0c\u4e14\u6bcf\u4e2a\u667a\u80fd\u4f53\u5fc5\u987b\u6709\u552f\u4e00\u7684\u540d\u79f0) tool_name Optional[str] \u5426 None \u5de5\u5177\u540d\u79f0 tool_description Optional[str] \u5426 None \u5de5\u5177\u63cf\u8ff0 pre_input_hooks - \u5426 None Agent \u8f93\u5165\u9884\u5904\u7406\u51fd\u6570 post_output_hooks - \u5426 None Agent \u8f93\u51fa\u540e\u5904\u7406\u51fd\u6570"},{"location":"zh/api-reference/agent/#_10","title":"\u793a\u4f8b","text":"<pre><code>tool = wrap_all_agents_as_tool([time_agent, weather_agent])\n</code></pre>"},{"location":"zh/api-reference/agent/#summarizationmiddleware","title":"SummarizationMiddleware","text":"<p>\u7528\u4e8e\u667a\u80fd\u4f53\u4e0a\u4e0b\u6587\u6458\u8981\u7684\u4e2d\u95f4\u4ef6\u3002</p>"},{"location":"zh/api-reference/agent/#_11","title":"\u7c7b\u5b9a\u4e49","text":"<pre><code>class SummarizationMiddleware(_SummarizationMiddleware):\n    def __init__(\n        self,\n        model: str,\n        *,\n        trigger: ContextSize | list[ContextSize] | None = None,\n        keep: ContextSize = (\"messages\", _DEFAULT_MESSAGES_TO_KEEP),\n        token_counter: TokenCounter = count_tokens_approximately,\n        summary_prompt: str = DEFAULT_SUMMARY_PROMPT,\n        trim_tokens_to_summarize: int | None = _DEFAULT_TRIM_TOKEN_LIMIT,\n        **deprecated_kwargs: Any,\n    ) -&gt; None\n</code></pre>"},{"location":"zh/api-reference/agent/#_12","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 model str \u662f - \u53ef\u7531 <code>load_chat_model</code> \u52a0\u8f7d\u7684\u6a21\u578b\u6807\u8bc6\u7b26\u5b57\u7b26\u4e32\u3002\u53ef\u6307\u5b9a\u4e3a \"provider:model-name\" \u683c\u5f0f trigger ContextSize | list[ContextSize] | None \u5426 None \u89e6\u53d1\u6458\u8981\u7684\u4e0a\u4e0b\u6587\u5927\u5c0f keep ContextSize \u5426 (\"messages\", _DEFAULT_MESSAGES_TO_KEEP) \u4fdd\u7559\u7684\u4e0a\u4e0b\u6587\u5927\u5c0f token_counter TokenCounter \u5426 count_tokens_approximately token \u8ba1\u6570\u5668 summary_prompt str \u5426 DEFAULT_SUMMARY_PROMPT \u6458\u8981\u63d0\u793a\u8bcd trim_tokens_to_summarize int | None \u5426 _DEFAULT_TRIM_TOKEN_LIMIT \u6458\u8981\u524d\u8981\u622a\u53d6\u7684 token \u6570"},{"location":"zh/api-reference/agent/#_13","title":"\u793a\u4f8b","text":"<pre><code>summarization_middleware = SummarizationMiddleware(model=\"vllm:qwen3-4b\")\n</code></pre>"},{"location":"zh/api-reference/agent/#llmtoolselectormiddleware","title":"LLMToolSelectorMiddleware","text":"<p>\u7528\u4e8e\u667a\u80fd\u4f53\u5de5\u5177\u9009\u62e9\u7684\u4e2d\u95f4\u4ef6\u3002</p>"},{"location":"zh/api-reference/agent/#_14","title":"\u7c7b\u5b9a\u4e49","text":"<pre><code>class LLMToolSelectorMiddleware(_LLMToolSelectorMiddleware):\n    def __init__(\n        self,\n        *,\n        model: str,\n        system_prompt: Optional[str] = None,\n        max_tools: Optional[int] = None,\n        always_include: Optional[list[str]] = None,\n    ) -&gt; None\n</code></pre>"},{"location":"zh/api-reference/agent/#_15","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 model str \u662f - \u53ef\u7531 <code>load_chat_model</code> \u52a0\u8f7d\u7684\u6a21\u578b\u6807\u8bc6\u7b26\u5b57\u7b26\u4e32\u3002\u53ef\u6307\u5b9a\u4e3a \"provider:model-name\" \u683c\u5f0f system_prompt Optional[str] \u5426 None \u7cfb\u7edf\u63d0\u793a\u8bcd max_tools Optional[int] \u5426 None \u6700\u5927\u5de5\u5177\u6570 always_include Optional[list[str]] \u5426 None \u603b\u662f\u5305\u542b\u7684\u5de5\u5177"},{"location":"zh/api-reference/agent/#_16","title":"\u793a\u4f8b","text":"<pre><code>llm_tool_selector_middleware = LLMToolSelectorMiddleware(model=\"vllm:qwen3-4b\")\n</code></pre>"},{"location":"zh/api-reference/agent/#planmiddleware","title":"PlanMiddleware","text":"<p>\u7528\u4e8e\u667a\u80fd\u4f53\u8ba1\u5212\u7ba1\u7406\u7684\u4e2d\u95f4\u4ef6\u3002</p>"},{"location":"zh/api-reference/agent/#_17","title":"\u7c7b\u5b9a\u4e49","text":"<pre><code>class PlanMiddleware(AgentMiddleware):\n    state_schema = PlanState\n    def __init__(\n        self,\n        *,\n        system_prompt: Optional[str] = None,\n        custom_plan_tool_descriptions: Optional[PlanToolDescription] = None,\n        use_read_plan_tool: bool = True,\n    ) -&gt; None:\n</code></pre>"},{"location":"zh/api-reference/agent/#_18","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 system_prompt Optional[str] \u5426 None \u7cfb\u7edf\u63d0\u793a\u8bcd custom_plan_tool_descriptions Optional[PlanToolDescription] \u5426 None \u81ea\u5b9a\u4e49\u8ba1\u5212\u5de5\u5177\u7684\u63cf\u8ff0 use_read_plan_tool bool \u5426 True \u662f\u5426\u4f7f\u7528\u8bfb\u8ba1\u5212\u5de5\u5177"},{"location":"zh/api-reference/agent/#_19","title":"\u793a\u4f8b","text":"<pre><code>plan_middleware = PlanMiddleware()\n</code></pre>"},{"location":"zh/api-reference/agent/#modelfallbackmiddleware","title":"ModelFallbackMiddleware","text":"<p>\u7528\u4e8e\u667a\u80fd\u4f53\u6a21\u578b\u56de\u9000\u7684\u4e2d\u95f4\u4ef6\u3002</p>"},{"location":"zh/api-reference/agent/#_20","title":"\u7c7b\u5b9a\u4e49","text":"<pre><code>class ModelFallbackMiddleware(_ModelFallbackMiddleware):\n    def __init__(\n        self,\n        first_model: str,\n        *additional_models: str,\n    ) -&gt; None\n</code></pre>"},{"location":"zh/api-reference/agent/#_21","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 first_model str \u662f - \u53ef\u7531 <code>load_chat_model</code> \u52a0\u8f7d\u7684\u6a21\u578b\u6807\u8bc6\u7b26\u5b57\u7b26\u4e32\u3002\u53ef\u6307\u5b9a\u4e3a \"provider:model-name\" \u683c\u5f0f additional_models str \u5426 - \u5907\u7528\u6a21\u578b\u5217\u8868"},{"location":"zh/api-reference/agent/#_22","title":"\u793a\u4f8b","text":"<pre><code>model_fallback_middleware = ModelFallbackMiddleware(\n    \"vllm:qwen3-4b\",\n    \"vllm:qwen3-8b\"\n)\n</code></pre>"},{"location":"zh/api-reference/agent/#llmtoolemulator","title":"LLMToolEmulator","text":"<p>\u7528\u4e8e\u4f7f\u7528\u5927\u6a21\u578b\u6765\u6a21\u62df\u5de5\u5177\u8c03\u7528\u7684\u4e2d\u95f4\u4ef6\u3002</p>"},{"location":"zh/api-reference/agent/#_23","title":"\u7c7b\u5b9a\u4e49","text":"<pre><code>class LLMToolEmulator(_LLMToolEmulator):\n    def __init__(\n        self,\n        *,\n        model: str,\n        tools: list[str | BaseTool] | None = None,\n    ) -&gt; None\n</code></pre>"},{"location":"zh/api-reference/agent/#_24","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 model str \u662f - \u53ef\u7531 <code>load_chat_model</code> \u52a0\u8f7d\u7684\u6a21\u578b\u6807\u8bc6\u7b26\u5b57\u7b26\u4e32\u3002\u53ef\u6307\u5b9a\u4e3a \"provider:model-name\" \u683c\u5f0f tools list[str | BaseTool] | None \u5426 None \u5de5\u5177\u5217\u8868"},{"location":"zh/api-reference/agent/#_25","title":"\u793a\u4f8b","text":"<pre><code>llm_tool_emulator = LLMToolEmulator(model=\"vllm:qwen3-4b\", tools=[get_current_time])\n</code></pre>"},{"location":"zh/api-reference/agent/#modelroutermiddleware","title":"ModelRouterMiddleware","text":"<p>\u7528\u4e8e\u6839\u636e\u8f93\u5165\u5185\u5bb9\u52a8\u6001\u8def\u7531\u5230\u5408\u9002\u6a21\u578b\u7684\u4e2d\u95f4\u4ef6\u3002</p>"},{"location":"zh/api-reference/agent/#_26","title":"\u7c7b\u5b9a\u4e49","text":"<pre><code>class ModelRouterMiddleware(AgentMiddleware):\n    state_schema = ModelRouterState\n    def __init__(\n        self,\n        router_model: str | BaseChatModel,\n        model_list: list[ModelDict],\n        router_prompt: Optional[str] = None,\n    ) -&gt; None\n</code></pre>"},{"location":"zh/api-reference/agent/#_27","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 router_model str | BaseChatModel \u662f - \u7528\u4e8e\u8def\u7531\u7684\u6a21\u578b\uff0c\u63a5\u6536\u5b57\u7b26\u4e32\u7c7b\u578b\uff08\u4f7f\u7528<code>load_chat_model</code>\u52a0\u8f7d\uff09\u6216\u8005\u76f4\u63a5\u4f20\u5165 ChatModel model_list list[ModelDict] \u662f - \u6a21\u578b\u5217\u8868\uff0c\u6bcf\u4e2a\u6a21\u578b\u9700\u8981\u5305\u542b <code>model_name</code> \u548c <code>model_description</code> \u4e24\u4e2a\u952e\uff0c\u540c\u65f6\u4e5f\u53ef\u4ee5\u9009\u62e9\u6027\u5730\u5305\u542b <code>tools</code>\u3001<code>model_kwargs</code>\u3001<code>model_instance</code>\u3001<code>model_system_prompt</code> \u8fd9\u56db\u4e2a\u952e router_prompt Optional[str] \u5426 None \u8def\u7531\u6a21\u578b\u7684\u63d0\u793a\u8bcd\uff0c\u5982\u679c\u4e3a None \u5219\u4f7f\u7528\u9ed8\u8ba4\u7684\u63d0\u793a\u8bcd"},{"location":"zh/api-reference/agent/#_28","title":"\u793a\u4f8b","text":"<pre><code>model_router_middleware = ModelRouterMiddleware(\n    router_model=\"vllm:qwen3-4b\",\n    model_list=[\n        {\n            \"model_name\": \"vllm:qwen3-4b\",\n            \"model_description\": \"\u9002\u5408\u666e\u901a\u4efb\u52a1\uff0c\u5982\u5bf9\u8bdd\u3001\u6587\u672c\u751f\u6210\u7b49\"\n        },\n        {\n            \"model_name\": \"vllm:qwen3-8b\",\n            \"model_description\": \"\u9002\u5408\u590d\u6742\u4efb\u52a1\uff0c\u5982\u4ee3\u7801\u751f\u6210\u3001\u6570\u636e\u5206\u6790\u7b49\",\n        },\n    ]\n)\n</code></pre>"},{"location":"zh/api-reference/agent/#handoffagentmiddleware","title":"HandoffAgentMiddleware","text":"<p>\u7528\u4e8e\u5b9e\u73b0\u591a\u667a\u80fd\u4f53\u5207\u6362\uff08handoffs\uff09\u7684\u4e2d\u95f4\u4ef6\u3002</p>"},{"location":"zh/api-reference/agent/#_29","title":"\u7c7b\u5b9a\u4e49","text":"<pre><code>class HandoffAgentMiddleware(AgentMiddleware):\n    state_schema = MultiAgentState\n    def __init__(\n        self,\n        agents_config: dict[str, AgentConfig],\n        custom_handoffs_tool_descriptions: Optional[dict[str, str]] = None,\n        handoffs_tool_overrides: Optional[dict[str, BaseTool]] = None,\n    ) -&gt; None:\n</code></pre>"},{"location":"zh/api-reference/agent/#_30","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 agents_config dict[str, AgentConfig] \u662f - \u667a\u80fd\u4f53\u914d\u7f6e\u5b57\u5178\uff0c\u952e\u4e3a\u667a\u80fd\u4f53\u540d\u79f0\uff0c\u503c\u4e3a\u667a\u80fd\u4f53\u914d\u7f6e custom_handoffs_tool_descriptions Optional[dict[str, str]] \u5426 None \u81ea\u5b9a\u4e49\u4ea4\u63a5\u5230\u5176\u5b83\u667a\u80fd\u4f53\u7684\u5de5\u5177\u63cf\u8ff0 handoffs_tool_overrides Optional[dict[str, BaseTool]] \u5426 None \u81ea\u5b9a\u4e49\u4ea4\u63a5\u5230\u5176\u5b83\u667a\u80fd\u4f53\u7684\u5de5\u5177"},{"location":"zh/api-reference/agent/#_31","title":"\u793a\u4f8b","text":"<pre><code>handoffs_agent_middleware = HandoffsAgentMiddleware({\n    \"time_agent\":{\n        \"model\":\"vllm:qwen3-4b\",\n        \"prompt\":\"\u4f60\u662f\u4e00\u4e2a\u65f6\u95f4\u667a\u80fd\u4f53\uff0c\u8d1f\u8d23\u56de\u7b54\u65f6\u95f4\u76f8\u5173\u7684\u95ee\u9898\u3002\",\n        \"tools\":[get_current_time, transfer_to_default_agent],\n        \"handoffs\":[\"default_agent\"]\n    },\n    \"default_agent\":{\n        \"model\":\"vllm:qwen3-8b\",\n        \"prompt\":\"\u4f60\u662f\u4e00\u4e2a\u590d\u6742\u4efb\u52a1\u667a\u80fd\u4f53\uff0c\u8d1f\u8d23\u56de\u7b54\u590d\u6742\u4efb\u52a1\u76f8\u5173\u7684\u95ee\u9898\u3002\",\n        \"default\":True,\n        \"handoffs\":[\"time_agent\"]\n    }\n})\n</code></pre>"},{"location":"zh/api-reference/agent/#toolcallrepairmiddleware","title":"ToolCallRepairMiddleware","text":"<p>\u7528\u4e8e\u4fee\u590d\u65e0\u6548\u5de5\u5177\u8c03\u7528\u7684\u4e2d\u95f4\u4ef6\u3002</p>"},{"location":"zh/api-reference/agent/#_32","title":"\u7c7b\u5b9a\u4e49","text":"<pre><code>class ToolCallRepairMiddleware(AgentMiddleware):\n</code></pre>"},{"location":"zh/api-reference/agent/#_33","title":"\u793a\u4f8b","text":"<pre><code>tool_call_repair_middleware = ToolCallRepairMiddleware()\n</code></pre>"},{"location":"zh/api-reference/agent/#formatpromptmiddleware","title":"FormatPromptMiddleware","text":"<p>\u7528\u4e8e\u683c\u5f0f\u5316\u63d0\u793a\u8bcd\u7684\u4e2d\u95f4\u4ef6\u3002</p>"},{"location":"zh/api-reference/agent/#_34","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>class FormatPromptMiddleware(AgentMiddleware):\n    def __init__(\n        self,\n        *,\n        template_format: Literal[\"f-string\", \"jinja2\"] = \"f-string\",\n    ) -&gt; None:\n</code></pre>"},{"location":"zh/api-reference/agent/#_35","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 template_format Literal[\"f-string\", \"jinja2\"] \u5426 <code>\"f-string\"</code> \u6a21\u677f\u8bed\u6cd5\uff0c\u53d6\u503c\u4e3a<code>f-string</code>\u6216<code>jinja2</code>"},{"location":"zh/api-reference/agent/#_36","title":"\u793a\u4f8b","text":"<pre><code>format_prompt_middleware = FormatPromptMiddleware(template_format=\"jinja2\")\n</code></pre>"},{"location":"zh/api-reference/agent/#planstate","title":"PlanState","text":"<p>\u7528\u4e8e Plan \u7684\u72b6\u6001 Schema\u3002</p>"},{"location":"zh/api-reference/agent/#_37","title":"\u7c7b\u5b9a\u4e49","text":"<pre><code>class Plan(TypedDict):\n    content: str\n    status: Literal[\"pending\", \"in_progress\", \"done\"]\n\n\nclass PlanState(AgentState):\n    plan: NotRequired[list[Plan]]\n</code></pre>"},{"location":"zh/api-reference/agent/#_38","title":"\u5c5e\u6027","text":"\u5c5e\u6027 \u7c7b\u578b \u63cf\u8ff0 plan NotRequired[list[Plan]] \u8ba1\u5212\u5217\u8868 plan.content str \u8ba1\u5212\u5185\u5bb9 plan.status Literal[\"pending\", \"in_progress\", \"done\"] \u8ba1\u5212\u72b6\u6001\uff0c\u53d6\u503c\u4e3a<code>pending</code>\u3001<code>in_progress</code>\u3001<code>done</code>"},{"location":"zh/api-reference/agent/#modeldict","title":"ModelDict","text":"<p>\u6a21\u578b\u5217\u8868\u7684\u7c7b\u578b\u3002</p>"},{"location":"zh/api-reference/agent/#_39","title":"\u7c7b\u5b9a\u4e49","text":"<pre><code>class ModelDict(TypedDict):\n    model_name: str\n    model_description: str\n    tools: NotRequired[list[BaseTool | dict[str, Any]]]\n    model_kwargs: NotRequired[dict[str, Any]]\n    model_instance: NotRequired[BaseChatModel]\n    model_system_prompt: NotRequired[str]\n</code></pre>"},{"location":"zh/api-reference/agent/#_40","title":"\u5c5e\u6027","text":"\u5c5e\u6027 \u7c7b\u578b \u5fc5\u586b \u63cf\u8ff0 model_name str \u662f \u6a21\u578b\u540d\u79f0 model_description str \u662f \u6a21\u578b\u63cf\u8ff0 tools NotRequired[list[BaseTool | dict[str, Any]]] \u5426 \u6a21\u578b\u53ef\u7528\u7684\u5de5\u5177 model_kwargs NotRequired[dict[str, Any]] \u5426 \u4f20\u9012\u7ed9\u6a21\u578b\u7684\u989d\u5916\u53c2\u6570 model_instance NotRequired[BaseChatModel] \u5426 \u6a21\u578b\u5b9e\u4f8b model_system_prompt NotRequired[str] \u5426 \u6a21\u578b\u7684\u7cfb\u7edf\u63d0\u793a\u8bcd"},{"location":"zh/api-reference/agent/#selectmodel","title":"SelectModel","text":"<p>\u7528\u4e8e\u9009\u62e9\u6a21\u578b\u7684\u5de5\u5177\u7c7b\u3002</p>"},{"location":"zh/api-reference/agent/#_41","title":"\u7c7b\u5b9a\u4e49","text":"<pre><code>class SelectModel(BaseModel):\n    \"\"\"Tool for model selection - Must call this tool to return the finally selected model\"\"\"\n\n    model_name: str = Field(\n        ...,\n        description=\"Selected model name (must be the full model name, for example, openai:gpt-4o)\",\n    )\n</code></pre>"},{"location":"zh/api-reference/agent/#_42","title":"\u5c5e\u6027","text":"\u5c5e\u6027 \u7c7b\u578b \u5fc5\u586b \u63cf\u8ff0 model_name str \u662f \u9009\u62e9\u7684\u6a21\u578b\u540d\u79f0\uff08\u5fc5\u987b\u662f\u5b8c\u6574\u7684\u6a21\u578b\u540d\u79f0\uff0c\u4f8b\u5982\uff0copenai:gpt-4o\uff09"},{"location":"zh/api-reference/agent/#multiagentstate","title":"MultiAgentState","text":"<p>\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u5207\u6362\u7684\u72b6\u6001 Schema\u3002</p>"},{"location":"zh/api-reference/agent/#_43","title":"\u7c7b\u5b9a\u4e49","text":"<pre><code>class MultiAgentState(AgentState):\n    active_agent: NotRequired[str]\n</code></pre>"},{"location":"zh/api-reference/agent/#_44","title":"\u5c5e\u6027","text":"\u5c5e\u6027 \u7c7b\u578b \u63cf\u8ff0 active_agent NotRequired[str] \u5f53\u524d\u6fc0\u6d3b\u7684\u667a\u80fd\u4f53\u540d\u79f0"},{"location":"zh/api-reference/agent/#agentconfig","title":"AgentConfig","text":"<p>\u667a\u80fd\u4f53\u914d\u7f6e\u7684\u7c7b\u578b\u3002</p>"},{"location":"zh/api-reference/agent/#_45","title":"\u7c7b\u5b9a\u4e49","text":"<pre><code>class AgentConfig(TypedDict):\n    model: NotRequired[str | BaseChatModel]\n    prompt: str | SystemMessage\n    tools: NotRequired[list[BaseTool | dict[str, Any]]]\n    default: NotRequired[bool]\n    handoffs: list[str] | Literal[\"all\"]\n</code></pre>"},{"location":"zh/api-reference/agent/#_46","title":"\u5c5e\u6027","text":"\u5c5e\u6027 \u7c7b\u578b \u5fc5\u586b \u63cf\u8ff0 model NotRequired[str | BaseChatModel] \u5426 \u6a21\u578b\u540d\u79f0\u6216\u6a21\u578b\u5b9e\u4f8b prompt str | SystemMessage \u662f \u667a\u80fd\u4f53\u7684\u63d0\u793a\u8bcd tools list[BaseTool | dict[str, Any]] \u662f \u667a\u80fd\u4f53\u53ef\u7528\u7684\u5de5\u5177 default NotRequired[bool] \u5426 \u662f\u5426\u4e3a\u9ed8\u8ba4\u667a\u80fd\u4f53 handoffs list[str] | Literal[\"all\"] \u662f \u53ef\u4ee5\u4ea4\u63a5\u5230\u7684\u667a\u80fd\u4f53\u540d\u79f0\u5217\u8868\uff0c\u6216\"all\"\u8868\u793a\u6240\u6709\u667a\u80fd\u4f53"},{"location":"zh/api-reference/chat_model/","title":"ChatModel \u6a21\u5757 API \u53c2\u8003\u6587\u6863","text":""},{"location":"zh/api-reference/chat_model/#register_model_provider","title":"register_model_provider","text":"<p>\u6ce8\u518c\u804a\u5929\u6a21\u578b\u7684\u63d0\u4f9b\u8005\u3002</p>"},{"location":"zh/api-reference/chat_model/#_1","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def register_model_provider(\n    provider_name: str,\n    chat_model: ChatModelType,\n    base_url: Optional[str] = None,\n    model_profiles: Optional[dict[str, dict[str, Any]]] = None,\n    compatibility_options: Optional[CompatibilityOptions] = None,\n) -&gt; None:\n</code></pre>"},{"location":"zh/api-reference/chat_model/#_2","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 provider_name str \u662f - \u81ea\u5b9a\u4e49\u63d0\u4f9b\u5546\u540d\u79f0 chat_model ChatModelType \u662f - ChatModel \u7c7b\u6216\u652f\u6301\u7684\u63d0\u4f9b\u8005\u5b57\u7b26\u4e32\u7c7b\u578b base_url Optional[str] \u5426 None \u63d0\u4f9b\u5546\u7684 BaseURL model_profiles Optional[dict[str, dict[str, Any]]] \u5426 None \u63d0\u4f9b\u5546\u6240\u652f\u6301\u7684\u6a21\u578b\u7684profile\uff0c\u683c\u5f0f\u4e3a <code>{model_name: model_profile}</code> compatibility_options Optional[CompatibilityOptions] \u5426 None \u517c\u5bb9\u6027\u9009\u9879"},{"location":"zh/api-reference/chat_model/#_3","title":"\u793a\u4f8b","text":"<pre><code>register_model_provider(\"fakechat\",FakeChatModel)\nregister_model_provider(\"vllm\", \"openai-compatible\", base_url=\"http://localhost:8000/v1\")\n</code></pre>"},{"location":"zh/api-reference/chat_model/#batch_register_model_provider","title":"batch_register_model_provider","text":"<p>\u6279\u91cf\u6ce8\u518c\u6a21\u578b\u63d0\u4f9b\u8005\u3002</p>"},{"location":"zh/api-reference/chat_model/#_4","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def batch_register_model_provider(\n    providers: list[ChatModelProvider],\n) -&gt; None:\n</code></pre>"},{"location":"zh/api-reference/chat_model/#_5","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 providers list[ChatModelProvider] \u662f - \u63d0\u4f9b\u8005\u914d\u7f6e\u5217\u8868"},{"location":"zh/api-reference/chat_model/#_6","title":"\u793a\u4f8b","text":"<pre><code>batch_register_model_provider([\n    {\"provider_name\": \"fakechat\", \"chat_model\": FakeChatModel},\n    {\"provider_name\": \"vllm\", \"chat_model\": \"openai-compatible\", \"base_url\": \"http://localhost:8000/v1\"},\n])\n</code></pre>"},{"location":"zh/api-reference/chat_model/#load_chat_model","title":"load_chat_model","text":"<p>\u4ece\u5df2\u6ce8\u518c\u7684\u63d0\u4f9b\u8005\u52a0\u8f7d\u804a\u5929\u6a21\u578b\u3002</p>"},{"location":"zh/api-reference/chat_model/#_7","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def load_chat_model(\n    model: str,\n    *,\n    model_provider: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; BaseChatModel:\n</code></pre>"},{"location":"zh/api-reference/chat_model/#_8","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 model str \u662f - \u6a21\u578b\u540d\u79f0\uff0c\u683c\u5f0f\u4e3a <code>model_name</code> \u6216 <code>provider_name:model_name</code> model_provider Optional[str] \u5426 None \u6a21\u578b\u63d0\u4f9b\u8005\u540d\u79f0 **kwargs Any \u5426 - \u989d\u5916\u7684\u6a21\u578b\u53c2\u6570"},{"location":"zh/api-reference/chat_model/#_9","title":"\u793a\u4f8b","text":"<pre><code>model = load_chat_model(\"vllm:qwen3-4b\")\n</code></pre>"},{"location":"zh/api-reference/chat_model/#create_openai_compatible_model","title":"create_openai_compatible_model","text":"<p>\u521b\u5efa\u4e00\u4e2a OpenAI \u517c\u5bb9\u7684\u804a\u5929\u6a21\u578b\u7c7b\u3002</p>"},{"location":"zh/api-reference/chat_model/#_10","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def create_openai_compatible_model(\n    model_provider: str,\n    base_url: Optional[str] = None,\n    compatibility_options: Optional[CompatibilityOptions] = None,\n    model_profiles: Optional[dict[str, dict[str, Any]]] = None,\n    chat_model_cls_name: Optional[str] = None,\n) -&gt; type[BaseChatModel]:\n</code></pre>"},{"location":"zh/api-reference/chat_model/#_11","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 model_provider str \u662f - \u6a21\u578b\u63d0\u4f9b\u8005\u540d\u79f0 base_url Optional[str] \u5426 None \u6a21\u578b\u63d0\u4f9b\u8005\u7684 BaseURL compatibility_options Optional[CompatibilityOptions] \u5426 None \u517c\u5bb9\u6027\u9009\u9879 model_profiles Optional[dict[str, dict[str, Any]]] \u5426 None \u6a21\u578b\u63d0\u4f9b\u8005\u6240\u652f\u6301\u7684\u6a21\u578b\u7684profile\uff0c\u683c\u5f0f\u4e3a <code>{model_name: model_profile}</code> chat_model_cls_name Optional[str] \u5426 None \u81ea\u5b9a\u4e49\u7684\u804a\u5929\u6a21\u578b\u7c7b\u540d"},{"location":"zh/api-reference/chat_model/#_12","title":"\u8fd4\u56de\u503c","text":"\u7c7b\u578b \u63cf\u8ff0 type[BaseChatModel] \u52a8\u6001\u521b\u5efa\u7684 OpenAI \u517c\u5bb9\u804a\u5929\u6a21\u578b\u7c7b"},{"location":"zh/api-reference/chat_model/#_13","title":"\u793a\u4f8b","text":"<pre><code>ChatVLLM = create_openai_compatible_model(\n    model_provider=\"vllm\",\n    base_url=\"http://localhost:8000/v1\",\n    chat_model_cls_name=\"ChatVLLM\",\n)\n</code></pre>"},{"location":"zh/api-reference/chat_model/#chatmodeltype","title":"ChatModelType","text":"<p>\u6ce8\u518c\u6a21\u578b\u63d0\u4f9b\u5546\u65f6<code>chat_model</code>\u53c2\u6570\u652f\u6301\u7684\u7c7b\u578b\u3002</p>"},{"location":"zh/api-reference/chat_model/#_14","title":"\u7c7b\u578b\u5b9a\u4e49","text":"<pre><code>ChatModelType = Union[type[BaseChatModel], Literal[\"openai-compatible\"]]\n</code></pre>"},{"location":"zh/api-reference/chat_model/#toolchoicetype","title":"ToolChoiceType","text":"<p><code>tool_choice</code>\u53c2\u6570\u652f\u6301\u7684\u7c7b\u578b\u3002</p>"},{"location":"zh/api-reference/chat_model/#_15","title":"\u7c7b\u578b\u5b9a\u4e49","text":"<pre><code>ToolChoiceType = list[Literal[\"auto\", \"none\", \"required\", \"specific\"]]\n</code></pre>"},{"location":"zh/api-reference/chat_model/#responseformattype","title":"ResponseFormatType","text":"<p><code>response_format</code>\u652f\u6301\u7684\u7c7b\u578b\u3002</p>"},{"location":"zh/api-reference/chat_model/#_16","title":"\u7c7b\u578b\u5b9a\u4e49","text":"<pre><code>ResponseFormatType = list[Literal[\"json_schema\", \"json_mode\"]]\n</code></pre>"},{"location":"zh/api-reference/chat_model/#reasoningkeeppolicy","title":"ReasoningKeepPolicy","text":"<p>messages\u5217\u8868\u4e2dreasoning_content\u5b57\u6bb5\u7684\u4fdd\u7559\u7b56\u7565\u3002</p>"},{"location":"zh/api-reference/chat_model/#_17","title":"\u7c7b\u578b\u5b9a\u4e49","text":"<pre><code>ReasoningKeepPolicy = Literal[\"never\", \"current\", \"all\"]\n</code></pre>"},{"location":"zh/api-reference/chat_model/#compatibilityoptions","title":"CompatibilityOptions","text":"<p>\u6a21\u578b\u63d0\u4f9b\u5546\u7684\u517c\u5bb9\u6027\u9009\u9879\u3002</p>"},{"location":"zh/api-reference/chat_model/#_18","title":"\u7c7b\u5b9a\u4e49","text":"<pre><code>class CompatibilityOptions(TypedDict):\n    supported_tool_choice: NotRequired[ToolChoiceType]\n    supported_response_format: NotRequired[ResponseFormatType]\n    reasoning_keep_policy: NotRequired[ReasoningKeepPolicy]\n    include_usage: NotRequired[bool]\n</code></pre>"},{"location":"zh/api-reference/chat_model/#_19","title":"\u5b57\u6bb5\u8bf4\u660e","text":"\u5b57\u6bb5 \u7c7b\u578b \u5fc5\u586b \u63cf\u8ff0 supported_tool_choice NotRequired[ToolChoiceType] \u5426 \u652f\u6301\u7684 <code>tool_choice</code> \u7b56\u7565\u5217\u8868 supported_response_format NotRequired[ResponseFormatType] \u5426 \u652f\u6301\u7684 <code>response_format</code> \u65b9\u6cd5\u5217\u8868 reasoning_keep_policy NotRequired[ReasoningKeepPolicy] \u5426 \u4f20\u7ed9\u6a21\u578b\u7684\u5386\u53f2\u6d88\u606f\uff08messages\uff09\u4e2d <code>reasoning_content</code> \u5b57\u6bb5\u7684\u4fdd\u7559\u7b56\u7565\u3002\u53ef\u9009\u503c\u6709<code>never</code>\u3001<code>current</code>\u3001<code>all</code> include_usage NotRequired[bool] \u5426 \u662f\u5426\u5728\u6700\u540e\u4e00\u6761\u6d41\u5f0f\u8fd4\u56de\u7ed3\u679c\u4e2d\u5305\u542b <code>usage</code> \u4fe1\u606f"},{"location":"zh/api-reference/chat_model/#chatmodelprovider","title":"ChatModelProvider","text":"<p>\u804a\u5929\u6a21\u578b\u63d0\u4f9b\u8005\u914d\u7f6e\u7c7b\u578b\u3002</p>"},{"location":"zh/api-reference/chat_model/#_20","title":"\u7c7b\u5b9a\u4e49","text":"<pre><code>class ChatModelProvider(TypedDict):\n    provider_name: str\n    chat_model: ChatModelType\n    base_url: NotRequired[str]\n    model_profiles: NotRequired[dict[str, dict[str, Any]]]\n    compatibility_options: NotRequired[CompatibilityOptions]\n</code></pre>"},{"location":"zh/api-reference/chat_model/#_21","title":"\u5b57\u6bb5\u8bf4\u660e","text":"\u5b57\u6bb5 \u7c7b\u578b \u5fc5\u586b \u63cf\u8ff0 provider_name str \u662f \u63d0\u4f9b\u8005\u540d\u79f0 chat_model ChatModelType \u662f \u652f\u6301\u4f20\u5165\u5bf9\u8bdd\u6a21\u578b\u7c7b\u6216\u5b57\u7b26\u4e32\uff08\u76ee\u524d\u53ea\u652f\u6301<code>openai-compatible</code>\uff09 base_url NotRequired[str] \u5426 \u57fa\u7840 URL model_profiles NotRequired[dict[str, dict[str, Any]]] \u5426 \u63d0\u4f9b\u5546\u6240\u652f\u6301\u7684\u6a21\u578b\u7684profile\uff0c\u683c\u5f0f\u4e3a <code>{model_name: model_profile}</code> compatibility_options NotRequired[CompatibilityOptions] \u5426 \u6a21\u578b\u63d0\u4f9b\u5546\u517c\u5bb9\u6027\u9009\u9879"},{"location":"zh/api-reference/embeddings/","title":"Embeddings \u6a21\u5757 API \u53c2\u8003\u6587\u6863","text":""},{"location":"zh/api-reference/embeddings/#register_embeddings_provider","title":"register_embeddings_provider","text":"<p>\u6ce8\u518c\u5d4c\u5165\u6a21\u578b\u7684\u63d0\u4f9b\u8005\u3002</p>"},{"location":"zh/api-reference/embeddings/#_1","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def register_embeddings_provider(\n    provider_name: str,\n    embeddings_model: EmbeddingsType,\n    base_url: Optional[str] = None,\n) -&gt; None:\n</code></pre>"},{"location":"zh/api-reference/embeddings/#_2","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 provider_name str \u662f - \u81ea\u5b9a\u4e49\u63d0\u4f9b\u8005\u540d\u79f0 embeddings_model EmbeddingsType \u662f - \u5d4c\u5165\u6a21\u578b\u7c7b\u6216\u652f\u6301\u7684\u63d0\u4f9b\u8005\u5b57\u7b26\u4e32\u7c7b\u578b base_url Optional[str] \u5426 None \u63d0\u4f9b\u8005\u7684 BaseURL"},{"location":"zh/api-reference/embeddings/#_3","title":"\u793a\u4f8b","text":"<pre><code>register_embeddings_provider(\"fakeembeddings\", FakeEmbeddings)\nregister_embeddings_provider(\"vllm\", \"openai-compatible\", base_url=\"http://localhost:8000/v1\")\n</code></pre>"},{"location":"zh/api-reference/embeddings/#batch_register_embeddings_provider","title":"batch_register_embeddings_provider","text":"<p>\u6279\u91cf\u6ce8\u518c\u5d4c\u5165\u6a21\u578b\u63d0\u4f9b\u8005\u3002</p>"},{"location":"zh/api-reference/embeddings/#_4","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def batch_register_embeddings_provider(\n    providers: list[EmbeddingProvider]\n) -&gt; None:\n</code></pre>"},{"location":"zh/api-reference/embeddings/#_5","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 providers list[EmbeddingProvider] \u662f - \u63d0\u4f9b\u8005\u914d\u7f6e\u5217\u8868"},{"location":"zh/api-reference/embeddings/#_6","title":"\u793a\u4f8b","text":"<pre><code>batch_register_embeddings_provider([\n    {\"provider_name\": \"fakeembeddings\", \"embeddings_model\": FakeEmbeddings},\n    {\"provider_name\": \"vllm\", \"embeddings_model\": \"openai-compatible\", \"base_url\": \"http://localhost:8000/v1\"},\n])\n</code></pre>"},{"location":"zh/api-reference/embeddings/#load_embeddings","title":"load_embeddings","text":"<p>\u4ece\u5df2\u6ce8\u518c\u7684\u63d0\u4f9b\u8005\u52a0\u8f7d\u5d4c\u5165\u6a21\u578b\u3002</p>"},{"location":"zh/api-reference/embeddings/#_7","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def load_embeddings(\n    model: str,\n    *,\n    provider: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; Embeddings:\n</code></pre>"},{"location":"zh/api-reference/embeddings/#_8","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 model str \u662f - \u6a21\u578b\u540d\u79f0\uff0c\u683c\u5f0f\u4e3a <code>model_name</code> \u6216 <code>provider_name:model_name</code> provider Optional[str] \u5426 None \u6a21\u578b\u63d0\u4f9b\u8005\u540d\u79f0 **kwargs Any \u5426 - \u989d\u5916\u7684\u6a21\u578b\u53c2\u6570"},{"location":"zh/api-reference/embeddings/#_9","title":"\u793a\u4f8b","text":"<pre><code>embeddings = load_embeddings(\"vllm:qwen3-embedding-4b\")\n</code></pre>"},{"location":"zh/api-reference/embeddings/#create_openai_compatible_embedding","title":"create_openai_compatible_embedding","text":"<p>\u521b\u5efa\u4e00\u4e2a OpenAI \u517c\u5bb9\u7684\u5d4c\u5165\u6a21\u578b\u7c7b\u3002</p>"},{"location":"zh/api-reference/embeddings/#_10","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def create_openai_compatible_embedding(\n    embedding_provider: str,\n    base_url: Optional[str] = None,\n    embedding_model_cls_name: Optional[str] = None,\n) -&gt; type[Embeddings]:\n</code></pre>"},{"location":"zh/api-reference/embeddings/#_11","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 embedding_provider str \u662f - \u5d4c\u5165\u6a21\u578b\u63d0\u4f9b\u8005\u540d\u79f0 base_url Optional[str] \u5426 None \u6a21\u578b\u63d0\u4f9b\u8005\u7684 BaseURL embedding_model_cls_name Optional[str] \u5426 None \u81ea\u5b9a\u4e49\u7684\u5d4c\u5165\u6a21\u578b\u7c7b\u540d"},{"location":"zh/api-reference/embeddings/#_12","title":"\u8fd4\u56de\u503c","text":"\u7c7b\u578b \u63cf\u8ff0 type[Embeddings] \u52a8\u6001\u521b\u5efa\u7684 OpenAI \u517c\u5bb9\u5d4c\u5165\u6a21\u578b\u7c7b"},{"location":"zh/api-reference/embeddings/#_13","title":"\u793a\u4f8b","text":""},{"location":"zh/api-reference/embeddings/#embeddingstype","title":"EmbeddingsType","text":"<p>\u6ce8\u518c\u5d4c\u5165\u63d0\u4f9b\u5546\u65f6<code>embeddings_model</code>\u53c2\u6570\u652f\u6301\u7684\u7c7b\u578b\u3002</p>"},{"location":"zh/api-reference/embeddings/#_14","title":"\u7c7b\u578b\u5b9a\u4e49","text":"<pre><code>EmbeddingsType = Union[type[Embeddings], Literal[\"openai-compatible\"]]\n</code></pre>"},{"location":"zh/api-reference/embeddings/#embeddingprovider","title":"EmbeddingProvider","text":"<p>\u5d4c\u5165\u6a21\u578b\u63d0\u4f9b\u8005\u914d\u7f6e\u7c7b\u578b\u3002</p>"},{"location":"zh/api-reference/embeddings/#_15","title":"\u7c7b\u5b9a\u4e49","text":"<pre><code>class EmbeddingProvider(TypedDict):\n    provider_name: str\n    embeddings_model: EmbeddingsType\n    base_url: NotRequired[str]\n</code></pre>"},{"location":"zh/api-reference/embeddings/#_16","title":"\u5b57\u6bb5\u8bf4\u660e","text":"\u5b57\u6bb5 \u7c7b\u578b \u5fc5\u586b \u63cf\u8ff0 provider_name str \u662f \u63d0\u4f9b\u8005\u540d\u79f0 embeddings_model EmbeddingsType \u662f \u5d4c\u5165\u6a21\u578b\u7c7b\u6216\u5b57\u7b26\u4e32 base_url NotRequired[str] \u5426 \u57fa\u7840 URL"},{"location":"zh/api-reference/graph/","title":"Graph \u6a21\u5757 API \u53c2\u8003\u6587\u6863","text":""},{"location":"zh/api-reference/graph/#create_sequential_graph","title":"create_sequential_graph","text":"<p>\u5c06\u591a\u4e2a\u8282\u70b9\u4ee5\u4e32\u884c\u65b9\u5f0f\u7ec4\u5408\u6210\u4e00\u4e2a\u72b6\u6001\u56fe\u3002</p>"},{"location":"zh/api-reference/graph/#_1","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def create_sequential_graph(\n    nodes: list[Node],\n    state_schema: type[StateT],\n    graph_name: Optional[str] = None,\n    context_schema: type[ContextT] | None = None,\n    input_schema: type[InputT] | None = None,\n    output_schema: type[OutputT] | None = None,\n    checkpointer: Checkpointer | None = None,\n    store: BaseStore | None = None,\n    cache: BaseCache | None = None,\n) -&gt; CompiledStateGraph[StateT, ContextT, InputT, OutputT]:\n</code></pre>"},{"location":"zh/api-reference/graph/#_2","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 nodes list[Node] \u662f - \u8981\u7ec4\u5408\u7684\u8282\u70b9\u5217\u8868\uff0c\u53ef\u4e3a\u8282\u70b9\u51fd\u6570\u6216\u7531\u8282\u70b9\u540d\u79f0\u4e0e\u8282\u70b9\u51fd\u6570\u7ec4\u6210\u7684\u4e8c\u5143\u7ec4\u3002 state_schema type[StateT] \u662f - \u6700\u7ec8\u751f\u6210\u56fe\u7684 State Schema graph_name Optional[str] \u5426 None \u6700\u7ec8\u751f\u6210\u56fe\u7684\u540d\u79f0 context_schema type[ContextT] | None \u5426 None \u6700\u7ec8\u751f\u6210\u56fe\u7684 Context Schema input_schema type[InputT] | None \u5426 None \u6700\u7ec8\u751f\u6210\u56fe\u7684\u8f93\u5165 Schema output_schema type[OutputT] | None \u5426 None \u6700\u7ec8\u751f\u6210\u56fe\u7684\u8f93\u51fa Schema checkpointer Checkpointer | None \u5426 None \u6700\u7ec8\u751f\u6210\u56fe\u7684 Checkpointer store BaseStore | None \u5426 None \u6700\u7ec8\u751f\u6210\u56fe\u7684 Store cache BaseCache | None \u5426 None \u6700\u7ec8\u751f\u6210\u56fe\u7684 Cache"},{"location":"zh/api-reference/graph/#_3","title":"\u793a\u4f8b","text":"<pre><code>create_sequential_graph(\n    nodes=[node1, node2],\n    state_schema=State,\n    graph_name=\"sequential_pipeline\",\n    context_schema=Context,\n    input_schema=Input,\n    output_schema=Output,\n)\n</code></pre>"},{"location":"zh/api-reference/graph/#create_parallel_graph","title":"create_parallel_graph","text":"<p>\u5c06\u591a\u4e2a\u8282\u70b9\u4ee5\u5e76\u884c\u65b9\u5f0f\u7ec4\u5408\u6210\u4e00\u4e2a\u72b6\u6001\u56fe\u3002</p>"},{"location":"zh/api-reference/graph/#_4","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def create_parallel_graph(\n    nodes: list[Node],\n    state_schema: type[StateT],\n    graph_name: Optional[str] = None,\n    branches_fn: Optional[\n        Union[\n            Callable[..., list[Send]],\n            Callable[..., Awaitable[list[Send]]],\n        ]\n    ] = None,\n    context_schema: type[ContextT] | None = None,\n    input_schema: type[InputT] | None = None,\n    output_schema: type[OutputT] | None = None,\n    checkpointer: Checkpointer | None = None,\n    store: BaseStore | None = None,\n    cache: BaseCache | None = None,\n) -&gt; CompiledStateGraph[StateT, ContextT, InputT, OutputT]:\n</code></pre>"},{"location":"zh/api-reference/graph/#_5","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 nodes list[Node] \u662f - \u8981\u7ec4\u5408\u7684\u8282\u70b9\u5217\u8868\uff0c\u53ef\u4e3a\u8282\u70b9\u51fd\u6570\u6216\u7531\u8282\u70b9\u540d\u79f0\u4e0e\u8282\u70b9\u51fd\u6570\u7ec4\u6210\u7684\u4e8c\u5143\u7ec4\u3002 state_schema type[StateT] \u662f - \u6700\u7ec8\u751f\u6210\u56fe\u7684 State Schema graph_name Optional[str] \u5426 None \u6700\u7ec8\u751f\u6210\u56fe\u7684\u540d\u79f0 branches_fn Optional[Union[Callable[..., list[Send]], Callable[..., Awaitable[list[Send]]]]] \u5426 None \u5e76\u884c\u5206\u652f\u51fd\u6570\uff0c\u8fd4\u56de Send \u5217\u8868\u63a7\u5236\u5e76\u884c\u6267\u884c context_schema type[ContextT] | None \u5426 None \u6700\u7ec8\u751f\u6210\u56fe\u7684 Context Schema input_schema type[InputT] | None \u5426 None \u6700\u7ec8\u751f\u6210\u56fe\u7684\u8f93\u5165 Schema output_schema type[OutputT] | None \u5426 None \u6700\u7ec8\u751f\u6210\u56fe\u7684\u8f93\u51fa Schema checkpointer Checkpointer | None \u5426 None \u6700\u7ec8\u751f\u6210\u56fe\u7684 Checkpointer store BaseStore | None \u5426 None \u6700\u7ec8\u751f\u6210\u56fe\u7684 Store cache BaseCache | None \u5426 None \u6700\u7ec8\u751f\u6210\u56fe\u7684 Cache"},{"location":"zh/api-reference/graph/#_6","title":"\u793a\u4f8b","text":"<pre><code>create_parallel_graph(\n    nodes=[node1, node2],\n    state_schema=State,\n    graph_name=\"parallel_pipeline\",\n    branches_fn=lambda state: [Send(\"node1\", state), Send(\"node2\", state)],\n    context_schema=Context,\n    input_schema=Input,\n    output_schema=Output,\n)\n</code></pre>"},{"location":"zh/api-reference/graph/#node","title":"Node \u7c7b\u578b","text":"<pre><code>Node = StateNode | tuple[str, StateNode]\n</code></pre>"},{"location":"zh/api-reference/message_convert/","title":"Message Convert \u6a21\u5757 API \u53c2\u8003\u6587\u6863","text":""},{"location":"zh/api-reference/message_convert/#convert_reasoning_content_for_ai_message","title":"convert_reasoning_content_for_ai_message","text":"<p>\u5c06\u601d\u7ef4\u94fe\u5408\u5e76\u5230\u6700\u7ec8\u56de\u590d\u4e2d\u3002</p>"},{"location":"zh/api-reference/message_convert/#_1","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def convert_reasoning_content_for_ai_message(\n    model_response: AIMessage,\n    think_tag: Tuple[str, str] = (\"&lt;think&gt;\", \"&lt;/think&gt;\"),\n) -&gt; AIMessage\n</code></pre>"},{"location":"zh/api-reference/message_convert/#_2","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 model_response AIMessage \u662f - \u5305\u542b\u63a8\u7406\u5185\u5bb9\u7684 AI \u6d88\u606f think_tag Tuple[str, str] \u5426 <code>(\"&lt;think&gt;\",\"&lt;/think&gt;\")</code> \u63a8\u7406\u5185\u5bb9\u7684\u5f00\u59cb\u548c\u7ed3\u675f\u6807\u7b7e"},{"location":"zh/api-reference/message_convert/#_3","title":"\u793a\u4f8b","text":"<pre><code>response = convert_reasoning_content_for_ai_message(\n    response, think_tag=(\"&lt;start&gt;\", \"&lt;end&gt;\")\n)\n</code></pre>"},{"location":"zh/api-reference/message_convert/#convert_reasoning_content_for_chunk_iterator","title":"convert_reasoning_content_for_chunk_iterator","text":"<p>\u4e3a\u6d41\u5f0f\u6d88\u606f\u5757\u5408\u5e76\u63a8\u7406\u5185\u5bb9\u3002</p>"},{"location":"zh/api-reference/message_convert/#_4","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def convert_reasoning_content_for_chunk_iterator(\n    model_response: Iterator[AIMessageChunk | AIMessage],\n    think_tag: Tuple[str, str] = (\"&lt;think&gt;\", \"&lt;/think&gt;\"),\n) -&gt; Iterator[AIMessageChunk | AIMessage]\n</code></pre>"},{"location":"zh/api-reference/message_convert/#_5","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 model_response Iterator[AIMessageChunk | AIMessage] \u662f - \u6d88\u606f\u5757\u7684\u8fed\u4ee3\u5668 think_tag Tuple[str, str] \u5426 <code>(\"&lt;think&gt;\",\"&lt;/think&gt;\")</code> \u63a8\u7406\u5185\u5bb9\u7684\u5f00\u59cb\u548c\u7ed3\u675f\u6807\u7b7e"},{"location":"zh/api-reference/message_convert/#_6","title":"\u793a\u4f8b","text":"<pre><code>for chunk in convert_reasoning_content_for_chunk_iterator(\n    model.stream(\"Hello\"), think_tag=(\"&lt;think&gt;\", \"&lt;/think&gt;\")\n):\n    print(chunk.content, end=\"\", flush=True)\n</code></pre>"},{"location":"zh/api-reference/message_convert/#aconvert_reasoning_content_for_chunk_iterator","title":"aconvert_reasoning_content_for_chunk_iterator","text":"<p><code>convert_reasoning_content_for_chunk_iterator</code> \u7684\u5f02\u6b65\u7248\u672c\u3002</p>"},{"location":"zh/api-reference/message_convert/#_7","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>async def aconvert_reasoning_content_for_chunk_iterator(\n    model_response: AsyncIterator[AIMessageChunk | AIMessage],\n    think_tag: Tuple[str, str] = (\"&lt;think&gt;\", \"&lt;/think&gt;\"),\n) -&gt; AsyncIterator[AIMessageChunk | AIMessage]\n</code></pre>"},{"location":"zh/api-reference/message_convert/#_8","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 model_response AsyncIterator[AIMessageChunk | AIMessage] \u662f - \u6d88\u606f\u5757\u7684\u5f02\u6b65\u8fed\u4ee3\u5668 think_tag Tuple[str, str] \u5426 <code>(\"&lt;think&gt;\",\"&lt;/think&gt;\")</code> \u63a8\u7406\u5185\u5bb9\u7684\u5f00\u59cb\u548c\u7ed3\u675f\u6807\u7b7e"},{"location":"zh/api-reference/message_convert/#_9","title":"\u793a\u4f8b","text":"<pre><code>async for chunk in aconvert_reasoning_content_for_chunk_iterator(\n    model.astream(\"Hello\"), think_tag=(\"&lt;think&gt;\", \"&lt;/think&gt;\")\n):\n    print(chunk.content, end=\"\", flush=True)\n</code></pre>"},{"location":"zh/api-reference/message_convert/#merge_ai_message_chunk","title":"merge_ai_message_chunk","text":"<p>\u5c06\u6d41\u5f0f\u8f93\u51fa\u7684 chunks \u5408\u5e76\u4e3a\u4e00\u4e2a AIMessage\u3002</p>"},{"location":"zh/api-reference/message_convert/#_10","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def merge_ai_message_chunk(\n    chunks: Sequence[AIMessageChunk]\n) -&gt; AIMessage\n</code></pre>"},{"location":"zh/api-reference/message_convert/#_11","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 chunks Sequence[AIMessageChunk] \u662f - \u5f85\u5408\u5e76\u7684\u6d88\u606f\u5757\u5217\u8868"},{"location":"zh/api-reference/message_convert/#_12","title":"\u793a\u4f8b","text":"<pre><code>chunks = list(model.stream(\"Hello\"))\nmerged = merge_ai_message_chunk(chunks)\n</code></pre>"},{"location":"zh/api-reference/message_convert/#format_sequence","title":"format_sequence","text":"<p>\u5c06 BaseMessage\u3001Document \u6216\u5b57\u7b26\u4e32\u5217\u8868\u683c\u5f0f\u5316\u4e3a\u5355\u4e2a\u5b57\u7b26\u4e32\u3002</p>"},{"location":"zh/api-reference/message_convert/#_13","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def format_sequence(\n    inputs: List[Union[BaseMessage, Document, str]],\n    separator: str = \"-\",\n    with_num: bool = False\n) -&gt; str\n</code></pre>"},{"location":"zh/api-reference/message_convert/#_14","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 inputs List[Union[BaseMessage, Document, str]] \u662f - \u5f85\u683c\u5f0f\u5316\u7684\u9879\u76ee\u5217\u8868 separator str \u5426 \"-\" \u5206\u9694\u7b26\u5b57\u7b26\u4e32 with_num bool \u5426 False \u662f\u5426\u6dfb\u52a0\u6570\u5b57\u524d\u7f00"},{"location":"zh/api-reference/message_convert/#_15","title":"\u793a\u4f8b","text":"<pre><code>formatted = format_sequence(messages, separator=\"\\n\", with_num=True)\n</code></pre>"},{"location":"zh/api-reference/tool_calling/","title":"Tool Calling \u6a21\u5757 API \u53c2\u8003\u6587\u6863","text":""},{"location":"zh/api-reference/tool_calling/#has_tool_calling","title":"has_tool_calling","text":"<p>\u68c0\u67e5\u6d88\u606f\u662f\u5426\u5305\u542b\u5de5\u5177\u8c03\u7528\u3002</p>"},{"location":"zh/api-reference/tool_calling/#_1","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def has_tool_calling(\n    message: AIMessage\n) -&gt; bool\n</code></pre>"},{"location":"zh/api-reference/tool_calling/#_2","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 message AIMessage \u662f - \u5f85\u68c0\u67e5\u7684\u6d88\u606f"},{"location":"zh/api-reference/tool_calling/#_3","title":"\u793a\u4f8b","text":"<pre><code>if has_tool_calling(response):\n    # \u5904\u7406\u5de5\u5177\u8c03\u7528\n    pass\n</code></pre>"},{"location":"zh/api-reference/tool_calling/#parse_tool_calling","title":"parse_tool_calling","text":"<p>\u4ece\u6d88\u606f\u4e2d\u89e3\u6790\u5de5\u5177\u8c03\u7528\u53c2\u6570\u3002</p>"},{"location":"zh/api-reference/tool_calling/#_4","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def parse_tool_calling(\n    message: AIMessage, first_tool_call_only: bool = False\n) -&gt; Union[tuple[str, dict], list[tuple[str, dict]]]\n</code></pre>"},{"location":"zh/api-reference/tool_calling/#_5","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 message AIMessage \u662f - \u5f85\u89e3\u6790\u7684\u6d88\u606f first_tool_call_only bool \u5426 False \u662f\u5426\u4ec5\u8fd4\u56de\u7b2c\u4e00\u4e2a\u5de5\u5177\u8c03\u7528"},{"location":"zh/api-reference/tool_calling/#_6","title":"\u793a\u4f8b","text":"<pre><code># \u83b7\u53d6\u6240\u6709\u5de5\u5177\u8c03\u7528\ntool_calls = parse_tool_calling(response)\n\n# \u4ec5\u83b7\u53d6\u7b2c\u4e00\u4e2a\u5de5\u5177\u8c03\u7528\nname, args = parse_tool_calling(response, first_tool_call_only=True)\n</code></pre>"},{"location":"zh/api-reference/tool_calling/#human_in_the_loop","title":"human_in_the_loop","text":"<p>\u4e3a\u540c\u6b65\u5de5\u5177\u51fd\u6570\u6dfb\u52a0\"\u4eba\u5728\u56de\u8def\"\u4eba\u5de5\u5ba1\u6838\u80fd\u529b\u7684\u88c5\u9970\u5668\u3002</p>"},{"location":"zh/api-reference/tool_calling/#_7","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def human_in_the_loop(\n    func: Optional[Callable] = None,\n    *,\n    handler: Optional[HumanInterruptHandler] = None\n) -&gt; Union[Callable[[Callable], BaseTool], BaseTool]\n</code></pre>"},{"location":"zh/api-reference/tool_calling/#_8","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 func Optional[Callable] \u5426 None \u5f85\u88c5\u9970\u7684\u540c\u6b65\u51fd\u6570\uff08\u88c5\u9970\u5668\u8bed\u6cd5\u7cd6\uff09 handler Optional[HumanInterruptHandler] \u5426 None \u81ea\u5b9a\u4e49\u4e2d\u65ad\u5904\u7406\u51fd\u6570"},{"location":"zh/api-reference/tool_calling/#_9","title":"\u793a\u4f8b","text":"<pre><code>@human_in_the_loop\ndef get_current_time():\n    \"\"\"\u83b7\u53d6\u5f53\u524d\u65f6\u95f4\"\"\"\n    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n</code></pre>"},{"location":"zh/api-reference/tool_calling/#human_in_the_loop_async","title":"human_in_the_loop_async","text":"<p>\u4e3a\u5f02\u6b65\u5de5\u5177\u51fd\u6570\u6dfb\u52a0\"\u4eba\u5728\u56de\u8def\"\u4eba\u5de5\u5ba1\u6838\u80fd\u529b\u7684\u88c5\u9970\u5668\u3002</p>"},{"location":"zh/api-reference/tool_calling/#_10","title":"\u51fd\u6570\u7b7e\u540d","text":"<pre><code>def human_in_the_loop_async(\n    func: Optional[Callable] = None,\n    *,\n    handler: Optional[HumanInterruptHandler] = None\n) -&gt; Union[Callable[[Callable], BaseTool], BaseTool]\n</code></pre>"},{"location":"zh/api-reference/tool_calling/#_11","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u5fc5\u586b \u9ed8\u8ba4\u503c \u63cf\u8ff0 func Optional[Callable] \u5426 None \u5f85\u88c5\u9970\u7684\u5f02\u6b65\u51fd\u6570\uff08\u88c5\u9970\u5668\u8bed\u6cd5\u7cd6\uff09 handler Optional[HumanInterruptHandler] \u5426 None \u81ea\u5b9a\u4e49\u4e2d\u65ad\u5904\u7406\u51fd\u6570"},{"location":"zh/api-reference/tool_calling/#_12","title":"\u793a\u4f8b","text":"<pre><code>@human_in_the_loop_async\nasync def get_current_time():\n    \"\"\"\u83b7\u53d6\u5f53\u524d\u65f6\u95f4\"\"\"\n    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n</code></pre>"},{"location":"zh/api-reference/tool_calling/#interruptparams","title":"InterruptParams","text":"<p>\u4f20\u9012\u7ed9\u4e2d\u65ad\u5904\u7406\u51fd\u6570\u7684\u53c2\u6570\u7c7b\u578b\u3002</p>"},{"location":"zh/api-reference/tool_calling/#_13","title":"\u7c7b\u5b9a\u4e49","text":"<pre><code>class InterruptParams(TypedDict):\n    tool_call_name: str\n    tool_call_args: Dict[str, Any]\n    tool: BaseTool\n</code></pre>"},{"location":"zh/api-reference/tool_calling/#_14","title":"\u5b57\u6bb5\u8bf4\u660e","text":"\u5b57\u6bb5 \u7c7b\u578b \u5fc5\u586b \u63cf\u8ff0 tool_call_name str \u662f \u5de5\u5177\u8c03\u7528\u540d\u79f0 tool_call_args Dict[str, Any] \u662f \u5de5\u5177\u8c03\u7528\u53c2\u6570 tool BaseTool \u662f \u5de5\u5177\u5b9e\u4f8b"},{"location":"zh/api-reference/tool_calling/#humaninterrupthandler","title":"HumanInterruptHandler","text":"<p>\u4e2d\u65ad\u5904\u7406\u5668\u51fd\u6570\u7684\u7c7b\u578b\u522b\u540d\u3002</p>"},{"location":"zh/api-reference/tool_calling/#_15","title":"\u7c7b\u578b\u5b9a\u4e49","text":"<pre><code>HumanInterruptHandler = Callable[[InterruptParams], Any]\n</code></pre>"},{"location":"zh/getting-started-guide/chat/","title":"\u5bf9\u8bdd\u6a21\u578b\u7ba1\u7406","text":""},{"location":"zh/getting-started-guide/chat/#_2","title":"\u6982\u8ff0","text":"<p>LangChain \u7684 <code>init_chat_model</code> \u51fd\u6570\u4ec5\u652f\u6301\u6709\u9650\u7684\u6a21\u578b\u63d0\u4f9b\u5546\u3002\u672c\u5e93\u63d0\u4f9b\u66f4\u7075\u6d3b\u7684\u5bf9\u8bdd\u6a21\u578b\u7ba1\u7406\u65b9\u6848\uff0c\u652f\u6301\u81ea\u5b9a\u4e49\u6a21\u578b\u63d0\u4f9b\u5546\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u63a5\u5165\u672a\u5185\u7f6e\u652f\u6301\u7684\u6a21\u578b\u670d\u52a1\uff08\u5982 vLLM\u7b49\uff09\u7684\u573a\u666f\u3002</p>"},{"location":"zh/getting-started-guide/chat/#_3","title":"\u6ce8\u518c\u6a21\u578b\u63d0\u4f9b\u5546","text":"<p>\u6ce8\u518c\u5bf9\u8bdd\u6a21\u578b\u63d0\u4f9b\u5546\u9700\u8c03\u7528 <code>register_model_provider</code>\u3002\u5bf9\u4e8e\u4e0d\u540c\u7684\u60c5\u51b5\uff0c\u6ce8\u518c\u6b65\u9aa4\u7565\u6709\u4e0d\u540c\u3002</p>"},{"location":"zh/getting-started-guide/chat/#langchain","title":"\u5df2\u6709 LangChain \u5bf9\u8bdd\u6a21\u578b\u7c7b","text":"<p>\u82e5\u6a21\u578b\u63d0\u4f9b\u5546\u5df2\u6709\u73b0\u6210\u4e14\u5408\u9002\u7684 LangChain \u96c6\u6210\uff08\u8be6\u89c1\u5bf9\u8bdd\u6a21\u578b\u7c7b\u96c6\u6210\uff09\uff0c\u8bf7\u5c06\u76f8\u5e94\u7684\u96c6\u6210\u5bf9\u8bdd\u6a21\u578b\u7c7b\u4f5c\u4e3a chat_model \u53c2\u6570\u4f20\u5165\u3002</p>"},{"location":"zh/getting-started-guide/chat/#_4","title":"\u4ee3\u7801\u793a\u4f8b","text":"<pre><code>from langchain_core.language_models.fake_chat_models import FakeChatModel\nfrom langchain_dev_utils.chat_models import register_model_provider\n\nregister_model_provider(\n    provider_name=\"fake_provider\",\n    chat_model=FakeChatModel,\n)\n\n# FakeChatModel\u4ec5\u7528\u4e8e\u6d4b\u8bd5\uff0c\u5b9e\u9645\u4f7f\u7528\u4e2d\u5fc5\u987b\u4f20\u5165\u5177\u5907\u771f\u5b9e\u529f\u80fd\u7684 ChatModel \u7c7b\u3002\n</code></pre> <p>\u53c2\u6570\u8bbe\u7f6e\u8bf4\u660e</p> <p><code>provider_name</code>\u4ee3\u8868\u6a21\u578b\u63d0\u4f9b\u5546\u7684\u540d\u79f0\uff0c\u7528\u4e8e\u540e\u7eed\u5728 <code>load_chat_model</code> \u4e2d\u5f15\u7528\u3002\u547d\u540d\u5fc5\u987b\u4ee5\u5b57\u6bcd\u6216\u6570\u5b57\u5f00\u5934\uff0c\u53ea\u80fd\u5305\u542b\u5b57\u6bcd\u3001\u6570\u5b57\u548c\u4e0b\u5212\u7ebf\uff0c\u957f\u5ea6\u4e0d\u8d85\u8fc7 20 \u4e2a\u5b57\u7b26\u3002</p>"},{"location":"zh/getting-started-guide/chat/#_5","title":"\u53ef\u9009\u53c2\u6570\u8bf4\u660e","text":"<p>base_url</p> <p>\u6b64\u53c2\u6570\u901a\u5e38\u65e0\u9700\u8bbe\u7f6e\uff08\u56e0\u4e3a\u5bf9\u8bdd\u6a21\u578b\u7c7b\u5185\u90e8\u4e00\u822c\u5df2\u5b9a\u4e49\u9ed8\u8ba4\u7684 API \u5730\u5740\uff09\uff0c\u4ec5\u5f53\u9700\u8981\u8986\u76d6\u5bf9\u8bdd\u6a21\u578b\u7c7b\u9ed8\u8ba4\u5730\u5740\u65f6\u624d\u4f20\u5165 <code>base_url</code>\uff0c\u4e14\u4ec5\u5bf9\u5b57\u6bb5\u540d\u4e3a <code>api_base</code> \u6216 <code>base_url</code>\uff08\u542b\u522b\u540d\uff09\u7684\u5c5e\u6027\u751f\u6548\u3002</p> <p>model_profiles</p> <p>\u5982\u679c\u4f60\u7684 LangChain \u96c6\u6210\u5bf9\u8bdd\u6a21\u578b\u7c7b\u5df2\u5168\u9762\u652f\u6301 <code>profile</code> \u53c2\u6570\uff08\u5373\u53ef\u4ee5\u901a\u8fc7 <code>model.profile</code> \u76f4\u63a5\u8bbf\u95ee\u6a21\u578b\u7684\u76f8\u5173\u5c5e\u6027\uff0c\u4f8b\u5982 <code>max_input_tokens</code>\u3001<code>tool_calling</code> \u7b49\uff09\uff0c\u5219\u65e0\u9700\u989d\u5916\u8bbe\u7f6e <code>model_profiles</code>\u3002</p> <p>\u5982\u679c\u901a\u8fc7 <code>model.profile</code> \u8bbf\u95ee\u65f6\u8fd4\u56de\u7684\u662f\u4e00\u4e2a\u7a7a\u5b57\u5178 <code>{}</code>\uff0c\u8bf4\u660e\u8be5 LangChain \u5bf9\u8bdd\u6a21\u578b\u7c7b\u53ef\u80fd\u6682\u65f6\u672a\u652f\u6301 <code>profile</code> \u53c2\u6570\uff0c\u6b64\u65f6\u53ef\u4ee5\u624b\u52a8\u63d0\u4f9b <code>model_profiles</code>\u3002</p> <p><code>model_profiles</code> \u662f\u4e00\u4e2a\u5b57\u5178\uff0c\u5176\u6bcf\u4e00\u4e2a\u952e\u4e3a\u6a21\u578b\u540d\u79f0\uff0c\u503c\u4e3a\u5bf9\u5e94\u6a21\u578b\u7684 profile \u914d\u7f6e:</p> <pre><code>{\n    \"model_name_1\": {\n        \"max_input_tokens\": 100_000,\n        \"tool_calling\": True,\n        \"structured_output\": True,\n        # ... \u5176\u4ed6\u53ef\u9009\u5b57\u6bb5\n    },\n    \"model_name_2\": {\n        \"max_input_tokens\": 32768,\n        \"image_inputs\": True,\n        \"tool_calling\": False,\n        # ... \u5176\u4ed6\u53ef\u9009\u5b57\u6bb5\n    },\n    # \u53ef\u4ee5\u6709\u4efb\u610f\u591a\u4e2a\u6a21\u578b\u914d\u7f6e\n}\n</code></pre> <p>\u63d0\u793a</p> <p>\u63a8\u8350\u4f7f\u7528 <code>langchain-model-profiles</code> \u5e93\u6765\u83b7\u53d6\u4f60\u6240\u7528\u6a21\u578b\u63d0\u4f9b\u5546\u7684 profiles\u3002</p>"},{"location":"zh/getting-started-guide/chat/#langchain-openai-api","title":"\u672a\u6709 LangChain \u5bf9\u8bdd\u6a21\u578b\u7c7b\uff0c\u4f46\u6a21\u578b\u63d0\u4f9b\u5546\u652f\u6301 OpenAI \u517c\u5bb9 API","text":"<p>\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u9700\u8981<code>chat_model</code>\u53c2\u6570\u5fc5\u987b\u8bbe\u4e3a <code>\"openai-compatible\"</code>\u3002</p>"},{"location":"zh/getting-started-guide/chat/#_6","title":"\u4ee3\u7801\u793a\u4f8b","text":"<pre><code>register_model_provider(\n    provider_name=\"vllm\",\n    chat_model=\"openai-compatible\",\n    base_url=\"http://localhost:8000/v1\"\n)\n</code></pre> <p>\u6ce8\u610f\uff1a\u5173\u4e8e\u8fd9\u90e8\u5206\u66f4\u591a\u7684\u7ec6\u8282\uff0c\u8bf7\u53c2\u8003OpenAI \u517c\u5bb9 API \u96c6\u6210\u3002</p>"},{"location":"zh/getting-started-guide/chat/#_7","title":"\u6279\u91cf\u6ce8\u518c","text":"<p>\u82e5\u9700\u6ce8\u518c\u591a\u4e2a\u63d0\u4f9b\u5546\uff0c\u53ef\u4f7f\u7528 <code>batch_register_model_provider</code> \u907f\u514d\u91cd\u590d\u8c03\u7528\u3002</p>"},{"location":"zh/getting-started-guide/chat/#_8","title":"\u4ee3\u7801\u793a\u4f8b","text":"<pre><code>from langchain_dev_utils.chat_models import batch_register_model_provider\nfrom langchain_core.language_models.fake_chat_models import FakeChatModel\n\nbatch_register_model_provider(\n    providers=[\n        {\n            \"provider_name\": \"fake_provider\",\n            \"chat_model\": FakeChatModel,\n        },\n        {\n            \"provider_name\": \"vllm\",\n            \"chat_model\": \"openai-compatible\",\n            \"base_url\": \"http://localhost:8000/v1\",\n        },\n    ]\n)\n</code></pre> <p>\u6ce8\u610f</p> <p>\u4e24\u4e2a\u6ce8\u518c\u51fd\u6570\u5747\u57fa\u4e8e\u5168\u5c40\u5b57\u5178\u5b9e\u73b0\u3002\u4e3a\u907f\u514d\u591a\u7ebf\u7a0b\u95ee\u9898\uff0c\u5fc5\u987b\u5728\u5e94\u7528\u542f\u52a8\u9636\u6bb5\u5b8c\u6210\u6240\u6709\u6ce8\u518c\uff0c\u7981\u6b62\u8fd0\u884c\u65f6\u52a8\u6001\u6ce8\u518c\u3002  </p> <p>\u6b64\u5916\uff0c\u6ce8\u518c\u65f6\u82e5\u5c06 <code>chat_model</code> \u8bbe\u4e3a <code>openai-compatible</code>\uff0c\u5185\u90e8\u4f1a\u901a\u8fc7 <code>pydantic.create_model</code> \u52a8\u6001\u521b\u5efa\u65b0\u7684\u6a21\u578b\u7c7b\uff08\u4ee5 <code>BaseChatOpenAICompatible</code> \u4e3a\u57fa\u7c7b\uff0c\u751f\u6210\u5bf9\u5e94\u7684\u5bf9\u8bdd\u6a21\u578b\u96c6\u6210\u7c7b\uff09\uff0c\u6b64\u8fc7\u7a0b\u6d89\u53ca Python \u5143\u7c7b\u64cd\u4f5c\u548c pydantic \u9a8c\u8bc1\u903b\u8f91\u521d\u59cb\u5316\uff0c\u5b58\u5728\u4e00\u5b9a\u6027\u80fd\u5f00\u9500\uff0c\u56e0\u6b64\u8bf7\u907f\u514d\u5728\u8fd0\u884c\u671f\u9891\u7e41\u6ce8\u518c\u3002</p>"},{"location":"zh/getting-started-guide/chat/#_9","title":"\u52a0\u8f7d\u5bf9\u8bdd\u6a21\u578b","text":"<p>\u4f7f\u7528 <code>load_chat_model</code> \u51fd\u6570\u52a0\u8f7d\u5bf9\u8bdd\u6a21\u578b\uff08\u521d\u59cb\u5316\u5bf9\u8bdd\u6a21\u578b\u5b9e\u4f8b\uff09\u3002</p> <p>\u8be5\u51fd\u6570\u63a5\u6536 <code>model</code> \u53c2\u6570\u7528\u4e8e\u6307\u5b9a\u6a21\u578b\u540d\u79f0\uff0c\u53ef\u9009\u7684 <code>model_provider</code> \u53c2\u6570\u7528\u4e8e\u6307\u5b9a\u6a21\u578b\u63d0\u4f9b\u5546\uff1b\u8fd8\u53ef\u4f20\u5165\u4efb\u610f\u6570\u91cf\u7684\u5173\u952e\u5b57\u53c2\u6570\uff0c\u7528\u4e8e\u4f20\u9012\u5bf9\u8bdd\u6a21\u578b\u7c7b\u7684\u989d\u5916\u53c2\u6570\u3002</p>"},{"location":"zh/getting-started-guide/chat/#_10","title":"\u53c2\u6570\u89c4\u5219","text":"<ul> <li>\u82e5\u672a\u4f20 <code>model_provider</code>\uff0c\u5219 <code>model</code> \u5fc5\u987b\u4e3a <code>provider_name:model_name</code> \u683c\u5f0f\uff1b</li> <li>\u82e5\u4f20 <code>model_provider</code>\uff0c\u5219 <code>model</code> \u5fc5\u987b\u4ec5\u4e3a <code>model_name</code>\u3002</li> </ul>"},{"location":"zh/getting-started-guide/chat/#_11","title":"\u4ee3\u7801\u793a\u4f8b","text":"<pre><code># \u65b9\u5f0f\u4e00\uff1amodel \u5305\u542b provider \u4fe1\u606f\nmodel = load_chat_model(\"vllm:qwen3-4b\")\n\n# \u65b9\u5f0f\u4e8c\uff1a\u5355\u72ec\u6307\u5b9a provider\nmodel = load_chat_model(\"qwen3-4b\", model_provider=\"vllm\")\n</code></pre>"},{"location":"zh/getting-started-guide/chat/#_12","title":"\u6a21\u578b\u65b9\u6cd5\u548c\u53c2\u6570","text":"<p>\u5bf9\u4e8e\u652f\u6301\u7684\u6a21\u578b\u65b9\u6cd5\u548c\u53c2\u6570\uff0c\u9700\u8981\u53c2\u8003\u5bf9\u5e94\u7684\u5bf9\u8bdd\u6a21\u578b\u7c7b\u7684\u4f7f\u7528\u8bf4\u660e\u3002\u5982\u679c\u91c7\u7528\u7684\u662f\u7b2c\u4e8c\u79cd\u60c5\u51b5\uff0c\u5219\u652f\u6301\u6240\u6709\u7684<code>BaseChatOpenAI</code> \u7c7b\u7684\u65b9\u6cd5\u548c\u53c2\u6570\u3002</p>"},{"location":"zh/getting-started-guide/chat/#_13","title":"\u517c\u5bb9\u5b98\u65b9\u63d0\u4f9b\u5546","text":"<p><code>load_chat_model</code> \u4f9d\u636e <code>model_provider</code>\u53c2\u6570\u67e5\u627e\u5168\u5c40\u6ce8\u518c\u5b57\u5178\uff1a\u547d\u4e2d\u5219\u4f7f\u7528\u8be5\u5b57\u5178\u5bf9\u5e94\u7684\u6a21\u578b\u7c7b\u5b9e\u4f8b\u5316\uff0c\u672a\u547d\u4e2d\u5219\u901a\u8fc7 <code>init_chat_model</code> \u521d\u59cb\u5316\u3002\u8fd9\u610f\u5473\u7740 LangChain \u5b98\u65b9\u652f\u6301\u7684\u63d0\u4f9b\u5546\uff08\u5982 openai\uff09\u53ef\u76f4\u63a5\u8c03\u7528\uff0c\u65e0\u9700\u6ce8\u518c\u3002</p> <pre><code>model = load_chat_model(\"openai:gpt-4o-mini\")\n# \u6216\nmodel = load_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n</code></pre> <p>\u6700\u4f73\u5b9e\u8df5</p> <p>\u5bf9\u4e8e\u672c\u6a21\u5757\u7684\u4f7f\u7528\uff0c\u53ef\u4ee5\u6839\u636e\u4e0b\u9762\u4e09\u79cd\u60c5\u51b5\u8fdb\u884c\u9009\u62e9\uff1a</p> <ol> <li> <p>\u82e5\u63a5\u5165\u7684\u6240\u6709\u6a21\u578b\u63d0\u4f9b\u5546\u5747\u88ab\u5b98\u65b9 <code>init_chat_model</code> \u652f\u6301\uff0c\u8bf7\u76f4\u63a5\u4f7f\u7528\u5b98\u65b9\u51fd\u6570\uff0c\u4ee5\u83b7\u5f97\u6700\u4f73\u517c\u5bb9\u6027\u548c\u7a33\u5b9a\u6027\u3002</p> </li> <li> <p>\u82e5\u63a5\u5165\u7684\u90e8\u5206\u6a21\u578b\u63d0\u4f9b\u5546\u4e3a\u975e\u5b98\u65b9\u652f\u6301\uff0c\u53ef\u4f7f\u7528\u672c\u6a21\u5757\u7684\u529f\u80fd\uff0c\u5148\u5229\u7528<code>register_model_provider</code>\u6ce8\u518c\u6a21\u578b\u63d0\u4f9b\u5546\uff0c\u7136\u540e\u4f7f\u7528<code>load_chat_model</code>\u52a0\u8f7d\u6a21\u578b\u3002</p> </li> <li> <p>\u82e5\u63a5\u5165\u7684\u6a21\u578b\u63d0\u4f9b\u5546\u6682\u65e0\u9002\u5408\u7684\u96c6\u6210\uff0c\u4f46\u63d0\u4f9b\u5546\u63d0\u4f9b\u4e86 OpenAI \u517c\u5bb9\u7684 API\uff08\u5982 vLLM\uff09\uff0c\u5219\u63a8\u8350\u4f7f\u7528\u672c\u6a21\u5757\u7684\u529f\u80fd\uff0c\u5148\u5229\u7528<code>register_model_provider</code>\u6ce8\u518c\u6a21\u578b\u63d0\u4f9b\u5546\uff08chat_model\u4f20\u5165<code>openai-compatible</code>\uff09\uff0c\u7136\u540e\u4f7f\u7528<code>load_chat_model</code>\u52a0\u8f7d\u6a21\u578b\u3002</p> </li> </ol>"},{"location":"zh/getting-started-guide/embedding/","title":"\u5d4c\u5165\u6a21\u578b\u7ba1\u7406","text":""},{"location":"zh/getting-started-guide/embedding/#_2","title":"\u6982\u8ff0","text":"<p>LangChain \u7684 <code>init_embeddings</code> \u51fd\u6570\u4ec5\u652f\u6301\u6709\u9650\u7684\u5d4c\u5165\u6a21\u578b\u63d0\u4f9b\u5546\u3002\u672c\u5e93\u63d0\u4f9b\u66f4\u7075\u6d3b\u7684\u5d4c\u5165\u6a21\u578b\u7ba1\u7406\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u63a5\u5165\u672a\u5185\u7f6e\u652f\u6301\u7684\u5d4c\u5165\u670d\u52a1\uff08\u5982 vLLM \u7b49\uff09\u7684\u573a\u666f\u3002</p>"},{"location":"zh/getting-started-guide/embedding/#_3","title":"\u6ce8\u518c\u5d4c\u5165\u6a21\u578b\u63d0\u4f9b\u5546","text":"<p>\u6ce8\u518c\u5d4c\u5165\u6a21\u578b\u63d0\u4f9b\u5546\u9700\u8c03\u7528 <code>register_embeddings_provider</code>\u3002\u6839\u636e <code>embeddings_model</code> \u7c7b\u578b\u4e0d\u540c\uff0c\u6ce8\u518c\u65b9\u5f0f\u7565\u6709\u5dee\u5f02\u3002</p>"},{"location":"zh/getting-started-guide/embedding/#langchain","title":"\u5df2\u6709 LangChain \u5d4c\u5165\u6a21\u578b\u7c7b","text":"<p>\u82e5\u5d4c\u5165\u6a21\u578b\u63d0\u4f9b\u5546\u5df2\u6709\u73b0\u6210\u4e14\u5408\u9002\u7684 LangChain \u96c6\u6210\uff08\u8be6\u89c1 \u5d4c\u5165\u6a21\u578b\u96c6\u6210\u5217\u8868\uff09\uff0c\u8bf7\u5c06\u76f8\u5e94\u7684\u5d4c\u5165\u6a21\u578b\u7c7b\u76f4\u63a5\u4f20\u5165 <code>embeddings_model</code> \u53c2\u6570\u3002</p>"},{"location":"zh/getting-started-guide/embedding/#_4","title":"\u4ee3\u7801\u793a\u4f8b","text":"<pre><code>from langchain_core.embeddings.fake import FakeEmbeddings\nfrom langchain_dev_utils.embeddings import register_embeddings_provider\n\nregister_embeddings_provider(\n    provider_name=\"fake_provider\",\n    embeddings_model=FakeEmbeddings,\n)\n\n# FakeEmbeddings \u4ec5\u7528\u4e8e\u6d4b\u8bd5\uff0c\u5b9e\u9645\u4f7f\u7528\u4e2d\u5fc5\u987b\u4f20\u5165\u5177\u5907\u771f\u5b9e\u529f\u80fd\u7684 Embeddings \u7c7b\u3002\n</code></pre> <p>\u53c2\u6570\u8bbe\u7f6e\u8bf4\u660e</p> <p><code>provider_name</code>\u4ee3\u8868\u6a21\u578b\u63d0\u4f9b\u5546\u7684\u540d\u79f0\uff0c\u7528\u4e8e\u540e\u7eed\u5728 <code>load_embeddings</code> \u4e2d\u5f15\u7528\u3002<code>provider_name</code> \u5fc5\u987b\u4ee5\u5b57\u6bcd\u6216\u6570\u5b57\u5f00\u5934\uff0c\u53ea\u80fd\u5305\u542b\u5b57\u6bcd\u3001\u6570\u5b57\u548c\u4e0b\u5212\u7ebf\uff0c\u957f\u5ea6\u4e0d\u8d85\u8fc7 20 \u4e2a\u5b57\u7b26\u3002</p>"},{"location":"zh/getting-started-guide/embedding/#_5","title":"\u53ef\u9009\u53c2\u6570\u8bf4\u660e","text":"<p>base_url</p> <p>\u6b64\u53c2\u6570\u901a\u5e38\u65e0\u9700\u8bbe\u7f6e\uff08\u56e0\u4e3a\u5d4c\u5165\u6a21\u578b\u7c7b\u5185\u90e8\u4e00\u822c\u5df2\u5b9a\u4e49\u9ed8\u8ba4\u7684 API \u5730\u5740\uff09\uff0c\u4ec5\u5f53\u9700\u8981\u8986\u76d6\u5d4c\u5165\u6a21\u578b\u7c7b\u9ed8\u8ba4\u5730\u5740\u65f6\u624d\u4f20\u5165 <code>base_url</code>\uff0c\u4e14\u4ec5\u5bf9\u5b57\u6bb5\u540d\u4e3a <code>api_base</code> \u6216 <code>base_url</code>\uff08\u542b\u522b\u540d\uff09\u7684\u5c5e\u6027\u751f\u6548\u3002</p>"},{"location":"zh/getting-started-guide/embedding/#langchain-openai-api","title":"\u672a\u6709 LangChain \u5d4c\u5165\u6a21\u578b\u7c7b\uff0c\u4f46\u63d0\u4f9b\u5546\u652f\u6301 OpenAI \u517c\u5bb9 API","text":"<p>\u4e0e\u5bf9\u8bdd\u6a21\u578b\u7ba1\u7406\u7c7b\u4f3c\uff0c\u9700\u8981\u8bbe\u7f6e<code>embeddings_model</code>\u53d6\u503c\u4e3a<code>\"openai-compatible\"</code>\u3002</p>"},{"location":"zh/getting-started-guide/embedding/#_6","title":"\u4ee3\u7801\u793a\u4f8b","text":"<pre><code>register_embeddings_provider(\n    provider_name=\"vllm\",\n    embeddings_model=\"openai-compatible\",\n    base_url=\"http://localhost:8000/v1\"\n)\n</code></pre> <p>\u6ce8\u610f\uff1a\u5173\u4e8e\u8fd9\u90e8\u5206\u66f4\u591a\u7684\u7ec6\u8282\uff0c\u8bf7\u53c2\u8003OpenAI \u517c\u5bb9 API \u96c6\u6210\u3002</p>"},{"location":"zh/getting-started-guide/embedding/#_7","title":"\u6279\u91cf\u6ce8\u518c","text":"<p>\u82e5\u9700\u6ce8\u518c\u591a\u4e2a\u63d0\u4f9b\u5546\uff0c\u53ef\u4f7f\u7528 <code>batch_register_embeddings_provider</code>\u3002</p>"},{"location":"zh/getting-started-guide/embedding/#_8","title":"\u4ee3\u7801\u793a\u4f8b","text":"<pre><code>from langchain_dev_utils.embeddings import batch_register_embeddings_provider\nfrom langchain_core.embeddings.fake import FakeEmbeddings\n\nbatch_register_embeddings_provider(\n    providers=[\n        {\n            \"provider_name\": \"fake_provider\",\n            \"embeddings_model\": FakeEmbeddings,\n        },\n        {\n            \"provider_name\": \"vllm\",\n            \"embeddings_model\": \"openai-compatible\",\n            \"base_url\": \"http://localhost:8000/v1\",\n        },\n    ]\n)\n</code></pre> <p>\u6ce8\u610f</p> <p>\u4e24\u4e2a\u6ce8\u518c\u51fd\u6570\u5747\u57fa\u4e8e\u5168\u5c40\u5b57\u5178\u5b9e\u73b0\u3002\u5fc5\u987b\u5728\u5e94\u7528\u542f\u52a8\u9636\u6bb5\u5b8c\u6210\u6240\u6709\u6ce8\u518c\uff0c\u7981\u6b62\u8fd0\u884c\u65f6\u52a8\u6001\u6ce8\u518c\uff0c\u4ee5\u907f\u514d\u591a\u7ebf\u7a0b\u95ee\u9898\u3002 </p> <p>\u6b64\u5916\uff0c\u6ce8\u518c\u65f6\u82e5\u5c06 <code>embeddings_model</code> \u8bbe\u4e3a <code>openai-compatible</code>\uff0c\u5185\u90e8\u4f1a\u901a\u8fc7 <code>pydantic.create_model</code> \u52a8\u6001\u521b\u5efa\u65b0\u7684\u6a21\u578b\u7c7b\uff08\u4ee5 <code>BaseEmbeddingOpenAICompatible</code> \u4e3a\u57fa\u7c7b\uff0c\u751f\u6210\u5bf9\u5e94\u7684\u5d4c\u5165\u6a21\u578b\u96c6\u6210\u7c7b\uff09\uff0c\u6b64\u8fc7\u7a0b\u6d89\u53ca Python \u5143\u7c7b\u64cd\u4f5c\u548c pydantic \u9a8c\u8bc1\u903b\u8f91\u521d\u59cb\u5316\uff0c\u5b58\u5728\u4e00\u5b9a\u6027\u80fd\u5f00\u9500\uff0c\u56e0\u6b64\u8bf7\u907f\u514d\u5728\u8fd0\u884c\u671f\u9891\u7e41\u6ce8\u518c\u3002</p>"},{"location":"zh/getting-started-guide/embedding/#_9","title":"\u52a0\u8f7d\u5d4c\u5165\u6a21\u578b","text":"<p>\u4f7f\u7528 <code>load_embeddings</code> \u521d\u59cb\u5316\u5d4c\u5165\u6a21\u578b\u5b9e\u4f8b\u3002</p> <p>\u8be5\u51fd\u6570\u63a5\u6536 <code>model</code> \u53c2\u6570\u7528\u4e8e\u6307\u5b9a\u6a21\u578b\u540d\u79f0\uff0c\u53ef\u9009\u7684 <code>provider</code> \u53c2\u6570\u7528\u4e8e\u6307\u5b9a\u6a21\u578b\u63d0\u4f9b\u5546\uff1b\u8fd8\u53ef\u4f20\u5165\u4efb\u610f\u6570\u91cf\u7684\u5173\u952e\u5b57\u53c2\u6570\uff0c\u7528\u4e8e\u4f20\u9012\u5d4c\u5165\u6a21\u578b\u7c7b\u7684\u989d\u5916\u53c2\u6570\u3002</p>"},{"location":"zh/getting-started-guide/embedding/#_10","title":"\u53c2\u6570\u89c4\u5219","text":"<ul> <li>\u82e5\u672a\u4f20 <code>provider</code>\uff0c\u5219 <code>model</code> \u5fc5\u987b\u4e3a <code>provider_name:embeddings_name</code> \u683c\u5f0f\uff1b</li> <li>\u82e5\u4f20 <code>provider</code>\uff0c\u5219 <code>model</code> \u4ec5\u4e3a <code>embeddings_name</code>\u3002</li> </ul>"},{"location":"zh/getting-started-guide/embedding/#_11","title":"\u4ee3\u7801\u793a\u4f8b","text":"<pre><code># \u65b9\u5f0f\u4e00\uff1amodel \u5305\u542b provider \u4fe1\u606f\nembedding = load_embeddings(\"vllm:qwen3-embedding-4b\")\n\n# \u65b9\u5f0f\u4e8c\uff1a\u5355\u72ec\u6307\u5b9a provider\nembedding = load_embeddings(\"qwen3-embedding-4b\", provider=\"vllm\")\n</code></pre>"},{"location":"zh/getting-started-guide/embedding/#_12","title":"\u6a21\u578b\u65b9\u6cd5\u548c\u53c2\u6570","text":"<p>\u5bf9\u4e8e\u652f\u6301\u7684\u6a21\u578b\u65b9\u6cd5\u548c\u53c2\u6570\uff0c\u9700\u8981\u53c2\u8003\u5bf9\u5e94\u7684\u5d4c\u5165\u6a21\u578b\u7c7b\u7684\u4f7f\u7528\u8bf4\u660e\u3002\u5982\u679c\u91c7\u7528\u7684\u662f\u7b2c\u4e8c\u79cd\u60c5\u51b5\uff0c\u5219\u652f\u6301\u6240\u6709\u7684<code>OpenAIEmbeddings</code> \u7c7b\u7684\u65b9\u6cd5\u548c\u53c2\u6570\u3002</p>"},{"location":"zh/getting-started-guide/embedding/#_13","title":"\u517c\u5bb9\u5b98\u65b9\u63d0\u4f9b\u5546","text":"<p><code>load_embeddings</code> \u4f9d\u636e <code>provider</code>\u53c2\u6570\u67e5\u627e\u5168\u5c40\u6ce8\u518c\u5b57\u5178\uff1a\u547d\u4e2d\u5219\u4f7f\u7528\u8be5\u5b57\u5178\u5bf9\u5e94\u7684\u6a21\u578b\u7c7b\u5b9e\u4f8b\u5316\uff0c\u672a\u547d\u4e2d\u5219\u901a\u8fc7 <code>init_embeddings</code> \u521d\u59cb\u5316\u3002\u8fd9\u610f\u5473\u7740 LangChain \u5b98\u65b9\u652f\u6301\u7684\u63d0\u4f9b\u5546\uff08\u5982 openai\uff09\u53ef\u76f4\u63a5\u8c03\u7528\uff0c\u65e0\u9700\u6ce8\u518c\u3002</p> <pre><code>model = load_embeddings(\"openai:text-embedding-3-large\")\n# \u6216\nmodel = load_embeddings(\"text-embedding-3-large\", provider=\"openai\")\n</code></pre> <p>\u6700\u4f73\u5b9e\u8df5</p> <p>\u5bf9\u4e8e\u672c\u6a21\u5757\u7684\u4f7f\u7528\uff0c\u53ef\u4ee5\u6839\u636e\u4e0b\u9762\u4e09\u79cd\u60c5\u51b5\u8fdb\u884c\u9009\u62e9\uff1a</p> <ol> <li> <p>\u82e5\u63a5\u5165\u7684\u6240\u6709\u5d4c\u5165\u6a21\u578b\u63d0\u4f9b\u5546\u5747\u88ab\u5b98\u65b9 <code>init_embeddings</code> \u652f\u6301\uff0c\u8bf7\u76f4\u63a5\u4f7f\u7528\u5b98\u65b9\u51fd\u6570\uff0c\u4ee5\u83b7\u5f97\u6700\u4f73\u517c\u5bb9\u6027\u3002</p> </li> <li> <p>\u82e5\u63a5\u5165\u7684\u90e8\u5206\u5d4c\u5165\u6a21\u578b\u63d0\u4f9b\u5546\u4e3a\u975e\u5b98\u65b9\u652f\u6301\uff0c\u53ef\u5229\u7528\u672c\u6a21\u5757\u7684\u6ce8\u518c\u4e0e\u52a0\u8f7d\u673a\u5236\uff0c\u5148\u5229\u7528<code>register_embeddings_provider</code>\u6ce8\u518c\u6a21\u578b\u63d0\u4f9b\u5546\uff0c\u7136\u540e\u4f7f\u7528<code>load_embeddings</code>\u52a0\u8f7d\u6a21\u578b\u3002</p> </li> <li> <p>\u82e5\u63a5\u5165\u7684\u5d4c\u5165\u6a21\u578b\u63d0\u4f9b\u5546\u6682\u65e0\u9002\u5408\u7684\u96c6\u6210\uff0c\u4f46\u63d0\u4f9b\u5546\u63d0\u4f9b\u4e86 OpenAI \u517c\u5bb9\u7684 API\uff08\u5982 vLLM\uff09\uff0c\u5219\u63a8\u8350\u5229\u7528\u672c\u6a21\u5757\u7684\u529f\u80fd\uff0c\u5148\u5229\u7528<code>register_embeddings_provider</code>\u6ce8\u518c\u6a21\u578b\u63d0\u4f9b\u5546\uff08embeddings_model\u4f20\u5165<code>openai-compatible</code>\uff09\uff0c\u7136\u540e\u4f7f\u7528<code>load_embeddings</code>\u52a0\u8f7d\u6a21\u578b\u3002</p> </li> </ol>"},{"location":"zh/getting-started-guide/format/","title":"\u683c\u5f0f\u5316\u5e8f\u5217","text":""},{"location":"zh/getting-started-guide/format/#_2","title":"\u6982\u8ff0","text":"<p>\u7528\u4e8e\u5c06\u7531 Document\u3001Message \u6216\u5b57\u7b26\u4e32\u7ec4\u6210\u7684\u5217\u8868\u683c\u5f0f\u5316\u4e3a\u5355\u4e2a\u6587\u672c\u5b57\u7b26\u4e32\u3002\u5177\u4f53\u51fd\u6570\u4e3a <code>format_sequence</code>\u3002</p>"},{"location":"zh/getting-started-guide/format/#_3","title":"\u4f7f\u7528\u793a\u4f8b","text":""},{"location":"zh/getting-started-guide/format/#_4","title":"\u5e94\u7528\u573a\u666f","text":"<ul> <li>\u5c06\u5bf9\u8bdd\u5386\u53f2\uff08system/human/ai/tool\uff09\u538b\u6210\u4e00\u6bb5\u53ef\u8bfb\u6587\u672c\uff0c\u4fbf\u4e8e\u6ce8\u5165\u5230\u4e0b\u4e00\u8f6e\u63d0\u793a\u8bcd\u3002</li> <li>\u6253\u5370\u8c03\u8bd5\uff1a\u5c06\u6d88\u606f\u5217\u8868\u6253\u5370\u6210\u53ef\u8bfb\u683c\u5f0f\uff0c\u65b9\u4fbf\u5728\u65e5\u5fd7\u91cc\u67e5\u770b\u3002</li> </ul>"},{"location":"zh/getting-started-guide/format/#_5","title":"\u4ee3\u7801\u793a\u4f8b","text":"<pre><code>from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, ToolMessage\n\nfrom langchain_dev_utils.message_convert import format_sequence\n\nformated1 = format_sequence(\n    [\n        SystemMessage(content=\"\u4f60\u662f\u4e00\u4e2a\u5929\u6c14\u67e5\u8be2\u52a9\u624b\"),\n        HumanMessage(content=\"\u67e5\u8be2\u4e00\u4e0b\u4f26\u6566\u548c\u65e7\u91d1\u5c71\u7684\u5929\u6c14\"),\n        AIMessage(\n            content=\"\u6211\u5c06\u4f7f\u7528get_weather\u5de5\u5177\u67e5\u8be2\u8fd9\u4e24\u4e2a\u57ce\u5e02\u7684\u5929\u6c14\",\n            tool_calls=[\n                {\"name\": \"get_weather\", \"args\": {\"location\": \"\u4f26\u6566\"}, \"id\": \"123\"},\n                {\"name\": \"get_weather\", \"args\": {\"location\": \"\u65e7\u91d1\u5c71\"}, \"id\": \"456\"},\n            ],\n        ),\n        ToolMessage(\n            content=\"\u4f26\u6566\u7684\u5929\u6c14\u662f25\u6444\u6c0f\u5ea6\",\n            tool_call_id=\"123\",\n        ),\n        ToolMessage(\n            content=\"\u65e7\u91d1\u5c71\u7684\u5929\u6c14\u662f22\u6444\u6c0f\u5ea6\",\n            tool_call_id=\"456\",\n        ),\n        AIMessage(\n            content=\"\u6839\u636e\u5de5\u5177\u8c03\u7528\u7684\u7ed3\u679c\uff0c\u4f26\u6566\u7684\u5929\u6c14\u662f25\u6444\u6c0f\u5ea6\uff0c\u65e7\u91d1\u5c71\u7684\u5929\u6c14\u662f22\u6444\u6c0f\u5ea6\",\n        ),\n    ]\n    # separator \u53c2\u6570\u9ed8\u8ba4\u4e3a \"-\"\uff0cwith_num \u53c2\u6570\u9ed8\u8ba4\u4e3a False\uff0c\u6b64\u5904\u4f7f\u7528\u9ed8\u8ba4\u503c\n)\nprint(formated1)\n</code></pre>"},{"location":"zh/getting-started-guide/format/#_6","title":"\u8f93\u51fa\u7ed3\u679c","text":"<pre><code>-System: \u4f60\u662f\u4e00\u4e2a\u5929\u6c14\u67e5\u8be2\u52a9\u624b\n-Human: \u67e5\u8be2\u4e00\u4e0b\u4f26\u6566\u548c\u65e7\u91d1\u5c71\u7684\u5929\u6c14\n-AI: \u6211\u5c06\u4f7f\u7528get_weather\u5de5\u5177\u67e5\u8be2\u8fd9\u4e24\u4e2a\u57ce\u5e02\u7684\u5929\u6c14\n&lt;tool_call&gt;get_weather&lt;/tool_call&gt;\n&lt;tool_call&gt;get_weather&lt;/tool_call&gt;\n-Tool: \u4f26\u6566\u7684\u5929\u6c14\u662f25\u6444\u6c0f\u5ea6\n-Tool: \u65e7\u91d1\u5c71\u7684\u5929\u6c14\u662f22\u6444\u6c0f\u5ea6\n-AI: \u6839\u636e\u5de5\u5177\u8c03\u7528\u7684\u7ed3\u679c\uff0c\u4f26\u6566\u7684\u5929\u6c14\u662f25\u6444\u6c0f\u5ea6\uff0c\u65e7\u91d1\u5c71\u7684\u5929\u6c14\u662f22\u6444\u6c0f\u5ea6\n</code></pre>"},{"location":"zh/getting-started-guide/format/#_7","title":"\u5e94\u7528\u573a\u666f","text":"<ul> <li>RAG\uff1a\u628a\u68c0\u7d22\u8fd4\u56de\u7684 <code>Document</code> \u5217\u8868\u683c\u5f0f\u5316\u4e3a\u4e00\u6bb5 <code>context</code> \u6587\u672c\uff0c\u76f4\u63a5\u62fc\u8fdb\u63d0\u793a\u8bcd\u3002</li> </ul>"},{"location":"zh/getting-started-guide/format/#_8","title":"\u4ee3\u7801\u793a\u4f8b","text":"<pre><code>from langchain_core.documents import Document\n\nfrom langchain_dev_utils.message_convert import format_sequence\n\nformat2 = format_sequence(\n    [\n        Document(page_content=\"\u3010\u6765\u6e90: \u4ea7\u54c1\u624b\u518c\u3011\u9000\u6b3e\u653f\u7b56\uff1a7 \u5929\u5185\u53ef\u65e0\u7406\u7531\u9000\u6b3e\u3002\"),\n        Document(page_content=\"\u3010\u6765\u6e90: FAQ\u3011\u9000\u6b3e\u5230\u8d26\u4e00\u822c\u9700\u8981 1-3 \u4e2a\u5de5\u4f5c\u65e5\u3002\"),\n        Document(page_content=\"\u3010\u6765\u6e90: \u5ba2\u670d\u89c4\u8303\u3011\u9047\u5230\u4e89\u8bae\u5148\u81f4\u6b49\u5e76\u5f15\u5bfc\u63d0\u4ea4\u8ba2\u5355\u53f7\u3002\"),\n    ],\n    separator=\"&gt;\", # \u6307\u5b9a\u5206\u9694\u7b26\u4e3a \"&gt;\"\uff0c\u6bcf\u884c\u6587\u6863\u524d\u90fd\u4f1a\u52a0\u4e0a\u8fd9\u4e2a\u7b26\u53f7\n    with_num=True,  # \u5f00\u542f\u5e8f\u53f7\uff0c\u6587\u6863\u5c06\u6309\u987a\u5e8f\u7f16\u53f7\u4e3a 1, 2, 3...\n)\nprint(format2)\n</code></pre>"},{"location":"zh/getting-started-guide/format/#_9","title":"\u8f93\u51fa\u7ed3\u679c","text":"<pre><code>&gt;1. \u3010\u6765\u6e90: \u4ea7\u54c1\u624b\u518c\u3011\u9000\u6b3e\u653f\u7b56\uff1a7 \u5929\u5185\u53ef\u65e0\u7406\u7531\u9000\u6b3e\u3002\n&gt;2. \u3010\u6765\u6e90: FAQ\u3011\u9000\u6b3e\u5230\u8d26\u4e00\u822c\u9700\u8981 1-3 \u4e2a\u5de5\u4f5c\u65e5\u3002\n&gt;3. \u3010\u6765\u6e90: \u5ba2\u670d\u89c4\u8303\u3011\u9047\u5230\u4e89\u8bae\u5148\u81f4\u6b49\u5e76\u5f15\u5bfc\u63d0\u4ea4\u8ba2\u5355\u53f7\u3002\n</code></pre>"},{"location":"zh/getting-started-guide/format/#_10","title":"\u5e94\u7528\u573a\u666f","text":"<ul> <li>\u5c06\u4e00\u7ec4\u8981\u70b9\uff08\u9700\u6c42\u70b9\u3001\u68c0\u67e5\u9879\u3001\u5f85\u529e\u5217\u8868\u7b49\uff09\u683c\u5f0f\u5316\u6210\u591a\u884c\u6587\u672c\uff0c\u65b9\u4fbf\u62fc\u8fdb\u63d0\u793a\u8bcd\u3002</li> </ul>"},{"location":"zh/getting-started-guide/format/#_11","title":"\u4ee3\u7801\u793a\u4f8b","text":"<pre><code>from langchain_dev_utils.message_convert import format_sequence\n\nformat3 = format_sequence(\n    [\n        \"\u53ea\u56de\u7b54\u7528\u6237\u95ee\u9898\uff0c\u4e0d\u8981\u6269\u5c55\",\n        \"\u4e0d\u786e\u5b9a\u65f6\u660e\u786e\u8bf4\u660e\u5047\u8bbe\",\n        \"\u8f93\u51fa\u4f7f\u7528 Markdown \u5217\u8868\",\n    ],\n    separator=\"&gt;\",\n    with_num=True,\n)\nprint(format3)\n</code></pre>"},{"location":"zh/getting-started-guide/format/#_12","title":"\u8f93\u51fa\u7ed3\u679c","text":"<pre><code>&gt;1. \u53ea\u56de\u7b54\u7528\u6237\u95ee\u9898\uff0c\u4e0d\u8981\u6269\u5c55\n&gt;2. \u4e0d\u786e\u5b9a\u65f6\u660e\u786e\u8bf4\u660e\u5047\u8bbe\n&gt;3. \u8f93\u51fa\u4f7f\u7528 Markdown \u5217\u8868\n</code></pre>"},{"location":"zh/getting-started-guide/human-in-the-loop/","title":"\u4e3a\u5de5\u5177\u8c03\u7528\u6dfb\u52a0\u4eba\u5de5\u5ba1\u6838","text":""},{"location":"zh/getting-started-guide/human-in-the-loop/#_2","title":"\u6982\u8ff0","text":"<p>\u672c\u5e93\u63d0\u4f9b\u4e86\u88c5\u9970\u5668\u51fd\u6570\uff0c\u7528\u4e8e\u4e3a\u5de5\u5177\u8c03\u7528\u6dfb\u52a0\"\u4eba\u5728\u56de\u8def\"\u5ba1\u6838\u652f\u6301\uff0c\u5728\u5de5\u5177\u6267\u884c\u671f\u95f4\u542f\u7528\u4eba\u5de5\u5ba1\u6838\u3002</p> \u88c5\u9970\u5668 \u9002\u7528\u573a\u666f <code>human_in_the_loop</code> \u7528\u4e8e\u540c\u6b65\u5de5\u5177\u51fd\u6570 <code>human_in_the_loop_async</code> \u7528\u4e8e\u5f02\u6b65\u5de5\u5177\u51fd\u6570"},{"location":"zh/getting-started-guide/human-in-the-loop/#_3","title":"\u4f7f\u7528\u793a\u4f8b","text":""},{"location":"zh/getting-started-guide/human-in-the-loop/#handler","title":"\u4f7f\u7528\u9ed8\u8ba4\u7684 handler","text":"<pre><code>from langchain_dev_utils.tool_calling import human_in_the_loop\nimport datetime\n\n\n@human_in_the_loop\ndef get_current_time() -&gt; str:\n    \"\"\"\u83b7\u53d6\u5f53\u524d\u65f6\u95f4\u6233\"\"\"\n    return str(datetime.datetime.now().timestamp())\n</code></pre>"},{"location":"zh/getting-started-guide/human-in-the-loop/#_4","title":"\u5f02\u6b65\u5de5\u5177\u793a\u4f8b","text":"<pre><code>from langchain_dev_utils.tool_calling import human_in_the_loop_async\nimport asyncio\nimport datetime\n\n\n@human_in_the_loop_async\nasync def async_get_current_time() -&gt; str:\n    \"\"\"\u5f02\u6b65\u83b7\u53d6\u5f53\u524d\u65f6\u95f4\u6233\"\"\"\n    await asyncio.sleep(1)\n    return str(datetime.datetime.now().timestamp())\n</code></pre>"},{"location":"zh/getting-started-guide/human-in-the-loop/#handler_1","title":"\u9ed8\u8ba4 handler \u7684\u5b9e\u73b0","text":"<p>\u9ed8\u8ba4 handler \u7684\u5b9e\u73b0\u5982\u4e0b\uff1a</p> <pre><code>def _get_human_in_the_loop_request(params: InterruptParams) -&gt; dict[str, Any]:\n    return {\n        \"action_request\": {\n            \"action\": params[\"tool_call_name\"],\n            \"args\": params[\"tool_call_args\"],\n        },\n        \"config\": {\n            \"allow_accept\": True,\n            \"allow_edit\": True,\n            \"allow_respond\": True,\n        },\n        \"description\": f\"Please review tool call: {params['tool_call_name']}\",\n    }\n\n\ndef default_handler(params: InterruptParams) -&gt; Any:\n    request = _get_human_in_the_loop_request(params)\n    response = interrupt(request)\n\n    if response[\"type\"] == \"accept\":\n        return params[\"tool\"].invoke(params[\"tool_call_args\"])\n    elif response[\"type\"] == \"edit\":\n        updated_args = response[\"args\"]\n        return params[\"tool\"].invoke(updated_args)\n    elif response[\"type\"] == \"response\":\n        return response[\"args\"]\n    else:\n        raise ValueError(f\"Unsupported interrupt response type: {response['type']}\")\n</code></pre>"},{"location":"zh/getting-started-guide/human-in-the-loop/#_5","title":"\u4e2d\u65ad\u8bf7\u6c42\u683c\u5f0f","text":"<p>\u4e2d\u65ad\u65f6\u4f1a\u53d1\u9001\u5982\u4e0b JSON Schema \u683c\u5f0f\u7684\u8bf7\u6c42\uff1a</p> \u5b57\u6bb5 \u8bf4\u660e <code>action_request.action</code> \u5de5\u5177\u8c03\u7528\u540d\u79f0\u3002\u7c7b\u578b: <code>str</code> <code>action_request.args</code> \u5de5\u5177\u8c03\u7528\u53c2\u6570\u3002\u7c7b\u578b: <code>dict</code> <code>config.allow_accept</code> \u662f\u5426\u5141\u8bb8\u63a5\u53d7\u64cd\u4f5c\u3002\u7c7b\u578b: <code>bool</code> <code>config.allow_edit</code> \u662f\u5426\u5141\u8bb8\u7f16\u8f91\u53c2\u6570\u3002\u7c7b\u578b: <code>bool</code> <code>config.allow_respond</code> \u662f\u5426\u5141\u8bb8\u76f4\u63a5\u54cd\u5e94\u3002\u7c7b\u578b: <code>bool</code> <code>description</code> \u64cd\u4f5c\u63cf\u8ff0\u3002\u7c7b\u578b: <code>str</code>"},{"location":"zh/getting-started-guide/human-in-the-loop/#_6","title":"\u4e2d\u65ad\u54cd\u5e94\u683c\u5f0f","text":"<p>\u54cd\u5e94\u65f6\u9700\u8981\u8fd4\u56de\u5982\u4e0b JSON Schema \u683c\u5f0f\u7684\u6570\u636e\uff1a</p> \u5b57\u6bb5 \u8bf4\u660e <code>type</code> \u54cd\u5e94\u7c7b\u578b\uff0c\u53ef\u9009\u503c\u4e3a <code>accept</code>\u3001<code>edit</code>\u3001<code>response</code>\u3002\u7c7b\u578b: <code>str</code>\u5fc5\u586b: \u662f <code>args</code> \u5f53 <code>type</code> \u4e3a <code>edit</code> \u6216 <code>response</code> \u65f6\uff0c\u5305\u542b\u66f4\u65b0\u540e\u7684\u53c2\u6570\u6216\u54cd\u5e94\u5185\u5bb9\u3002\u7c7b\u578b: <code>dict</code>\u5fc5\u586b: \u5426"},{"location":"zh/getting-started-guide/human-in-the-loop/#handler_2","title":"\u81ea\u5b9a\u4e49 Handler \u793a\u4f8b","text":"<p>\u4f60\u53ef\u4ee5\u5b8c\u5168\u63a7\u5236\u4e2d\u65ad\u884c\u4e3a\uff0c\u4f8b\u5982\u53ea\u5141\u8bb8\"\u63a5\u53d7/\u62d2\u7edd\"\uff0c\u6216\u81ea\u5b9a\u4e49\u63d0\u793a\u8bed\uff1a</p> <pre><code>from typing import Any\nfrom langchain_dev_utils.tool_calling import human_in_the_loop_async, InterruptParams\nfrom langgraph.types import interrupt\n\n\nasync def custom_handler(params: InterruptParams) -&gt; Any:\n    response = interrupt(\n        f\"\u6211\u8981\u8c03\u7528\u5de5\u5177 {params['tool_call_name']}\uff0c\u53c2\u6570\u4e3a {params['tool_call_args']}\uff0c\u8bf7\u786e\u8ba4\u662f\u5426\u8c03\u7528\"\n    )\n    if response[\"type\"] == \"accept\":\n        return await params[\"tool\"].ainvoke(params[\"tool_call_args\"])\n    elif response[\"type\"] == \"reject\":\n        return \"\u7528\u6237\u62d2\u7edd\u8c03\u7528\u8be5\u5de5\u5177\"\n    else:\n        raise ValueError(f\"\u4e0d\u652f\u6301\u7684\u54cd\u5e94\u7c7b\u578b: {response['type']}\")\n\n\n@human_in_the_loop_async(handler=custom_handler)\nasync def get_weather(city: str) -&gt; str:\n    \"\"\"\u83b7\u53d6\u5929\u6c14\u4fe1\u606f\"\"\"\n    return f\"{city}\u5929\u6c14\u6674\u6717\"\n</code></pre> <p>\u6700\u4f73\u5b9e\u8df5</p> <p>\u8be5\u88c5\u9970\u5668\u5728\u5b9e\u73b0\u81ea\u5b9a\u4e49\u4eba\u5728\u56de\u8def\u903b\u8f91\u65f6\uff0c\u9700\u8981\u4f20\u5165 <code>handler</code> \u53c2\u6570\u3002\u6b64 <code>handler</code> \u53c2\u6570\u662f\u4e00\u4e2a\u51fd\u6570\uff0c\u5185\u90e8\u5fc5\u987b\u4f7f\u7528 LangGraph \u7684 <code>interrupt</code> \u51fd\u6570\u6765\u6267\u884c\u4e2d\u65ad\u64cd\u4f5c\u3002\u56e0\u6b64\uff0c\u5982\u679c\u4ec5\u4e3a\u5355\u4e2a\u5de5\u5177\u6dfb\u52a0\u81ea\u5b9a\u4e49\u7684\u4eba\u5728\u56de\u8def\u903b\u8f91\uff0c\u5efa\u8bae\u76f4\u63a5\u4f7f\u7528 LangGraph \u7684 <code>interrupt</code> \u51fd\u6570\u3002\u5f53\u591a\u4e2a\u5de5\u5177\u9700\u8981\u76f8\u540c\u81ea\u5b9a\u4e49\u4eba\u5728\u56de\u8def\u903b\u8f91\u65f6\uff0c\u4f7f\u7528\u672c\u88c5\u9970\u5668\u53ef\u4ee5\u6709\u6548\u907f\u514d\u4ee3\u7801\u91cd\u590d\u3002</p>"},{"location":"zh/getting-started-guide/installation/","title":"\u5b89\u88c5","text":"<p><code>langchain-dev-utils</code>\u652f\u6301\u4f7f\u7528<code>pip</code>\u3001<code>poetry</code>\u3001<code>uv</code>\u7b49\u591a\u79cd\u5305\u7ba1\u7406\u5668\u8fdb\u884c\u5b89\u88c5\u3002</p> <p>\u5b89\u88c5\u57fa\u7840\u7248\u672c\u7684<code>langchain-dev-utils</code>\uff1a</p> pippoetryuv <pre><code>pip install -U langchain-dev-utils\n</code></pre> <pre><code>poetry add langchain-dev-utils\n</code></pre> <pre><code>uv add langchain-dev-utils\n</code></pre> <p>\u5b89\u88c5\u5b8c\u6574\u529f\u80fd\u7248\u672c\u7684<code>langchain-dev-utils</code>\uff1a</p> pippoetryuv <pre><code>pip install -U \"langchain-dev-utils[standard]\"\n</code></pre> <pre><code>poetry add \"langchain-dev-utils[standard]\"\n</code></pre> <pre><code>uv add langchain-dev-utils[standard]\n</code></pre>"},{"location":"zh/getting-started-guide/installation/#_2","title":"\u9a8c\u8bc1\u5b89\u88c5","text":"<p>\u5b89\u88c5\u540e\uff0c\u9a8c\u8bc1\u5305\u662f\u5426\u6b63\u786e\u5b89\u88c5\uff1a</p> <pre><code>import langchain_dev_utils\nprint(langchain_dev_utils.__version__)\n</code></pre>"},{"location":"zh/getting-started-guide/installation/#_3","title":"\u4f9d\u8d56\u9879","text":"<p>\u8be5\u5305\u4f1a\u81ea\u52a8\u5b89\u88c5\u4ee5\u4e0b\u4f9d\u8d56\u9879\uff1a</p> <ul> <li><code>langchain</code></li> <li><code>langgraph</code> (\u5b89\u88c5<code>langchain</code>\u65f6\u4f1a\u540c\u65f6\u4e5f\u4f1a\u5b89\u88c5)</li> </ul> <p>\u5982\u679c\u662f standard \u7248\u672c\uff0c\u8fd8\u4f1a\u5b89\u88c5\u4ee5\u4e0b\u4f9d\u8d56\u9879\uff1a</p> <ul> <li><code>langchain-openai</code>\uff08\u7528\u4e8e\u6a21\u578b\u7ba1\u7406\uff09</li> <li><code>json-repair</code>(\u7528\u4e8e\u4e2d\u95f4\u4ef6\u7684\u5de5\u5177\u8c03\u7528\u9519\u8bef\u4fee\u590d)</li> <li><code>jinja2</code>(\u7528\u4e8e\u4e2d\u95f4\u4ef6\u7684\u683c\u5f0f\u5316\u7cfb\u7edf\u63d0\u793a\u8bcd\u6a21\u677f)</li> </ul>"},{"location":"zh/getting-started-guide/message/","title":"\u6d88\u606f\u5904\u7406","text":""},{"location":"zh/getting-started-guide/message/#_2","title":"\u6982\u8ff0","text":"<p>\u4e3b\u8981\u529f\u80fd\u5305\u62ec\uff1a</p> <ul> <li>\u5408\u5e76\u63a8\u7406\u5185\u5bb9\u81f3\u6700\u7ec8\u56de\u590d</li> <li>\u5408\u5e76\u6d41\u5f0f\u8f93\u51fa\u7684 Chunks</li> </ul>"},{"location":"zh/getting-started-guide/message/#_3","title":"\u5408\u5e76\u63a8\u7406\u5185\u5bb9\u81f3\u6700\u7ec8\u56de\u590d","text":"<p>\u7528\u4e8e\u5c06\u63a8\u7406\u5185\u5bb9\uff08<code>reasoning_content</code>\uff09\u5408\u5e76\u81f3\u6700\u7ec8\u56de\u590d\uff08<code>content</code>\uff09\u3002</p>"},{"location":"zh/getting-started-guide/message/#_4","title":"\u529f\u80fd\u8bf4\u660e","text":"\u51fd\u6570 \u8bf4\u660e <code>convert_reasoning_content_for_ai_message</code> \u5c06 AIMessage \u4e2d\u7684\u63a8\u7406\u5185\u5bb9\u5408\u5e76\u5230\u5185\u5bb9\u5b57\u6bb5\uff08\u7528\u4e8e\u6a21\u578b\u7684 invoke \u548c ainvoke\uff09 <code>convert_reasoning_content_for_chunk_iterator</code> \u5c06\u6d41\u5f0f\u54cd\u5e94\u4e2d\u7684\u63a8\u7406\u5185\u5bb9\u5408\u5e76\u5230\u5185\u5bb9\u5b57\u6bb5\uff08\u7528\u4e8e\u6a21\u578b\u7684 stream\uff09 <code>aconvert_reasoning_content_for_chunk_iterator</code> <code>convert_reasoning_content_for_chunk_iterator</code> \u7684\u5f02\u6b65\u7248\u672c\uff0c\u7528\u4e8e\u5f02\u6b65\u6d41\u5f0f\u5904\u7406\uff08\u7528\u4e8e\u6a21\u578b\u7684 astream\uff09"},{"location":"zh/getting-started-guide/message/#_5","title":"\u4ee3\u7801\u793a\u4f8b","text":"<pre><code>from langchain_dev_utils.message_convert import (\n    convert_reasoning_content_for_ai_message,\n    convert_reasoning_content_for_chunk_iterator,\n)\n\nresponse = model.invoke(\"\u4f60\u597d\")\nconverted_response = convert_reasoning_content_for_ai_message(\n    response, think_tag=(\"&lt;start&gt;\", \"&lt;end&gt;\")\n)\nprint(converted_response.content)\n\nfor chunk in convert_reasoning_content_for_chunk_iterator(\n    model.stream(\"\u4f60\u597d\"), think_tag=(\"&lt;start&gt;\", \"&lt;end&gt;\")\n):\n    print(chunk.content, end=\"\", flush=True)\n</code></pre>"},{"location":"zh/getting-started-guide/message/#chunks","title":"\u5408\u5e76\u6d41\u5f0f\u8f93\u51fa\u7684 Chunks","text":"<p>\u63d0\u4f9b\u5c06\u591a\u4e2a\u56e0\u4e3a\u6d41\u5f0f\u8f93\u51fa\u800c\u4ea7\u751f\u7684 AIMessageChunk \u5408\u5e76\u4e3a\u5355\u4e2a AIMessage \u7684\u5de5\u5177\u51fd\u6570\u3002</p>"},{"location":"zh/getting-started-guide/message/#_6","title":"\u6838\u5fc3\u51fd\u6570","text":"\u51fd\u6570 \u8bf4\u660e <code>merge_ai_message_chunk</code> \u5408\u5e76 AI \u6d88\u606f\u5757"},{"location":"zh/getting-started-guide/message/#_7","title":"\u4ee3\u7801\u793a\u4f8b","text":"<pre><code>from langchain_dev_utils.message_convert import merge_ai_message_chunk\n\nchunks = []\nfor chunk in model.stream(\"\u4f60\u597d\"):\n    chunks.append(chunk)\n\nmerged_message = merge_ai_message_chunk(chunks)\nprint(merged_message)\n</code></pre>"},{"location":"zh/getting-started-guide/tool/","title":"\u5de5\u5177\u8c03\u7528\u5904\u7406","text":""},{"location":"zh/getting-started-guide/tool/#_2","title":"\u6982\u8ff0","text":"<p>\u63d0\u4f9b\u68c0\u6d4b\u4ee5\u53ca\u89e3\u6790\u5de5\u5177\u8c03\u7528\u53c2\u6570\u7684\u5b9e\u7528\u5de5\u5177\u3002</p>"},{"location":"zh/getting-started-guide/tool/#_3","title":"\u68c0\u6d4b\u5de5\u5177\u8c03\u7528","text":"<p>\u68c0\u6d4b\u6d88\u606f\u662f\u5426\u5305\u542b\u5de5\u5177\u8c03\u7528\uff0c\u6838\u5fc3\u51fd\u6570\u662f <code>has_tool_calling</code>\u3002</p>"},{"location":"zh/getting-started-guide/tool/#_4","title":"\u4ee3\u7801\u793a\u4f8b","text":"<pre><code>import datetime\nfrom langchain_core.tools import tool\nfrom langchain_dev_utils.tool_calling import has_tool_calling\n\n@tool\ndef get_current_time() -&gt; str:\n    \"\"\"\u83b7\u53d6\u5f53\u524d\u65f6\u95f4\u6233\"\"\"\n    return str(datetime.datetime.now().timestamp())\n\nresponse = model.bind_tools([get_current_time]).invoke(\"\u73b0\u5728\u51e0\u70b9\u4e86\uff1f\")\nprint(has_tool_calling(response))\n</code></pre>"},{"location":"zh/getting-started-guide/tool/#_5","title":"\u89e3\u6790\u5de5\u5177\u8c03\u7528\u53c2\u6570","text":"<p>\u63d0\u4f9b\u4e00\u4e2a\u5b9e\u7528\u51fd\u6570\u6765\u89e3\u6790\u5de5\u5177\u8c03\u7528\u53c2\u6570\uff0c\u4ece\u6d88\u606f\u4e2d\u63d0\u53d6\u53c2\u6570\u4fe1\u606f\uff0c\u6838\u5fc3\u51fd\u6570\u662f <code>parse_tool_calling</code>\u3002</p>"},{"location":"zh/getting-started-guide/tool/#_6","title":"\u4ee3\u7801\u793a\u4f8b","text":"<pre><code>import datetime\nfrom langchain_core.tools import tool\nfrom langchain_dev_utils.tool_calling import has_tool_calling, parse_tool_calling\n\n@tool\ndef get_current_time() -&gt; str:\n    \"\"\"\u83b7\u53d6\u5f53\u524d\u65f6\u95f4\u6233\"\"\"\n    return str(datetime.datetime.now().timestamp())\n\nresponse = model.bind_tools([get_current_time]).invoke(\"\u73b0\u5728\u51e0\u70b9\u4e86\uff1f\")\n\nif has_tool_calling(response):\n    name, args = parse_tool_calling(\n        response, first_tool_call_only=True\n    )\n    print(name, args)\n</code></pre>"},{"location":"zh/resource/coding/","title":"\u4ee5\u7f16\u7a0b\u65b9\u5f0f\u8c03\u7528\u6587\u6863","text":"<p>\u672c\u6587\u6863\u65e8\u5728\u6df1\u5ea6\u96c6\u6210 AI \u5de5\u4f5c\u6d41\u3002\u60a8\u53ef\u4ee5\u901a\u8fc7 AI \u52a9\u624b\u3001IDE \u63d2\u4ef6\u6216 Model Context Protocol (MCP) \u5c06\u672c\u6587\u6863\u65e0\u7f1d\u63a5\u5165\u5f00\u53d1\u73af\u5883\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u81ea\u52a8\u5316\u7684\u5185\u5bb9\u68c0\u7d22\u4e0e\u8c03\u7528\u3002</p>"},{"location":"zh/resource/coding/#markdown","title":"\u5feb\u901f\u83b7\u53d6 Markdown \u5185\u5bb9","text":"<p>\u6587\u6863\u9875\u9762\u53f3\u4e0a\u89d2\u63d0\u4f9b\u4e86\u4fbf\u6377\u7684\u590d\u5236\u6309\u94ae\uff1a</p> <p> </p> <p>\u64cd\u4f5c\u6b65\u9aa4\uff1a</p> <ol> <li> <p>\u70b9\u51fb\u53f3\u4e0a\u89d2\u7684\u590d\u5236\u6309\u94ae\u3002</p> </li> <li> <p>\u5f53\u524d\u9875\u9762\u7684\u5b8c\u6574 Markdown \u5185\u5bb9\u5c06\u81ea\u52a8\u590d\u5236\u81f3\u526a\u8d34\u677f\u3002</p> </li> <li> <p>\u60a8\u53ef\u76f4\u63a5\u5c06\u5176\u7c98\u8d34\u81f3\u4ee3\u7801\u7f16\u8f91\u5668\uff08\u5982 VS Code\uff09\u6216 AI \u52a9\u624b\uff08\u5982 ChatGPT\u3001Claude\uff09\u7684\u5bf9\u8bdd\u6846\u4e2d\u5f00\u59cb\u4f7f\u7528\u3002</p> </li> </ol>"},{"location":"zh/resource/coding/#mcp","title":"\u901a\u8fc7 MCP \u96c6\u6210","text":"<p>\u672c\u6587\u6863\u5df2\u540c\u6b65\u81f3 Context7 \u5e73\u53f0\uff0c\u652f\u6301\u901a\u8fc7 Model Context Protocol (MCP) \u5de5\u5177\u8fdb\u884c\u76f4\u63a5\u8c03\u7528\u3002</p> <p>\u5982\u4f55\u914d\u7f6e\uff1a \u8bf7\u53c2\u9605 Context7 MCP \u5de5\u5177\u914d\u7f6e\u6587\u6863\uff0c\u4e86\u89e3\u5982\u4f55\u5728\u60a8\u7684\u4ee3\u7801\u7f16\u8f91\u5668\u6216 AI \u52a9\u624b\u4e2d\u5b8c\u6210 MCP \u5de5\u5177\u7684\u8fde\u63a5\u4e0e\u914d\u7f6e\u3002</p> <p>\u68c0\u7d22\u63d0\u793a</p> <p>\u7531\u4e8e Context7 \u6587\u6863\u5e93\u6536\u5f55\u4e86\u672c\u9879\u76ee\u7684\u4e09\u4e2a\u5386\u53f2\u7248\u672c\uff0c\u4e3a\u4e86\u786e\u4fdd AI \u83b7\u53d6\u7684\u662f\u6700\u65b0\u5185\u5bb9\uff0c\u5efa\u8bae\u5728\u63d0\u793a\u8bcd\u4e2d\u660e\u786e\u6307\u5b9a\uff1a\u201c\u8bf7\u4f18\u5148\u67e5\u9605\u6700\u65b0\u66f4\u65b0\u7684\u6587\u6863\u7248\u672c\u201d\u3002</p>"},{"location":"zh/resource/coding/#_2","title":"\u6700\u4f73\u5b9e\u8df5\u5efa\u8bae","text":"<p>\u9274\u4e8e Context7 \u7684\u5185\u5bb9\u540c\u6b65\u5468\u671f\u7ea6\u4e3a 2 \u5468\uff0c\u68c0\u7d22\u7ed3\u679c\u53ef\u80fd\u5b58\u5728\u65f6\u6548\u6027\u5dee\u5f02\u3002\u4e3a\u786e\u4fdd\u5185\u5bb9\u7684\u51c6\u786e\u6027\u4e0e\u5b9e\u65f6\u6027\uff0c\u5f3a\u70c8\u5efa\u8bae\u4f18\u5148\u4f7f\u7528\u9875\u9762\u53f3\u4e0a\u89d2\u7684\u201c\u590d\u5236\u201d\u6309\u94ae\u83b7\u53d6\u6700\u65b0 Markdown \u5185\u5bb9\uff0c\u800c\u975e\u4f9d\u8d56 Context7 \u7684\u7f13\u5b58\u6570\u636e\u3002</p>"},{"location":"zh/resource/example-project/","title":"Langchain-dev-utils Example Project","text":"<p>\u8be5\u4ed3\u5e93\u63d0\u4f9b\u4e86\u4e00\u4e2a\u793a\u4f8b\u9879\u76ee<code>langchain-dev-utils-example</code>\uff0c\u76ee\u7684\u662f\u4e3a\u4e86\u5e2e\u52a9\u5f00\u53d1\u8005\u5feb\u901f\u4e86\u89e3\u5982\u4f55\u5229\u7528 <code>langchain-dev-utils</code> \u63d0\u4f9b\u7684\u5de5\u5177\u51fd\u6570\uff0c\u9ad8\u6548\u6784\u5efa\u4e24\u79cd\u5178\u578b\u7684\u667a\u80fd\u4f53\uff08agent\uff09\u7cfb\u7edf\uff1a</p> <ul> <li>\u5355\u667a\u80fd\u4f53\uff08Single Agent\uff09\uff1a\u9002\u7528\u4e8e\u6267\u884c\u7b80\u5355\u4efb\u52a1\u4ee5\u53ca\u957f\u671f\u8bb0\u5fc6\u5b58\u50a8\u76f8\u5173\u7684\u4efb\u52a1\u3002</li> <li>\u76d1\u7763\u8005-\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff08Supervisor-Multi-Agent Architecture\uff09\uff1a\u901a\u8fc7\u4e00\u4e2a\u4e2d\u592e\u76d1\u7763\u8005\u534f\u8c03\u591a\u4e2a\u4e13\u4e1a\u5316\u667a\u80fd\u4f53\uff0c\u9002\u7528\u4e8e\u9700\u8981\u4efb\u52a1\u5206\u89e3\u3001\u89c4\u5212\u548c\u8fed\u4ee3\u4f18\u5316\u7684\u590d\u6742\u573a\u666f\u3002</li> </ul> <p> </p>"},{"location":"zh/resource/example-project/#_1","title":"\u5feb\u901f\u5f00\u59cb","text":"<ol> <li>\u514b\u9686\u672c\u4ed3\u5e93\uff1a <pre><code>git clone https://github.com/TBice123123/langchain-dev-utils-example.git  \ncd langchain-dev-utils-example\n</code></pre></li> <li>\u4f7f\u7528 uv \u5b89\u88c5\u4f9d\u8d56\uff1a <pre><code>uv sync\n</code></pre></li> <li>\u521b\u5efa.env\u6587\u4ef6 <pre><code>cp .env.example .env\n</code></pre></li> <li> <p>\u7f16\u8f91 <code>.env</code> \u6587\u4ef6\uff0c\u586b\u5165\u4f60\u7684 API \u5bc6\u94a5\uff08\u9700\u8981 <code>ZhipuAI</code> \u548c <code>Tavily</code> \u7684 API \u5bc6\u94a5\uff09\u3002</p> </li> <li> <p>\u542f\u52a8\u9879\u76ee <pre><code>langgraph dev\n</code></pre></p> </li> </ol>"},{"location":"zh/resource/example-project/#_2","title":"\u4f7f\u7528\u7684\u529f\u80fd","text":"<p>\u5355\u667a\u80fd\u4f53\uff08Simple Agent\uff09\uff1a</p> <p>\u4f7f\u7528\u7684\u672c\u5e93\u7684\u529f\u80fd\uff1a</p> <ul> <li>\u5bf9\u8bdd\u6a21\u578b\u7ba1\u7406\uff08\u542bOpenAI\u517c\u5bb9API\u96c6\u6210\uff09\uff1a<code>register_model_provider</code>\u3001<code>load_chat_model</code></li> <li>\u5d4c\u5165\u6a21\u578b\u7ba1\u7406\uff1a<code>register_embeddings_provider</code>\u3001<code>load_embeddings</code></li> <li>\u683c\u5f0f\u5316\u5e8f\u5217\uff1a<code>format_sequence</code></li> <li>\u4e2d\u95f4\u4ef6\uff1a<code>format_prompt</code></li> </ul> <p>\u76d1\u7763\u8005-\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff08Supervisor-Multi-Agent Architecture\uff09\uff1a</p> <p>\u4f7f\u7528\u7684\u672c\u5e93\u7684\u529f\u80fd\uff1a</p> <ul> <li>\u5bf9\u8bdd\u6a21\u578b\u7ba1\u7406\uff08\u542bOpenAI\u517c\u5bb9API\u96c6\u6210\uff09\uff1a<code>register_model_provider</code>\u3001<code>load_chat_model</code></li> <li>\u591a\u667a\u80fd\u4f53\u6784\u5efa\uff1a<code>wrap_agent_as_tool</code></li> </ul>"},{"location":"zh/resource/example-project/#_3","title":"\u5982\u4f55\u81ea\u5b9a\u4e49","text":"<p>\u53ef\u4ee5\u6839\u636e\u5b9e\u9645\u7684\u9700\u6c42\uff0c\u5bf9\u672c\u9879\u76ee\u8fdb\u884c\u81ea\u5b9a\u4e49\u4fee\u6539\u3002</p>"},{"location":"zh/resource/example-project/#1","title":"1. \u66ff\u6362\u5bf9\u8bdd\u6a21\u578b\u63d0\u4f9b\u5546","text":"<p>\u672c\u9879\u76ee\u9ed8\u8ba4\u4f7f\u7528\u667a\u8c31AI\u7684GLM\u7cfb\u5217\u4f5c\u4e3a\u6838\u5fc3\u6a21\u578b\uff0c\u5177\u4f53\u5982\u4e0b\uff1a</p> <ul> <li><code>GLM-5</code>\uff1a\u7528\u4e8e<code>simple-agent</code>\u4ee5\u53ca<code>supervisor-agent</code>\u7684\u4e3b\u667a\u80fd\u4f53</li> <li><code>GLM-4.7-Flash</code>\uff1a\u7528\u4e8e<code>supervisor-agent</code>\u7684<code>subagent</code></li> <li><code>GLM-4.6V</code>\uff1a\u7528\u4e8e<code>supervisor-agent</code>\u7684<code>vision subagent</code></li> </ul> <p>\u5982\u9700\u81ea\u5b9a\u4e49\u6a21\u578b\u63d0\u4f9b\u5546\uff0c\u8bf7\u4fee\u6539<code>src/utils/providers/chat_models/register.py</code>\uff0c\u5728<code>register_all_model_providers</code>\u51fd\u6570\u4e2d\u4f7f\u7528<code>register_model_provider</code>\u51fd\u6570\u6ce8\u518c\u4f60\u7684\u6a21\u578b\u63d0\u4f9b\u5546\u3002</p> <p>\u540c\u65f6\u5efa\u8bae\u4fee\u6539<code>src/utils/providers/chat_models/load.py</code>\uff0c\u5728<code>load_chat_model</code>\u51fd\u6570\u4e2d\u6dfb\u52a0\u5bf9\u5e94\u7684\u52a0\u8f7d\u903b\u8f91\u3002</p> <p>\u5bf9\u8bdd\u6a21\u578b\u7ba1\u7406\u6700\u4f73\u5b9e\u8df5</p> <p><code>load_chat_model</code>\u51fd\u6570\u91c7\u7528\u5173\u952e\u5b57\u53c2\u6570\u63a5\u6536\u4e0d\u540c\u5bf9\u8bdd\u6a21\u578b\u7c7b\u7684\u989d\u5916\u53c2\u6570\uff08LangChain\u5b98\u65b9\u51fd\u6570\u4e5f\u91c7\u7528\u6b64\u65b9\u5f0f\uff09\u3002\u8fd9\u79cd\u65b9\u5f0f\u63d0\u5347\u4e86\u901a\u7528\u6027\uff0c\u4f46\u4f1a\u524a\u5f31IDE\u7c7b\u578b\u63d0\u793a\uff0c\u589e\u52a0\u53c2\u6570\u8bef\u7528\u98ce\u9669\u3002\u56e0\u6b64\uff0c\u82e5\u5df2\u786e\u5b9a\u5177\u4f53\u63d0\u4f9b\u5546\uff0c\u53ef\u9488\u5bf9\u5176\u96c6\u6210\u5bf9\u8bdd\u6a21\u578b\u7c7b\uff08\u6216\u5d4c\u5165\u6a21\u578b\u7c7b\uff09\u6269\u5c55\u53c2\u6570\u7b7e\u540d\u4ee5\u6062\u590d\u7c7b\u578b\u63d0\u793a\uff0c\u53c2\u8003<code>src/utils/providers/chat_models/load.py</code>\u8fdb\u884c\u9488\u5bf9\u6027\u4fee\u6539\u3002</p>"},{"location":"zh/resource/example-project/#2","title":"2. \u6ce8\u518c\u5d4c\u5165\u6a21\u578b\u63d0\u4f9b\u5546","text":"<p>\u5d4c\u5165\u6a21\u578b\u63d0\u4f9b\u5546\u7684\u6ce8\u518c\u65b9\u5f0f\u4e0e\u5bf9\u8bdd\u6a21\u578b\u7c7b\u4f3c\u3002\u8bf7\u4fee\u6539<code>src/utils/providers/embeddings/register.py</code>\uff0c\u5728<code>register_all_embeddings_providers</code>\u51fd\u6570\u4e2d\u4f7f\u7528<code>register_embeddings_provider</code>\u51fd\u6570\u6ce8\u518c\u4f60\u7684\u5d4c\u5165\u6a21\u578b\u63d0\u4f9b\u5546\u3002</p> <p>\u5982\u9700\u81ea\u5b9a\u4e49\u52a0\u8f7d\u903b\u8f91\uff0c\u53ef\u4fee\u6539<code>src/utils/providers/embeddings/load.py</code>\uff0c\u5728<code>load_embeddings</code>\u51fd\u6570\u4e2d\u6dfb\u52a0\u76f8\u5e94\u7684\u52a0\u8f7d\u903b\u8f91\u3002</p>"},{"location":"zh/resource/example-project/#3","title":"3. \u81ea\u5b9a\u4e49\u5de5\u5177","text":"<p>\u5355\u667a\u80fd\u4f53\uff08simple-agent\uff09 \u5de5\u5177\u5b9e\u73b0\u4f4d\u4e8e <code>src/agents/simple_agent/tools.py</code>\uff0c\u5df2\u5185\u7f6e\uff1a - <code>save_user_memory</code> \u2014\u2014 \u6301\u4e45\u5316\u7528\u6237\u8bb0\u5fc6 - <code>get_user_memory</code> \u2014\u2014 \u8bfb\u53d6\u7528\u6237\u8bb0\u5fc6  </p> <p>\u5982\u9700\u6269\u5c55\uff0c\u76f4\u63a5\u5728\u8be5\u6587\u4ef6\u5185\u65b0\u589e\u5bf9\u5e94\u7684\u5de5\u5177\u5b9e\u73b0\u5373\u53ef\u3002</p> <p>\u76d1\u7763\u8005-\u591a\u667a\u80fd\u4f53\uff08supervisor-agent\uff09 \u5de5\u5177\u5b9e\u73b0\u4f4d\u4e8e <code>src/agents/supervisor/subagent/tools.py</code>\u3002\u662f\u5b50\u667a\u80fd\u4f53\u7684\u5de5\u5177\u5b9e\u73b0\uff0c\u5982\u9700\u4e3a\u5b50\u667a\u80fd\u4f53\u6dfb\u52a0\u81ea\u5b9a\u4e49\u5de5\u5177\uff0c\u76f4\u63a5\u5728\u8be5\u6587\u4ef6\u5185\u65b0\u589e\u5bf9\u5e94\u7684\u5de5\u5177\u5b9e\u73b0\u5373\u53ef\u3002</p> <p>\u6ce8\u610f\uff1a<code>supervisor</code> \u9ed8\u8ba4\u4ec5\u6301\u6709\u201c\u8c03\u7528\u5b50\u667a\u80fd\u4f53\u201d\u7684\u4e24\u4e2a\u5de5\u5177\u3002\u82e5\u9700\u4e3a <code>supervisor</code> \u8ffd\u52a0\u81ea\u5b9a\u4e49\u5de5\u5177\uff0c\u5efa\u8bae\u5728 <code>src/agents/supervisor/</code> \u4e0b\u65b0\u5efa <code>tools.py</code>\uff0c\u7f16\u5199\u5b8c\u6210\u540e\u5728 <code>src/agents/supervisor/agent.py</code> \u4e2d\u5bfc\u5165\u5e76\u4f20\u9012\u7ed9 <code>create_agent</code> \u51fd\u6570\u5373\u53ef\u3002</p>"}]}