
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://tbice123123.github.io/langchain-dev-utils/adavance-guide/openai-compatible/">
      
      
        <link rel="prev" href="../../getting-started-guide/tool/">
      
      
        <link rel="next" href="../multi-agent/">
      
      
        
          <link rel="alternate" href="./" hreflang="en">
        
          <link rel="alternate" href="../../zh/adavance-guide/openai-compatible/" hreflang="zh">
        
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.0">
    
    
      
        <title>OpenAI Compatible API Integration - Langchain Dev Utils</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.618322db.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.ab4e12ef.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#openai-compatible-api-model-provider-integration" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Langchain Dev Utils" class="md-header__button md-logo" aria-label="Langchain Dev Utils" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Langchain Dev Utils
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              OpenAI Compatible API Integration
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="切换到深色模式"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="切换到深色模式" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="切换到浅色模式"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="切换到浅色模式" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
      <div class="md-header__option">
  <div class="md-select">
    
    <button class="md-header__button md-icon" aria-label="Select language">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m12.87 15.07-2.54-2.51.03-.03A17.5 17.5 0 0 0 14.07 6H17V4h-7V2H8v2H1v2h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2zm-2.62 7 1.62-4.33L19.12 17z"/></svg>
    </button>
    <div class="md-select__inner">
      <ul class="md-select__list">
        
          <li class="md-select__item">
            <a href="./" hreflang="en" class="md-select__link">
              English
            </a>
          </li>
        
          <li class="md-select__item">
            <a href="../../zh/adavance-guide/openai-compatible/" hreflang="zh" class="md-select__link">
              中文
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</div>
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/tbice123123/langchain-dev-utils" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    tbice123123/langchain-dev-utils
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
  
    
  
  Overview

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../getting-started-guide/installation/" class="md-tabs__link">
          
  
  
  Getting Started

        </a>
      </li>
    
  

      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="./" class="md-tabs__link">
          
  
  
  Advanced Guides

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../api-reference/agent/" class="md-tabs__link">
          
  
  
  API Reference

        </a>
      </li>
    
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../example-project/" class="md-tabs__link">
        
  
  
    
  
  Usage Examples

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Langchain Dev Utils" class="md-nav__button md-logo" aria-label="Langchain Dev Utils" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Langchain Dev Utils
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/tbice123123/langchain-dev-utils" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    tbice123123/langchain-dev-utils
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Overview
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Getting Started
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Getting Started
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../getting-started-guide/installation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Installation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../getting-started-guide/chat/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chat Model Management
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../getting-started-guide/embedding/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Embedding Model Management
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../getting-started-guide/format/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Formatting Sequences
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../getting-started-guide/message/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Message Processing
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../getting-started-guide/tool/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Tool Calling Processing
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    
  
    Advanced Guides
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Advanced Guides
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    
  
    OpenAI Compatible API Integration
  

    
  </span>
  
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    
  
    OpenAI Compatible API Integration
  

    
  </span>
  
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#overview" class="md-nav__link">
    <span class="md-ellipsis">
      
        Overview
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#creating-corresponding-integration-classes" class="md-nav__link">
    <span class="md-ellipsis">
      
        Creating Corresponding Integration Classes
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Creating Corresponding Integration Classes">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#creating-chat-model-class" class="md-nav__link">
    <span class="md-ellipsis">
      
        Creating Chat Model Class
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Creating Chat Model Class">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#code-example" class="md-nav__link">
    <span class="md-ellipsis">
      
        Code Example
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compatibility-parameters" class="md-nav__link">
    <span class="md-ellipsis">
      
        Compatibility Parameters
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model_profiles-parameter-setting" class="md-nav__link">
    <span class="md-ellipsis">
      
        model_profiles Parameter Setting
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#creating-embedding-model-class" class="md-nav__link">
    <span class="md-ellipsis">
      
        Creating Embedding Model Class
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Creating Embedding Model Class">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#example-code" class="md-nav__link">
    <span class="md-ellipsis">
      
        Example Code
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#using-integration-classes" class="md-nav__link">
    <span class="md-ellipsis">
      
        Using Integration Classes
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Using Integration Classes">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#using-chat-model-classes" class="md-nav__link">
    <span class="md-ellipsis">
      
        Using Chat Model Classes
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#using-embedding-model-classes" class="md-nav__link">
    <span class="md-ellipsis">
      
        Using Embedding Model Classes
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#integration-with-model-management-feature" class="md-nav__link">
    <span class="md-ellipsis">
      
        Integration with Model Management Feature
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Integration with Model Management Feature">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#chat-model-class-registration" class="md-nav__link">
    <span class="md-ellipsis">
      
        Chat Model Class Registration
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#embedding-model-class-registration" class="md-nav__link">
    <span class="md-ellipsis">
      
        Embedding Model Class Registration
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../multi-agent/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Multi-Agent Construction
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../middleware/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Middleware
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipeline/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    State-Graph Orchestration
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../human-in-the-loop/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Add Human-in-the-Loop Support
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    API Reference
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            
  
    API Reference
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../api-reference/agent/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Agent Development Module API Reference
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../api-reference/chat_model/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chat Model Management Module API Reference
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../api-reference/embeddings/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Embedding Model Management Module API Reference
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../api-reference/message_convert/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Message Handling Module API Reference
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../api-reference/tool_calling/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Tool Calling Handling Module API Reference
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../api-reference/pipeline/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    State-Graph Orchestration Module API Reference
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../example-project/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Usage Examples
  

    
  </span>
  
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#overview" class="md-nav__link">
    <span class="md-ellipsis">
      
        Overview
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#creating-corresponding-integration-classes" class="md-nav__link">
    <span class="md-ellipsis">
      
        Creating Corresponding Integration Classes
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Creating Corresponding Integration Classes">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#creating-chat-model-class" class="md-nav__link">
    <span class="md-ellipsis">
      
        Creating Chat Model Class
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Creating Chat Model Class">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#code-example" class="md-nav__link">
    <span class="md-ellipsis">
      
        Code Example
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compatibility-parameters" class="md-nav__link">
    <span class="md-ellipsis">
      
        Compatibility Parameters
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model_profiles-parameter-setting" class="md-nav__link">
    <span class="md-ellipsis">
      
        model_profiles Parameter Setting
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#creating-embedding-model-class" class="md-nav__link">
    <span class="md-ellipsis">
      
        Creating Embedding Model Class
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Creating Embedding Model Class">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#example-code" class="md-nav__link">
    <span class="md-ellipsis">
      
        Example Code
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#using-integration-classes" class="md-nav__link">
    <span class="md-ellipsis">
      
        Using Integration Classes
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Using Integration Classes">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#using-chat-model-classes" class="md-nav__link">
    <span class="md-ellipsis">
      
        Using Chat Model Classes
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#using-embedding-model-classes" class="md-nav__link">
    <span class="md-ellipsis">
      
        Using Embedding Model Classes
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#integration-with-model-management-feature" class="md-nav__link">
    <span class="md-ellipsis">
      
        Integration with Model Management Feature
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Integration with Model Management Feature">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#chat-model-class-registration" class="md-nav__link">
    <span class="md-ellipsis">
      
        Chat Model Class Registration
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#embedding-model-class-registration" class="md-nav__link">
    <span class="md-ellipsis">
      
        Embedding Model Class Registration
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="openai-compatible-api-model-provider-integration">OpenAI Compatible API Model Provider Integration</h1>
<h2 id="overview">Overview</h2>
<p>Many model providers support <strong>OpenAI-compatible</strong> API services, such as: <a href="https://github.com/vllm-project/vllm">vLLM</a>, <a href="https://openrouter.ai/">OpenRouter</a>, <a href="https://www.together.ai/">Together AI</a>, etc. This library provides a complete OpenAI-compatible API integration solution, supporting both chat models and embedding models. It is particularly suitable for scenarios where there is no corresponding LangChain integration yet but the provider offers an OpenAI-compatible API.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>The common approach to integrate OpenAI-compatible APIs is to directly use <code>ChatOpenAI</code> or <code>OpenAIEmbeddings</code> from <code>langchain-openai</code>, simply by passing <code>base_url</code> and <code>api_key</code>. However, this approach only works for simple scenarios and has many compatibility issues, especially with chat models, including:</p>
<ol>
<li>Inability to display reasoning chains (<code>reasoning_content</code>) from non-OpenAI official inference models</li>
<li>No support for video type content_block</li>
<li>Low coverage rate for default structured output strategies</li>
</ol>
<p>This library provides this functionality to address the above compatibility issues. For simple scenarios (especially those with low compatibility requirements), you can directly use <code>ChatOpenAI</code> without using this feature. <code>OpenAIEmbeddings</code> has good compatibility, just set <code>check_embedding_ctx_length</code> to <code>False</code>. Additionally, for developers' convenience, we also provide embedding model OpenAI-compatible integration class functionality.</p>
</div>
<h2 id="creating-corresponding-integration-classes">Creating Corresponding Integration Classes</h2>
<p>This library provides two utility functions for creating corresponding chat model integration classes and embedding model integration classes. Specifically:</p>
<table>
<thead>
<tr>
<th>Function Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>create_openai_compatible_model</code></td>
<td>Create chat model integration class</td>
</tr>
<tr>
<td><code>create_openai_compatible_embedding</code></td>
<td>Create embedding model integration class</td>
</tr>
</tbody>
</table>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>The two utility functions provided by this library are inspired by the third-party library <a href="https://ai-sdk.dev/providers/openai-compatible-providers">@ai-sdk/openai-compatible</a> in the JavaScript ecosystem.</p>
</div>
<h3 id="creating-chat-model-class">Creating Chat Model Class</h3>
<p>Using the <code>create_openai_compatible_model</code> function, you can create a chat model integration class. This function accepts the following parameters:</p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>model_provider</code></td>
<td>Model provider name, e.g., <code>vllm</code>.<br><br><strong>Type</strong>: <code>str</code><br><strong>Required</strong>: Yes</td>
</tr>
<tr>
<td><code>base_url</code></td>
<td>Default API address of the model provider.<br><br><strong>Type</strong>: <code>str</code><br><strong>Required</strong>: No</td>
</tr>
<tr>
<td><code>compatibility_options</code></td>
<td>Compatibility options configuration.<br><br><strong>Type</strong>: <code>dict</code><br><strong>Required</strong>: No</td>
</tr>
<tr>
<td><code>model_profiles</code></td>
<td>Profiles corresponding to models provided by this model provider.<br><br><strong>Type</strong>: <code>dict</code><br><strong>Required</strong>: No</td>
</tr>
<tr>
<td><code>chat_model_cls_name</code></td>
<td>Chat model class name(must follow Python class name conventions), default value is <code>Chat{model_provider}</code> (where <code>{model_provider}</code> is capitalized).<br><br><strong>Type</strong>: <code>str</code><br><strong>Required</strong>: No</td>
</tr>
</tbody>
</table>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><code>model_provider</code> must start with a letter or number, can only contain letters, numbers, and underscores, and must be 20 characters or fewer. (<code>embedding_provider</code> is also subject to the same constraints)</p>
</div>
<p>This library builds a chat model class corresponding to a specific provider using the built-in <code>BaseChatOpenAICompatible</code> class based on the parameters provided by the user. This class inherits from <code>BaseChatOpenAI</code> of <code>langchain-openai</code> and enhances the following capabilities:</p>
<ul>
<li><strong>Support for more formats of reasoning content</strong>: Compared to <code>ChatOpenAI</code> which can only output official reasoning content, this class also supports outputting more formats of reasoning content (e.g., <code>vLLM</code>).</li>
<li><strong>Support for <code>video</code> type content_block</strong>: <code>ChatOpenAI</code> cannot convert <code>type=video</code> <code>content_block</code>, which this implementation has completed support for.</li>
<li><strong>Dynamic adaptation and selection of the most suitable structured output method</strong>: By default, it can automatically select the optimal structured output method (<code>function_calling</code> or <code>json_schema</code>) based on the actual support of the model provider.</li>
<li><strong>Fine-tuning differences through compatibility_options</strong>: By configuring provider compatibility options, resolve support differences for parameters like <code>tool_choice</code>, <code>response_format</code>, etc.</li>
</ul>
<div class="admonition warning">
<p class="admonition-title">Note</p>
<p>When using this feature, you must install the standard version of the <code>langchain-dev-utils</code> library. For details, refer to the installation section.</p>
</div>
<h4 id="code-example">Code Example</h4>
<p>We'll use the integration of vLLM as an example to show how to use the <code>create_openai_compatible_model</code> function to create a chat model integration class.</p>
<div class="admonition note">
<p class="admonition-title">Additional Information</p>
<p>vLLM is a commonly used large model inference framework that can deploy large models as OpenAI-compatible APIs, such as Qwen3-4B in this example:</p>
<p><div class="highlight"><pre><span></span><code>vllm<span class="w"> </span>serve<span class="w"> </span>Qwen/Qwen3-4B<span class="w"> </span><span class="se">\</span>
--reasoning-parser<span class="w"> </span>qwen3<span class="w"> </span><span class="se">\</span>
--enable-auto-tool-choice<span class="w"> </span>--tool-call-parser<span class="w"> </span>hermes<span class="w"> </span><span class="se">\</span>
--host<span class="w"> </span><span class="m">0</span>.0.0.0<span class="w"> </span>--port<span class="w"> </span><span class="m">8000</span><span class="w"> </span><span class="se">\</span>
--served-model-name<span class="w"> </span>qwen3-4b
</code></pre></div>
The service address is <code>http://localhost:8000/v1</code>.</p>
</div>
<p>Use the following code to create a chat model class:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_dev_utils.chat_models.adapters</span><span class="w"> </span><span class="kn">import</span> <span class="n">create_openai_compatible_model</span>

<span class="n">ChatVLLM</span> <span class="o">=</span> <span class="n">create_openai_compatible_model</span><span class="p">(</span>
    <span class="n">model_provider</span><span class="o">=</span><span class="s2">&quot;vllm&quot;</span><span class="p">,</span>
    <span class="n">base_url</span><span class="o">=</span><span class="s2">&quot;http://localhost:8000/v1&quot;</span><span class="p">,</span>
    <span class="n">chat_model_cls_name</span><span class="o">=</span><span class="s2">&quot;ChatVLLM&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">ChatVLLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;qwen3-4b&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">&quot;你好&quot;</span><span class="p">))</span>
</code></pre></div>
<p>It's worth noting that when creating a chat model class, the <code>base_url</code> parameter can be omitted. If not passed, the library will default to reading the corresponding environment variable. For example:</p>
<div class="highlight"><pre><span></span><code><span class="nb">export</span><span class="w"> </span><span class="nv">VLLM_API_BASE</span><span class="o">=</span>http://localhost:8000/v1
</code></pre></div>
<p>In this case, the code can omit the <code>base_url</code> parameter:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_dev_utils.chat_models.adapters</span><span class="w"> </span><span class="kn">import</span> <span class="n">create_openai_compatible_model</span>

<span class="n">ChatVLLM</span> <span class="o">=</span> <span class="n">create_openai_compatible_model</span><span class="p">(</span>
    <span class="n">model_provider</span><span class="o">=</span><span class="s2">&quot;vllm&quot;</span><span class="p">,</span>
    <span class="n">chat_model_cls_name</span><span class="o">=</span><span class="s2">&quot;ChatVLLM&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">ChatVLLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;qwen3-4b&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">&quot;你好&quot;</span><span class="p">))</span>
</code></pre></div>
<p><strong>Note</strong>: For the code above to run successfully, you must set the environment variable <code>VLLM_API_KEY</code>. Although vLLM itself does not require an API key, the chat model class must receive one during initialization, so please configure this variable first. For example:</p>
<div class="highlight"><pre><span></span><code><span class="nb">export</span><span class="w"> </span><span class="nv">VLLM_API_KEY</span><span class="o">=</span>vllm_api_key
</code></pre></div>
<div class="admonition info">
<p class="admonition-title">Tip</p>
<p>Naming rules for environment variables of the created chat model class (embedding model class also follows this naming rule):</p>
<ul>
<li>
<p>API address: <code>${PROVIDER_NAME}_API_BASE</code> (all uppercase, separated by underscores).</p>
</li>
<li>
<p>API Key: <code>${PROVIDER_NAME}_API_KEY</code> (all uppercase, separated by underscores).</p>
</li>
</ul>
</div>
<h4 id="compatibility-parameters">Compatibility Parameters</h4>
<p><code>compatibility_options</code> is a dictionary used to declare the provider's support for certain features of the OpenAI API to improve compatibility and stability.</p>
<p>Currently supported configuration items:</p>
<table>
<thead>
<tr>
<th>Configuration Item</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>supported_tool_choice</code></td>
<td>List of supported <code>tool_choice</code> strategies.<br><br><strong>Type</strong>: <code>list[str]</code><br><strong>Default Value</strong>: <code>["auto"]</code></td>
</tr>
<tr>
<td><code>supported_response_format</code></td>
<td>List of supported <code>response_format</code> formats (<code>json_schema</code>, <code>json_object</code>).<br><br><strong>Type</strong>: <code>list[str]</code><br><strong>Default Value</strong>: <code>[]</code></td>
</tr>
<tr>
<td><code>reasoning_keep_policy</code></td>
<td>Retention policy for <code>reasoning_content</code> field in historical messages.<br><br><strong>Type</strong>: <code>str</code><br><strong>Default Value</strong>: <code>"never"</code></td>
</tr>
<tr>
<td><code>include_usage</code></td>
<td>Whether to include <code>usage</code> information in streaming return results.<br><br><strong>Type</strong>: <code>bool</code><br><strong>Default Value</strong>: <code>True</code></td>
</tr>
</tbody>
</table>
<div class="admonition info">
<p class="admonition-title">Additional Information</p>
<p>Since different models from the same provider may have different support for parameters like <code>tool_choice</code>, <code>response_format</code>, etc., these four compatibility options are <strong>instance attributes</strong> of the class. Therefore, when creating a chat model class, values can be passed as global defaults (representing the configuration supported by most models of this provider). If fine-tuning is needed for specific models later, the same parameters can be overridden during instantiation.</p>
</div>
<p>Detailed introduction to these configuration items:</p>
<details class="note">
<summary>1. supported_tool_choice</summary>
<p><code>tool_choice</code> is used to control whether and which external tools the large model calls during response to improve accuracy, reliability, and controllability. Common values include:</p>
<ul>
<li><code>"auto"</code>: Model autonomously decides whether to call tools (default behavior);</li>
<li><code>"none"</code>: Prohibit tool calling;</li>
<li><code>"required"</code>: Force calling at least one tool;</li>
<li>Specify a specific tool (in OpenAI-compatible API, specifically <code>{"type": "function", "function": {"name": "xxx"}}</code>).</li>
</ul>
<p>Different providers support different ranges. To avoid errors, this library defaults <code>supported_tool_choice</code> to <code>["auto"]</code>, meaning when <code>bind_tools</code>, the <code>tool_choice</code> parameter can only be passed as <code>auto</code>, and other values will be filtered out.</p>
<p>To support passing other <code>tool_choice</code> values, you must configure the supported items. The configuration value is a list of strings, with optional values for each string:</p>
<ul>
<li><code>"auto"</code>, <code>"none"</code>, <code>"required"</code>: Corresponding to standard strategies;</li>
<li><code>"specific"</code>: Unique identifier for this library, indicating support for specifying specific tools.</li>
</ul>
<p>For example, vLLM supports all strategies:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_dev_utils.chat_models.adapters</span><span class="w"> </span><span class="kn">import</span> <span class="n">create_openai_compatible_model</span>

<span class="n">ChatVLLM</span> <span class="o">=</span> <span class="n">create_openai_compatible_model</span><span class="p">(</span>
    <span class="n">model_provider</span><span class="o">=</span><span class="s2">&quot;vllm&quot;</span><span class="p">,</span>
    <span class="n">chat_model_cls_name</span><span class="o">=</span><span class="s2">&quot;ChatVLLM&quot;</span><span class="p">,</span>
    <span class="n">compatibility_options</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;supported_tool_choice&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="s2">&quot;required&quot;</span><span class="p">,</span> <span class="s2">&quot;none&quot;</span><span class="p">,</span> <span class="s2">&quot;specific&quot;</span><span class="p">]</span>
    <span class="p">},</span>
<span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">ChatVLLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;qwen3-4b&quot;</span><span class="p">)</span>
</code></pre></div>
<div class="admonition info">
<p class="admonition-title">Tip</p>
<p>If no special requirements, keep the default (i.e., <code>["auto"]</code>). If business scenarios require the model to <strong>must call specific tools</strong> or <strong>select from a given list</strong>, and the model provider supports the corresponding strategy, enable as needed:</p>
<ol>
<li>
<p>If requiring <strong>at least one</strong> tool call, and the model provider supports <code>required</code>, it can be set to <code>["required"]</code> (and when calling <code>bind_tools</code>, you need to explicitly pass <code>tool_choice="required"</code>)</p>
</li>
<li>
<p>If requiring to call a <strong>specified</strong> tool, and the model provider supports specifying a specific tool call, it can be set to <code>["specific"]</code> (in <code>function_calling</code> structured output, this configuration is very useful to ensure the model calls the specified structured output tool to guarantee the stability of structured output. Because in the <code>with_structured_output</code> method, its internal implementation will pass a <code>tool_choice</code> value that <strong>forces the call to the specified tool</strong> when calling <code>bind_tools</code>, but if <code>"specific"</code> is not in <code>supported_tool_choice</code>, this parameter will be filtered out. Therefore, to ensure <code>tool_choice</code> can be passed normally, <code>"specific"</code> must be added to <code>supported_tool_choice</code>.)</p>
</li>
</ol>
<p>This parameter can be set uniformly during creation or dynamically overridden for individual models during instantiation; it's recommended to declare the <code>tool_choice</code> support for most models of this provider during creation, and for models with different support, specify separately during instantiation.</p>
</div>
</details>
<details class="note">
<summary>2. supported_response_format</summary>
<p>Currently, there are three common methods for structured output.</p>
<ul>
<li><code>function_calling</code>: Generate structured output by calling a tool that conforms to a specified schema.</li>
<li><code>json_schema</code>: A feature provided by the model provider specifically for generating structured output, in OpenAI-compatible API, specifically <code>response_format={"type": "json_schema", "json_schema": {...}}</code>.</li>
<li><code>json_mode</code>: A feature provided by some providers before launching <code>json_schema</code> that can generate valid JSON, but the schema must be described in the prompt. In OpenAI-compatible API, specifically <code>response_format={"type": "json_object"}</code>).</li>
</ul>
<p>Among these, <code>json_schema</code> is supported by only a few OpenAI-compatible API providers (such as <code>OpenRouter</code>, <code>TogetherAI</code>); <code>json_mode</code> has higher support and is compatible with most providers; while <code>function_calling</code> is the most universal, usable as long as the model supports tool calling.</p>
<p>This parameter is used to declare the model provider's support for <code>response_format</code>. By default, it's <code>[]</code>, representing that the model provider supports neither <code>json_mode</code> nor <code>json_schema</code>. In this case, the <code>method</code> parameter in the <code>with_structured_output</code> method can only be passed as <code>function_calling</code>. If <code>json_mode</code> or <code>json_schema</code> is passed, it will be automatically converted to <code>function_calling</code>. If you want to enable <code>json_mode</code> or <code>json_schema</code> structured output implementation, you need to explicitly set this parameter.</p>
<p>For example, if the model deployed by vLLM supports the <code>json_schema</code> structured output method, you can declare it during registration:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_dev_utils.chat_models.adapters</span><span class="w"> </span><span class="kn">import</span> <span class="n">create_openai_compatible_model</span>

<span class="n">ChatVLLM</span> <span class="o">=</span> <span class="n">create_openai_compatible_model</span><span class="p">(</span>
    <span class="n">model_provider</span><span class="o">=</span><span class="s2">&quot;vllm&quot;</span><span class="p">,</span>
    <span class="n">chat_model_cls_name</span><span class="o">=</span><span class="s2">&quot;ChatVLLM&quot;</span><span class="p">,</span>
    <span class="n">compatibility_options</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;supported_response_format&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;json_schema&quot;</span><span class="p">]},</span>
<span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">ChatVLLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;qwen3-4b&quot;</span><span class="p">)</span>
</code></pre></div>
<div class="admonition info">
<p class="admonition-title">Tip</p>
<p>Generally, no configuration is needed. Only when using the <code>with_structured_output</code> method do you need to consider configuring this parameter. If the model provider supports <code>json_schema</code>, you can consider configuring this parameter (because the stability of <code>json_schema</code> structured output is better than <code>function_calling</code>). For <code>json_mode</code>, since it only guarantees JSON output, there's generally no need to set it. Only when the model doesn't support tool calling and only supports setting <code>response_format={"type":"json_object"}</code> do you need to configure this parameter to include <code>json_mode</code>.</p>
<p>Similarly, this parameter can be set uniformly during creation or dynamically overridden for individual models during instantiation; it's recommended to declare the <code>response_format</code> support for most models of this provider during creation, and for models with different support, specify separately during instantiation.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Note</p>
<p>This parameter currently only affects the <code>model.with_structured_output</code> method. For structured output in <code>create_agent</code>, if you need to use the <code>json_schema</code> implementation, you need to ensure the corresponding model's <code>profile</code> contains the <code>structured_output</code> field with a value of <code>True</code>.</p>
</div>
</details>
<details class="note">
<summary>3. reasoning_keep_policy</summary>
<p>Used to control the retention policy of the <code>reasoning_content</code> field in historical messages (messages), mainly adapted to different thinking modes of models from different providers.</p>
<p>Supports the following values:</p>
<ul>
<li>
<p><code>never</code>: <strong>Do not retain any</strong> reasoning content in historical messages (default);</p>
</li>
<li>
<p><code>current</code>: Only retain the <code>reasoning_content</code> field in the <strong>current conversation</strong>;</p>
</li>
<li>
<p><code>all</code>: Retain the <code>reasoning_content</code> field in <strong>all conversations</strong>.</p>
</li>
</ul>
<p>For example:
The user first asks "What's the weather in New York?", then follows up with "What's the weather in London?", currently in the second round of conversation, and about to make the final model call.</p>
<ul>
<li>When the value is <code>never</code></li>
</ul>
<p>When the value is <code>never</code>, the final messages passed to the model will <strong>not have any</strong> <code>reasoning_content</code> fields. The final messages received by the model are:</p>
<div class="highlight"><pre><span></span><code><span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;What&#39;s the weather in New York?&quot;</span><span class="p">,</span> <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span> <span class="s2">&quot;tool_calls&quot;</span><span class="p">:</span> <span class="p">[</span><span class="o">...</span><span class="p">]},</span>
    <span class="p">{</span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Cloudy, 7~13°C&quot;</span><span class="p">,</span> <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;tool&quot;</span><span class="p">,</span> <span class="s2">&quot;tool_call_id&quot;</span><span class="p">:</span> <span class="s2">&quot;...&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Today&#39;s weather in New York is cloudy, 7~13°C.&quot;</span><span class="p">,</span> <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;What&#39;s the weather in London?&quot;</span><span class="p">,</span> <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span> <span class="s2">&quot;tool_calls&quot;</span><span class="p">:</span> <span class="p">[</span><span class="o">...</span><span class="p">]},</span>
    <span class="p">{</span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Rainy, 14~20°C&quot;</span><span class="p">,</span> <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;tool&quot;</span><span class="p">,</span> <span class="s2">&quot;tool_call_id&quot;</span><span class="p">:</span> <span class="s2">&quot;...&quot;</span><span class="p">},</span>
<span class="p">]</span>
</code></pre></div>
<ul>
<li>When the value is <code>current</code></li>
</ul>
<p>When the value is <code>current</code>, only the <code>reasoning_content</code> field in the <strong>current conversation</strong> is retained. The final messages received by the model are:
<div class="highlight"><pre><span></span><code><span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;What&#39;s the weather in New York?&quot;</span><span class="p">,</span> <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span> <span class="s2">&quot;tool_calls&quot;</span><span class="p">:</span> <span class="p">[</span><span class="o">...</span><span class="p">]},</span>
    <span class="p">{</span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Cloudy, 7~13°C&quot;</span><span class="p">,</span> <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;tool&quot;</span><span class="p">,</span> <span class="s2">&quot;tool_call_id&quot;</span><span class="p">:</span> <span class="s2">&quot;...&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Today&#39;s weather in New York is cloudy, 7~13°C.&quot;</span><span class="p">,</span> <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;What&#39;s the weather in London?&quot;</span><span class="p">,</span> <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">},</span>
    <span class="p">{</span>
        <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;&quot;</span><span class="p">,</span>
        <span class="s2">&quot;reasoning_content&quot;</span><span class="p">:</span> <span class="s2">&quot;To check London&#39;s weather, need to directly call the weather tool.&quot;</span><span class="p">,</span>  <span class="c1"># Only retain reasoning_content for this round of conversation</span>
        <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span>
        <span class="s2">&quot;tool_calls&quot;</span><span class="p">:</span> <span class="p">[</span><span class="o">...</span><span class="p">],</span>
    <span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Rainy, 14~20°C&quot;</span><span class="p">,</span> <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;tool&quot;</span><span class="p">,</span> <span class="s2">&quot;tool_call_id&quot;</span><span class="p">:</span> <span class="s2">&quot;...&quot;</span><span class="p">},</span>
<span class="p">]</span>
</code></pre></div></p>
<ul>
<li>When the value is <code>all</code></li>
</ul>
<p>When the value is <code>all</code>, the <code>reasoning_content</code> field in <strong>all</strong> conversations is retained. The final messages received by the model are:
<div class="highlight"><pre><span></span><code><span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;What&#39;s the weather in New York?&quot;</span><span class="p">,</span> <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">},</span>
    <span class="p">{</span>
        <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;&quot;</span><span class="p">,</span>
        <span class="s2">&quot;reasoning_content&quot;</span><span class="p">:</span> <span class="s2">&quot;To check New York&#39;s weather, need to directly call the weather tool.&quot;</span><span class="p">,</span>  <span class="c1"># Retain reasoning_content</span>
        <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span>
        <span class="s2">&quot;tool_calls&quot;</span><span class="p">:</span> <span class="p">[</span><span class="o">...</span><span class="p">],</span>
    <span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Cloudy, 7~13°C&quot;</span><span class="p">,</span> <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;tool&quot;</span><span class="p">,</span> <span class="s2">&quot;tool_call_id&quot;</span><span class="p">:</span> <span class="s2">&quot;...&quot;</span><span class="p">},</span>
    <span class="p">{</span>
        <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Today&#39;s weather in New York is cloudy, 7~13°C.&quot;</span><span class="p">,</span>
        <span class="s2">&quot;reasoning_content&quot;</span><span class="p">:</span> <span class="s2">&quot;Directly return New York weather result.&quot;</span><span class="p">,</span>  <span class="c1"># Retain reasoning_content</span>
        <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;What&#39;s the weather in London?&quot;</span><span class="p">,</span> <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">},</span>
    <span class="p">{</span>
        <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;&quot;</span><span class="p">,</span>
        <span class="s2">&quot;reasoning_content&quot;</span><span class="p">:</span> <span class="s2">&quot;To check London&#39;s weather, need to directly call the weather tool.&quot;</span><span class="p">,</span>  <span class="c1"># Retain reasoning_content</span>
        <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span>
        <span class="s2">&quot;tool_calls&quot;</span><span class="p">:</span> <span class="p">[</span><span class="o">...</span><span class="p">],</span>
    <span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Rainy, 14~20°C&quot;</span><span class="p">,</span> <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;tool&quot;</span><span class="p">,</span> <span class="s2">&quot;tool_call_id&quot;</span><span class="p">:</span> <span class="s2">&quot;...&quot;</span><span class="p">},</span>
<span class="p">]</span>
</code></pre></div></p>
<p><strong>Note</strong>: If the current conversation doesn't involve tool calling, the effect of <code>current</code> is the same as <code>never</code>.</p>
<div class="admonition info">
<p class="admonition-title">Tip</p>
<p>Configure flexibly based on the provider's requirements for <code>reasoning_content</code> retention:</p>
<ul>
<li>If the provider requires <strong>retaining reasoning content throughout</strong>, set to <code>all</code>;  </li>
<li>If only required to retain in <strong>this round of tool calling</strong>, set to <code>current</code>;  </li>
<li>If no special requirements, keep the default <code>never</code>.</li>
</ul>
<p>This parameter can be set uniformly during creation or dynamically overridden for individual models during instantiation; since different models from the same provider may have different <code>reasoning_content</code> retention policies, and even the same model may need different policies in different scenarios, <strong>it's recommended to specify explicitly during instantiation</strong>, no need to assign when creating the class.</p>
</div>
</details>
<details class="note">
<summary>4. include_usage</summary>
<p>Controls whether to append token usage information (<code>prompt_tokens</code> and <code>completion_tokens</code>) at the end of streaming responses.</p>
<p>For OpenAI-compatible APIs, this is typically set via <code>stream_options={"include_usage": true}</code>, so only providers that support the <code>stream_options</code> parameter can use it. The default value is <code>True</code>, and if you encounter a provider that does not support it, or if you do not want to record token usage, you can explicitly set it to <code>False</code>.</p>
</details>
<h4 id="model_profiles-parameter-setting">model_profiles Parameter Setting</h4>
<p>If you want to use the <code>model.profile</code> parameter, you must explicitly pass it during creation.</p>
<p>For example:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_dev_utils.chat_models.adapters</span><span class="w"> </span><span class="kn">import</span> <span class="n">create_openai_compatible_model</span>

<span class="n">model_profiles</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;qwen3-4b&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;max_input_tokens&quot;</span><span class="p">:</span> <span class="mi">131072</span><span class="p">,</span>
        <span class="s2">&quot;max_output_tokens&quot;</span><span class="p">:</span> <span class="mi">8192</span><span class="p">,</span>
        <span class="s2">&quot;image_inputs&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s2">&quot;audio_inputs&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s2">&quot;video_inputs&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s2">&quot;image_outputs&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s2">&quot;audio_outputs&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s2">&quot;video_outputs&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s2">&quot;reasoning_output&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
        <span class="s2">&quot;tool_calling&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="c1"># More model profiles can be written here</span>
<span class="p">}</span>

<span class="n">ChatVLLM</span> <span class="o">=</span> <span class="n">create_openai_compatible_model</span><span class="p">(</span>
    <span class="n">model_provider</span><span class="o">=</span><span class="s2">&quot;vllm&quot;</span><span class="p">,</span>
    <span class="n">base_url</span><span class="o">=</span><span class="s2">&quot;http://localhost:8000/v1&quot;</span><span class="p">,</span>
    <span class="n">model_profiles</span><span class="o">=</span><span class="n">model_profiles</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">ChatVLLM</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;qwen3-4b&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">profile</span><span class="p">)</span>
</code></pre></div>
<div class="admonition warning">
<p class="admonition-title">Note</p>
<p>Despite providing the above compatibility configurations, this library cannot guarantee 100% compatibility with all OpenAI-compatible interfaces. If the model provider already has an official or community integration class, please prioritize using that integration class. If you encounter any compatibility issues, feel free to submit an issue in this library's GitHub repository.</p>
</div>
<h3 id="creating-embedding-model-class">Creating Embedding Model Class</h3>
<p>Similar to chat model classes, you can use <code>create_openai_compatible_embedding</code> to create an embedding model class.</p>
<h4 id="example-code">Example Code</h4>
<p>Similarly, we use <code>create_openai_compatible_embedding</code> to integrate vLLM's embedding model.</p>
<div class="admonition note">
<p class="admonition-title">Additional Information</p>
<p>vLLM can deploy embedding models and expose OpenAI-compatible interfaces, for example:</p>
<div class="highlight"><pre><span></span><code>vllm<span class="w"> </span>serve<span class="w"> </span>Qwen/Qwen3-Embedding-4B<span class="w"> </span><span class="se">\</span>
--task<span class="w"> </span>embed<span class="w"> </span><span class="se">\</span>
--served-model-name<span class="w"> </span>qwen3-embedding-4b<span class="w"> </span><span class="se">\</span>
--host<span class="w"> </span><span class="m">0</span>.0.0.0<span class="w"> </span>--port<span class="w"> </span><span class="m">8000</span>
</code></pre></div>
<p>The service address is <code>http://localhost:8000/v1</code>.</p>
</div>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_dev_utils.embeddings.adapters</span><span class="w"> </span><span class="kn">import</span> <span class="n">create_openai_compatible_embedding</span>

<span class="n">VLLMEmbedding</span> <span class="o">=</span> <span class="n">create_openai_compatible_embedding</span><span class="p">(</span>
    <span class="n">embedding_provider</span><span class="o">=</span><span class="s2">&quot;vllm&quot;</span><span class="p">,</span>
    <span class="n">base_url</span><span class="o">=</span><span class="s2">&quot;http://localhost:8000/v1&quot;</span><span class="p">,</span>
    <span class="n">embedding_model_cls_name</span><span class="o">=</span><span class="s2">&quot;VLLMEmbedding&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">embedding</span> <span class="o">=</span> <span class="n">VLLMEmbedding</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;qwen3-embedding-8b&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">embedding</span><span class="o">.</span><span class="n">embed_query</span><span class="p">(</span><span class="s2">&quot;你好&quot;</span><span class="p">))</span>
</code></pre></div>
<p>Similarly, <code>base_url</code> can be omitted, in which case you need to set the environment variable <code>VLLM_API_BASE</code>.</p>
<div class="highlight"><pre><span></span><code><span class="nb">export</span><span class="w"> </span><span class="nv">VLLM_API_BASE</span><span class="o">=</span><span class="s2">&quot;http://localhost:8000/v1&quot;</span>
</code></pre></div>
<p>The code can omit <code>base_url</code>.</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_dev_utils.embeddings.adapters</span><span class="w"> </span><span class="kn">import</span> <span class="n">create_openai_compatible_embedding</span>

<span class="n">VLLMEmbedding</span> <span class="o">=</span> <span class="n">create_openai_compatible_embedding</span><span class="p">(</span>
    <span class="n">embedding_provider</span><span class="o">=</span><span class="s2">&quot;vllm&quot;</span><span class="p">,</span>
    <span class="n">embedding_model_cls_name</span><span class="o">=</span><span class="s2">&quot;VLLMEmbedding&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">embedding</span> <span class="o">=</span> <span class="n">VLLMEmbedding</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;qwen3-embedding-8b&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">embedding</span><span class="o">.</span><span class="n">embed_query</span><span class="p">(</span><span class="s2">&quot;你好&quot;</span><span class="p">))</span>
</code></pre></div>
<div class="admonition warning">
<p class="admonition-title">Note</p>
<p>Similar to model management requirements, since the two functions above use <code>pydantic.create_model</code> to create model classes at the bottom layer, they bring certain performance overhead, and <code>create_openai_compatible_model</code> also relies on a global dictionary to record the <code>profiles</code> corresponding to each model provider at the bottom layer, so there are also multi-threading concurrency issues in use. Therefore, it's recommended to create the corresponding integration classes in the project's startup phase, and not dynamically create them later.</p>
</div>
<h2 id="using-integration-classes">Using Integration Classes</h2>
<h3 id="using-chat-model-classes">Using Chat Model Classes</h3>
<p>First, we need to create a chat model class. We'll use the previously created <code>ChatVLLM</code> class.</p>
<ul>
<li>Supports methods like <code>invoke</code>, <code>ainvoke</code>, <code>stream</code>, <code>astream</code>, etc.</li>
</ul>
<details class="example">
<summary>Regular Call</summary>
<p>Supports <code>invoke</code> for simple calls:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">HumanMessage</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">ChatVLLM</span><span class="p">(</span><span class="s2">&quot;qwen3-4b&quot;</span><span class="p">)</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">invoke</span><span class="p">([</span><span class="n">HumanMessage</span><span class="p">(</span><span class="s2">&quot;Hello&quot;</span><span class="p">)])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</code></pre></div>
<p>Also supports <code>ainvoke</code> for asynchronous calls:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">HumanMessage</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">ChatVLLM</span><span class="p">(</span><span class="s2">&quot;qwen3-4b&quot;</span><span class="p">)</span>
<span class="n">response</span> <span class="o">=</span> <span class="k">await</span> <span class="n">model</span><span class="o">.</span><span class="n">ainvoke</span><span class="p">([</span><span class="n">HumanMessage</span><span class="p">(</span><span class="s2">&quot;Hello&quot;</span><span class="p">)])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</code></pre></div>
</details>
<details class="example">
<summary>Streaming Output</summary>
<p>Supports <code>stream</code> for streaming output:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">HumanMessage</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">ChatVLLM</span><span class="p">(</span><span class="s2">&quot;qwen3-4b&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">stream</span><span class="p">([</span><span class="n">HumanMessage</span><span class="p">(</span><span class="s2">&quot;Hello&quot;</span><span class="p">)]):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">chunk</span><span class="p">)</span>
</code></pre></div>
<p>And <code>astream</code> for asynchronous streaming calls:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">HumanMessage</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">ChatVLLM</span><span class="p">(</span><span class="s2">&quot;qwen3-4b&quot;</span><span class="p">)</span>
<span class="k">async</span> <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">astream</span><span class="p">([</span><span class="n">HumanMessage</span><span class="p">(</span><span class="s2">&quot;Hello&quot;</span><span class="p">)]):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">chunk</span><span class="p">)</span>
</code></pre></div>
</details>
<ul>
<li>Supports the <code>bind_tools</code> method for tool calling.</li>
</ul>
<p>If the model itself supports tool calling, you can directly use the <code>bind_tools</code> method for tool calling:</p>
<details class="example">
<summary>Tool Calling</summary>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">HumanMessage</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.tools</span><span class="w"> </span><span class="kn">import</span> <span class="n">tool</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">datetime</span>

<span class="nd">@tool</span>
<span class="k">def</span><span class="w"> </span><span class="nf">get_current_time</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get current timestamp&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span><span class="o">.</span><span class="n">timestamp</span><span class="p">())</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">ChatVLLM</span><span class="p">(</span><span class="s2">&quot;qwen3-4b&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">bind_tools</span><span class="p">([</span><span class="n">get_current_time</span><span class="p">])</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">invoke</span><span class="p">([</span><span class="n">HumanMessage</span><span class="p">(</span><span class="s2">&quot;Get current timestamp&quot;</span><span class="p">)])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</code></pre></div>
</details>
<ul>
<li>Supports the <code>with_structured_output</code> method for structured output.</li>
</ul>
<p>If the <code>supported_response_format</code> parameter of this model class contains <code>json_schema</code>, then <code>with_structured_output</code> will prioritize using <code>json_schema</code> for structured output, otherwise fall back to <code>function_calling</code>; if you need <code>json_mode</code>, explicitly specify <code>method="json_mode"</code> and ensure <code>json_mode</code> is included during registration.</p>
<details class="example">
<summary>Structured Output</summary>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">HumanMessage</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.tools</span><span class="w"> </span><span class="kn">import</span> <span class="n">tool</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pydantic</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseModel</span>

<span class="k">class</span><span class="w"> </span><span class="nc">User</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">name</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">age</span><span class="p">:</span> <span class="nb">int</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">ChatVLLM</span><span class="p">(</span><span class="s2">&quot;qwen3-4b&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">with_structured_output</span><span class="p">(</span><span class="n">User</span><span class="p">)</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">invoke</span><span class="p">([</span><span class="n">HumanMessage</span><span class="p">(</span><span class="s2">&quot;Hello, my name is Zhang San, I&#39;m 25 years old&quot;</span><span class="p">)])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</code></pre></div>
</details>
<ul>
<li>Supports passing parameters of <code>BaseChatOpenAI</code>, such as <code>temperature</code>, <code>top_p</code>, <code>max_tokens</code>, etc.</li>
</ul>
<p>In addition, since this class inherits from <code>BaseChatOpenAI</code>, it supports passing model parameters of <code>BaseChatOpenAI</code>, such as <code>temperature</code>, <code>extra_body</code>, etc.:</p>
<details class="example">
<summary>Passing Model Parameters</summary>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">HumanMessage</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">ChatVLLM</span><span class="p">(</span><span class="s2">&quot;qwen3-4b&quot;</span><span class="p">,</span> <span class="n">extra_body</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;chat_template_kwargs&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;enable_thinking&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">}})</span> <span class="c1"># Use extra_body to pass additional parameters, here to disable thinking mode</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">invoke</span><span class="p">([</span><span class="n">HumanMessage</span><span class="p">(</span><span class="s2">&quot;Hello&quot;</span><span class="p">)])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</code></pre></div>
</details>
<ul>
<li>Supports passing multimodal data</li>
</ul>
<p>Supports passing multimodal data, you can use OpenAI-compatible multimodal data format or directly use <code>content_block</code> in <code>langchain</code>.</p>
<details class="example">
<summary>Passing Multimodal Data</summary>
<p><strong>Passing image data</strong>:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">HumanMessage</span>
<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">HumanMessage</span><span class="p">(</span>
        <span class="n">content_blocks</span><span class="o">=</span><span class="p">[</span>
            <span class="p">{</span>
                <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;image&quot;</span><span class="p">,</span>
                <span class="s2">&quot;url&quot;</span><span class="p">:</span> <span class="s2">&quot;https://example.com/image.png&quot;</span><span class="p">,</span>
            <span class="p">},</span>
            <span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;text&quot;</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Describe this image&quot;</span><span class="p">},</span>
        <span class="p">]</span>
    <span class="p">)</span>
<span class="p">]</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">ChatVLLM</span><span class="p">(</span><span class="s2">&quot;qwen3-vl-2b&quot;</span><span class="p">)</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</code></pre></div>
<p><strong>Passing video data</strong>:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">HumanMessage</span>

<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">HumanMessage</span><span class="p">(</span>
        <span class="n">content_blocks</span><span class="o">=</span><span class="p">[</span>
            <span class="p">{</span>
                <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;video&quot;</span><span class="p">,</span>
                <span class="s2">&quot;url&quot;</span><span class="p">:</span> <span class="s2">&quot;https://example.com/video.mp4&quot;</span><span class="p">,</span>
            <span class="p">},</span>
            <span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;text&quot;</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Describe this video&quot;</span><span class="p">},</span>
        <span class="p">]</span>
    <span class="p">)</span>
<span class="p">]</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">ChatVLLM</span><span class="p">(</span><span class="s2">&quot;qwen3-vl-2b&quot;</span><span class="p">)</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</code></pre></div>
</details>
<div class="admonition note">
<p class="admonition-title">Additional Information</p>
<p>vllm also supports deploying multimodal models, such as <code>qwen3-vl-2b</code>:
<div class="highlight"><pre><span></span><code>vllm<span class="w"> </span>serve<span class="w"> </span>Qwen/Qwen3-VL-2B-Instruct<span class="w"> </span><span class="se">\</span>
--trust-remote-code<span class="w"> </span><span class="se">\</span>
--host<span class="w"> </span><span class="m">0</span>.0.0.0<span class="w"> </span>--port<span class="w"> </span><span class="m">8000</span><span class="w"> </span><span class="se">\</span>
--served-model-name<span class="w"> </span>qwen3-vl-2b
</code></pre></div></p>
</div>
<ul>
<li>Supports OpenAI's latest <code>responses api</code> (not yet fully guaranteed to be supported, can be used for simple testing, but not for production environments)</li>
</ul>
<p>This model class also supports OpenAI's latest <code>responses_api</code>. However, currently only a few providers support this API style. If your model provider supports this API style, you can pass <code>use_responses_api</code> parameter as <code>True</code>.
    For example, if vllm supports <code>responses_api</code>, you can use it like this:</p>
<details class="example">
<summary>OpenAI's latest <code>responses_api</code></summary>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">HumanMessage</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">ChatVLLM</span><span class="p">(</span><span class="s2">&quot;qwen3-4b&quot;</span><span class="p">,</span> <span class="n">use_responses_api</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">invoke</span><span class="p">([</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;Hello&quot;</span><span class="p">)])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</code></pre></div>
</details>
<h3 id="using-embedding-model-classes">Using Embedding Model Classes</h3>
<p>We use the previously created <code>VLLMEmbeddings</code> class to initialize an embedding model instance.</p>
<ul>
<li>Vectorizing queries</li>
</ul>
<details class="example">
<summary>Vectorizing Queries</summary>
<div class="highlight"><pre><span></span><code><span class="n">embedding</span> <span class="o">=</span> <span class="n">VLLMEmbeddings</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;qwen3-embedding-4b&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">embedding</span><span class="o">.</span><span class="n">embed_query</span><span class="p">(</span><span class="s2">&quot;Hello&quot;</span><span class="p">))</span>
</code></pre></div>
<p><strong>Asynchronous version</strong></p>
<div class="highlight"><pre><span></span><code><span class="n">embedding</span> <span class="o">=</span> <span class="n">VLLMEmbeddings</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;qwen3-embedding-4b&quot;</span><span class="p">)</span>
<span class="n">res</span> <span class="o">=</span> <span class="k">await</span> <span class="n">embedding</span><span class="o">.</span><span class="n">aembed_query</span><span class="p">(</span><span class="s2">&quot;Hello&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
</code></pre></div>
</details>
<ul>
<li>Vectorizing string lists</li>
</ul>
<details class="example">
<summary>Vectorizing String Lists</summary>
<div class="highlight"><pre><span></span><code><span class="n">documents</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Hello&quot;</span><span class="p">,</span> <span class="s2">&quot;Hello, I&#39;m Zhang San&quot;</span><span class="p">]</span>
<span class="n">embedding</span> <span class="o">=</span> <span class="n">VLLMEmbeddings</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;qwen3-embedding-4b&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">embedding</span><span class="o">.</span><span class="n">embed_documents</span><span class="p">(</span><span class="n">documents</span><span class="p">))</span>
</code></pre></div>
<p><strong>Asynchronous version</strong></p>
<div class="highlight"><pre><span></span><code><span class="n">documents</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Hello&quot;</span><span class="p">,</span> <span class="s2">&quot;Hello, I&#39;m Zhang San&quot;</span><span class="p">]</span>
<span class="n">embedding</span> <span class="o">=</span> <span class="n">VLLMEmbeddings</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;qwen3-embedding-4b&quot;</span><span class="p">)</span>
<span class="n">res</span> <span class="o">=</span> <span class="k">await</span> <span class="n">embedding</span><span class="o">.</span><span class="n">aembed_documents</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
</code></pre></div>
</details>
<p><strong>Note</strong>: The chat model classes and embedding model classes created using this feature support passing any parameters of <code>BaseChatOpenAI</code> and <code>OpenAIEmbeddings</code>, such as <code>temperature</code>, <code>extra_body</code>, <code>dimensions</code>, etc.</p>
<h2 id="integration-with-model-management-feature">Integration with Model Management Feature</h2>
<p>This library has seamlessly integrated this feature with the model management functionality. When registering a chat model, just set <code>chat_model</code> to <code>"openai-compatible"</code>; when registering an embedding model, set <code>embeddings_model</code> to <code>"openai-compatible"</code>.</p>
<h3 id="chat-model-class-registration">Chat Model Class Registration</h3>
<p>Specific code is as follows:</p>
<p><strong>Method 1: Explicit Parameter Passing</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_dev_utils.chat_models</span><span class="w"> </span><span class="kn">import</span> <span class="n">register_model_provider</span>

<span class="n">register_model_provider</span><span class="p">(</span>
    <span class="n">provider_name</span><span class="o">=</span><span class="s2">&quot;vllm&quot;</span><span class="p">,</span>
    <span class="n">chat_model</span><span class="o">=</span><span class="s2">&quot;openai-compatible&quot;</span><span class="p">,</span>
    <span class="n">base_url</span><span class="o">=</span><span class="s2">&quot;http://localhost:8000/v1&quot;</span>
<span class="p">)</span>
</code></pre></div>
<p><strong>Method 2: Through Environment Variables (Recommended for Configuration Management)</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_dev_utils.chat_models</span><span class="w"> </span><span class="kn">import</span> <span class="n">register_model_provider</span>

<span class="n">register_model_provider</span><span class="p">(</span>
    <span class="n">provider_name</span><span class="o">=</span><span class="s2">&quot;vllm&quot;</span><span class="p">,</span>
    <span class="n">chat_model</span><span class="o">=</span><span class="s2">&quot;openai-compatible&quot;</span>
    <span class="c1"># Automatically reads VLLM_API_BASE</span>
<span class="p">)</span>
</code></pre></div>
<p>At the same time, parameters like <code>base_url</code>, <code>compatibility_options</code>, <code>model_profiles</code> in the <code>create_openai_compatible_model</code> function also support being passed. Just pass the corresponding parameters in the <code>register_model_provider</code> function.</p>
<h3 id="embedding-model-class-registration">Embedding Model Class Registration</h3>
<p>Similar to chat model class registration:</p>
<p><strong>Method 1: Explicit Parameter Passing</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_dev_utils.embeddings</span><span class="w"> </span><span class="kn">import</span> <span class="n">register_embeddings_provider</span>

<span class="n">register_embeddings_provider</span><span class="p">(</span>
    <span class="n">provider_name</span><span class="o">=</span><span class="s2">&quot;vllm&quot;</span><span class="p">,</span>
    <span class="n">embeddings_model</span><span class="o">=</span><span class="s2">&quot;openai-compatible&quot;</span><span class="p">,</span>
    <span class="n">base_url</span><span class="o">=</span><span class="s2">&quot;http://localhost:8000/v1&quot;</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>
<p><strong>Method 2: Environment Variables (Recommended)</strong></p>
<div class="highlight"><pre><span></span><code><span class="nb">export</span><span class="w"> </span><span class="nv">VLLM_API_BASE</span><span class="o">=</span>http://localhost:8000/v1
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_dev_utils.embeddings</span><span class="w"> </span><span class="kn">import</span> <span class="n">register_embeddings_provider</span>

<span class="n">register_embeddings_provider</span><span class="p">(</span>
    <span class="n">provider_name</span><span class="o">=</span><span class="s2">&quot;vllm&quot;</span><span class="p">,</span>
    <span class="n">embeddings_model</span><span class="o">=</span><span class="s2">&quot;openai-compatible&quot;</span>
<span class="p">)</span>
</code></pre></div>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "../..", "features": ["navigation.tabs", "navigation.sections", "navigation.expand", "navigation.instant", "navigation.tracking", "navigation.top", "search.suggest", "search.highlight", "search.share", "content.code.copy", "content.code.annotate", "content.tabs.link", "content.tooltips", "content.footnote.tooltips"], "search": "../../assets/javascripts/workers/search.7a47a382.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.e71a0d61.min.js"></script>
      
    
  </body>
</html>