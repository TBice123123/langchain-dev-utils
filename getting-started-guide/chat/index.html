
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://tbice123123.github.io/langchain-dev-utils/getting-started-guide/chat/">
      
      
        <link rel="prev" href="../installation/">
      
      
        <link rel="next" href="../embedding/">
      
      
        
          <link rel="alternate" href="./" hreflang="en">
        
          <link rel="alternate" href="../../zh/getting-started-guide/chat/" hreflang="zh">
        
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.0">
    
    
      
        <title>Chat Model Management - Langchain Dev Utils</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.618322db.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.ab4e12ef.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#chat-model-management" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Langchain Dev Utils" class="md-header__button md-logo" aria-label="Langchain Dev Utils" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Langchain Dev Utils
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Chat Model Management
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="切换到深色模式"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="切换到深色模式" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="切换到浅色模式"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="切换到浅色模式" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
      <div class="md-header__option">
  <div class="md-select">
    
    <button class="md-header__button md-icon" aria-label="Select language">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m12.87 15.07-2.54-2.51.03-.03A17.5 17.5 0 0 0 14.07 6H17V4h-7V2H8v2H1v2h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2zm-2.62 7 1.62-4.33L19.12 17z"/></svg>
    </button>
    <div class="md-select__inner">
      <ul class="md-select__list">
        
          <li class="md-select__item">
            <a href="./" hreflang="en" class="md-select__link">
              English
            </a>
          </li>
        
          <li class="md-select__item">
            <a href="../../zh/getting-started-guide/chat/" hreflang="zh" class="md-select__link">
              中文
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</div>
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/tbice123123/langchain-dev-utils" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    tbice123123/langchain-dev-utils
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
  
    
  
  Overview

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../installation/" class="md-tabs__link">
          
  
  
  Getting Started

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../adavance-guide/multi-agent/" class="md-tabs__link">
          
  
  
  Advanced Guides

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../api-reference/agent/" class="md-tabs__link">
          
  
  
  API Reference

        </a>
      </li>
    
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../example-project/" class="md-tabs__link">
        
  
  
    
  
  Usage Examples

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Langchain Dev Utils" class="md-nav__button md-logo" aria-label="Langchain Dev Utils" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Langchain Dev Utils
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/tbice123123/langchain-dev-utils" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    tbice123123/langchain-dev-utils
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Overview
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    
  
    Getting Started
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Getting Started
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../installation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Installation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    
  
    Chat Model Management
  

    
  </span>
  
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    
  
    Chat Model Management
  

    
  </span>
  
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#overview" class="md-nav__link">
    <span class="md-ellipsis">
      
        Overview
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#registering-model-providers" class="md-nav__link">
    <span class="md-ellipsis">
      
        Registering Model Providers
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Registering Model Providers">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#existing-langchain-chat-model-class" class="md-nav__link">
    <span class="md-ellipsis">
      
        Existing LangChain Chat Model Class
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Existing LangChain Chat Model Class">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameter-description" class="md-nav__link">
    <span class="md-ellipsis">
      
        Parameter Description
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#code-example" class="md-nav__link">
    <span class="md-ellipsis">
      
        Code Example
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#usage-instructions" class="md-nav__link">
    <span class="md-ellipsis">
      
        Usage Instructions
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optional-parameter-description" class="md-nav__link">
    <span class="md-ellipsis">
      
        Optional Parameter Description
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#no-langchain-chat-model-class-but-provider-supports-openai-compatible-api" class="md-nav__link">
    <span class="md-ellipsis">
      
        No LangChain Chat Model Class, but Provider Supports OpenAI Compatible API
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="No LangChain Chat Model Class, but Provider Supports OpenAI Compatible API">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameter-description_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Parameter Description
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#code-example_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Code Example
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#environment-variable-description" class="md-nav__link">
    <span class="md-ellipsis">
      
        Environment Variable Description
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optional-parameter-description_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Optional Parameter Description
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#batch-registration" class="md-nav__link">
    <span class="md-ellipsis">
      
        Batch Registration
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Batch Registration">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameter-description_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Parameter Description
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#code-example_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Code Example
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#loading-chat-models" class="md-nav__link">
    <span class="md-ellipsis">
      
        Loading Chat Models
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Loading Chat Models">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameter-description_3" class="md-nav__link">
    <span class="md-ellipsis">
      
        Parameter Description
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#parameter-rules" class="md-nav__link">
    <span class="md-ellipsis">
      
        Parameter Rules
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#code-example_3" class="md-nav__link">
    <span class="md-ellipsis">
      
        Code Example
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-methods-and-parameters" class="md-nav__link">
    <span class="md-ellipsis">
      
        Model Methods and Parameters
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compatibility-with-official-providers" class="md-nav__link">
    <span class="md-ellipsis">
      
        Compatibility with Official Providers
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../embedding/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Embedding Model Management
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../format/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Formatting Sequences
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../message/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Message Processing
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tool/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Tool Calling Processing
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Advanced Guides
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Advanced Guides
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../adavance-guide/multi-agent/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Multi-Agent Construction
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../adavance-guide/middleware/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Middleware
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../adavance-guide/pipeline/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    State-Graph Orchestration
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../adavance-guide/human-in-the-loop/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Add Human-in-the-Loop Support
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    API Reference
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            
  
    API Reference
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../api-reference/agent/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Agent Development Module API Reference
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../api-reference/chat_model/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chat Model Management Module API Reference
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../api-reference/embeddings/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Embedding Model Management Module API Reference
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../api-reference/message_convert/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Message Handling Module API Reference
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../api-reference/tool_calling/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Tool Calling Handling Module API Reference
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../api-reference/pipeline/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    State-Graph Orchestration Module API Reference
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../example-project/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Usage Examples
  

    
  </span>
  
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#overview" class="md-nav__link">
    <span class="md-ellipsis">
      
        Overview
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#registering-model-providers" class="md-nav__link">
    <span class="md-ellipsis">
      
        Registering Model Providers
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Registering Model Providers">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#existing-langchain-chat-model-class" class="md-nav__link">
    <span class="md-ellipsis">
      
        Existing LangChain Chat Model Class
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Existing LangChain Chat Model Class">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameter-description" class="md-nav__link">
    <span class="md-ellipsis">
      
        Parameter Description
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#code-example" class="md-nav__link">
    <span class="md-ellipsis">
      
        Code Example
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#usage-instructions" class="md-nav__link">
    <span class="md-ellipsis">
      
        Usage Instructions
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optional-parameter-description" class="md-nav__link">
    <span class="md-ellipsis">
      
        Optional Parameter Description
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#no-langchain-chat-model-class-but-provider-supports-openai-compatible-api" class="md-nav__link">
    <span class="md-ellipsis">
      
        No LangChain Chat Model Class, but Provider Supports OpenAI Compatible API
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="No LangChain Chat Model Class, but Provider Supports OpenAI Compatible API">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameter-description_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Parameter Description
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#code-example_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Code Example
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#environment-variable-description" class="md-nav__link">
    <span class="md-ellipsis">
      
        Environment Variable Description
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optional-parameter-description_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Optional Parameter Description
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#batch-registration" class="md-nav__link">
    <span class="md-ellipsis">
      
        Batch Registration
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Batch Registration">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameter-description_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Parameter Description
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#code-example_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Code Example
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#loading-chat-models" class="md-nav__link">
    <span class="md-ellipsis">
      
        Loading Chat Models
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Loading Chat Models">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameter-description_3" class="md-nav__link">
    <span class="md-ellipsis">
      
        Parameter Description
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#parameter-rules" class="md-nav__link">
    <span class="md-ellipsis">
      
        Parameter Rules
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#code-example_3" class="md-nav__link">
    <span class="md-ellipsis">
      
        Code Example
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-methods-and-parameters" class="md-nav__link">
    <span class="md-ellipsis">
      
        Model Methods and Parameters
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compatibility-with-official-providers" class="md-nav__link">
    <span class="md-ellipsis">
      
        Compatibility with Official Providers
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="chat-model-management">Chat Model Management</h1>
<h2 id="overview">Overview</h2>
<p>LangChain's <code>init_chat_model</code> function only supports a limited number of model providers. This library offers a more flexible chat model management solution that supports custom model providers, especially suitable for scenarios where you need to integrate model services that are not natively supported (such as vLLM, etc.).</p>
<h2 id="registering-model-providers">Registering Model Providers</h2>
<p>To register a chat model provider, you need to call <code>register_model_provider</code>. The registration steps vary slightly depending on different situations.</p>
<h3 id="existing-langchain-chat-model-class">Existing LangChain Chat Model Class</h3>
<p>If the model provider already has a suitable and ready-to-use LangChain integration (see <a href="https://docs.langchain.com/oss/python/integrations/chat">Chat Model Class Integration</a>), please pass the corresponding integrated chat model class as the chat_model parameter.</p>
<h4 id="parameter-description">Parameter Description</h4>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Type</th>
<th>Required</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>provider_name</code></td>
<td><code>str</code></td>
<td>Yes</td>
<td>-</td>
<td>Model provider name, used for reference in <code>load_chat_model</code> later</td>
</tr>
<tr>
<td><code>chat_model</code></td>
<td><code>type[BaseChatModel]</code></td>
<td>Yes</td>
<td>-</td>
<td>LangChain chat model class</td>
</tr>
<tr>
<td><code>base_url</code></td>
<td><code>str</code></td>
<td>No</td>
<td><code>None</code></td>
<td>API base URL, usually no need to set manually</td>
</tr>
<tr>
<td><code>model_profiles</code></td>
<td><code>dict</code></td>
<td>No</td>
<td><code>None</code></td>
<td>Dictionary of model configuration information</td>
</tr>
</tbody>
</table>
<h4 id="code-example">Code Example</h4>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.language_models.fake_chat_models</span><span class="w"> </span><span class="kn">import</span> <span class="n">FakeChatModel</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_dev_utils.chat_models</span><span class="w"> </span><span class="kn">import</span> <span class="n">register_model_provider</span>

<span class="n">register_model_provider</span><span class="p">(</span>
    <span class="n">provider_name</span><span class="o">=</span><span class="s2">&quot;fake_provider&quot;</span><span class="p">,</span>
    <span class="n">chat_model</span><span class="o">=</span><span class="n">FakeChatModel</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="usage-instructions">Usage Instructions</h4>
<ul>
<li><code>FakeChatModel</code> is only for testing purposes. In actual use, you must pass a <code>ChatModel</code> class with real functionality.</li>
<li><code>provider_name</code> represents the name of the model provider, used for reference in <code>load_chat_model</code> later. The name can be customized, but should not contain special characters such as <code>:</code>, <code>-</code>, etc.</li>
</ul>
<h4 id="optional-parameter-description">Optional Parameter Description</h4>
<p><strong>base_url</strong></p>
<p>This parameter usually doesn't need to be set (since the model class generally already defines a default API address), only pass <code>base_url</code> when you need to override the default address of the model class, and it only works for attributes with field names <code>api_base</code> or <code>base_url</code> (including aliases).</p>
<p><strong>model_profiles</strong></p>
<p>If your LangChain integrated chat model class already fully supports the <code>profile</code> parameter (i.e., you can directly access model-related properties through <code>model.profile</code>, such as <code>max_input_tokens</code>, <code>tool_calling</code>, etc.), then there's no need to set <code>model_profiles</code> additionally.</p>
<p>If accessing through <code>model.profile</code> returns an empty dictionary <code>{}</code>, it indicates that this LangChain chat model class may not support the <code>profile</code> parameter temporarily. In this case, you can manually provide <code>model_profiles</code>.</p>
<p><code>model_profiles</code> is a dictionary where each key is a model name, and the value is the profile configuration of the corresponding model:</p>
<div class="highlight"><pre><span></span><code><span class="p">{</span>
    <span class="s2">&quot;model_name_1&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;max_input_tokens&quot;</span><span class="p">:</span> <span class="mi">100_000</span><span class="p">,</span>
        <span class="s2">&quot;tool_calling&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
        <span class="s2">&quot;structured_output&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
        <span class="c1"># ... other optional fields</span>
    <span class="p">},</span>
    <span class="s2">&quot;model_name_2&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;max_input_tokens&quot;</span><span class="p">:</span> <span class="mi">32768</span><span class="p">,</span>
        <span class="s2">&quot;image_inputs&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
        <span class="s2">&quot;tool_calling&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
        <span class="c1"># ... other optional fields</span>
    <span class="p">},</span>
    <span class="c1"># can have any number of model configurations</span>
<span class="p">}</span>
</code></pre></div>
<div class="admonition info">
<p class="admonition-title">Tip</p>
<p>It is recommended to use the <code>langchain-model-profiles</code> library to get the profiles of your model provider.</p>
</div>
<h3 id="no-langchain-chat-model-class-but-provider-supports-openai-compatible-api">No LangChain Chat Model Class, but Provider Supports OpenAI Compatible API</h3>
<p>Many model providers support <strong>OpenAI Compatible API</strong> services, such as: <a href="https://github.com/vllm-project/vllm">vLLM</a>, <a href="https://openrouter.ai/">OpenRouter</a>, <a href="https://www.together.ai/">Together AI</a>, etc. When the model provider you're integrating doesn't have a suitable LangChain chat model class, but the provider supports OpenAI Compatible API, you can consider using this situation.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>A common way to integrate OpenAI Compatible API is to directly use <code>ChatOpenAI</code> from <code>langchain-openai</code>, just pass in <code>base_url</code> and <code>api_key</code>. However, this approach is only suitable for simple scenarios and has many compatibility issues: it cannot display the reasoning chain (<code>reasoning_content</code>) of non-OpenAI official inference models, does not support video type content_block, and has low default coverage for structured output. Therefore, this library specifically provides this functionality to solve the above problems. So, for simpler scenarios (especially those with low compatibility requirements), you can completely use <code>ChatOpenAI</code> without using this feature.</p>
</div>
<p>This library will build a chat model class corresponding to a specific provider using the built-in <code>BaseChatOpenAICompatible</code> class based on user-related input. This class inherits from <code>BaseChatOpenAI</code> of <code>langchain-openai</code> and enhances the following capabilities:</p>
<ul>
<li><strong>Support for more formats of reasoning content</strong>: Compared to <code>ChatOpenAI</code> which can only output official reasoning content, this class also supports outputting more formats of reasoning content (e.g., <code>vLLM</code>).</li>
<li><strong>Support for <code>video</code> type content_block</strong>: <code>ChatOpenAI</code> cannot convert <code>type=video</code> <code>content_block</code>, this implementation has completed support.</li>
<li><strong>Dynamically adapt and select the most suitable structured output method</strong>: By default, it can automatically select the optimal structured output method (<code>function_calling</code> or <code>json_schema</code>) based on the actual support of the model provider.</li>
<li><strong>Fine-tune compatibility differences through compatibility_options</strong>: By configuring provider compatibility options, solve support differences for parameters such as <code>tool_choice</code>, <code>response_format</code>, etc.</li>
</ul>
<div class="admonition warning">
<p class="admonition-title">Note</p>
<p>When using this situation, you must install the standard version of the <code>langchain-dev-utils</code> library. For details, please refer to the installation section.</p>
</div>
<h4 id="parameter-description_1">Parameter Description</h4>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Type</th>
<th>Required</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>provider_name</code></td>
<td><code>str</code></td>
<td>Yes</td>
<td>-</td>
<td>Model provider name</td>
</tr>
<tr>
<td><code>chat_model</code></td>
<td><code>str</code></td>
<td>Yes</td>
<td>-</td>
<td>Fixed value <code>"openai-compatible"</code></td>
</tr>
<tr>
<td><code>base_url</code></td>
<td><code>str</code></td>
<td>No</td>
<td><code>None</code></td>
<td>API base URL</td>
</tr>
<tr>
<td><code>model_profiles</code></td>
<td><code>dict</code></td>
<td>No</td>
<td><code>None</code></td>
<td>Dictionary of model configuration information</td>
</tr>
<tr>
<td><code>compatibility_options</code></td>
<td><code>dict</code></td>
<td>No</td>
<td><code>None</code></td>
<td>Compatibility options configuration</td>
</tr>
</tbody>
</table>
<h4 id="code-example_1">Code Example</h4>
<p><strong>Method 1: Explicit Parameter Passing</strong></p>
<div class="highlight"><pre><span></span><code><span class="n">register_model_provider</span><span class="p">(</span>
    <span class="n">provider_name</span><span class="o">=</span><span class="s2">&quot;vllm&quot;</span><span class="p">,</span>
    <span class="n">chat_model</span><span class="o">=</span><span class="s2">&quot;openai-compatible&quot;</span><span class="p">,</span>
    <span class="n">base_url</span><span class="o">=</span><span class="s2">&quot;http://localhost:8000/v1&quot;</span>
<span class="p">)</span>
</code></pre></div>
<p><strong>Method 2: Through Environment Variables (Recommended for Configuration Management)</strong></p>
<div class="highlight"><pre><span></span><code><span class="nb">export</span><span class="w"> </span><span class="nv">VLLM_API_BASE</span><span class="o">=</span>http://localhost:8000/v1
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">register_model_provider</span><span class="p">(</span>
    <span class="n">provider_name</span><span class="o">=</span><span class="s2">&quot;vllm&quot;</span><span class="p">,</span>
    <span class="n">chat_model</span><span class="o">=</span><span class="s2">&quot;openai-compatible&quot;</span>
    <span class="c1"># Automatically reads VLLM_API_BASE</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="environment-variable-description">Environment Variable Description</h4>
<table>
<thead>
<tr>
<th>Environment Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>${PROVIDER_NAME}_API_BASE</code></td>
<td>API base URL (all caps, underscore separated)</td>
</tr>
<tr>
<td><code>${PROVIDER_NAME}_API_KEY</code></td>
<td>API key</td>
</tr>
</tbody>
</table>
<div class="admonition info">
<p class="admonition-title">Tip</p>
<p>In this situation, the naming rule for environment variables of the model provider's API endpoint is <code>${PROVIDER_NAME}_API_BASE</code> (all caps, underscore separated). The corresponding naming rule for the API_KEY environment variable is <code>${PROVIDER_NAME}_API_KEY</code> (all caps, underscore separated).</p>
</div>
<div class="admonition note">
<p class="admonition-title">Supplement</p>
<p>vLLM is a commonly used large model inference framework, which can deploy large models as OpenAI Compatible APIs, such as the Qwen3-4B in this example:</p>
<p><div class="highlight"><pre><span></span><code>vllm<span class="w"> </span>serve<span class="w"> </span>Qwen/Qwen3-4B<span class="w"> </span><span class="se">\</span>
--reasoning-parser<span class="w"> </span>qwen3<span class="w"> </span><span class="se">\</span>
--enable-auto-tool-choice<span class="w"> </span>--tool-call-parser<span class="w"> </span>hermes<span class="w"> </span><span class="se">\</span>
--host<span class="w"> </span><span class="m">0</span>.0.0.0<span class="w"> </span>--port<span class="w"> </span><span class="m">8000</span><span class="w"> </span><span class="se">\</span>
--served-model-name<span class="w"> </span>qwen3-4b
</code></pre></div>
The service address is <code>http://localhost:8000/v1</code>.</p>
</div>
<h4 id="optional-parameter-description_1">Optional Parameter Description</h4>
<p><strong>model_profiles</strong></p>
<p>In this situation, if <code>model_profiles</code> is not manually set, <code>model.profile</code> will return an empty dictionary <code>{}</code>. Therefore, if you need to get the configuration information of a specific model through <code>model.profile</code>, you must first explicitly set <code>model_profiles</code>.</p>
<p><strong>compatibility_options</strong></p>
<p>Only effective in this situation. Used to <strong>declare</strong> the provider's support for some features of the <strong>OpenAI API</strong> to improve compatibility and stability.</p>
<p>Currently supports the following configuration items:</p>
<table>
<thead>
<tr>
<th>Configuration Item</th>
<th>Type</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>supported_tool_choice</code></td>
<td><code>list[str]</code></td>
<td><code>["auto"]</code></td>
<td>List of supported <code>tool_choice</code> strategies</td>
</tr>
<tr>
<td><code>supported_response_format</code></td>
<td><code>list[str]</code></td>
<td><code>[]</code></td>
<td>List of supported <code>response_format</code> formats (<code>json_schema</code>, <code>json_object</code>)</td>
</tr>
<tr>
<td><code>reasoning_keep_policy</code></td>
<td><code>str</code></td>
<td><code>"never"</code></td>
<td>Retention policy for <code>reasoning_content</code> field in historical messages</td>
</tr>
<tr>
<td><code>include_usage</code></td>
<td><code>bool</code></td>
<td><code>True</code></td>
<td>Whether to include <code>usage</code> information in streaming return results</td>
</tr>
</tbody>
</table>
<div class="admonition info">
<p class="admonition-title">Supplement</p>
<p>Because different models from the same model provider have different support for parameters such as <code>tool_choice</code>, <code>response_format</code>, etc. Therefore, these four compatibility options will ultimately become <strong>instance attributes</strong> of the class. When registering, you can pass values as global defaults (representing the configuration supported by most models of this provider), and if you need to fine-tune when loading, you can override parameters with the same name in <code>load_chat_model</code>.</p>
</div>
<details class="note">
<summary>1. supported_tool_choice</summary>
<p><code>tool_choice</code> is used to control whether and which external tools the large model calls during response to improve accuracy, reliability, and controllability. Common values are:</p>
<ul>
<li><code>"auto"</code>: The model autonomously decides whether to call tools (default behavior);</li>
<li><code>"none"</code>: Prohibit calling tools;</li>
<li><code>"required"</code>: Force calling at least one tool;</li>
<li>Specify a specific tool (in OpenAI Compatible API, specifically <code>{"type": "function", "function": {"name": "xxx"}}</code>).</li>
</ul>
<p>Different providers support different ranges. To avoid errors, this library defaults <code>supported_tool_choice</code> to <code>["auto"]</code>, so when calling <code>bind_tools</code>, the <code>tool_choice</code> parameter can only be passed <code>auto</code>, and other values will be filtered out.</p>
<p>If you need to support passing other <code>tool_choice</code> values, you must configure the supported items. The configuration value is a string list, with each string's optional values:</p>
<ul>
<li><code>"auto"</code>, <code>"none"</code>, <code>"required"</code>: Corresponding to standard strategies;</li>
<li><code>"specific"</code>: Unique identifier of this library, indicating support for specifying specific tools.</li>
</ul>
<p>For example, vLLM supports all strategies:</p>
<div class="highlight"><pre><span></span><code><span class="n">register_model_provider</span><span class="p">(</span>
    <span class="n">provider_name</span><span class="o">=</span><span class="s2">&quot;vllm&quot;</span><span class="p">,</span>
    <span class="n">chat_model</span><span class="o">=</span><span class="s2">&quot;openai-compatible&quot;</span><span class="p">,</span>
    <span class="n">compatibility_options</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;supported_tool_choice&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="s2">&quot;none&quot;</span><span class="p">,</span> <span class="s2">&quot;required&quot;</span><span class="p">,</span> <span class="s2">&quot;specific&quot;</span><span class="p">]},</span>
<span class="p">)</span>
</code></pre></div>
<div class="admonition info">
<p class="admonition-title">Tip</p>
<p>If there are no special requirements, you can keep the default (i.e., <code>["auto"]</code>). If the business scenario requires the model to <strong>must call a specific tool</strong> or <strong>select one from a given list</strong>, and the model provider supports the corresponding strategy, then enable as needed:</p>
<ol>
<li>
<p>If you require <strong>at least one</strong> tool to be called, and the model provider supports <code>required</code>, you can set it to <code>["required"]</code> (at the same time, when calling <code>bind_tools</code>, you need to explicitly pass <code>tool_choice="required"</code>)</p>
</li>
<li>
<p>If you require <strong>calling a specific</strong> tool, and the model provider supports specifying a specific tool call, you can set it to <code>["specific"]</code> (in <code>function_calling</code> structured output, this configuration is very useful to ensure the model calls the specified structured output tool to ensure the stability of structured output. Because in the <code>with_structured_output</code> method, its internal implementation will pass in <strong>a <code>tool_choice</code> value that can force the call to the specified tool</strong> when calling <code>bind_tools</code>, but if <code>"specific"</code> is not in <code>supported_tool_choice</code>, this parameter will be filtered out. Therefore, if you want to ensure that <code>tool_choice</code> can be passed normally, you must add <code>"specific"</code> to <code>supported_tool_choice</code>.)</p>
</li>
</ol>
<p>This parameter can be set uniformly in <code>register_model_provider</code> or dynamically overridden for a single model when loading with <code>load_chat_model</code>; it is recommended to declare the <code>tool_choice</code> support situation of most models of this provider in <code>register_model_provider</code> at once, and for models with different support situations, specify them separately in <code>load_chat_model</code>.</p>
</div>
</details>
<details class="note">
<summary>2. supported_response_format</summary>
<p>Currently, there are three common methods for structured output.</p>
<ul>
<li><code>function_calling</code>: Generate structured output by calling a tool that conforms to the specified schema.</li>
<li><code>json_schema</code>: A feature provided by the model provider specifically for generating structured output. In OpenAI Compatible API, specifically <code>response_format={"type": "json_schema", "json_schema": {...}}</code>.</li>
<li><code>json_mode</code>: A feature provided by some providers before they launched <code>json_schema</code>. It can generate valid JSON, but the schema must be described in the prompt. In OpenAI Compatible API, specifically <code>response_format={"type": "json_object"}</code>).</li>
</ul>
<p>Among them, <code>json_schema</code> is only supported by a few OpenAI Compatible API providers (such as <code>OpenRouter</code>, <code>TogetherAI</code>); <code>json_mode</code> has higher support and is compatible with most providers; while <code>function_calling</code> is the most universal, as long as the model supports tool calls, it can be used.</p>
<p>This parameter is used to declare the model provider's support for <code>response_format</code>. By default, it is <code>[]</code>, representing that the model provider neither supports <code>json_mode</code> nor <code>json_schema</code>. In this case, the <code>method</code> parameter in the <code>with_structured_output</code> method can only be passed <code>function_calling</code>. If <code>json_mode</code> or <code>json_schema</code> is passed, it will be automatically converted to <code>function_calling</code>. If you want to enable the <code>json_mode</code> or <code>json_schema</code> structured output implementation method, you need to explicitly set this parameter.</p>
<p>For example, if the model deployed by vLLM supports the <code>json_schema</code> structured output method, you can declare it when registering:</p>
<div class="highlight"><pre><span></span><code><span class="n">register_model_provider</span><span class="p">(</span>
    <span class="n">provider_name</span><span class="o">=</span><span class="s2">&quot;vllm&quot;</span><span class="p">,</span>
    <span class="n">chat_model</span><span class="o">=</span><span class="s2">&quot;openai-compatible&quot;</span><span class="p">,</span>
    <span class="n">compatibility_options</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;supported_response_format&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;json_schema&quot;</span><span class="p">]},</span>
<span class="p">)</span>
</code></pre></div>
<div class="admonition info">
<p class="admonition-title">Tip</p>
<p>Generally, there is no need to configure this. It is only needed when using the <code>with_structured_output</code> method. At this time, if the model provider supports <code>json_schema</code>, you can consider configuring this parameter (because the stability of <code>json_schema</code> structured output is better than <code>function_calling</code>). To ensure the stability of structured output. For <code>json_mode</code>, because it can only guarantee output JSON, there is generally no need to set it. Only when the model does not support tool calls and only supports setting <code>response_format={"type":"json_object"}</code>, do you need to configure this parameter to include <code>json_mode</code>.</p>
<p>Similarly, this parameter can be set uniformly in <code>register_model_provider</code> or dynamically overridden for a single model when loading with <code>load_chat_model</code>; it is recommended to declare the <code>response_format</code> support situation of most models of this provider in <code>register_model_provider</code> at once, and for models with different support situations, specify them separately in <code>load_chat_model</code>.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Note</p>
<p>This parameter currently only affects the <code>model.with_structured_output</code> method. For structured output in <code>create_agent</code>, if you need to use the <code>json_schema</code> implementation method, you need to ensure that the corresponding model's <code>profile</code> contains the <code>structured_output</code> field, and the value is <code>True</code>.</p>
</div>
</details>
<details class="note">
<summary>3. reasoning_keep_policy</summary>
<p>Used to control the retention policy of the <code>reasoning_content</code> field in historical messages (messages), mainly adapted to the different thinking modes of different model providers' models.</p>
<p>Supports the following values:</p>
<ul>
<li>
<p><code>never</code>: <strong>Do not retain any</strong> reasoning content in historical messages (default);</p>
</li>
<li>
<p><code>current</code>: Only retain the <code>reasoning_content</code> field in the <strong>current conversation</strong>;</p>
</li>
<li>
<p><code>all</code>: Retain the <code>reasoning_content</code> field in <strong>all conversations</strong>.</p>
</li>
</ul>
<p>For example:
For example, the user first asks "What's the weather in New York?", then follows up with "What's the weather in London?", currently about to start the second round of conversation, and about to make the last model call.</p>
<ul>
<li>When the value is <code>never</code></li>
</ul>
<p>When the value is <code>never</code>, then the messages passed to the model will <strong>not have any</strong> <code>reasoning_content</code> fields. The final messages received by the model are:</p>
<div class="highlight"><pre><span></span><code><span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;What&#39;s the weather in New York?&quot;</span><span class="p">,</span> <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span> <span class="s2">&quot;tool_calls&quot;</span><span class="p">:</span> <span class="p">[</span><span class="o">...</span><span class="p">]},</span>
    <span class="p">{</span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Cloudy, 7~13°C&quot;</span><span class="p">,</span> <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;tool&quot;</span><span class="p">,</span> <span class="s2">&quot;tool_call_id&quot;</span><span class="p">:</span> <span class="s2">&quot;...&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;New York&#39;s weather today is cloudy, 7~13°C.&quot;</span><span class="p">,</span> <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;What&#39;s the weather in London?&quot;</span><span class="p">,</span> <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span> <span class="s2">&quot;tool_calls&quot;</span><span class="p">:</span> <span class="p">[</span><span class="o">...</span><span class="p">]},</span>
    <span class="p">{</span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Rainy, 14~20°C&quot;</span><span class="p">,</span> <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;tool&quot;</span><span class="p">,</span> <span class="s2">&quot;tool_call_id&quot;</span><span class="p">:</span> <span class="s2">&quot;...&quot;</span><span class="p">},</span>
<span class="p">]</span>
</code></pre></div>
<ul>
<li>When the value is <code>current</code></li>
</ul>
<p>When the value is <code>current</code>, only the <code>reasoning_content</code> field in the <strong>current conversation</strong> is retained. The final messages received by the model are:
<div class="highlight"><pre><span></span><code><span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;What&#39;s the weather in New York?&quot;</span><span class="p">,</span> <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span> <span class="s2">&quot;tool_calls&quot;</span><span class="p">:</span> <span class="p">[</span><span class="o">...</span><span class="p">]},</span>
    <span class="p">{</span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Cloudy, 7~13°C&quot;</span><span class="p">,</span> <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;tool&quot;</span><span class="p">,</span> <span class="s2">&quot;tool_call_id&quot;</span><span class="p">:</span> <span class="s2">&quot;...&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;New York&#39;s weather today is cloudy, 7~13°C.&quot;</span><span class="p">,</span> <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;What&#39;s the weather in London?&quot;</span><span class="p">,</span> <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">},</span>
    <span class="p">{</span>
        <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;&quot;</span><span class="p">,</span>
        <span class="s2">&quot;reasoning_content&quot;</span><span class="p">:</span> <span class="s2">&quot;To check London&#39;s weather, need to directly call the weather tool.&quot;</span><span class="p">,</span>  <span class="c1"># Only retain reasoning_content of this round of conversation</span>
        <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span>
        <span class="s2">&quot;tool_calls&quot;</span><span class="p">:</span> <span class="p">[</span><span class="o">...</span><span class="p">],</span>
    <span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Rainy, 14~20°C&quot;</span><span class="p">,</span> <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;tool&quot;</span><span class="p">,</span> <span class="s2">&quot;tool_call_id&quot;</span><span class="p">:</span> <span class="s2">&quot;...&quot;</span><span class="p">},</span>
<span class="p">]</span>
</code></pre></div></p>
<ul>
<li>When the value is <code>all</code></li>
</ul>
<p>When the value is <code>all</code>, the <code>reasoning_content</code> field in <strong>all</strong> conversations is retained. The final messages received by the model are:
<div class="highlight"><pre><span></span><code><span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;What&#39;s the weather in New York?&quot;</span><span class="p">,</span> <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">},</span>
    <span class="p">{</span>
        <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;&quot;</span><span class="p">,</span>
        <span class="s2">&quot;reasoning_content&quot;</span><span class="p">:</span> <span class="s2">&quot;To check New York&#39;s weather, need to directly call the weather tool.&quot;</span><span class="p">,</span>  <span class="c1"># Retain reasoning_content</span>
        <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span>
        <span class="s2">&quot;tool_calls&quot;</span><span class="p">:</span> <span class="p">[</span><span class="o">...</span><span class="p">],</span>
    <span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Cloudy, 7~13°C&quot;</span><span class="p">,</span> <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;tool&quot;</span><span class="p">,</span> <span class="s2">&quot;tool_call_id&quot;</span><span class="p">:</span> <span class="s2">&quot;...&quot;</span><span class="p">},</span>
    <span class="p">{</span>
        <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;New York&#39;s weather today is cloudy, 7~13°C.&quot;</span><span class="p">,</span>
        <span class="s2">&quot;reasoning_content&quot;</span><span class="p">:</span> <span class="s2">&quot;Directly return New York weather result.&quot;</span><span class="p">,</span>  <span class="c1"># Retain reasoning_content</span>
        <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;What&#39;s the weather in London?&quot;</span><span class="p">,</span> <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">},</span>
    <span class="p">{</span>
        <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;&quot;</span><span class="p">,</span>
        <span class="s2">&quot;reasoning_content&quot;</span><span class="p">:</span> <span class="s2">&quot;To check London&#39;s weather, need to directly call the weather tool.&quot;</span><span class="p">,</span>  <span class="c1"># Retain reasoning_content</span>
        <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span>
        <span class="s2">&quot;tool_calls&quot;</span><span class="p">:</span> <span class="p">[</span><span class="o">...</span><span class="p">],</span>
    <span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Rainy, 14~20°C&quot;</span><span class="p">,</span> <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;tool&quot;</span><span class="p">,</span> <span class="s2">&quot;tool_call_id&quot;</span><span class="p">:</span> <span class="s2">&quot;...&quot;</span><span class="p">},</span>
<span class="p">]</span>
</code></pre></div></p>
<p><strong>Note</strong>: If the current round of conversation does not involve tool calls, the effect of <code>current</code> is the same as <code>never</code>.</p>
<div class="admonition info">
<p class="admonition-title">Tip</p>
<p>Flexibly configure according to the model provider's requirements for retaining <code>reasoning_content</code>:</p>
<ul>
<li>If the provider requires <strong>retaining throughout the entire process</strong>, set to <code>all</code>;  </li>
<li>If only required to retain in <strong>this round of tool calls</strong>, set to <code>current</code>;  </li>
<li>If there are no special requirements, keep the default <code>never</code>.</li>
</ul>
<p>Similarly, this parameter can be set uniformly in <code>register_model_provider</code> or dynamically overridden for a single model when loading with <code>load_chat_model</code>; it is generally recommended to specify separately in <code>load_chat_model</code>, in which case <code>register_model_provider</code> does not need to be set.</p>
</div>
</details>
<details class="note">
<summary>4. include_usage</summary>
<p><code>include_usage</code> is a parameter in the OpenAI Compatible API used to control whether to append a message containing token usage information (such as <code>prompt_tokens</code> and <code>completion_tokens</code>) at the end of the streaming response. Since standard streaming responses do not return usage information by default, enabling this option allows clients to directly obtain complete token consumption data, facilitating billing, monitoring, or logging.</p>
<p>Usually enabled through <code>stream_options={"include_usage": true}</code>. Considering that some model providers do not support this parameter, this library sets it as a compatibility option with a default value of <code>True</code>, because most model providers support this parameter. If not supported, it can be explicitly set to <code>False</code>.</p>
<div class="admonition info">
<p class="admonition-title">Tip</p>
<p>This parameter generally does not need to be set, just keep the default value. Only when the model provider does not support it, it needs to be set to <code>False</code>.</p>
</div>
</details>
<div class="admonition warning">
<p class="admonition-title">Note</p>
<p>Although the above compatibility configurations have been provided, this library still cannot guarantee 100% compatibility with all OpenAI Compatible interfaces. If the model provider already has an official or community integration class, please prioritize using that integration class. If you encounter any compatibility issues, you are welcome to submit an issue in this library's GitHub repository.</p>
</div>
<h2 id="batch-registration">Batch Registration</h2>
<p>If you need to register multiple providers, you can use <code>batch_register_model_provider</code> to avoid repeated calls.</p>
<h4 id="parameter-description_2">Parameter Description</h4>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Type</th>
<th>Required</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>providers</code></td>
<td><code>list[dict]</code></td>
<td>Yes</td>
<td>-</td>
<td>List of provider configurations, each dictionary contains registration parameters</td>
</tr>
</tbody>
</table>
<h4 id="code-example_2">Code Example</h4>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_dev_utils.chat_models</span><span class="w"> </span><span class="kn">import</span> <span class="n">batch_register_model_provider</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.language_models.fake_chat_models</span><span class="w"> </span><span class="kn">import</span> <span class="n">FakeChatModel</span>

<span class="n">batch_register_model_provider</span><span class="p">(</span>
    <span class="n">providers</span><span class="o">=</span><span class="p">[</span>
        <span class="p">{</span>
            <span class="s2">&quot;provider_name&quot;</span><span class="p">:</span> <span class="s2">&quot;fake_provider&quot;</span><span class="p">,</span>
            <span class="s2">&quot;chat_model&quot;</span><span class="p">:</span> <span class="n">FakeChatModel</span><span class="p">,</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;provider_name&quot;</span><span class="p">:</span> <span class="s2">&quot;vllm&quot;</span><span class="p">,</span>
            <span class="s2">&quot;chat_model&quot;</span><span class="p">:</span> <span class="s2">&quot;openai-compatible&quot;</span><span class="p">,</span>
            <span class="s2">&quot;base_url&quot;</span><span class="p">:</span> <span class="s2">&quot;http://localhost:8000/v1&quot;</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">]</span>
<span class="p">)</span>
</code></pre></div>
<div class="admonition warning">
<p class="admonition-title">Note</p>
<p>Both registration functions are implemented based on a global dictionary. To avoid multi-threading issues, <strong>all registrations must be completed during the application startup phase</strong>, and dynamic registration during runtime is prohibited.  </p>
<p>Additionally, when registering, if <code>chat_model</code> is set to <code>openai-compatible</code>, the library will dynamically create a new model class internally through <code>pydantic.create_model</code> (with <code>BaseChatOpenAICompatible</code> as the base class, generating the corresponding chat model integration class). This process involves Python metaclass operations and pydantic validation logic initialization, which has certain performance overhead, so please avoid frequent registration during runtime.</p>
</div>
<h2 id="loading-chat-models">Loading Chat Models</h2>
<p>Use the <code>load_chat_model</code> function to load chat models (initialize chat model instances).</p>
<h4 id="parameter-description_3">Parameter Description</h4>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Type</th>
<th>Required</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>model</code></td>
<td><code>str</code></td>
<td>Yes</td>
<td>-</td>
<td>Model name</td>
</tr>
<tr>
<td><code>model_provider</code></td>
<td><code>str</code></td>
<td>No</td>
<td><code>None</code></td>
<td>Model provider name</td>
</tr>
</tbody>
</table>
<p><strong>In addition, you can pass any number of keyword arguments for additional parameters of the chat model class.</strong></p>
<h4 id="parameter-rules">Parameter Rules</h4>
<ul>
<li>If <code>model_provider</code> is not passed, then <code>model</code> must be in the format <code>provider_name:model_name</code>;</li>
<li>If <code>model_provider</code> is passed, then <code>model</code> must be only <code>model_name</code>.</li>
</ul>
<h4 id="code-example_3">Code Example</h4>
<div class="highlight"><pre><span></span><code><span class="c1"># Method 1: model includes provider information</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">load_chat_model</span><span class="p">(</span><span class="s2">&quot;vllm:qwen3-4b&quot;</span><span class="p">)</span>

<span class="c1"># Method 2: specify provider separately</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">load_chat_model</span><span class="p">(</span><span class="s2">&quot;qwen3-4b&quot;</span><span class="p">,</span> <span class="n">model_provider</span><span class="o">=</span><span class="s2">&quot;vllm&quot;</span><span class="p">)</span>
</code></pre></div>
<p>Although <code>vLLM</code> doesn't strictly require an API Key, LangChain still requires it to be set. You can set it in environment variables:</p>
<div class="highlight"><pre><span></span><code><span class="nb">export</span><span class="w"> </span><span class="nv">VLLM_API_KEY</span><span class="o">=</span>vllm
</code></pre></div>
<h3 id="model-methods-and-parameters">Model Methods and Parameters</h3>
<p>For <strong>situation one</strong>, all its methods and parameters are consistent with the corresponding chat model class.
For <strong>situation two</strong>, the model's methods and parameters are as follows:</p>
<ul>
<li>Supports methods such as <code>invoke</code>, <code>ainvoke</code>, <code>stream</code>, <code>astream</code>, etc.</li>
</ul>
<details class="example">
<summary>Regular Call</summary>
<p>Supports <code>invoke</code> for simple calls:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_dev_utils.chat_models</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_chat_model</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">HumanMessage</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">load_chat_model</span><span class="p">(</span><span class="s2">&quot;vllm:qwen3-4b&quot;</span><span class="p">)</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">invoke</span><span class="p">([</span><span class="n">HumanMessage</span><span class="p">(</span><span class="s2">&quot;Hello&quot;</span><span class="p">)])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</code></pre></div>
<p>Also supports <code>ainvoke</code> for asynchronous calls:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_dev_utils.chat_models</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_chat_model</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">HumanMessage</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">load_chat_model</span><span class="p">(</span><span class="s2">&quot;vllm:qwen3-4b&quot;</span><span class="p">)</span>
<span class="n">response</span> <span class="o">=</span> <span class="k">await</span> <span class="n">model</span><span class="o">.</span><span class="n">ainvoke</span><span class="p">([</span><span class="n">HumanMessage</span><span class="p">(</span><span class="s2">&quot;Hello&quot;</span><span class="p">)])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</code></pre></div>
</details>
<details class="example">
<summary>Streaming Output</summary>
<p>Supports <code>stream</code> for streaming output:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_dev_utils.chat_models</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_chat_model</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">HumanMessage</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">load_chat_model</span><span class="p">(</span><span class="s2">&quot;vllm:qwen3-4b&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">stream</span><span class="p">([</span><span class="n">HumanMessage</span><span class="p">(</span><span class="s2">&quot;Hello&quot;</span><span class="p">)]):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">chunk</span><span class="p">)</span>
</code></pre></div>
<p>And <code>astream</code> for asynchronous streaming calls:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_dev_utils.chat_models</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_chat_model</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">HumanMessage</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">load_chat_model</span><span class="p">(</span><span class="s2">&quot;vllm:qwen3-4b&quot;</span><span class="p">)</span>
<span class="k">async</span> <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">astream</span><span class="p">([</span><span class="n">HumanMessage</span><span class="p">(</span><span class="s2">&quot;Hello&quot;</span><span class="p">)]):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">chunk</span><span class="p">)</span>
</code></pre></div>
</details>
<ul>
<li>Supports the <code>bind_tools</code> method for tool calling.</li>
</ul>
<p>If the model itself supports tool calling, you can directly use the <code>bind_tools</code> method for tool calling:</p>
<details class="example">
<summary>Tool Calling</summary>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_dev_utils.chat_models</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_chat_model</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">HumanMessage</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.tools</span><span class="w"> </span><span class="kn">import</span> <span class="n">tool</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">datetime</span>

<span class="nd">@tool</span>
<span class="k">def</span><span class="w"> </span><span class="nf">get_current_time</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get current timestamp&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span><span class="o">.</span><span class="n">timestamp</span><span class="p">())</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">load_chat_model</span><span class="p">(</span><span class="s2">&quot;vllm:qwen3-4b&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">bind_tools</span><span class="p">([</span><span class="n">get_current_time</span><span class="p">])</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">invoke</span><span class="p">([</span><span class="n">HumanMessage</span><span class="p">(</span><span class="s2">&quot;Get current timestamp&quot;</span><span class="p">)])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</code></pre></div>
</details>
<ul>
<li>Supports the <code>with_structured_output</code> method for structured output.</li>
</ul>
<p>If the model class's <code>supported_response_format</code> parameter contains <code>json_schema</code>, then <code>with_structured_output</code> prioritizes using <code>json_schema</code> for structured output, otherwise falls back to <code>function_calling</code>; if you need <code>json_mode</code>, explicitly specify <code>method="json_mode"</code> and ensure registration includes <code>json_mode</code>.</p>
<details class="example">
<summary>Structured Output</summary>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_dev_utils.chat_models</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_chat_model</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">HumanMessage</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.tools</span><span class="w"> </span><span class="kn">import</span> <span class="n">tool</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pydantic</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseModel</span>

<span class="k">class</span><span class="w"> </span><span class="nc">User</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">name</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">age</span><span class="p">:</span> <span class="nb">int</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">load_chat_model</span><span class="p">(</span><span class="s2">&quot;vllm:qwen3-4b&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">with_structured_output</span><span class="p">(</span><span class="n">User</span><span class="p">)</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">invoke</span><span class="p">([</span><span class="n">HumanMessage</span><span class="p">(</span><span class="s2">&quot;Hello, my name is Zhang San, I&#39;m 25 years old&quot;</span><span class="p">)])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</code></pre></div>
</details>
<ul>
<li>Supports passing parameters of <code>BaseChatOpenAI</code>, such as <code>temperature</code>, <code>top_p</code>, <code>max_tokens</code>, etc.</li>
</ul>
<p>In addition, since this class inherits from <code>BaseChatOpenAI</code>, it supports passing model parameters of <code>BaseChatOpenAI</code>, such as <code>temperature</code>, <code>extra_body</code>, etc.:</p>
<details class="example">
<summary>Passing Model Parameters</summary>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_dev_utils.chat_models</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_chat_model</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">HumanMessage</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">load_chat_model</span><span class="p">(</span><span class="s2">&quot;vllm:qwen3-4b&quot;</span><span class="p">,</span><span class="n">extra_body</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;chat_template_kwargs&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;enable_thinking&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">}})</span> <span class="c1"># Use extra_body to pass additional parameters, here is to turn off thinking mode</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">invoke</span><span class="p">([</span><span class="n">HumanMessage</span><span class="p">(</span><span class="s2">&quot;Hello&quot;</span><span class="p">)])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</code></pre></div>
</details>
<ul>
<li>Supports passing multimodal data</li>
</ul>
<p>Supports passing multimodal data, you can use OpenAI Compatible multimodal data format or directly use <code>content_block</code> in <code>langchain</code>:</p>
<details class="example">
<summary>Passing Multimodal Data</summary>
<p><strong>Passing image type data</strong>:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_dev_utils.chat_models</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_chat_model</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">HumanMessage</span>
<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">HumanMessage</span><span class="p">(</span>
        <span class="n">content_blocks</span><span class="o">=</span><span class="p">[</span>
            <span class="p">{</span>
                <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;image&quot;</span><span class="p">,</span>
                <span class="s2">&quot;url&quot;</span><span class="p">:</span> <span class="s2">&quot;https://example.com/image.png&quot;</span><span class="p">,</span>
            <span class="p">},</span>
            <span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;text&quot;</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Describe this image&quot;</span><span class="p">},</span>
        <span class="p">]</span>
    <span class="p">)</span>
<span class="p">]</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">load_chat_model</span><span class="p">(</span><span class="s2">&quot;vllm:qwen3-vl-2b&quot;</span><span class="p">)</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</code></pre></div>
<p><strong>Passing video type data</strong>:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_dev_utils.chat_models</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_chat_model</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">HumanMessage</span>

<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">HumanMessage</span><span class="p">(</span>
        <span class="n">content_blocks</span><span class="o">=</span><span class="p">[</span>
            <span class="p">{</span>
                <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;video&quot;</span><span class="p">,</span>
                <span class="s2">&quot;url&quot;</span><span class="p">:</span> <span class="s2">&quot;https://example.com/video.mp4&quot;</span><span class="p">,</span>
            <span class="p">},</span>
            <span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;text&quot;</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Describe this video&quot;</span><span class="p">},</span>
        <span class="p">]</span>
    <span class="p">)</span>
<span class="p">]</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">load_chat_model</span><span class="p">(</span><span class="s2">&quot;vllm:qwen3-vl-2b&quot;</span><span class="p">)</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</code></pre></div>
</details>
<div class="admonition note">
<p class="admonition-title">Supplement</p>
<p>vllm also supports deploying multimodal models, such as <code>qwen3-vl-2b</code>:
<div class="highlight"><pre><span></span><code>vllm<span class="w"> </span>serve<span class="w"> </span>Qwen/Qwen3-VL-2B-Instruct<span class="w"> </span><span class="se">\</span>
--trust-remote-code<span class="w"> </span><span class="se">\</span>
--host<span class="w"> </span><span class="m">0</span>.0.0.0<span class="w"> </span>--port<span class="w"> </span><span class="m">8000</span><span class="w"> </span><span class="se">\</span>
--served-model-name<span class="w"> </span>qwen3-vl-2b
</code></pre></div></p>
</div>
<ul>
<li>Supports OpenAI's latest <code>responses api</code> (not yet guaranteed to be fully supported, can be used for simple testing, but not for production environments)</li>
</ul>
<p>This model class also supports OpenAI's latest <code>responses_api</code>. However, currently only a few providers support this API style. If your model provider supports this API style, you can pass in <code>use_responses_api=True</code> as a parameter.
    For example, if vllm supports <code>responses_api</code>, you can use it like this:</p>
<details class="example">
<summary>OpenAI's latest <code>responses_api</code></summary>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_dev_utils.chat_models</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_chat_model</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">HumanMessage</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">load_chat_model</span><span class="p">(</span><span class="s2">&quot;vllm:qwen3-4b&quot;</span><span class="p">,</span> <span class="n">use_responses_api</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">invoke</span><span class="p">([</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;Hello&quot;</span><span class="p">)])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</code></pre></div>
</details>
<div class="admonition info">
<p class="admonition-title">Tip</p>
<p>Regardless of the situation, you can pass any number of keyword arguments as additional parameters for the model, such as <code>temperature</code>, <code>extra_body</code>, etc.</p>
</div>
<h3 id="compatibility-with-official-providers">Compatibility with Official Providers</h3>
<p>For providers already supported by LangChain officially (such as <code>openai</code>), you can directly use <code>load_chat_model</code> without registration:</p>
<div class="highlight"><pre><span></span><code><span class="n">model</span> <span class="o">=</span> <span class="n">load_chat_model</span><span class="p">(</span><span class="s2">&quot;openai:gpt-4o-mini&quot;</span><span class="p">)</span>
<span class="c1"># or</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">load_chat_model</span><span class="p">(</span><span class="s2">&quot;gpt-4o-mini&quot;</span><span class="p">,</span> <span class="n">model_provider</span><span class="o">=</span><span class="s2">&quot;openai&quot;</span><span class="p">)</span>
</code></pre></div>
<div class="admonition success">
<p class="admonition-title">Best Practice</p>
<p>For the use of this module, you can choose according to the following three situations:</p>
<ol>
<li>
<p>If all model providers you're integrating are supported by the official <code>init_chat_model</code>, please directly use the official function to get the best compatibility and stability.</p>
</li>
<li>
<p>If some of the model providers you're integrating are not officially supported, you can use the functionality of this module, first use <code>register_model_provider</code> to register the model provider, then use <code>load_chat_model</code> to load the model.</p>
</li>
<li>
<p>If the model provider you're integrating does not have a suitable integration, but the provider provides an OpenAI Compatible API (such as vLLM), it is recommended to use the functionality of this module, first use <code>register_model_provider</code> to register the model provider (pass <code>openai-compatible</code> as chat_model), then use <code>load_chat_model</code> to load the model.</p>
</li>
</ol>
</div>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "../..", "features": ["navigation.tabs", "navigation.sections", "navigation.expand", "navigation.instant", "navigation.tracking", "navigation.top", "search.suggest", "search.highlight", "search.share", "content.code.copy", "content.code.annotate", "content.tabs.link", "content.tooltips", "content.footnote.tooltips"], "search": "../../assets/javascripts/workers/search.7a47a382.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.e71a0d61.min.js"></script>
      
    
  </body>
</html>